---
title: "Multiple_Regression"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(effects)
library(broom)
```

### Multiple regression

Multiple regression is an extension of simple regression.  Now, we have more than one predictor variable for our outcome variable.  We follow the same process as a simple regression, and test the variances explained by the model set of predictors with one $F$ value in an "omnibus" test.  This is unlike ANOVA where we can calculate an $F$ value for each factor. For individual predictors and their significance on the outcome variable, we can calculate $t$ values.   

A multiple regression analysis finds the best possible set of values that *predict* values of the outcome variable given the observed values.

#### Model for simple regression

Remember the equation for simple regression:

$$
Y = b_0 + b_1*X+e
$$
You may notice that the intercept term here is $b_0$ where in the simple regression equation it was $a$.  They are equivalent,  but I have noted the differences in convention here so that if you come across equations, you are aware that different people use different notation conventions.  There is usually an explanation of the notation when it is first introduced - so read carefully and you should be ok.

#### Model for multiple regression

Now, the multiple regression equation simply adds further slope terms (extra $b$s) - one for each extra predictor variable

So a two predictor regression analysis equation becomes:

$$
Y_i = b_0 + b_1 * X_1 + b_2*X_2 + e_i
$$
As before:

 - $Y_i$ is the outcome variable - the values to be predicted.  Sometimes called the *criterion* variable.
 - $X_1$ and $X_2$ are the predictor variables.  
 - $b_0$ is the intercept term;
   - In ANOVA terms $b_0$ is the grand mean.
 - $b_1$ and $b_2$ are the beta weights or the slopes for change in $Y$ when $X_1$ and $X_2$ change.  You can see that the beta weights and x's are paired by matching subscripts - each predictor gets its own beta weight.
   - the sign of $b$ tells the direction of the effect - positive or negative
 - $e_i$ is the error term,  the distance of each data point from the line of best fit or the difference of the observed value of $Y$ from the *fitted* / *predicted* value of $Y$

All the relations of the terms in the equation remain the same as with simple regression, there are literally just more predictors.  The interpretation of an individual beta weight is slightly different from that in simple regression (explained later).

You may be relieved to hear that the computation by hand becomes intractable at this point.  We use the computer and functions to do the heavy lifting of the model estimation for us.

### Read in the data

This is a truncated set of values retrieved from Margriet's first lecture in January 2021.  We used these to review correlation and simple regression. It makes sense to use them again, because we know the results of at least one of the predictors, so the addition of extra predictors may demonstrate that multiple regression is an extension of simple regression.

```{r cars}
d <- read_csv("~/Downloads/Covariance_Data.csv") # calling my dataframe d to save typing!
```
 
That is not to say that is *all* multiple regression is.  There are practices that are needed with the presentation of more than one predictor but for now we will simple perform a regression with more than one predictor.  The future lectures will visit these practices to fully inform the regression workflow.

```{r}
head(d) # inspect the first 6 observations of `d`
```

We know some information about this dataset from looking at the first few rows, however, without a supporting article or a codebook, there is little to tell us about the driving research question or the hypothesis for this data.  So we cannot know for sure what the primary variables are and what the supporting or auxiliary variables may be. 

But remember your PSYC123 learning from last year. If this was a real analysis - and you were working with an inherited data set, before peeking at the data, we need to make some assumptions, and it's a good idea to write those down, so that they are clear and communicated / logged for prosperity (interested researchers). Otherwise, you are adopting a completely exploratory framework.  It's good to be clear before you begin to work with inherited data and state this very clearly in your notebook.

For the purposes of demonstration - I'll put myself in the exploratory analysis space!

We need more than two variables to complete an example. We know from the simple regression performed earlier the `IQ` has very little relationship with `Performance`, so we could just go for it and add `Motivation` and `Social_Support`. 

`Performance` can still be $Y$, with all other variables acting as predictors in this model.


Here is a repeat of the simple regression linear model command:

```{r}
(m3 <- lm(Performance ~ IQ, d))
```
The multiple regression command is simply an addition of the predictor terms.  I have called this m3 because it has three predictor terms (explained later).

```{r}
(m3 <- lm(Performance ~ IQ + Motivation + Social_Support, d))
```
I haven't called the summary on the model, because that would give us a lot more useful information.  We only have the regression coefficients here, which will help us to keep our focus at this point.  All of this information is contained in the `summary(m3)` call which you would use more often.

But lets take a look at the difference between the two models.  The coefficients from the simple regression have changed in the context of a larger set of predictors:

  - the intercept term is smaller in m3, compared to m1
  - the IQ predictor is now positive but is of a smaller magnitude. The interpretation of this predictor, given its magnitude, is probably the same in m3 as m1.  A one unit increase in IQ indicates a 0.005 increase in Performance, holding Motivation and Social_Support predictors at a constant level.  The full summary would give us an inferential test to check this assumption.
  
The added predictors of Motivation and Social_Support

 - Motivation looks to have a positive relationship with Performance. A one unit increase in Motivation indicates a 0.06 unit increase in Performance, holding IQ and Social_Support predictors at a constant level.
 - Social_Support also looks to have a positive relationship with Performance.  A one unit increase in Social_Support indicates a 0.17 increase in Performance, holding IQ and Motivation predictors at a constant level.
 
Let's call the summary of the model and look at some inferential information:

```{r}
summary(m3)
```
From top line working down the summary information:

 - As with the earlier model output, at the top we have a reminder of the model that we called.
 - We can see the distribution of the *Residuals*.  This is not quite symmetric - the median is not 0 (but it's close); also first quartile (1Q) and third quartile (3Q) should be similar in magnitude (though not sign, obviously), as should the Min and Max values.  It looks like there could be some interesting data points at the extremes of the distribution...you can boxplot residuals to check this:
 
The following two plots are identical - except for the code used to call the residual values from the model object
 
```{r}
boxplot(m3[['residuals']], # uses double brackets to identify the residuals from the model object
        main='Boxplot: Residuals', # main title label
        ylab='residual value') # y axis label
```

```{r}
boxplot(m3$residuals, # uses dollar sign to identify the residuals from the model object
        main='Boxplot: Residuals', # main title label
        ylab='residual value') # y axis label
```


You can see that the distribution is not symmetric - a shorter bottom whisker and a longer top whisker indicated a skewed distribution.

Back to the summary information:

 - Next, The *regression coefficients* are tabulated:
   - the first column is the coefficient labels
   - the estimate column is the magnitude of the coefficient - all positive here but could also be negative, denoting the direction of the effect.  It may be tempting to think that `Social_Support` is a stronger predictor than the other predictors given its magnitude but remember that the units here for each predictor are on different scales (that we have no information about either).  We need to standardise the predictors if we are to be able to make such comparisons (explained later). 
   - the standard error column gives us a precision of the measurement
   - *t* value - obtained by dividing the coefficient by the standard error, and under the null hypothesis that each beta value = 0,  it is a *t* with $n - p - 1$ degrees of freedom ($n$ = number of observations, and $p$ is the number of parameters).
   - *p* value for a two tailed *t* statistic (more about this later).
 - Key for any asterisks that indicate statistical significance on the *t* values
 - The bottom block of information has:
   - Residual standard error term and its associated degrees of freedom.  This is the spread of the residuals.
   - $R^2$ and adjusted $R^2$ tell us how much of the variability in $Y$ is explained by the set of predictors in the model. Adjusted $R^2$ has some noise added to its calculation so is generally smaller than $R^2$.
   - $F$ statistic and its associated *p* value, testing the set of predictors for significance. Under the null hypothesis,  the F has a distribution of ($p-1, n-p$) degrees of freedom.


#### Interpretation of the model

Lets reprint the summary to save scrolling but also save the summary to an object so that we can call the values while interpreting and reporting the model

```{r}
(m3_sum <- summary(m3))
```
When the $F_b$ is not significant then $b$ = 0 may be true.  The data do not demonstrate any tendency for larger values of $X$ to be associated with smaller or larger values of $Y$.  

None of the predictors show any significant ability to predict variance in Job Performance (IQ: *t* = `r round(m3_sum$coefficients[[10]], 2)`, *p* = `r round(m3_sum$coefficients[[14]], 2)`; Motivation: *t* = `r round(m3_sum$coefficients[[11]], 2)`, *p* = `r round(m3_sum$coefficients[[15]], 2)` and Social Support: *t* = `r round(m3_sum$coefficients[[12]], 2)`, *p* = `r round(m3_sum$coefficients[[16]], 2)`). For this sample, the predictors and the job performance appear to be causally unrelated. 

The model is not significant either ($F{_(}{_3,_9}{_)}$ = `r round(m3_sum$fstatistic[[1]], 2)`, *p* = 0.834). $F_{critical}$ for df = (3, 9) at the .05 level = 3.86.  This means that $F_{observed}$ is not larger than $F_{critical}$ so we fail to reject the null hypothesis.  


##### Plotting predictions

Plotting an individual effect (or lack of it) is very simple if we are not too worried about being pretty.

```{r}
plot(predictorEffect("IQ", m3))
plot(predictorEffect("Motivation", m3))
plot(predictorEffect("Social_Support", m3))
```

Plotting it in this way, also allows us, with a simple additional argument to the plot call, to see the partial residuals - thereby checking that we have residual values that are normally distributed - I think the pink line probably tells us that we do not!  We should be looking for an alternative way to analyse these two variables....or some transformations!

```{r}
plot(predictorEffect("IQ", m3, partial.residuals = TRUE))
plot(predictorEffect("Motivation", m3, partial.residuals = TRUE))
plot(predictorEffect("Social_Support", m3, partial.residuals = TRUE))
```

##### Checking that assumptions of linear regression are not violated

We can collect some diagnostic measures to help with assumption checking using the `augment()` function from the `broom` package:

```{r}
(m3_metrics <- augment(m3))
```

Columns 1 - 4 are our variables. `fitted` and `residuals` are the predicted values of Y and the `error` values.  The final four columns we use below with some explanation.


Every regression model is built upon the following assumptions:

  1. The relationship between $X$ and $Y$ is assumed to be linear (additive)
  2. The residual errors are normally distributed
  3. The residuals have constant variance (homoscedasticity)
  4. The residuals are not correlated (assumption of independence)
  
```{r}
par(mfrow = c(2, 2)) # display plots in a 2 x 2 panel
plot(m3) # plot diagnostic plots for m3
```


**Residuals vs Fitted**: "fitted" here means the predicted values. We want the pink line to align pretty closely with the horizontal dashed line. Comparing this plot with that from the simple regression, this plot looks better. Take note of observation 13 - that was also labelled in the simple regression plots.

**Normal Q-Q**: If the residual points (open circles) follow the dashed line, you can assume the residuals are normally distributed

**Scale-Location**: This is checking for constant variance in the residuals - not much here.  A good indication would be a horizontal pink line with equally spread points.  Our graph is not good.

**Residuals vs Leverage** - are there any points that are having a large influence on the regression results.  They will be numbered and you can then inspect them in your data file. Observations that show standardised residuals (see the table above) above 3 would be problematic.  As would observations of a hat value above $2(p+1)/n$ where $p$ = is the number of predictors and $n$ = is the number of observations

```{r}
p <- 4
N <- nrow(d)
(m3_hat <- (2*(p+1))/N)  # model hat value
```
None of the datapoints above have hat values larger than the model hat threshold value.  

Checking for observations that are influential follows a similar pattern:  observations that exceed the *Cook's distance* value = $4/(n-p-1)$ are likely to have high influence and the regression results may change if you exclude them.  In the presence of such observations that exceed Cook's distance, unless you know the observation are errors, you probably need to estimate the model without the observations and report both sets of results.

```{r}
(m3_Cooks <- 4/(N-p-1)) # model Cook's distance value
```
Observation no. 13 is looking influential. Remember the skewed distribution of the residuals with the long whisker... Time to inspect, possibly exclude 13 and rerun!

There are lots of things to do as we run a multiple regression model - both before, while and after building the model, which will be the focus of the future lectures.  We need to look at:

  - a model with some significant predictors!
  - correlation matrices
  - the properties of multicollinearity
  - centering predictors
  - standardising predictors
  - models with interaction terms
  - models with non-continuous predictors
    - interpreting models with non-continuous predictors
  - choosing between different models
  - reporting models.


