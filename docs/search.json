[
  {
    "objectID": "PSYC121/Week8.html",
    "href": "PSYC121/Week8.html",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "Today we will take a look at summarising means and standard errors (SEs) from our data. We will look at how we plot these together on the one graph (using ggplot() commands that allow us to share mappings between different geoms. We will explore our data on the famous “Stroop Task” and we will use a related-samples t-test to examine the differences between the means of our different conditions in this task."
  },
  {
    "objectID": "PSYC121/Week8.html#pre-lab-work-online-tutorial",
    "href": "PSYC121/Week8.html#pre-lab-work-online-tutorial",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "Pre-lab work: online tutorial",
    "text": "Pre-lab work: online tutorial\nOnline tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\n\nCreate a folder for Week 8 and download the Week_8.zip file and upload it into this new folder in RStudio Server."
  },
  {
    "objectID": "PSYC121/Week8.html#rstudio-tasks",
    "href": "PSYC121/Week8.html#rstudio-tasks",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "RStudio Tasks",
    "text": "RStudio Tasks\n\nCalculating means and SEs\nThe “Stroop Effect” is a classic demonstration of automaticity of behaviour. Participants have to say the colour a word is printed in, which is an easy task for a “compatible” stimulus like GREEN, and a much more difficult task for an “incompatible” stimulus like BLUE. We can’t help but read the text - it has seemingly become an automatic process.\n In this task we will calculate the means and standard errors of the means and then we will then plot them using ggplot(). First though, we’ll need to inspect the data and maybe do a bit of data wrangling by using our filter() command.\n\nOpen the script “Week_8_script.R” (see prep work)\nRun the library and add a read_csv line to read in the data set. Call your data something meaningful (perhaps data_w8 or data_stroop but maybe not bestest_most_fantastic_data_on_the_stroop_test_eva_init)\nView the data with View(data). You will see that the data are a little different from the data we have worked with previously. We have an pID variable, which gives a unique number for each person and each person has 3 rows. This is because the different conditions of the Stroop task reflect a within-subjects variable (related samples). For data like this it is often useful to have them arranged in what is referred to as “long format”, with multiple rows for each response the participant provides. For the current data that means we have a variable called condition, which is our IV, and one called time which is our DV.\nLet’s look at the distribution of time (our DV) as a function of condition. Complete the next chunk of code by mapping x to time and fill to condition for our geom_density() plot. You can play around with the alpha parameter, setting it to a value between 0 and 1 - note that this is done OUTSIDE of the aes() command.\nWe seem to have some outlier values at both the high and the low ends. It’s probably best if we remove data for the whole participant if their average time is unusual. To do that, we’ll need to create a new column. Here we will introduce you to a new function called mutate(). This function will create a new variable (column) from an old one. Run this next block of code (you don’t need to edit this one) to create the new column, avg_time. We are using this in combination with group_by(pID) because we want to calculate the average time for each participant. Use view() to take a look at the data to check the column has been created correctly.\nEdit the geom_histogram() code to plot the distribution of values in the new avg_time column.\nWe now need the filter out the values in our data that we feel are unusual. Like last week, we will do this (for now) in a fairly unprincipled manner, by “eyeballing” the data (next week we’ll consider something a bit more “scientific”). Complete the filter command so that it removes both the very low values in the avg_time column, and also those that are very high. Because you want to filter out low AND high values, you are using an AND expression (&). You will therefore need to enter in two numerical values, based on your assessment of the histogram produced for Q6. Note that you need to think about how you are storing the result of this filter process. Do you want to create a new object, or overwrite the existing object?\nCheck the new distribution of average times after this filter has been applied to the data.\n\n\n\nRunning related samples t-tests\nWe have seen in our density plots that the reaction times (DV) look different in the three different Stroop conditions (our IV). But now we need to look at whether there are statistically significant differences between the means of the three conditions.\nTo do this, we will first summarise the mean time taken by each condition in the Stroop task. Remember from Week 3 that we can use group_by() and summarise() to get summary stats (e.g., mean, sd) at each level of the IV. That’s what we want to do now:\n\nEdit the group_by() code to specify the IV and the summarise() to calculate the mean() of our DV. If you do this correctly, you’ll get three values - a mean value for each level (condition) of our IV. Do these means reflect what you would expect in the Stroop task?\nNext we need to test if these differences between our means are real. To do that, we can run a related samples t-test; remember that the data for each level of the IV in this experiment came from the same person. We must use a filter() to restrict the data to just two levels of the IV: the condition column/variable in the data. This is because a related samples t-test looks at the difference between two means (and only two), so the column we use for the t-test needs to have just two levels of the IV (two of the conditions).\nThe filter command is already set up to restrict the data to two of the conditions. Note that the filter uses an “|” symbol, which means “or”, because we want the data that is the same as (==) one condition OR is the same as (==) the other condition.\nRun the t-test on this selection of data, to compare the means from these two levels of the IV. Is the result significant? Note the t-value and the p-value.\nWrite out a statement to express this result. Here’s a template you can use, where you need to edit the bits in the []:\n\n\n“There [was a / was no] significant difference between the [describe the variables that were compared], t([degrees of freedom here]) = [t value here], p &lt; [p value here].”\n\n\nWith 3 levels to the IV condition there are 3 possible comparisons we can make (1 vs. 2; 1 vs. 3; 2 vs. 3). Complete all three tests, by copying and pasting the commands, editing each to make a different filter selection, and then to run the t-test. Write out a report statement (Q5) for each of your comparisons.\n\n\n\nPlotting the means and SEs\n\nIn Task 2 you calculated the means for each condition in the Stroop task. We’ve seen in lectures that “standard error” provides an estimate of how variable that mean will be across the samples we collect. A very typical way to plot a mean value is to plot it with the standard error of the mean (SEM):\n\n\n\nThe code from Task 2-Q1 will give the mean. We will now complete the second line of this code to give the standard error values. TO do this, you simply need to add the correct variable (DV) to the sd() command to calculate the SE values (note that you don’t need to put anything in n(), as this simply calculates how many rows there are).\nView the new object summary object you have created. Check that the means and SEs are different for the 3 conditions. If they are the same, you probably summarised the wrong column!\nWe will now plot these 3 mean values in a figure. Let’s use geom_point() so that our means and SEs look a bit like the figure above. Complete the ggplot command to plot our summarised value called stroop_mean (y), as a function of the IV, condition (x).\nNow we need to add some “error bars” which provide a visual guide as to how much uncertainty we have in our mean value. Edit the code for the ggplot() command to plot both geom_point() (same as Q4) and geom_errorbar. You will need to calculate a ymin and a ymax value. Use the illustration above to work out how to combine the mean value and the SE value (hint: add or subtract) to create the right ymin and ymax.\n\nEXTRA: These next steps can be completed to practice customising your plot\n\nAdd a labs() layer to the plot to change the axis titles, and the title of the plot.\nChange the theme of the plot (e.g., theme_void())\nMap the fill aesthetic to the variable stroop_condition. You can do this for geom_point or geom_errorbar or both at once by putting it in the aes() within the ggplot() command.\nManually change the colours of the columns with using scale_fill_brewer(). Take a look at the Week 6 (6.3.7) for instructions on setting colours.\nTry changing your geom_point() to geom_col.\n\n\n\nSaving your work\nScripts: By now you are hopefully getting used to editing and working within the script. As you know, to save a script, you simply click the save icon, or press ctrl+S (cmd+s on a mac).\nPlots: To save a graph you have produced, click the “Export” button in the plot window, then “Save as Image”. You can resize the graph and give it an appropriate filename. If you’ve set the working directory correctly, then the new file should appear in the current folder.\nData: The data objects you create (in the Environment) only exist within RStudio, and are temporary (with a script and the csv file, you can always redo the analysis). But what if you want to use the data elsewhere? For example you may want to share the data with your project (PEP?) supervisor. To do this, we need to write the data to a csv file (like those we use to import the data). You can do this with the following command: write_csv(the_data_object, \"the_filename.csv\").\nExporting from RStudio: The above save operations save files to a folder within RStudio Server. At some stage you will need to get these files out of RStudio Server, for example if you need a graph for your report, or you need to share the data or the scripts. Or maybe you want to make the csv file available to other researchers. To get files out of RStudio, simply select the files you want in the Files pane, click “More” and then “Export”. Selecting multiple files will produce a “.zip” file, which will need to be “unzipped” on your computer to access the individual files (instructions for Windows and instructions for Mac)"
  },
  {
    "objectID": "PSYC121/Week8.html#week-8-quiz",
    "href": "PSYC121/Week8.html#week-8-quiz",
    "title": "8. Related-samples t-tests, plotting means and SE bars",
    "section": "Week 8 Quiz",
    "text": "Week 8 Quiz\nYou can access a set of quiz questions related to this week here."
  },
  {
    "objectID": "PSYC121/Week9.html",
    "href": "PSYC121/Week9.html",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "",
    "text": "Online tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\nCreate a folder for Week 9. and download the week_9.zip file and upload it into this new folder in RStudio Server."
  },
  {
    "objectID": "PSYC121/Week9.html#pre-lab-work-online-tutorial",
    "href": "PSYC121/Week9.html#pre-lab-work-online-tutorial",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "",
    "text": "Online tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\nCreate a folder for Week 9. and download the week_9.zip file and upload it into this new folder in RStudio Server."
  },
  {
    "objectID": "PSYC121/Week9.html#rstudio-tasks",
    "href": "PSYC121/Week9.html#rstudio-tasks",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "RStudio tasks",
    "text": "RStudio tasks\n\nExploring the data\nIn this class we will be exploring some data on people’s estimations on aspects of the UK population. We asked people 4 different questions:\n\nOut of every 100 people, about how many do you think are:\n\n\n\nChristian?\nMuslim?\nOver the age of 65?\n\n\nWe also asked a related question about immigration:\n\nWhat percentage of the UK population do you think are immigrants to this country? (i.e. not born in UK)\n\n\nRead in the data “data_wk9.csv” using read_csv().\nTake a look at the summary statistics for all of the columns in our data using summary()\nAt this stage you could try using ggplot() with geom_histogram() (hint: not sure? look at last week) or geom_density() to explore the different columns.\nYou may be tempted at this stage to apply the filter() commands to remove any outliers. If so, yes, that’s entirely sensible. But note that we are going to remove outliers a little further into our tasks after we’ve first done some visualisation.\n\n\n\nVisualising relationships in the data\nTo what extent are people’s estimations of these population parameters related? Let’s look at this by plotting these data as geom_point():\n\nComplete the code to add one numeric column in the data to x and another numeric column to y. You can pick any of the columns you like, but it’s important that you understand what research question you are asking with your choice. For example, you might be asking “Do people who estimate there are more Christians in the population also think there are more people over the age of 65?”\nNote the general pattern in the data. Is there a postive relationship: do people who give high estimations for one variable tend to give high estimations for the other variable? Or is there a negative relationship: do people who give high estimations for one variable tend to give lower estimations for the other variable? Or is there no discernable relationship at all?\nCopy the code and edit it to explore relationships between the other numeric variables, each time noting the research question you are asking, and discussing on your table what kind of relationship you can see in the data.\n\n\n\nUsing z-scores to remove outliers\n\nYou may have noticed that there are some fairly extreme values in some of these numeric estimations of the population. As we’ve discussed in previous weeks, these outlier values can be problematic when we run our statistical tests, so (like last week) we probably want to control their influence by removing them. As you saw in your online tutorial, we can convert the data to z scores, and then remove z values above and below certain values.\nLet’s create a “z-transform column” called z_imm for the estimates of the percentage of immigrants. Complete the code you have been given by adding the relevant variable (column) name to the code. You may want to create a new data object at this point.\nView the new data object to check this column has been created correctly. Like in the online tutorial, it would be a good idea to calculate some descriptive statistics for these new columns to check it conforms to what we know about z-scores (e.g., mean() should be very close to 0, sd() = 1). Note, if you want to change the output in r from scientific notation to non-scientific notation, you can run the command options(scipen=999).\nWe know from our lectures on the z distribution that values of greater than 2 (or less than -2) reflect around 5% of the distribution, and values greater than 3 (or less than -3) represent less than 1% of the distribution:\n\n\n\nLet’s consider an outlier any value that has a z of 2.5 (a conventional cutoff). Plot a histogram of the z_imm column in order to inspect whether there are data that are above 2.5 or below -2.5.\nAdd a filter command to remove the values in the z_imm column are greater than 2.5 or less than -2.5."
  },
  {
    "objectID": "PSYC121/Week9.html#unrelated-samples-t-test",
    "href": "PSYC121/Week9.html#unrelated-samples-t-test",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "Unrelated samples t-test",
    "text": "Unrelated samples t-test\nWe have also included a categorical variable in our data this week, which is one you have seen before in our analysis classes: the home location in the UK of the respondent, home_location_in_UK. For this data object we have included only those responses from those people from the “North” (NW and NE) and those from the “South” (SW and SE). Other respondents from elsewhere have been removed from the data. We can therefore look at whether people’s home location determines their population estimations.\n\nFirst we will look at the mean population estimations, split by home location. To do this, complete the group_by() and summarise() commands to give the mean estimates of the proportion of muslims in the population by home location. You don’t need to edit the N = n() line - this provides the number of participants at each level of the home_location_in_UK variable.\nWhat do the means suggest? Do people in the North and South give different estimations?\nLet’s test if these differences are real. First, it is worth noting that many more respondents originate from the North than from the South (see the N column in the summary). We have unequal sample sizes, and potentially unequal variances. Run the var.test() code to check if the variances of the two samples are similar (homogeneity of variance). If this test produces a p value less that .05, then the variances in the two samples are unequal. That will have consequences for how we run the t-test() in the next step.\nNow let’s run the t-test. This week we are comparing data from different samples of participants (those who are from the North and South). We need to tell the t-test that the data are NOT paired (paired = FALSE). The result of the var.test() in the last step will tell you whether the var.equal value should be TRUE or FALSE. Set var.equal = FALSE or var.equal = TRUE depending on whether the variances are equal. When you’re happy with the parameters, run the t-test. What result did you get and what does this mean? Discuss this with your table, or with the staff in the lab.\nIn that t-test we looked at the pop_est_muslim variable, but we can do this test for all of our population estimates. Copy the code to run another var.test() and t.test(), for the pop_est_christian variable. Note the t and p values; What do these tell us about the relationship between music preference and social media use? You can also run two further tests on the other population estimations."
  },
  {
    "objectID": "PSYC121/Week9.html#power-and-effect-size-d-calculations",
    "href": "PSYC121/Week9.html#power-and-effect-size-d-calculations",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "Power and effect size (d) calculations",
    "text": "Power and effect size (d) calculations\n\nWe saw in last week’s lab tasks that there was a significant effect in our Stroop task data: participants were faster to say the colour names of the compatible list compared to the incompatible list (there were significant differences with the control list too). We will now use these data to calculate an effect size (Cohen’s d) for the t-statistic that we observed in that test.\nImport the stroop data. We have reduced the data down to just the compatible and incompatible conditions.\nRun the cohens_d() code to calculate the effect size, which is reported as effsize. You can ignore any negative sign, taking note of the absolute value.\nWe already know that this large effect size was significant with our large sample of participants. What might we have expected with a much smaller sample size? Use the pwr.t.test() function to add in the effect size that you calculated (Cohen’s d) and set the N to 20. What power would we have achieved with this sample size, to detect this large effect? Discuss with your table, or staff, what this power means.\nLet’s say we wanted our next experiment to have an 80% chance of finding an effect at least as large as the one we found. Complete the pwr.t.test() function to work out the minimum sample size we would need to achieve power of .8, with this effect size.\nLet’s say we are looking to run a new experiment in which we give people a stressful task to complete simultaneously. We will ask them to put their hands in a bucket of ice cold water while doing the Stroop task (this is a real “stressor task” people use!). We are unsure of what consequence this will have for our effect size, but we want to estimate the effect size that could be detected in our experiment. We decide to run 40 participants, and want to achieve a power of .90 (90% chance to find an effect at least this large). What is the minimum effect size we could hope to detect under these conditions?"
  },
  {
    "objectID": "PSYC121/Week9.html#week-9-quiz",
    "href": "PSYC121/Week9.html#week-9-quiz",
    "title": "9. Unrelated-samples t-test and Power",
    "section": "Week 9 Quiz",
    "text": "Week 9 Quiz\nYou can access a set of quiz questions related to this week here."
  },
  {
    "objectID": "PSYC121/index.html",
    "href": "PSYC121/index.html",
    "title": "Statistics for Psychologists I",
    "section": "",
    "text": "Welcome\nThis is filler text introducing students to the course in general. Perhaps what’s expected of them, the course aims, and other information of use.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam in est non nisi eleifend vulputate a et magna. Mauris vulputate felis lacus, ut bibendum lectus sollicitudin vel. Integer vestibulum arcu et risus vestibulum, in ullamcorper magna consectetur. Donec ut tempus enim, at vulputate libero. Integer porta metus eget elit cursus, sit amet hendrerit ipsum facilisis. Maecenas mollis, elit gravida ornare tempus, est lectus mattis velit, et commodo nunc lectus eu enim. Aenean luctus, felis ac sodales accumsan, dolor nunc euismod lorem, vel vulputate ipsum orci quis ipsum. Quisque placerat, velit vitae dictum feugiat, lectus lorem rutrum ligula, at fringilla ex enim consectetur felis. Aliquam erat volutpat. Nunc a nisi eget ex ornare dictum.\n\n\nCourse Contacts\n\n\n\n\nEmail Address\n\n\n\n\nTom Beesley\nt.beesley at lancaster dot ac dot uk\n\n\nJohn Towse\nj.towse at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC121/Week1.html",
    "href": "PSYC121/Week1.html",
    "title": "1. Introduction to PSYC121",
    "section": "",
    "text": "For each lab session that you have been scheduled to attend, please come “prepared’. By this we mean\n\nYou have watched the lecture video, made notes, and developed questions where you have them.\nYou have worked through the online “prelab” preparation materials - Here is the one for Week 1\nYou read the lab sheet (this one) to get a sense of what we’re going to be doing, and you anticipate potential problems so that you can focus on these in the session.\n\nThe lecture is designed to deliver important ideas and procedures for learning about analysis. Pre-lab material is then designed to help consolidate this learning, or enhance, expand and apply it in ways that set the scene for the lab session activity. We want to prepare you to be ready to go in the session itself and make the most of your time there.\nWant additional support? Keep in mind that the Department has endorsed and will use the Statistics textbook by David Howell called “Fundamental statistics for the behavioral sciences”\nThe book covers principles of statistics as well as some tutorials on using R. You can access a library copy from the library pages"
  },
  {
    "objectID": "PSYC121/Week1.html#analysis-labs-and-pre-lab-activity",
    "href": "PSYC121/Week1.html#analysis-labs-and-pre-lab-activity",
    "title": "1. Introduction to PSYC121",
    "section": "",
    "text": "For each lab session that you have been scheduled to attend, please come “prepared’. By this we mean\n\nYou have watched the lecture video, made notes, and developed questions where you have them.\nYou have worked through the online “prelab” preparation materials - Here is the one for Week 1\nYou read the lab sheet (this one) to get a sense of what we’re going to be doing, and you anticipate potential problems so that you can focus on these in the session.\n\nThe lecture is designed to deliver important ideas and procedures for learning about analysis. Pre-lab material is then designed to help consolidate this learning, or enhance, expand and apply it in ways that set the scene for the lab session activity. We want to prepare you to be ready to go in the session itself and make the most of your time there.\nWant additional support? Keep in mind that the Department has endorsed and will use the Statistics textbook by David Howell called “Fundamental statistics for the behavioral sciences”\nThe book covers principles of statistics as well as some tutorials on using R. You can access a library copy from the library pages"
  },
  {
    "objectID": "PSYC121/Week1.html#activities-for-this-week",
    "href": "PSYC121/Week1.html#activities-for-this-week",
    "title": "1. Introduction to PSYC121",
    "section": "Activities for this week",
    "text": "Activities for this week"
  },
  {
    "objectID": "PSYC121/Week1.html#task-1---check-in-with-the-university-attendance-register",
    "href": "PSYC121/Week1.html#task-1---check-in-with-the-university-attendance-register",
    "title": "1. Introduction to PSYC121",
    "section": "Task 1 - check-in with the University attendance register",
    "text": "Task 1 - check-in with the University attendance register\nWhen you arrive, make sure you have checked-in to your Analysis session in the Levy lab. All students are required by the University to confirm attendance at taught session\nStaff will remind you of this in your class."
  },
  {
    "objectID": "PSYC121/Week1.html#task-2---getting-dicy",
    "href": "PSYC121/Week1.html#task-2---getting-dicy",
    "title": "1. Introduction to PSYC121",
    "section": "Task 2 - Getting dicy",
    "text": "Task 2 - Getting dicy\n\nHere’s a simple task for you to complete as a group around each of the workstations;\nYou will be given a pair of dice\n\nWorking in pairs, one person rolls both dice.\nAdd up the total on each of them and have someone record that total (if you don’t have some spare paper or a pen, use your computer)\nRepeat those steps 20 times.\nThen swap over your roles (the person rolling the dice, the person recording the outcome)\nOnce everyone at the workstation has had a turn at this, each person should attempt to work out (a) the mean and (b) the median of their dice roll total.\nCheck each others working, and discuss any differences or problems you have.\n\nAre all your answers the same? Why / why not? If not, are they very different or very similar?"
  },
  {
    "objectID": "PSYC121/Week1.html#task-3---using-rstudio",
    "href": "PSYC121/Week1.html#task-3---using-rstudio",
    "title": "1. Introduction to PSYC121",
    "section": "Task 3 - Using RStudio",
    "text": "Task 3 - Using RStudio\n\nIntroducing R Studio\nR and RStudio is the software that we will be using to explore and learn about analysis in your Psychology degree. It’s a computational engine: a very snazzy calculator that you should see as your friend and ally in the journey to understand and appreciate psychology. It sits alongside what we teach about the concepts and interpretation of statistical analysis.\nR is the core software, RStudio is the interface for interacting with it. Put another way, R is the engine, RStudio is the cockpit.\nLike even a simplest calculator, it just does what you ask (at least when you ask nicely!) but it requires the user to know what they want from it and to understand what it is telling you. A calculator can’t help a kid get the right answer to a multiplication problem if they don’t know the difference between multiplication and division and addition etc. And whilst a calculator is brilliant at doing the number crunching (and as a bonus, R Studio can help with turning the numbers into beautiful graphs and images too), even a calculator requires a thoughtful person to take the answers and make sensible interpretations from them.\nTherefore, we need to learn both about the concepts of statistical analysis on the one hand, and the processing of statistical information -through R- on the other. The lectures will provide the starting point and the direction for statistical concepts, whilst these analysis labs provide the more practical experiences in how to use R, and how to make R your ally. Over the next year, in these labs we will increasingly be using RStudio to focus on the latter, processing side, which will allow you to focus your energies on the conceptual side and its relevance for appreciating psychology.\n\n\nGetting started with RStudio\nFor Lancaster University Psychology Students in 2022, we will be learning about R Studio through a simple but powerful web server architecture. That is, through the power of the internet, you can access and use R Studio by logging into a free account that we have provided and we will maintain for your use.\n\nHere’s a little secret: There are several different ways to access RStudio. For example, you can download a copy of the software onto your computer, or use a Virtual Machine set up to run a copy. There’s nothing to stop you having your local copy, but please note - we can’t support your own version through lab classes. We’re using the web server to make sure everyone has the same, controlled experience.\n\nYou will have received an email with your account information to log onto From a computer on the campus wifi, you can access R Studio at:\nLancaster Psychology R Studio Server\nAt the login screen, use your university username (e.g., bloggsj)\nYour password for R Studio is: [password here]\nAs it says in the subject line, please keep your account details safe!\n\n\nWhat does RStudio look like?\nWhen RStudio starts, it will look something like this: \nRStudio has three panels or windows: there are tabs for Console (taking up the left hand side), Environment (and History top right) , Current file (bottom right). You will also see a 4th window for a script or set of commands you develop, also (on the left hand side).\n\n\nLet’s get started!\nThe first thing we want to do in RStudio is to create a folder for this week so that we can put the relevant material there and keep it tidy.\nFrom the lower-right panel of RStudio, click the files tab.\nSelect the “new folder” option and create a new folder (eg “week 1”)\nClick on that folder to open it\nNext, we’ve prepared some instructions for RStudio to use - this is called a “script”. So we need to get this script into the server for you to explore and play with\n\nDownload the “zip” file by clicking this link\nFind the location of the file on your computer and check it is saved as a “.zip” file\nReturn to RStudio\nClick “Upload”\nClick choose file and find the file on your computer.\nSelect the file and click “Open”. Click “OK”\n\nYou should now see the files extracted in the directory. If you receive an “unexpected server error” please try this process in a different browser. If you still have trouble, send your username to t.beesley@lancaster.ac.uk for support.\nYou should now have the script available in RStudio.\nUse “Save…As” to create a new version of the script. By doing this, you’ll be able to have a “before” and “after” version of the script and can go back over the changes\nIn the script, select or highlight the first line of text, which is this one:\n\nRun your first ever R instruction!\n\n5 + 5\n\nand “run” this line. That tells RStudio to carry out the instruction.\nYou should see that in the console tab, RStudio calculates the answer to this incredibly hard maths challenge! (amazing huh? OK, maybe not *that* amazing…).\n\n\nModify your first ever R instruction!\nUse your imagination – add a new line to the script and ask a different simple arithmetic question of your own choosing! What happens?\n\n\nCalculate descriptive stats in R for the first time!\nIn this week’s analysis lecture, we looked at measures of central tendency and how to calculate them. So let’s get R to do these calculations also!\nFirst, we tell R about the data used in the lecture. We’ve already created the instruction that will do exacly this and it is in the script, so run this line from the script\n\nweek_1_lecture_data &lt;- c(7,8,8,7,3,1,6,9,3,8)\n\nThis creates an “object” called week_1_lecture_data. We can then perform calculations on this object. For example, we can find the mean by running the following command (use the script to do this)\n\nmean(week_1_lecture_data)\n\nCheck the answer is the same we found in the lecture (it should be 6!).\nNext, let’s ask for the median by running this line from the script:\n\nmedian(week_1_lecture_data)\n\nThis also should be the answer from the lecture (7)\nR doesn’t have a single corresponding command for the mode, but we can use the block of code in the script for this that starts and ends with the “getmode” text (there are 6 lines of text)\nThis is just a bit of clever jiggery-pokery that gets the mode. What does R say the mode is?\n\n\nYour challenge\nHow can you get RStudio to verify / check the dice calculations that you attempted earlier? Think about how you might solve this problem, on the basis of what we have covered so far.\nWe will discuss this in class and attempt to get RStudio to check your answers. In doing so, annotate the script (add notes for you - not RStudio) using the “#” command"
  },
  {
    "objectID": "PSYC121/Week1.html#before-you-finish",
    "href": "PSYC121/Week1.html#before-you-finish",
    "title": "1. Introduction to PSYC121",
    "section": "Before you finish",
    "text": "Before you finish\n\nMake sure you save a copy of the script that you have been working on by the end of the session. This provides you with the record - the digital trace - on what you have done. And it means you can come back and repeat any of the work you have performed.\nEnd your session on the RStudio server, this logs you out of the server and stops any ongoing activities and tasks you have set up, maybe in the background.\n\nThere is a red “power” button near the top right of the R studio window (do ask for help if you can’t find it). It’s a good habit to get into to turn the session off\n\n\nExtra content for outside the lab class\n\nIn the Howell text book on statistics, there’s some R code on descriptive statistics. It is included in the script for you to look at and play with.\nin your own time and think about the following:\n\nIn R, “&lt;-” is the assignment operator as in the command we used:\n\nPSYC121_week_1_data &lt;- c(7,8,8,7,3,1,6,9,3,8)\n\nWe create the variable label on the left (Analysis_week1_data) and we give it those numbers on the right. The nameAnalysis_week1_data` is largely arbitrary: try use a variable of your own naming (your own name?) instead - and then use that alternative name for the other commands.\n\nThroughout this year, we’ll use the convention of the “underscore” to separate words in labels (it_makes_them_easier_to_read than ifyoudidn’thaveanyspaces)"
  },
  {
    "objectID": "PSYC121/Week1.html#task-4-review-the-learnr-sample-practice-questions",
    "href": "PSYC121/Week1.html#task-4-review-the-learnr-sample-practice-questions",
    "title": "1. Introduction to PSYC121",
    "section": "Task 4 – Review the learnr sample / practice questions",
    "text": "Task 4 – Review the learnr sample / practice questions\nAfter every block of teaching in part-1 analysis (specifically, we mean in week 5, week 10, week 15 and week 20) there will be a class test. This will assess your knowledge and your understanding of the material that has been covered.\nThe class test will comprise a set of Multiple Choice Questions (and the set of questions will be different for each student, as the test will involve random selection from a larger pool) under timed conditions.\nIn order to help you get (a) a broad or basic feel for the sort of questions you might get in the class test (b) self-review your progress through the term, we will provide MCQs each week for you to attempt.\nSo these are for your benefit… you can take the questions when you choose to, and the learnr quiz will provide feedback on the answers your provide. Just bear in mind:\n\nWe place a set of questions at the end of the learnr pages so that you can attempt these at the end of each week, after you’ve completed lab activities, follow-up work, weekly Q&As etc. But it’s up to you when you answer the questions\nThese are meant as indicative questions. There’s no point in learning/ memorising these questions (they won’t be on the quiz!) and our advice is to reflect on how the teaching and content links to the sorts of questions that get posed."
  },
  {
    "objectID": "PSYC121/Week1.html#task-5-data-collection-exercise",
    "href": "PSYC121/Week1.html#task-5-data-collection-exercise",
    "title": "1. Introduction to PSYC121",
    "section": "Task 5 – Data collection exercise",
    "text": "Task 5 – Data collection exercise\nIn order to learn about psychology and data analysis techniques, we need data! Rather than rely too much on artificial data (certainly it is sometimes useful to say “Here are a bunch of numbers and this is what we can do with them” – think about the R Studio example for this week’s lab) for the most part, we prefer to draw on datasets that are a bit more engaging and meaningful that you have a stake in yourself! By using a common data set, that we can return to over the year, we can also build up familiarity and confidence in the data and remove a potential obstacle to thinking about the more important analysis part.\nSo a key task will be for everyone to have a go at taking our online survey, and contribute to a dataset that can be used throughout the year.\nWe would like you to complete the survey via your Department Sona system account. This way, you will receive a research credit for doing so, to “jump start” your credit account.\nThe sona system is can be accessed by following this link"
  },
  {
    "objectID": "PSYC121/Week2.html",
    "href": "PSYC121/Week2.html",
    "title": "2. Descriptive statistics in RStudio",
    "section": "",
    "text": "Last week, among other things, we asked you to\n\nRoll some dice, carry out some (relatively) straightforward ‘hand’ calculations of central tendency\nConnect to the RStudio server and create a folder\nWithin RStudio upload and run a script - a set of instructions, and explore annotations\nAdapt the script commands to perform calculations on the dice rolls within RStudio\nComplete a survey so that we can collect data for analysis teaching\n\nYour progress was great! We start with small steps and build up - but this is a nice start and we’re pleased how things went!\nBefore the lab, make sure you have worked through the material in the week2 learnr tutorial. The link is here\n\n\nFor a reminder of how to start RStudio, see week 1 lab sheet\n(remember: off campus, you will need to be on the VPN)\n\nA word of advice (from David Howell’s statistics book: “One more word of advice”I can’t resist adding what is perhaps the best advice I have. If there is something that you don’t understand, just remember that “Google is your friend.” She certainly is mine. (Well, maybe Google is getting a bit pushy, but there are many other search sites.) If you don’t understand what Fisher’s Exact Test is, or you don’t like my explanation, go to Google and type in Fisher’s Exact Test. I just did that and had 260,000 hits. You can’t tell me that there isn’t going to be something useful in there.)”\n\n\n\n\nIn week 1, we had a tiny dataset (relatively speaking) that we entered into R through a script line. That worked for what is was. But it’s going to become painful and tedious when (a) we want to work with larger datasets (b) we have data more complex than a 1-dimensional list of numbers (think about some 2-dimensional data sheets you might have encountered in excel for example)\nR can handle data files, and this week we’re going to explore them. Within R, we can specify ‘data frames’ which can have, essentially, multiple columns of data, and we can link data files to data frames for processing\nTo make things straightforward, each week we’ll provide students with a “zip” file that contains the script to start from (which you can expand and annotate etc, and save on your file space). We’ll also provide a data file or data files for you to use in the zip file. R can then import these files into the RStudio environment. So when you upload the zip file, you can import the data AND you can open up the script"
  },
  {
    "objectID": "PSYC121/Week2.html#pre-lab-work",
    "href": "PSYC121/Week2.html#pre-lab-work",
    "title": "2. Descriptive statistics in RStudio",
    "section": "",
    "text": "Last week, among other things, we asked you to\n\nRoll some dice, carry out some (relatively) straightforward ‘hand’ calculations of central tendency\nConnect to the RStudio server and create a folder\nWithin RStudio upload and run a script - a set of instructions, and explore annotations\nAdapt the script commands to perform calculations on the dice rolls within RStudio\nComplete a survey so that we can collect data for analysis teaching\n\nYour progress was great! We start with small steps and build up - but this is a nice start and we’re pleased how things went!\nBefore the lab, make sure you have worked through the material in the week2 learnr tutorial. The link is here\n\n\nFor a reminder of how to start RStudio, see week 1 lab sheet\n(remember: off campus, you will need to be on the VPN)\n\nA word of advice (from David Howell’s statistics book: “One more word of advice”I can’t resist adding what is perhaps the best advice I have. If there is something that you don’t understand, just remember that “Google is your friend.” She certainly is mine. (Well, maybe Google is getting a bit pushy, but there are many other search sites.) If you don’t understand what Fisher’s Exact Test is, or you don’t like my explanation, go to Google and type in Fisher’s Exact Test. I just did that and had 260,000 hits. You can’t tell me that there isn’t going to be something useful in there.)”\n\n\n\n\nIn week 1, we had a tiny dataset (relatively speaking) that we entered into R through a script line. That worked for what is was. But it’s going to become painful and tedious when (a) we want to work with larger datasets (b) we have data more complex than a 1-dimensional list of numbers (think about some 2-dimensional data sheets you might have encountered in excel for example)\nR can handle data files, and this week we’re going to explore them. Within R, we can specify ‘data frames’ which can have, essentially, multiple columns of data, and we can link data files to data frames for processing\nTo make things straightforward, each week we’ll provide students with a “zip” file that contains the script to start from (which you can expand and annotate etc, and save on your file space). We’ll also provide a data file or data files for you to use in the zip file. R can then import these files into the RStudio environment. So when you upload the zip file, you can import the data AND you can open up the script"
  },
  {
    "objectID": "PSYC121/Week2.html#lab-exercises---descriptive-information-in-r-studio",
    "href": "PSYC121/Week2.html#lab-exercises---descriptive-information-in-r-studio",
    "title": "2. Descriptive statistics in RStudio",
    "section": "Lab exercises - descriptive information in R Studio",
    "text": "Lab exercises - descriptive information in R Studio\nSome years ago, a large group of participants gave an estimate of the weight of Penelope the cow. Just over 17,000 guesses. And the distribution of guesses was something like this: \nWhat we can see from this graph is that:\n\nGuesses formed a roughly normal distribution. There is a bit of a skew with a right-hand tail, but this is inevitable as a weight of less than 0 is physically impossible, but there is no limit of the semantics of a large guess.\nThe mean guess weight (1,287 lbs) is very close to the actual (true) weight of the cow (1,355 lbs). So even though lots of people were inaccurate, a central tendency measure has a pretty good alignment with the true weight. This is known as the Wisdom of Crowds phenomenon, first identified by Galton in 1907 (though he suggested using the median weight)\n\nLet’s look at (a sample of) the PSYC121 student data collected on guessing the weight of Penelope, and ask whether it resembles the properties of this large dataset.\n\nLoading the data\nUsing the instructions and advice given on Moodle, get the week2.zip file and bring the data and R script into R Studio. The week 2 zip file is here\n\n\nUsing the R script\nStep 1. Download the week_2.zip file (If you are using a mac, there is a video guide on Moodle to explain how to download the zip file as is)\nStep 2. Open the appropriate folder on the RStudio server\nStep 3. Upload the zip file\nLet’s start working with our data, by opening up (clicking on) the script “Week_2.R” file.\nThe first command is to load a library of functions:\n\nlibrary(tidyverse)\n\nTo run this, simply click anywhere on line 1 of the R script to put the cursor there, and press ctrl+enter (cmd+enter on a mac) or click the button called run. You will see a number of messages appear in the console. Don’t worry about these, or worry too much about what exactly this command is doing. Essentially this is giving us some useful tools for our analysis. We will introduce the features of the tidyverse gradually during this course.\n(side note: If you were using a local version of R studio on your computer, it might not have the ‘tidyverse’ library already installed. You would need to install the package first)\nThe data are on the RStudio server if you have followed all the lab sheet to this point. Note that when you imported the data into the R environment, a command line was generated at the console\n\ncows &lt;- read_csv(\"~/penelope22.csv\")\n\nWhat this command accomplished was to read the spreadsheet called ‘penelope22’ into an object in R called cows. You could use any object label, but it’s important to then keep that name consistent in what you do next.\nThe command was also generated\n\n View(cows)\n\nwhich presents the data in a window of RStudio. Note that “NA” means not available or missing data. Does this file structure make some sense to you?\n\n\nFinding the mean and median estimates\nUse the data to answer the following questions…\n\nWhat is the mean weight estimates?\nWhat is the standard deviation of the estimates?\nWhat is the median weight of the estimates?\nWhich of these central tendency measures is the more accurate measure of the true cow weight? (make a judgement)\nWhat is the mean weight estimate (and standard deviation) for female respondents and non-female (male / non-binary /prefer not to say) respondents?\n\nYou may be thinking, how do I possibly do any of this?! Well this week most of the commands you need are contained in the R script you have downloaded. Also, remember from last week, we explored the R command:\n\nmean(week_1_lecture_data)\n\nThat gave us the mean of the small dataset week_1_lecture_data. This time, we want to explore the penelope dataset. But also, the lecture_data was just a single list of numbers. The penelope22 object is more like a datasheet. So we need to tell R Studio which column we are interested in. RStudio uses the format data$column. So run the followinbg line in the script\n\nmean(cows$estimate) \n\n\nsd(cows$estimate)\n\nSo from this, can you work out what you would do to get the median value (remember from last week how we got the median value?)? Part fo the command is given to you, can you change the text so that it works?\n\n\nCalculations from a range of columns\nWe have seen that:\n\nmean(cows$estimate) \n\nwill provide a mean of the column “estimate”. In the third column, named “female_estimate”, we have the estimates of just the female respondents. In the fourth column, named “other_estimate”, we have the estimates of the “other” respondents (males and non-binary and prefer not to say).\nSo can you now figure out how you might get information about the estimate from the female data (only) or the non-female data? Try it, based on what you have just done. Does it work?\nYou will find that the result of the this command produces an “NA” result. This means that the answer is “Not Available”, or in other words, is a “missing value”. This is because some of the values in this column are NA, and the mean of a column with NAs will always lead to the result NA.\nInstead, try change the script so it looks like this:\n\nmean(cows$female_estimate, na.rm = TRUE )\n\nAny different? The na.rm = TRUE instruction tells RStudio that missing data can be ignored in this mean calculation. (in technical language, na.rm is a parameter of the function mean that removes the NAs if set to TRUE)\n\n\nSimple graphs\nRStudio can be used to create graphical data plots that can help interpret datasets\nThe first thing we can do is create a histogram distribution of guesses from the sample student data to compare with the previous large sample study (i.e. the 17,000 guesses):\n\nhist(cows$estimate)\n\nOne way to alter or adjust the histogram is to change the width of the bars, the intervals, between each plot section. Try run this line from the script\n\nhist(cows$estimate, breaks = ??)\n\nDoes it work? No? What you need to do is replace the two question marks in the script (or better still, create a new instruction line in which you amend this to have a numerical value representing the number of different plot bars. Try at least 3 different values. Look at and think about how this affects the visual distribution.\nWe can also create a “box and whisker plot”. Here’s a general simple description of a box-and-whisker plot as a graphical representation of data:\n\n\n\nAdditional challenge\nIn the zip file, we also provide data on the estimates of the percentage of immigrants in the UK. This will allow you to explore the data discussed in the analysis lecture, and create visualisations of the data and its spread. Can you apply the analysis of the penelope data to the immigration data?"
  },
  {
    "objectID": "PSYC121/Week3.html",
    "href": "PSYC121/Week3.html",
    "title": "3. DVs and IVs in RStudio",
    "section": "",
    "text": "Last week we asked you to\n\nUse a script to run instructions in RStudio\nPut data into RStudio form a data file and explore how to run descriptive stats\n\nThis week - again, there’s a learnr tutorial to follow and help prep for this week’s activities You can find it here"
  },
  {
    "objectID": "PSYC121/Week3.html#pre-lab-work",
    "href": "PSYC121/Week3.html#pre-lab-work",
    "title": "3. DVs and IVs in RStudio",
    "section": "",
    "text": "Last week we asked you to\n\nUse a script to run instructions in RStudio\nPut data into RStudio form a data file and explore how to run descriptive stats\n\nThis week - again, there’s a learnr tutorial to follow and help prep for this week’s activities You can find it here"
  },
  {
    "objectID": "PSYC121/Week3.html#r-studio-tasks",
    "href": "PSYC121/Week3.html#r-studio-tasks",
    "title": "3. DVs and IVs in RStudio",
    "section": "R Studio tasks",
    "text": "R Studio tasks\nFor the “penelope22” data in week 2, we provided you with estimates data, and you were able to generate descriptive statistics for the estimates (if not, please go back and work through that part of the week 2 lab sheet again). You also found the weight estimates for the female and non-female guesses, right?\nHowever, in order to find the estimates separately for gender identity, we needed to have a column for each gender category. Whilst that worked, it could get cumbersome over time always to work with data created like that.\nThere is a better way…\n\nTask 1 - More with the penelope22 data\nStep 1. This week, we again want to explore commands from the tidyverse library (toolkit) which can help us do more powerful things more elegantly. So let’s get R to work with the tidyverse library\nlibrary(tidyverse)\nStep 2. Explore help() commands. R can give you more information about how it works.\nStep 3. Creating a project and a folder, and setting the working directory\nStep 4. Bring the week3.zip file into R Studio server. Like last week, upload the zip file, and launch the R script.You can get the file here\nStep 5. Read or load the penelope data into R. Have a look at it using the View() command in the script\nThis time, let’s ask for the estimate data arranged by identity:\naggregate(x = *MISSING*2$estimate, by = list(*MISSING*$identity), FUN = mean)\nFirst, let’s try this (you will need to use your object name in place of MISSING). What do you get? Does this match what we did last week when we calculated the mean for the female and for the other (i.e., non-female) group?\nSecond, let’s look at what is happening here:\naggregate This is a command to call for descriptive statistics\nx= This defines what column we are analyzing\nby=list Now we tell R how to group the estimate data, and which column does that\nFUN=mean Specifies which descriptive function is being asked for Can you explore whether you can call on alternate measures?)\n\ngroup_by()\nThere’s another way that also allows us to group scores by a (nominal) variable. This is explored in the learnr tutorial, which should help you create the command the get weight estimates broken down by gender identity. You need to define the data frame for the estimates data, and the gender IV and the estimates DV\n*MISSING* %&gt;% group_by(*MISSING*) %&gt;% summarise(mean_estimate = mean(*MISSING*))\nFirst, try this command and see what you get. If you run this command as entered, it won’t work. So now use your experience at skills from the above and the learnr tutorial to work out what is required.\nNote\n%&gt;% This is called a “pipe operator”, basically take the output from the left and feed it into the requests on the right. Summarise Provide summary statistics information for the specified variable as specified (whether mean, median etc)\n\n\nThe assignment operator\nAs well as learning about the pipe operator, we want to remind you /draw attention explicitly to another important element of the R command line syntax: the assignment operator. Uing a command such as\ncows &lt;- read_csv(\"penelope22.csv\")\nlooks for the csv datafile called ‘penelope22’, and assign it to an object or variable called ‘cows’\nWe could create any object name we wanted (within limits of names already known to RStudio). It isn’t just for reading in data files, we can perform a whole range of functions and assign them to an object.\n\n\nTask 2 - Salary data\nUsing aggregate and summarise may not seem like much progress, because they are just replicating what we had already done with mean() is week 2. However (a) this emphasizes that there are often several ways to get at the same thing in R (b) now we know about grouping, about working with 2-dimensional data, we can start to do more efficient and informative things.\nNow, let’s turn to the guesses made about median salary in the UK. We can get the data from the file “wages” in the week_3.zip file (you will need to adapt the code in the script for the penelope data so that it will load in the wages data)\nLet’s take a peek at the dataset with\nglimpse(wages)\nGlimpse pretty much does what you might think from the meaning of the word – it just gives us a data sample (handy because this is a much bigger dataset) and shows that we have 3 columns; uk_region (where someone lives, note ‘other’ probably equals Northern Ireland, Europe, China, etc), family_position (age relationship with siblings), and salary (estimate).\nBy the way, the govt statistics say the actual median income in 2019 was approx. £30,350 see this link\n\nWriting into your script, use the working “aggregate” commands from task 1 with the penelope weight data to find out the salary guesses as a function of where someone lives? That is, can you adapt that code for this problem? First, make sure you read in the data file into an R object.\nCan you use the aggregate command to find out salary guess as a function of family relationships? (if you are the youngest child maybe you have older siblings earning money that changes your evaluation?)\nCan you get a breakdown of guess as a function of BOTH UK region AND family relationship together?\nCan you use the group by() command to display salary guesses as a function of where someone lives? Check this this gives you the same answer\n\n\n\n\nTask 3 - Phone Use data\n\nDatset 3: Use the dataset of phone screen time usage to further explore and consolidate the group_by() command (ie we’ll drop the aggregate command for this task to focus on group_by(). Adapt script lines from the above two tasks to read in and calculate screen time usage as a function of the type of phone.\nUse RStudio to figure out the overall mean salary estimate and the standard deviation. Calculate by hand what salary estimate would have a z score of z=-1.5?\n\n\n\nTask 4 - Final challenge: visualisation\nCan you find a way to visualise the screentime usage data that you have been working with above? The script provides two ways to consider doing this - boxplots (which we have looked at in script commands already) and ggplot, which we have spent less time with but is an extremely powerful engine for creating plots. We’ve provided the start of the code in each case, leaving you to work out the specifics. Remember to annotate your creations!\n\n\nNow you’re finished …\nRemember to complete your notes in the script, save the script file, and end the server session."
  },
  {
    "objectID": "PSYC121/Week7.html",
    "href": "PSYC121/Week7.html",
    "title": "7.Filtering data and testing means (one-sample t-test)",
    "section": "",
    "text": "Today we will look in a bit more detail at people’s estimates of the average UK salary. We will first plot this data using geom_histogram() and also geom_boxplot(). When we do this, we’ll see that there are some unusual values, and we’ll need to do a bit of data wrangling to remove them, using the filter() command. We’ll then turn to the conceptual ideas of the lecture - how can we tell if the mean of our sample is unusual, or whether we would actually expect this mean value under the null hypothesis? Finally, we’ll continue to develop our skills in data visualisation by exploring geom_density() plots."
  },
  {
    "objectID": "PSYC121/Week7.html#pre-lab-work",
    "href": "PSYC121/Week7.html#pre-lab-work",
    "title": "7.Filtering data and testing means (one-sample t-test)",
    "section": "Pre-lab work",
    "text": "Pre-lab work\n\nThere is an online tutorial. Please make every attempt to complete this before the lab!\nCreate a folder for Week 7\nDownload the Week_7.zip and upload it into this new folder in RStudio Server."
  },
  {
    "objectID": "PSYC121/Week7.html#rstudio-tasks",
    "href": "PSYC121/Week7.html#rstudio-tasks",
    "title": "7.Filtering data and testing means (one-sample t-test)",
    "section": "RStudio tasks",
    "text": "RStudio tasks\n\nPlotting and filtering\n\nOpen the Week_7_script and run the library, options commands. The options command is new. It is very cryptic and you don’t need to worry too much about this - it is making sure that the values in the graphs are displayed as regular numbers and not as scientific notation. Add a read_csv() command to get the data - you’ve done this every week, so this should be familiar.\nComplete the geom_histogram() code to plot the distribution of salary data. Try setting bins=20 within your geom_histogram() code. Play around with the number of bins for the histogram.\nOK - we’ve got some pretty funky values here! Some people think the average salary is over £400,000!!! Well, maybe they just added too many zeros (let’s give them the benefit of the doubt). Quite often when we get our “raw” data, it contains weird values like this that we need to consider removing. Let’s run the arrange() code now to see what exactly those high values are.\nWe’ll need to remove these high values to get a better sense of the distribution. Let’s use a filter() command to do this. We need to make a decision about what values to exclude. In later labs we’ll look at a more systematic process of removing outliers, but for now, let’s just remove any that are over £200,000. Edit the filter() command to keep only those estimates that are below £200,000 (&lt;). Remember that the filter command keeps the data that is TRUE according to the expression.\nSTOP! OK, this next bit is very important. If you completed that last step correctly you’ll see an output in the console showing a “tibble” (a data frame) which has 194 rows and 5 columns. However, your object has not actually changed in the environment. This will still be showing as 197 observations (row). So the filter command will take out those large estimations, but we haven’t actually changed the data object. To do this we need to assign (&lt;-)the result of our filter command to the object. To do this, you need to put in some code at the start of this small chunk of code you’ve just run, so that the result of your commands will be assigned to the object (you can make this the same object name you’ve been using -overwrite it-, or call it a new name).\nWhen you run this command again you should see the (new) object has changed to 194 rows. We can now plot the data again as a histogram. To do this, copy the earlier code for the histogram and edit it as necessary (if you’re using a new object for the filtered data, you’ll need to edit the code to reflect that). .\nAnd as you know, we can also look at the distribution as a boxplot, a violin, or a density plot. Feel free to add in your own code for other visualisations you might like to try.\n\n\n\nOne-sample t-test\nWe now want to know if the salary estimates are different to the actual average salary in the UK (which is approx. £30,000). Our hypothesis might be that people are inaccurate - they overestimate or underestimate the average UK salary. Let’s test that.\n\nFirst, let’s calculate the mean and sd of the column.\nNow we can compute a t-statistic and check whether this mean is significantly different from the expected mean. We do this with t.test(). Edit the code on this line to conduct a one-sample t-test. You need to provide the sample of data on which you want to conduct the test, and the expected mean under the null hypothesis. Remember our hypothesis is that people are not accurate. Your calculation of the mean should tell you whether they numerically overestimated or underestimated. But would we expect such a result under the null hypothesis? Run the t-test and note the p value. How likely is it that we would see this sample of data (this mean value and the distribution of data - the SD) under the null hypothesis? The p value ranges from 0 to 1. If it is very low - typically we say p &lt; .05 - then we conclude our result is unlikely under the null hypothesis and it is therefore a significant result.\nWhat is the critical value of t in the t-distribution table, for this sample size? Degrees of freedom is N - 1.\n\n\n\n\nPractising filtering\n\nFiltering can also be useful for selecting certain sub-sets of our data. In the script we have given you an example of how we select a sub-set of data based on two conditions from two different columns:\n\ndata_set_name %&gt;% filter(home_location_in_UK == \"NW\" & sibling_order == \"oldest\")\nWe have given you a few different columns to look at and to use in practicing your filter commands:\nsibling_order: what position in age was the respondent within their siblings home_location: UK / Asia / Europe, etc home_location_in_UK: NW, NE, etc (NA is non-UK residents) attention_check: respondents were asked “click strongly agree to show you are paying attention” - some people failed this!!!\nComplete the following filters. We’ve put in () the number of rows you should see in the resulting object\n\nJust those people who come from the North East (16 rows)\nThose people who come from Wales and are the middle child within their siblings (2 rows)\nThose people passed the attention check, are from the UK, and are an only child (21 rows)\nThose people who are NOT from the North West; you’ll need to use != (84 rows)\nThose people who failed the attention check (that is, did not say strongly agree) (14 rows)\nThose people who are from the South East or (|) the South West (38 rows)"
  },
  {
    "objectID": "PSYC121/Week7.html#sample-size-size-of-effect-and-the-one-sample-t-test",
    "href": "PSYC121/Week7.html#sample-size-size-of-effect-and-the-one-sample-t-test",
    "title": "7.Filtering data and testing means (one-sample t-test)",
    "section": "Sample size, size of effect, and the one sample t-test",
    "text": "Sample size, size of effect, and the one sample t-test\nIn the lecture this week, Tom used an application to show the process of sampling data. You can access this application at the link below. There are three “parameters” you can change in this:\n\nThe true mean of the effect: Think of this like the bias that was set up in your deck of cards last week. There is some true state out there in the world, and we are going to draw samples from a distribution of data that has a mean that equals this value. If you make this 100, then the true mean is equal to that under the null hypothesis (there is no effect).\nThe standard deviation of the data: This sets how variable the data are in this population. If the data are more variable, then our samples will produce estimations that are less accurate of the true mean value.\nThe sample size: How many observations are drawn in the sample. These are represented by the yellow circles in the plot.\n\nEach time you draw a sample the data points are plotted in yellow and the mean of the sample is marked with the red line. The application also runs a one-sample t-test against the expected mean under the null hypothesis, of 100. The null hypothesis is also represented by the static distribution presented in grey, centred on 100.\nThings to try:\n\nStart with a sample size of 10, and a mean of the effect of 110 (SD = 15). How often do you get a significant result (p &lt; .05) when you draw a new sample?\nNow try changing the mean of the effect to 120. Does this increase or decrease the likelihood of getting significant results? What about changing to 130?\nNow keep the mean effect constant (say 110), but increase the sample size. Try 5, then 10, 15, and so on. Does this increase or decrease the likelihood of getting significant results?\nSet the mean of the effect to 100 and the sample size to 10. Keep drawing new samples, noting each time the p value. You will evenutally get a p value of &lt; .05. What type of error is this?\n\nClick here for the one-sample t-test application"
  },
  {
    "objectID": "PSYC121/Week7.html#week-7-quiz",
    "href": "PSYC121/Week7.html#week-7-quiz",
    "title": "7.Filtering data and testing means (one-sample t-test)",
    "section": "Week 7 Quiz",
    "text": "Week 7 Quiz\nYou can access a set of quiz questions related to this week here."
  },
  {
    "objectID": "PSYC121/Week6.html",
    "href": "PSYC121/Week6.html",
    "title": "6. Sampling, probability and binomial tests",
    "section": "",
    "text": "Ensure you have watched the lecture for Week 6.\nThere is a learnr tutorial to complete which will help you to think about binomial tests. You : You can find it here.\nSet up a new folder in RStudio and upload the files from the Week 6 zip file\nIf you create a folder and upload the file into RStudio before the lab class you’ll be even more ready to follow along!"
  },
  {
    "objectID": "PSYC121/Week6.html#pre-lab-work",
    "href": "PSYC121/Week6.html#pre-lab-work",
    "title": "6. Sampling, probability and binomial tests",
    "section": "",
    "text": "Ensure you have watched the lecture for Week 6.\nThere is a learnr tutorial to complete which will help you to think about binomial tests. You : You can find it here.\nSet up a new folder in RStudio and upload the files from the Week 6 zip file\nIf you create a folder and upload the file into RStudio before the lab class you’ll be even more ready to follow along!"
  },
  {
    "objectID": "PSYC121/Week6.html#card-sampling-task",
    "href": "PSYC121/Week6.html#card-sampling-task",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Card sampling task",
    "text": "Card sampling task\n\nIn the first task this week we will look at the sampling of events and we will apply the basic statistical test of the binomial test: binom.test()\nEach table has a set of cards. These will be 13 red cards and 13 black cards - please check your set to ensure you have the right number of each colour (it doesn’t matter what suit the cards are).\nIn this task we will be drawing samples from the deck, and trying to tell whether the deck is biased towards red or black. Think of the cards as the population of scores out there in world, and when you can only see back of the cards, that data is unobserved.\nAs an experimenter we are trying to estimate what is true about the world, and in order to do this we need to draw a sample. So each time you draw a card, you are observing one data point from the population, and based on a collection of data (the sample) you are going to draw an inference about what is true about the population.\n\nSet up and instructions:\n\nOne person on each table should act as the “world” (the dealer). Congratulations, you are God. That is, this person will determine what is true about the state of things in the world. In this example, that means they control what is contained in the deck of cards. For each experiment, secretly remove some cards from the deck and place those face down. It’s important that no one sees what these cards are. For example, you might take out 5 red cards, so that 13 black and 8 red are left; this deck is now biased to black. Or you might take out 3 red and 3 black; this deck is not biased.\nThe remaining people (1 or more) will act as the experimenters. Your job is to draw samples and work out whether you think the deck is biased or not towards either red or black.\nFor each experiment, go through the following steps:\n\n\nThe world removes some cards from the full deck (the number and colour of the cards removed is up to them) and they place these face down on the table. They shuffle the deck ready to start the “experiment”.\nThe experimenters “pre-register” their sample size. That is, they state how many cards they are going to draw.\nDraw samples one at a time. Importantly - make sure you replace the cards each time and the world/dealer should give the pack a quick shuffle.\nMark down whether the card was red or black in your logbook\nThe world should shuffle the cards after each draw. Repeat 2.3 and 2.4 until you have reached your sample size.\nAt the end of each experiment, the experimenters should draw a conclusion based initially on their own “gut feeling” about the data. Do you think the deck was biased towards red, black, or was it unbiased?\nAs a group, use RStudio to run a binom.test() to provide a statistical result (you can do this in the console, or create a new script to save your tests and results - it’s up to you). Was this result unusual? How likely were the data given the null hypothesis? Note down the p value that this test gives you.\nThe “world” can then reveal the hidden cards. Was the deck actually biased or not? How does this sit with a) your initial conclusions, and b) the result of the binomial test?\n\n\nRepeat all steps in part 3 again for a new experiment, making sure that you try different parameters for the experiment. So vary a) how many cards are removed from the deck, b) the combination of cards removed from the deck, and c) the pre-registered sample size. Feel free to swap the roles around.\nOnce you’ve conducted a few experiments, discuss on your table the results you found. It might be useful to think about the following things:\n\n\nwere there times when your intuitions were different to the statistical result? For example, you were sure there was a bias, but in fact the statistics told you this was not that unusual (p was &gt; .05)?\nwere there times when the deck was actually biased, but you failed to prove this with your experiment (you failed to see p &lt; .05)? Do you remember what this type of error is called?\nwere there times when the deck was not biased, but the test result suggested it was (p &lt; .05)? Do you remember what type of error this is called?"
  },
  {
    "objectID": "PSYC121/Week6.html#rstudio-tasks",
    "href": "PSYC121/Week6.html#rstudio-tasks",
    "title": "6. Sampling, probability and binomial tests",
    "section": "RStudio tasks",
    "text": "RStudio tasks\nFor the second exercise today we will take a look at the phone data again. If you haven’t already, set up a new folder for Week 6 and upload the data from the Week 6 zip (see pre-lab instructions).\n\nRead in the csv data file\nYou should know how to do this by now. But if not, try searching “csv” at the top. Remember that what you name your data set is important for the following commands.\n\n\nTake a look at the data\nThere are lots of ways to get a quick look at the data. Here are a few useful ones (some you’ve come across, some that might be new): glimpse(), summary(), View(), head().\n\n\nCreate a boxplot of the phone data\nComplete the code to create a boxplot of the estimated phone use. Note that you can put the boxes on either the x or you y axis. Copy and paste the code and edit it so you can also plot the actual phone use.\n\n\nCreate a density plot and/or hisotgram of the phone data\nIt’s very easy to convert the boxplot code into either a density plot (geom_density()) or a histogram (geom_histogram()). Have a play around with these different types of graphs. Which one communicates the spread of the data most clearly? Is it better to plot these on the x or y axis?\n\n\nPlot the relationship between estimated and actual phone use\nSo far we’ve looked at the phone use estimate and the actual phone use separately. But if people are at all accurate in their estimates, we’d expect these two things to be related (those people who use their phone more probably know they do). Let’s use a scatter plot to see if this relationship exists in our data. In ggplot() we can do with with geom_point(). Each point on the graph needs an x and y value, so with the code you’ve been given, you just need to add in the two variables we want to plot.\n\nIs there a relationship between these variables?\nHow would you describe this relationship in words?\n\n\n\nBinomial test of the accuracy of phone use estimates\nSo did people overestimate or underestimate their phone use on average? We have given you a column called accuracy which simply says whether each participant underestimated or overestimated. Add your dataset name to this code to use the count() function to get the total who overestimated and underestimated. With these totals, you have enough information to run a binomial test:\n\nfor this test focus on the totals for “underestimate” and “overestimate” (for simplicity we can ignore people who were “accurate” and the NA values)\nThese give you your first two parameters for binom.test()\nto get the probability, we consider the null hypothesis\nthat is what’s the probability of overestimating or underestimating under the null hypothesis?\nwhat is the result of the binomial test?\nYou can also include the UK_region within your count, and then run binomial tests on these sub-groups. Do you find any interesting reuslts? What issues might we have with running the binom.test() on these sub groups?\n\n\n\nMore things to try with your scatter plot\nWe can customise our plot even further:\n\nTry adding/changing the size = within geom_point() to make the points bigger or smaller (values from .1 to 30)\nTry adding/changing the alpha = within geom_point() to make the points transparent (try values between 0.1 and 1)\ntry adding colour = within ggplot(aes( )) to map the colour to the ‘UK_region’ variable.\nyou can change the colours used by ggplot by adding + scale_colour_brewer() to your plot code. Within this, try setting the pallete parameter to one of these options (e.g., scale_colour_brewer(palette = \"Set3\"))\n\n\n\nremember you can add labels using +labs()\nremember you can set a new theme, such as + theme_minimal()"
  },
  {
    "objectID": "PSYC121/Week6.html#week-6-quiz",
    "href": "PSYC121/Week6.html#week-6-quiz",
    "title": "6. Sampling, probability and binomial tests",
    "section": "Week 6 Quiz",
    "text": "Week 6 Quiz\nYou can access a set of quiz questions related to this week here."
  },
  {
    "objectID": "PSYC121/Week4.html",
    "href": "PSYC121/Week4.html",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "",
    "text": "Complete materials from sessions in previous week (materials from week 1 /2 / 3 remain fully available for anyone who wants them). Consolidate what we have already covered.\nThis week - again, there’s a learnr tutorial to follow and help prep for what we are covering: You can find it here.\nMake sure you have access to the week4.zip file for the RStudio server.You can get the file here.\nIf you create a folder and upload the file into RStudio before the lab class you’ll be even more ready to follow along!"
  },
  {
    "objectID": "PSYC121/Week4.html#pre-lab-work",
    "href": "PSYC121/Week4.html#pre-lab-work",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "",
    "text": "Complete materials from sessions in previous week (materials from week 1 /2 / 3 remain fully available for anyone who wants them). Consolidate what we have already covered.\nThis week - again, there’s a learnr tutorial to follow and help prep for what we are covering: You can find it here.\nMake sure you have access to the week4.zip file for the RStudio server.You can get the file here.\nIf you create a folder and upload the file into RStudio before the lab class you’ll be even more ready to follow along!"
  },
  {
    "objectID": "PSYC121/Week4.html#r-studio-tasks",
    "href": "PSYC121/Week4.html#r-studio-tasks",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "R Studio tasks",
    "text": "R Studio tasks\nLast week we introduced two different ways to get descriptive information about a variable / column of scores investigated as a function of a separate piece of information. In others words, describe the DV a function of an IV\nStudents were generally very good at utilising each of these;\naggregate(x = DV, by = list(IV), FUN = mean)\nand\ntibble %&gt;% group_by(IV) %&gt;% summarise(mean_estimate = mean(DV))\n[tibble = technical name for the data in R]}\nThis week, we’re focusing on how you can edit or customise a graph to be more useful to a viewer."
  },
  {
    "objectID": "PSYC121/Week4.html#section-1---customisation-of-data-plots",
    "href": "PSYC121/Week4.html#section-1---customisation-of-data-plots",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "Section 1 - Customisation of data plots",
    "text": "Section 1 - Customisation of data plots\nStep 1. Set up a folder for this week in the R Project that you created last week.\nStep 2. Bring the week4.zip file into R Studio server. Like last week, upload the zip file, and load in the data file. Launch the week_4 R script as before.\n{If you’ve done Step 1 &2 already as a pre-lab preparation, super, pat yourself on the back, skip these steps an move on)}\nStep 3. Once again, we’re gong to be using commands from the tidyverse library (the pipe operator is one example) so we need to ensure that it’s active. Run the command\nlibrary(tidyverse)\nStep 4. Read in the datafiles that will be on the server. There’s already a script line for this, you just need to change the file name (see the comments for advice on this)\nStep 5. We’ve provided a suggestion of how you can complete the visualisation challenge task from week 3.\nStep 6. Customize you graph work. We’ve provided some suggestions about adding titles and labels for your graph. Edit and play with the script lines to make them useful to you and to understand how they work.\n\nTry change the text, the colours, and so on of the graphs.\nAdd comments for yourself about what the different commands do. The idea is to learn by trying different things out (changing values, taking out elements of the command, putting other is) and record for yourself.\nIf you are struggling or not sue, try look at help files."
  },
  {
    "objectID": "PSYC121/Week4.html#section-2-z-scores",
    "href": "PSYC121/Week4.html#section-2-z-scores",
    "title": "4. Customisation of graphs, and z-scores",
    "section": "Section 2: z-scores",
    "text": "Section 2: z-scores\n\nHint / Reminder: Sketch a normal (z score) distribution and mark the mean/mode, and mark off the relevant parts of the question so you know what you are trying to achieve and how to interpret any calculations you make.\n\n\nHint/ Reminder 2. For questions 6 & 7, remember that from the week 4 lecture material, typically in psychology we use the 5% level as a cutoff to decide, in broadly described terms, whether something is extreme or unlikely vs. at least somewhat plausible or likely.\n\n\nz-scores 1\nz-score distributions\nQ1. What is the relationship between the sign of a z-score and its position in a distribution?\nQ2. If a distribution has a mean of 100 and a standard deviation of 10, what is the raw score equivalent to a z-score of 1.96?\nQ3. If a distribution has a mean of 157 and a standard deviation of 19, what is the raw score equivalent to a z-score of 1?\n\n\nz-scores 2 Using z-score tables\nQ4. What proportion of scores lie between the mean and a z-score of 0.5?\nQ5. What is the combined proportion of scores lying between z=-1.2 and z=.85?\n\n\nz-scores 3 Applying z-scores to inferential problems\nQ6. A Neuropsychologist has presented a test of face recognition to 200 neurotypical participants and finds that the scores are normally distributed with a mean of 85 and the standard deviation of 12. Two brain-damaged patients are also given the test. The one with right hemisphere brain damage scored 58 and the one with left hemisphere damage scored 67.\n\nWhat is the z score of the right hemisphere patient when compared to the neurotypical group?\nWhat proportion of neurotypical participants score lower that this patient?\nIs this patient likely to belong to the population of neurotypical participants? (justify your answer)\nWhat is the z score of the left hemisphere patient when compared to the neurotypical group?\nWhat proportion of neurotypical participants score lower than this patient?\nIs this patient likely to belong to the population of neurotypical participants? (justify your answer)\n\n\n\nExtra activity\nCome back to this afterwards for some extra practice if you want:\nQ7. Tom Bunion has completed a huge research study and measured the foot size of men and women and found each to be normally distributed. The men have a mean size of 55 with a standard deviation of 5 and the women a mean of 33 and a standard deviation of 5. Joanna Toes has foolishly measured two individuals but forgotten to note their gender. These have foot sizes of 37 and 47. To which gender is each more likely to belong? What evidence is there for this?"
  },
  {
    "objectID": "PSYC121/Week5.html",
    "href": "PSYC121/Week5.html",
    "title": "5. Class test",
    "section": "",
    "text": "No materials!\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC401/Week8.html",
    "href": "PSYC401/Week8.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC401/Week9.html",
    "href": "PSYC401/Week9.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC401/index.html",
    "href": "PSYC401/index.html",
    "title": "Analysing and Interpreting Psychological Data I",
    "section": "",
    "text": "Welcome\nThis is filler text introducing students to the course in general. Perhaps what’s expected of them, the course aims, and other information of use.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam in est non nisi eleifend vulputate a et magna. Mauris vulputate felis lacus, ut bibendum lectus sollicitudin vel. Integer vestibulum arcu et risus vestibulum, in ullamcorper magna consectetur. Donec ut tempus enim, at vulputate libero. Integer porta metus eget elit cursus, sit amet hendrerit ipsum facilisis. Maecenas mollis, elit gravida ornare tempus, est lectus mattis velit, et commodo nunc lectus eu enim. Aenean luctus, felis ac sodales accumsan, dolor nunc euismod lorem, vel vulputate ipsum orci quis ipsum. Quisque placerat, velit vitae dictum feugiat, lectus lorem rutrum ligula, at fringilla ex enim consectetur felis. Aliquam erat volutpat. Nunc a nisi eget ex ornare dictum.\n\n\nCourse Contacts\n\n\n\n\nEmail Address\n\n\n\n\nTom Beesley\nt.beesley at lancaster dot ac dot uk\n\n\nJohn Towse\nj.towse at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC401/Week1.html",
    "href": "PSYC401/Week1.html",
    "title": "1. Introduction to R",
    "section": "",
    "text": "In your group, work through this workbook, note any problems and questions you have, and come prepared to the online practical class to go through the tasks and ask your questions.\nThe first part of this workbook reproduces what you saw in the week 1 part 3 lecture. The second part gives you some more exercises in using R studio for finding means, standard deviations, z scores, and drawing histograms. The third part explores some more tasks you can do to practise exploring what R studio can do."
  },
  {
    "objectID": "PSYC401/Week1.html#task-one-open-rstudio",
    "href": "PSYC401/Week1.html#task-one-open-rstudio",
    "title": "1. Introduction to R",
    "section": "Task One: Open Rstudio",
    "text": "Task One: Open Rstudio\nYou will have received an email with your account information to log onto From a computer on the campus wifi, you can access R Studio at:\nLancaster Psychology R Studio Server\nAt the login screen, use your university username (e.g., bloggsj)\nYour password for R Studio is: [password here]\nAs it says in the subject line, please keep your account details safe!\n\nWhat does RStudio look like?\nWhen RStudio starts, it will look something like this: \nRStudio has three panels or windows: there are tabs for Console (taking up the left hand side), Environment (and History top right), Current file (bottom right). You will also see a 4th window for a script or set of commands you develop, also (on the left hand side)."
  },
  {
    "objectID": "PSYC401/Week1.html#task-two-using-the-console",
    "href": "PSYC401/Week1.html#task-two-using-the-console",
    "title": "1. Introduction to R",
    "section": "Task Two: using the console",
    "text": "Task Two: using the console\n\n\n\n\n\n\nTip\n\n\n\nText that is highlighted with a grey background denotes code, rather than typiucal prose. Code is different to other forms of writing, such as essays, because the syntax, order and words need to be quite specific. For some longer chunks of code, as you will see below, they are formatted slightly differently.\n\n\n\nIn the “console” part of the R window, next to the &gt;, type 10 + 30. Press return.\n\n\n10 + 40                        \n\n\n\n\n\n\n\nTip\n\n\n\nIf you hover your mouse over the box that includes the code snippet, a ‘copy to clipboard’ icon will appear in the top right corner of the box. Click that to copy the code. Now you can easily paste it into your script.\n\n\nIt should give you the answer 40.\n\nIn the console, type a &lt;- 40 and press Return.\n\n\na &lt;- 40                      \n\nNow type a and press return. It should give you the answer 40. a is called an object, think of it like a bucket that you can keep a number, or some numbers, or actually all kinds of stuff in.\n\nNow let’s look at a function, sqrt. sqrt is a function that takes the square root of whatever is inside the brackets. In the console, type sqrt(13). Press Return.\nNow find the square root of the object a by typing sqrt(a). Press return."
  },
  {
    "objectID": "PSYC401/Week1.html#task-three-finding-distributions",
    "href": "PSYC401/Week1.html#task-three-finding-distributions",
    "title": "1. Introduction to R",
    "section": "Task Three: finding distributions",
    "text": "Task Three: finding distributions\n\nMake a new object b, and put the following list of children’s attachment scores into it\n\n\nb &lt;- c( 4, 1, 5, 3, 8, 2, 2, 6, 8, 5, 4, 1, 6, 5, 4, 5, 7, 9, 10, 1, 1, 3, 5, 4, 6, 4, 8, 6, 5, 5, 7, 8, 9, 8, 8, 2, 1, 4, 3, 2, 5, 1, 5, 6, 8, 6, 7, 2, 7)\n\n\nCheck it works by typing b, press return.\nFind the mean of these numbers by typing mean(b).\nFind the median of these numbers by typing median(b).\nFind the standard deviation of these numbers by typing sd(b).\nDraw a histogram of these numbers by typing hist(b)."
  },
  {
    "objectID": "PSYC401/Week1.html#task-four-z-scores",
    "href": "PSYC401/Week1.html#task-four-z-scores",
    "title": "1. Introduction to R",
    "section": "Task Four: z scores",
    "text": "Task Four: z scores\n\nMake a new object b_z and assign to it the z scores of the values from b: ``\n\n\nb_z &lt;- scale(b)\n\n\nCheck that it worked by typing b_z.\nDraw a histogram of b_z by typing hist(b_z)."
  },
  {
    "objectID": "PSYC401/Week1.html#task-five-investigating-distributions",
    "href": "PSYC401/Week1.html#task-five-investigating-distributions",
    "title": "1. Introduction to R",
    "section": "Task Five: investigating distributions",
    "text": "Task Five: investigating distributions\n\nLet’s make three new objects, with the marks from three people’s university masters courses. They are called annie, saj, and carrie and they took 10 courses each. We use the special notation c() to indicate a list, each number in the list is separated by a comma. Type the following into the console:\n\n\nannie &lt;- c(55, 95, 85, 65, 65, 85, 65, 95, 65, 75)\nsaj &lt;- c(65, 85, 95, 75, 65, 55, 55, 75, 95, 85)\ncarrie &lt;- c(75, 65, 95, 95, 55, 85, 75, 55, 95, 55)\n\n\nWho has the highest average (mean) score for their course?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nuse the mean() function\n\n\n\n\nWho has the most variable scores for their course?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nuse the sd() function\n\n\n\n\nWhat is the median score for each student?. What does this mean about the distribution of each students’ scores? Use the function hist() to draw the distributions to help you see.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nyou can use the summary() function, or the median() function"
  },
  {
    "objectID": "PSYC401/Week1.html#task-six-standardised-scores-z-scores",
    "href": "PSYC401/Week1.html#task-six-standardised-scores-z-scores",
    "title": "1. Introduction to R",
    "section": "Task Six: standardised scores: Z scores",
    "text": "Task Six: standardised scores: Z scores\n\nMake a new object called annie_z and use the function scale to convert annie’s scores to z-scores: in the console type:\n\n\nannie_z &lt;- scale(annie)\n\n\nYou can have a look at the standardised scores of annie, by just typing annie_z. To what did annie’s highest initial score of 95 convert to?\nWhat is the mean and standard deviation of annie_z’s standardised scores?\nDraw a histogram of annie’s standardised scores, in the console type hist(annie_z). What is the peak frequency value?\nBonus extra: If you want to find out the proportion of scores lower than a particular score you can do it like this in R-studio: pnorm(x) where x is the z-score you’re interested in. What is the proportion of scores lower than annie’s highest grade score?"
  },
  {
    "objectID": "PSYC401/Week1.html#task-seven-exploring-operators.",
    "href": "PSYC401/Week1.html#task-seven-exploring-operators.",
    "title": "1. Introduction to R",
    "section": "Task Seven: Exploring operators.",
    "text": "Task Seven: Exploring operators.\nSo far, we’ve just looked at + as an operator. Go to this page: https://www.statmethods.net/management/operators.html\n\nIn the console, assign the object d to be 100 multiplied by 246.\nIn the console, assign the object e to be 84 divided by 32.1.\nAssign the variable f to 8 to the power of 4 (in R this is called exponentiation).\nWhat is the result of d added to e all divided by f\n\nTask 9: Exploring functions\nSo far, we’ve just looked at the square root function sqrt(). Go to this page: https://www.statmethods.net/management/functions.html 27. What is the result of abs(-5.3)? What does the abs function do?\n\nUsing the seq() function, generate a sequence of numbers from 0 to 30 in intervals of 3.\nAssign the sequence generated in step 28 to a new object. Now compute the mean of the sequence of numbers. (remember that objects can be a single number, or a sequence of numbers (called an array or a vector) or anything you want to put into it – remember, think of objects as buckets).\n\n\n\n\n\n\n\nStuck? Here’s the solution\n\n\n\n\n\nTry out the following code, pay special attention to how the sentences above “convert” into R code.\n\nsequence &lt;- seq(0,30, 3)\nmean(sequence)"
  },
  {
    "objectID": "PSYC401/Week2.html",
    "href": "PSYC401/Week2.html",
    "title": "2. Data Manipulation",
    "section": "",
    "text": "Open up the R server at psy-rstudio.lancaster.ac.uk\nIn R studio, at the File menu, select New File, then R script. Now, we can put in our favourite sum to check it’s working. At the top of the R script window type 10.5 + 7. With the cursor on the same line as the sum click on the Run button (or press Ctrl+Enter on Windows, Cmd+Enter on Mac). In the console, you should see the sum being run, and the answer produced.\nSave the R source. Click on the Save icon, call the R script “psyc401_week2.r”. The .r subscript indicates to R studio that this is an R script file.\nClose the R script, by clicking on the little x next to the R script filename just above the R script window.\nNow open it again. In the R studio window File menu, select Open File, and browse to where you saved your R script file and open it.\nTo make it easier to save and open files we can set what is called the “working directory” for R studio. Click “Session” in the menu at the top of the R studio screen, select “Working directory”, then select “Choose directory”. Browse to the Folder where you are going to save your PSYC401 R studio files (for me this is Documents/PSYC401), and click Open. Over on the right lower panel, you should now see all the files that are in this Folder - including psyc401_week2.r\n\n\n\n\n\nDownload the answers file by right-clicking the link and saving it. psyc401_week1_workbook_answers.r\nOpen psyc401_week1_workbook_answers.r in R studio (File &gt; Open File). You can now look through this document, check your commands and answers to the tasks from last week.\nClose the file psyc401_week1_workbook_answers.r\n\n\n\n\n\nReopen your script file psyc401_week2.r in R studio.\nIt’s always a good idea to refresh and clear out R studio when you start a new session, so add this line to the beginning of your R studio file, rm(list=ls())\nNow we are going to open a new data file. Download the file “PSYC401-shipley-scores-anonymous-17_18.csv” from moodle PSYC401 site, from the Practical Week2 folder. Note: DO NOT OPEN THIS FILE IN EXCEL – IF YOU DO, DELETE IT THEN DOWNLOAD IT AGAIN. Each row is one person’s data, and each column is a measure taken from the person. Columns are separated by commas, which is what the “csv” refers to” comma-separated values.\nIn R studio, we open csv files using a function called read_csv() which comes from a set of functions called “tidyverse”, we can install these functions by putting library(tidyverse) at the very top of our R file. Type this command in the script file and then run it:\n\n\n\n\n\n\n\nWant to know more about library()?\n\n\n\n\n\nR comes with certain functions pre-installed, such as mean(), but part of the charm of R is that we can install different functions that give us the opportunity to do almost anything! Collections of functions are called packages, and collections of packages are called libraries, but for the purpose of practicality, we typically refer to them interchangably. We install libraries using library() and enter the name of the library in the brackets.\n\n\n\n\nlibrary(tidyverse)\n\ndat &lt;- read_csv(\"PSYC401-shipley-scores-anonymous-17_18.csv\")\n\nRemember the dat &lt;- notation, which means we put the data into an object called “dat”\n\nIf all went well, we can then look at the data, using the function View: In the script file type View(dat) and run it. It should open a spreadsheet where we can see the data.\nNext, we just focus on two variables from the data: subject_ID which is the participants’ anonymised number, and Gent_1_score, which is the participants’ score on the Gent vocabulary test, the first time they had a go (that’s what the 1 stands for). We do this using the command select. The command select is in the library “tidyverse”. In the script file type and run:\n\n\nsummarydat &lt;- select(.data = dat, subject_ID, Gent_1_score)\n\n\nFinally, let’s just have a quick look at these data. In the script file type hist(summarydat$Gent_1_score) and run it.\nThis will draw a histogram of the Gent vocabulary scores. summarydat$Gent_1_score means that we look at the Gent_1_score values from the summarydat data – the $ indicates that this is one of the measures from the data. What kind of pattern does the histogram show?"
  },
  {
    "objectID": "PSYC401/Week2.html#task-zero-making-and-opening-an-r-script-file",
    "href": "PSYC401/Week2.html#task-zero-making-and-opening-an-r-script-file",
    "title": "2. Data Manipulation",
    "section": "",
    "text": "Open up the R server at psy-rstudio.lancaster.ac.uk\nIn R studio, at the File menu, select New File, then R script. Now, we can put in our favourite sum to check it’s working. At the top of the R script window type 10.5 + 7. With the cursor on the same line as the sum click on the Run button (or press Ctrl+Enter on Windows, Cmd+Enter on Mac). In the console, you should see the sum being run, and the answer produced.\nSave the R source. Click on the Save icon, call the R script “psyc401_week2.r”. The .r subscript indicates to R studio that this is an R script file.\nClose the R script, by clicking on the little x next to the R script filename just above the R script window.\nNow open it again. In the R studio window File menu, select Open File, and browse to where you saved your R script file and open it.\nTo make it easier to save and open files we can set what is called the “working directory” for R studio. Click “Session” in the menu at the top of the R studio screen, select “Working directory”, then select “Choose directory”. Browse to the Folder where you are going to save your PSYC401 R studio files (for me this is Documents/PSYC401), and click Open. Over on the right lower panel, you should now see all the files that are in this Folder - including psyc401_week2.r"
  },
  {
    "objectID": "PSYC401/Week2.html#task-one-open-and-check-practical-week1-workbook-answers-script-file",
    "href": "PSYC401/Week2.html#task-one-open-and-check-practical-week1-workbook-answers-script-file",
    "title": "2. Data Manipulation",
    "section": "",
    "text": "Download the answers file by right-clicking the link and saving it. psyc401_week1_workbook_answers.r\nOpen psyc401_week1_workbook_answers.r in R studio (File &gt; Open File). You can now look through this document, check your commands and answers to the tasks from last week.\nClose the file psyc401_week1_workbook_answers.r"
  },
  {
    "objectID": "PSYC401/Week2.html#task-two-opening-a-data-file",
    "href": "PSYC401/Week2.html#task-two-opening-a-data-file",
    "title": "2. Data Manipulation",
    "section": "",
    "text": "Reopen your script file psyc401_week2.r in R studio.\nIt’s always a good idea to refresh and clear out R studio when you start a new session, so add this line to the beginning of your R studio file, rm(list=ls())\nNow we are going to open a new data file. Download the file “PSYC401-shipley-scores-anonymous-17_18.csv” from moodle PSYC401 site, from the Practical Week2 folder. Note: DO NOT OPEN THIS FILE IN EXCEL – IF YOU DO, DELETE IT THEN DOWNLOAD IT AGAIN. Each row is one person’s data, and each column is a measure taken from the person. Columns are separated by commas, which is what the “csv” refers to” comma-separated values.\nIn R studio, we open csv files using a function called read_csv() which comes from a set of functions called “tidyverse”, we can install these functions by putting library(tidyverse) at the very top of our R file. Type this command in the script file and then run it:\n\n\n\n\n\n\n\nWant to know more about library()?\n\n\n\n\n\nR comes with certain functions pre-installed, such as mean(), but part of the charm of R is that we can install different functions that give us the opportunity to do almost anything! Collections of functions are called packages, and collections of packages are called libraries, but for the purpose of practicality, we typically refer to them interchangably. We install libraries using library() and enter the name of the library in the brackets.\n\n\n\n\nlibrary(tidyverse)\n\ndat &lt;- read_csv(\"PSYC401-shipley-scores-anonymous-17_18.csv\")\n\nRemember the dat &lt;- notation, which means we put the data into an object called “dat”\n\nIf all went well, we can then look at the data, using the function View: In the script file type View(dat) and run it. It should open a spreadsheet where we can see the data.\nNext, we just focus on two variables from the data: subject_ID which is the participants’ anonymised number, and Gent_1_score, which is the participants’ score on the Gent vocabulary test, the first time they had a go (that’s what the 1 stands for). We do this using the command select. The command select is in the library “tidyverse”. In the script file type and run:\n\n\nsummarydat &lt;- select(.data = dat, subject_ID, Gent_1_score)\n\n\nFinally, let’s just have a quick look at these data. In the script file type hist(summarydat$Gent_1_score) and run it.\nThis will draw a histogram of the Gent vocabulary scores. summarydat$Gent_1_score means that we look at the Gent_1_score values from the summarydat data – the $ indicates that this is one of the measures from the data. What kind of pattern does the histogram show?"
  },
  {
    "objectID": "PSYC401/Week2.html#task-four-examining-and-manipulating-data",
    "href": "PSYC401/Week2.html#task-four-examining-and-manipulating-data",
    "title": "2. Data Manipulation",
    "section": "Task Four: Examining and manipulating data",
    "text": "Task Four: Examining and manipulating data\n\nLet’s have a look at the data now. Type View(dat) in the source and Run it in the console, and you should see the data appear above the console window. Have a good long hard look at it.\nThe data shows id which is the participant number, occasion which is whether this is the first (0), second (1), up to sixth (5) time they filled in the questionnaires, intervention is which intervention they took part in with respect to attempting to promote their mood, ahi01-ahi24 are the 24 items on the AHI happiness scale, cesd01-cesd20 are the 20 items on the CESD depression scale. Way over on the right are the total scores on the AHI and the CESD questionnaires.\nNow, view the pinfo data. How can you look at it?\nLooking at the data replaced the source window, but the source is still there. Just above the View panel you should see a tab named “psyc401_week2.r”, click on that to get your source panel back. It will have a star/asterisk after the file name if it is unsaved. Remember it’s a good idea to regularly save your source file so you don’t lose work.\nNow, we are going to join together the two files. Type this:\n\n\nall_dat &lt;- inner_join(x = dat, y = pinfo, by = c('id', 'intervention'))\n\n\nQuestion: what does the c(“id”, “intervention”) bit mean?\n\n::: {.callout-warning icon=false collapse=“true”} ## Answer this means we match by two variables – id and intervention. We use the c() notation to indicate that this is a list of things.\nWe’ve now made a new data set called “all_dat”. The “x = dat” bit is the name of the first datafile we want to join, the “y = pinfo” is the name of the second datafile we want to join, the “by = ‘id’, ‘intervention’” bit is the names of variables that the two datasets have in common. :::\n\nHow would you join two data sets one called “datamad” the other called “datasane” together if they both have the variable “participantname” in common?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndata_full &lt;- inner_join(x = datamad, y = datasane, by = c(\"participantname\"))\n\n\n\n\n\nNow we just want to keep a few of the variables – we’re not interested in the individual questionnaire items. So, let’s select the variables we want to keep:\n\n\nsummarydata &lt;- select(.data = all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, intervention)\n\nWhere “all_dat” is the name of the object to take data from, and “ahiTotal, cesdTotal, sex, age, educ, income, occasion, intervention” are all the variables we want to keep.\n\nHave a look at the summarydata in the View. How do you do that?"
  },
  {
    "objectID": "PSYC401/Week2.html#task-five-investigate-data",
    "href": "PSYC401/Week2.html#task-five-investigate-data",
    "title": "2. Data Manipulation",
    "section": "Task Five: Investigate data",
    "text": "Task Five: Investigate data\n\nThe next task is to have a closer look at the distributions of the data. Let’s focus on the age of participants. To investigate one column of data from a dataset, you have to refer to it using the “$” symbol. So, to investigate the “age” column from the “summarydata” dataset, you would look at summarydata$age. Draw a histogram (bar graph) of the distribution of age in the participant sample.\nNow, let’s look at how the AHI and the CESD scores relate. To gain an impression of how two variables relate we can draw a scatter graph. In the console, type plot(summarydata$ahiTotal, summarydata$cesdTotal), press Return. What does the “$” do in this command? What relationship do you find between these two variables?\nNow make sure you save psyc401_week2.r that contains all these commands that you ran. Close Rstudio, and Open Rstudio and make sure its saved all your work. The list of commands is extremely useful for making science open and accessible to other researchers. It’s more and more common for psychology articles to make the R source files available so other researchers can reproduce the data manipulations and analyses used in the paper precisely."
  },
  {
    "objectID": "PSYC401/Week2.html#task-six-wide-to-long-format-conversion",
    "href": "PSYC401/Week2.html#task-six-wide-to-long-format-conversion",
    "title": "2. Data Manipulation",
    "section": "Task Six: Wide to long format conversion",
    "text": "Task Six: Wide to long format conversion\n\nLet’s go back to the psyc401-shipley-scores-anonymous-17_18.csv data. This should still be in the object called “dat”. If it’s not, then load the data again into dat, using the read_csv() function.\nMake sure the library(tidyverse) is loaded, if not, run the command library(tidyverse).\nThe aim here is to convert the data so that each Gent vocabulary score is on a separate line, we use the pivot_longer function for this. First we specify what the new object should be (datlong), then we say where the old data is (dat), then we make a new variable to keep the names of the tests (names_to = “test”), then we make a new variable to keep the scores from the tests (values_to = “vocab”), then we specify the list of old variables to combine into the new scores variable (c(“Gent_1_score”, “Gent_2_score”) ) – remember lists are written as c(). So, run this command:\n\n\ndatlong &lt;- pivot_longer(dat, names_to = \"test\", values_to = \"vocab\", cols = c(\"Gent_1_score\", \"Gent_2_score\")) \n\n\nHave a look at the new object datlong that results: View(datlong). This function pivot_longer has taken as input the data in dat, it has created a new variable called “test” which reports whether it is the Gent_1_score or the Gent_2_score that is the measurement, and a new variable called “vocab” where the actual scores are listed. Then, the following list of variables let’s the function pivot_longer know which variables from the object dat we are converting (or lengthening). It also includes all the other variables, but unconverted. How many rows of data are there now corresponding to data from subject_ID number 1?\nLet’s tidy things up so we only have subject_ID, and the Gent vocabulary scores by using select:\n\n\ndatlongsummary &lt;- select(datlong, subject_ID, test, vocab)"
  },
  {
    "objectID": "PSYC401/Week2.html#task-seven-long-to-wide-format-conversion",
    "href": "PSYC401/Week2.html#task-seven-long-to-wide-format-conversion",
    "title": "2. Data Manipulation",
    "section": "Task Seven: Long to wide format conversion",
    "text": "Task Seven: Long to wide format conversion\n\nNow, we will have a go at converting from long to wide format. Let’s start with the datlongsummary object. We will convert this so that Gent_1_score and Gent_2_score are listed alongside each other – one row per person. The command for this is the reverse of pivot_longer, called pivot_wider. Run this command: datwide &lt;- pivot_wider(datlongsummary, names_from = \"test\", values_from = \"vocab\"). This command takes the data from datlongsummary and puts the different measures reported in the variable test into different columns again, filling in the values from the variable vocab."
  },
  {
    "objectID": "PSYC401/Week3.html",
    "href": "PSYC401/Week3.html",
    "title": "3. Workbook 3",
    "section": "",
    "text": "Your take-home task was to download a data set that accompanied a paper published in Psychological Science. Describe this data set to the rest of your group.\nLoad your data set into Rstudio.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nuse read_csv(), or one of the other read functions from the last part of the Practical week2 workbook. You may need to load library(tidyverse).\n\n\n\n\nView the data set, and then make a new data set from this data set, by selecting just two variables.\nIs it appropriate to draw a histogram or a scatter plot of the two variables? If so, draw it. If not, why not?\nMake sure these commands are in the source window, save them as a new R file, e.g., “mypsychscidata.r”\n\nPart 2: Reproduce the Lecture week3 part3 analyses\n\n\n\n\nCreate a new r script, called psyc401_week3.r, and clear out R studio ready for a new script using rm(list=ls()).\nDownload the data files on the vocabulary tests here: PSYC401-shipley-scores-anonymous-17_18.zip. You should then upload the entire zip folder to the R server.\nLoad the data into an object called “dat” using read_csv(), what command line do you use? (remember to set the working directory)\nView the data. What command do you use?\nWe can make a histogram of the first time people took the Gent vocabulary test:\n\n\nhist(dat$Gent_1_score)\n\n\nAnd a histogram of the second time people took the Gent test, what command line do you use?\nWe can find out means and standard deviations. We will use the mean() and the sd() functions. However, we need to tell R studio what to do with the missing values (called NA in the View), to do that we have to add an extra bit to the command:\n\n\nmean(dat$Gent_1_score, na.rm=TRUE)\n\nThis tells R studio to remove the NA values before computing the mean. What happens if you don’t add na.rm=TRUE?\nWhat is the mean and standard deviation of Gent_1_score and Gent_2_score?\n\n\n\n\nNow we are going to use another way of making graphs. This is more flexible than the hist function. Here is how to make a histogram of the Gent vocabulary scores:\n\n\nggplot(dat, aes(x = Gent_1_score) ) + geom_histogram(fill=\"blue\") + labs(title=\"Gent Vocabulary Test 1\", x = \"Vocabulary Score\", y = \"Frequency\")\n\n\nAnd for the second Gent test:\n\n\nggplot(dat, aes(x = Gent_2_score) ) + geom_histogram(fill=\"red\") + labs(title=\"Gent Vocabulary Test 2\", x = \"Vocabulary Score\", y = \"Frequency\")\n\n\n\n\n\n\n\nNote\n\n\n\nBreaking it down: ggplot(dat, aes(x = Gent_1_score)): this calls the plotting function ggplot we specify the data set we will use, dat and we set the data for the plot, in this case we say that the x value (so that’s what will be along the x-axis in the graph) is the Gent_1_score. We put this inside aes(), which stands for “aesthetic”. + geom_histogram(fill=\"blue\"): this adds a graph of type histogram and colours it blue + labs(title=\"Gent Vocabulary Test\", x = \"Vocabulary Score\", y = \"Frequency\"): this adds labels to the graph: title, the x-axis label and the y-axis label."
  },
  {
    "objectID": "PSYC401/Week3.html#task-1-describe-and-load-the-data-set-you-found-for-your-take-home-task",
    "href": "PSYC401/Week3.html#task-1-describe-and-load-the-data-set-you-found-for-your-take-home-task",
    "title": "3. Workbook 3",
    "section": "",
    "text": "Your take-home task was to download a data set that accompanied a paper published in Psychological Science. Describe this data set to the rest of your group.\nLoad your data set into Rstudio.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nuse read_csv(), or one of the other read functions from the last part of the Practical week2 workbook. You may need to load library(tidyverse).\n\n\n\n\nView the data set, and then make a new data set from this data set, by selecting just two variables.\nIs it appropriate to draw a histogram or a scatter plot of the two variables? If so, draw it. If not, why not?\nMake sure these commands are in the source window, save them as a new R file, e.g., “mypsychscidata.r”\n\nPart 2: Reproduce the Lecture week3 part3 analyses"
  },
  {
    "objectID": "PSYC401/Week3.html#task-2-load-in-the-data-draw-a-histogram-find-means-and-standard-deviations",
    "href": "PSYC401/Week3.html#task-2-load-in-the-data-draw-a-histogram-find-means-and-standard-deviations",
    "title": "3. Workbook 3",
    "section": "",
    "text": "Create a new r script, called psyc401_week3.r, and clear out R studio ready for a new script using rm(list=ls()).\nDownload the data files on the vocabulary tests here: PSYC401-shipley-scores-anonymous-17_18.zip. You should then upload the entire zip folder to the R server.\nLoad the data into an object called “dat” using read_csv(), what command line do you use? (remember to set the working directory)\nView the data. What command do you use?\nWe can make a histogram of the first time people took the Gent vocabulary test:\n\n\nhist(dat$Gent_1_score)\n\n\nAnd a histogram of the second time people took the Gent test, what command line do you use?\nWe can find out means and standard deviations. We will use the mean() and the sd() functions. However, we need to tell R studio what to do with the missing values (called NA in the View), to do that we have to add an extra bit to the command:\n\n\nmean(dat$Gent_1_score, na.rm=TRUE)\n\nThis tells R studio to remove the NA values before computing the mean. What happens if you don’t add na.rm=TRUE?\nWhat is the mean and standard deviation of Gent_1_score and Gent_2_score?"
  },
  {
    "objectID": "PSYC401/Week3.html#task-3-use-ggplot-to-draw-some-histograms",
    "href": "PSYC401/Week3.html#task-3-use-ggplot-to-draw-some-histograms",
    "title": "3. Workbook 3",
    "section": "",
    "text": "Now we are going to use another way of making graphs. This is more flexible than the hist function. Here is how to make a histogram of the Gent vocabulary scores:\n\n\nggplot(dat, aes(x = Gent_1_score) ) + geom_histogram(fill=\"blue\") + labs(title=\"Gent Vocabulary Test 1\", x = \"Vocabulary Score\", y = \"Frequency\")\n\n\nAnd for the second Gent test:\n\n\nggplot(dat, aes(x = Gent_2_score) ) + geom_histogram(fill=\"red\") + labs(title=\"Gent Vocabulary Test 2\", x = \"Vocabulary Score\", y = \"Frequency\")\n\n\n\n\n\n\n\nNote\n\n\n\nBreaking it down: ggplot(dat, aes(x = Gent_1_score)): this calls the plotting function ggplot we specify the data set we will use, dat and we set the data for the plot, in this case we say that the x value (so that’s what will be along the x-axis in the graph) is the Gent_1_score. We put this inside aes(), which stands for “aesthetic”. + geom_histogram(fill=\"blue\"): this adds a graph of type histogram and colours it blue + labs(title=\"Gent Vocabulary Test\", x = \"Vocabulary Score\", y = \"Frequency\"): this adds labels to the graph: title, the x-axis label and the y-axis label."
  },
  {
    "objectID": "PSYC401/Week3.html#task-4-practise-manipulating-data",
    "href": "PSYC401/Week3.html#task-4-practise-manipulating-data",
    "title": "3. Workbook 3",
    "section": "Task 4: Practise manipulating data",
    "text": "Task 4: Practise manipulating data\n\nLet’s keep only some of the variables from the dataset dat - let’s remove Gender_code, and Dyslexia_diagnosis. Keep the other variables using select() and load this into summarydata\n\n\nsummarydata &lt;- select(.data = dat, subject_ID, Age, english_status, Gender, Shipley_Voc_Score, Gent_1_score, Gent_2_score, academic_year)\n\n\nNext we will have a bit more of a wander around the data to get a feel for it. We will first use the function arrange(), which changes the order of observations (rows):\n\n\narrange(.data = summarydata, Shipley_Voc_Score)\n\nWhat is the lowest score of a participant on the Shipley Vocabulary questionnaire? (You may like to make a new object, which is the result of the arrange function, then look at it in View).\n\nIf you want to order from highest to lowest, you have to use the desc() function:\n\n\narrange(.data = summarydata, desc(Shipley_Voc_Score))\n\nWhat is the highest value on the Shipley Vocabulary Test? How many participants have this highest score?\n\nNext we will use the filter() function. This includes or excludes certain observations (rows). Let’s just include the participants with English as a first language and put this into a new object, called summarydata_enl. What are the mean and SD values of the Shipley Vocabulary test for the native speakers?\n\n\nsummarydata_enl &lt;- filter(.data = summarydata, english_status == 'native')\n\n\nMake another variable with the z-scores of the Shipley Vocabulary test (see week 1 workbook). What are the maximum and minimum z-scores?\nRemember to save your script file."
  },
  {
    "objectID": "PSYC401/Week3.html#task-5-graphing-data-using-histograms",
    "href": "PSYC401/Week3.html#task-5-graphing-data-using-histograms",
    "title": "3. Workbook 3",
    "section": "Task 5: graphing data using histograms",
    "text": "Task 5: graphing data using histograms\n\nPreviously we used plot to draw a scatter plot, and hist to draw a histogram. Now, we’re going to use ggplot which can draw all kinds of graphs, with a great deal more flexibility. We are going to represent the data to reflect the following relations:\n\n\nEnglish status and gender\nAge and vocabulary score\nGender and vocabulary score\nAcademic year and vocabulary score\nAcademic year and age\nEnglish status and vocabulary score\nEnglish status and age\n\nBut first, let’s repeat reproducing the histogram from the overhead slides to look at the distribution of variables:\n\nggplot(summarydata, aes(x = Gent_1_score)) +\n  geom_histogram(fill=\"blue\") + \n  labs(title=\"Gent Vocabulary Test 1\", x = \"Vocabulary Score\", y = \"Frequency\")\n\n\nNow draw a histogram with Shipley_Voc_Score as the variable and colour it orange. Remember to change the title to something appropriate."
  },
  {
    "objectID": "PSYC401/Week3.html#task-6-graphing-data-using-bar-graphs",
    "href": "PSYC401/Week3.html#task-6-graphing-data-using-bar-graphs",
    "title": "3. Workbook 3",
    "section": "Task 6: graphing data using bar graphs",
    "text": "Task 6: graphing data using bar graphs\n\nNext let’s look at English status and gender. What types of variable are these? Nominal? Ordinal? Interval/ratio?\nWe will draw a bar graph of the counts. We use geom_bar() for this:\n\n\nFirst try this:\n\n\nggplot(summarydata, aes(x = Gender)) + \n  geom_bar()\n\n\nThis just draws counts of Gender\nNow let’s draw Gender and English Status together:\n\n\nggplot(summarydata, aes(x = Gender, fill = english_status)) + \n  geom_bar(position = \"dodge\")\n\nNote 1: We use position dodge so that it puts the bars next to each other (what happens if you leave out position = dodge?)\nNote 2: We use fill = english_status so that it fills the different bars with different colours according to different english statuses.\nWhat is the general pattern of counts? Are there proportional differences by English status according to gender?"
  },
  {
    "objectID": "PSYC401/Week3.html#task-7-graphing-data-using-scatterplot",
    "href": "PSYC401/Week3.html#task-7-graphing-data-using-scatterplot",
    "title": "3. Workbook 3",
    "section": "Task 7: graphing data using scatterplot",
    "text": "Task 7: graphing data using scatterplot\n\nNext we’ll look at Age and Shipley Vocabulary Score. What types of data are these?\nWe will draw a point plot of these values:\n\n\nggplot(summarydata, aes(x= Age, y = Shipley_Voc_Score)) + geom_point()\n\nWe can add + labs(title = \"Age by Shipley Vocabulary Score\", x = \"Age\", y = \"Shipley Vocabulary Score\") to tidy up presentation a bit.\n\nggplot(summarydata, aes(x= Age, y = Shipley_Voc_Score)) + \n  geom_point() + \n  labs(title = \"Age by Shipley Vocabulary Score\", x = \"Age\", y = \"Shipley Vocabulary Score\")\n\n\nWhat is the relation between age and Shipley Vocabulary score?"
  },
  {
    "objectID": "PSYC401/Week3.html#task-8-draw-and-interpret-a-box-plot",
    "href": "PSYC401/Week3.html#task-8-draw-and-interpret-a-box-plot",
    "title": "3. Workbook 3",
    "section": "Task 8: Draw and interpret a box plot",
    "text": "Task 8: Draw and interpret a box plot\n\nNext on the list of relations to check is gender and vocabulary score. Let’s look at Gent_1_score against Gender. What type of variables are these?\nWe will draw a box plot (you could draw a bar graph, but box plots tend to be preferred for these combinations of variables - use a bar graph for counts):\n\n\nggplot(summarydata, aes(x= Gender, y = Gent_1_score)) + \n  geom_boxplot() \n\n\nAgain we can tidy this up by adding labels:\n\n\nggplot(summarydata, aes(x= Gender, y = Gent_1_score)) + geom_boxplot() + labs(title = \"Vocabulary Score by Gender\", x = \"Gender\", y = \"Gent Vocabulary Score Test 1\")\n\n\nInterpreting box plots: The horizontal line indicates the median. The box indicates where 50% of the data lie. The lines indicate an estimate of the range of the data (minimum and maximum values). The dots indicate outliers. A large box indicates larger standard deviation. If the boxes don’t overlap much then this indicates there may be a difference between the groups.\n\n\nAre there differences in Vocabulary according to gender?\n\n\nNow for the other relations:\n\n\nAcademic year and vocabulary score\nAcademic year and age\nEnglish status and vocabulary score\nEnglish status and age\n\nAt the moment, R is interpreting Academic year as a number. We need to turn it into a nominal variable (called a “factor” in R studio):\n\nsummarydata$academic_year &lt;- as.factor(summarydata$academic_year)\n\nDraw a graph for each of these relations.\n\nSave your R script."
  },
  {
    "objectID": "PSYC401/Week7.html",
    "href": "PSYC401/Week7.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC401/Week6.html",
    "href": "PSYC401/Week6.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC401/Week4.html",
    "href": "PSYC401/Week4.html",
    "title": "4. Worksheet 4",
    "section": "",
    "text": "The materials in this workbook share some material with Glasgow University Psychology Department Teaching in R website"
  },
  {
    "objectID": "PSYC401/Week4.html#task-1-your-data-from-the-paper-in-psychological-science",
    "href": "PSYC401/Week4.html#task-1-your-data-from-the-paper-in-psychological-science",
    "title": "4. Worksheet 4",
    "section": "Task 1: Your data from the paper in Psychological Science",
    "text": "Task 1: Your data from the paper in Psychological Science\n\nYour take-home task was to produce some graphs of the data set downloaded from a paper in Psychological Science. Show your graphs and R script to the rest of your group."
  },
  {
    "objectID": "PSYC401/Week4.html#task-2-load-in-the-data",
    "href": "PSYC401/Week4.html#task-2-load-in-the-data",
    "title": "4. Worksheet 4",
    "section": "Task 2: Load in the Data",
    "text": "Task 2: Load in the Data\n\nRemember to clear out R first:\n\n\nrm(list=ls())\n\nThe data set on the Shipley and Gent vocabulary scores is now updated with the data from your group, so it now contains five years of PSYC401 students’ data. I’ve omitted Age as this might impact anonymity of the data. Download the data from the week 4 moodle folder: “PSYC401-shipley-scores-anonymous-17_22.csv” and read the data into an object in R studio called vdat (for vocabulary data).\n\nAs a reminder, when we want to look at a particular variable (a column) in an object in R studio, we refer to it using the $ notation. So, for the object vdat and the variable academic_year you would refer to it as vdat$academic_year. For this data set, we need to change academic year to be a nominal (factor) variable. Why does academic year have to be nominal and not interval/ratio?:\n\n\nvdat$academic_year &lt;- as.factor(vdat$academic_year)\n\nview(vdat) #view the data\n\n\nMake sure the tidyverse library is loaded. Select all the variables apart from Dyslexia_diagnosis and Age and save as a new object called “summaryvdat”. We will omit these variables because they are not complete for the dataset.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nlibrary(tidyverse)\n\nsummaryvdat &lt;- select(vdat, -c(Dyslexia_diagnosis, Age))\n\n-c(Dyslexia_diagnosis, Age) is a quicker way to “unselect” variables. The negative sign means select all columns except for these listed in c(). The alternative is to type every column you want to keep.\n\n\n\n\nArrange the data according to Gent_2_score, from highest to lowest. Save this as a new object called “summaryvdat_sort”"
  },
  {
    "objectID": "PSYC401/Week4.html#task-3-draw-graphs-of-the-vocabulary-data",
    "href": "PSYC401/Week4.html#task-3-draw-graphs-of-the-vocabulary-data",
    "title": "4. Worksheet 4",
    "section": "Task 3: Draw Graphs of the Vocabulary Data",
    "text": "Task 3: Draw Graphs of the Vocabulary Data\n\nDraw graphs of the following relations:\n\n\nEnglish status and academic year\nGender and academic year\nVocabulary score and academic year\n\n\nSave your script file."
  },
  {
    "objectID": "PSYC401/Week4.html#task-4-loading-and-joining-data-in-r-studio",
    "href": "PSYC401/Week4.html#task-4-loading-and-joining-data-in-r-studio",
    "title": "4. Worksheet 4",
    "section": "Task 4: Loading and joining data in R studio",
    "text": "Task 4: Loading and joining data in R studio\n\nNow, let’s clear out R-studio before we get started again using rm(list=ls()).\nGo to the data files from week 2 and load them into Rstudio again (“ahicesd.csv”, and “participantinfo.csv”). You can redownload them here.\n\n\nRemember these data come from this study: Woodworth, R.J., O’Brien-Malone, A., Diamond, M.R. and Schuz, B. (2018). Data from, “Web-based Positive Psychology Interventions: A Reexamination of Effectiveness”. Journal of Open Psychology Data, 6(1).\nRemind yourself of the aim of the study and the variables that are in the data set (see end of this script file for repeat description on the study).\n\n\nNext, load and join the ahicesd.csv and participantinfo.csv data in R studio. Call the joined data set “all_dat” (see week 2 workbook for reminders about this)"
  },
  {
    "objectID": "PSYC401/Week4.html#task-5-selecting-and-manipulating-data",
    "href": "PSYC401/Week4.html#task-5-selecting-and-manipulating-data",
    "title": "4. Worksheet 4",
    "section": "Task 5: Selecting and manipulating data",
    "text": "Task 5: Selecting and manipulating data\n\nWe’re not interested in the individual questionnaire items. So, let’s select all the variables we want to keep (omitting the individual questionnaire items), and save this to an object called summary_all_dat (again see week 2 workbook for reminder)\nNext, we will add another variable to the data. We use the function mutate() for this. Let’s scale the ahiTotal and cesdTotal values and add them to the summary_all_dat set.\n\n\nsummary_all_dat_scale &lt;- mutate(.data = summary_all_dat, ahiTotalscale = scale(ahiTotal), cesdTotalscale = scale(cesdTotal))\n\n\nWhat are the minimum and maximum values of the new variable ahiTotalscale?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nhint: use the arrange() function, or the min() and max() functions.\n\n\n\n\nWhat do these scale values mean? (reminder: they are Z scores).\n\n\nThe next way we will work with the data is to organise the observations into different groups. First of all, here is the function summarise(). This works by summarising the results of a data set according to a particular measure. So, instead of mean(summary_all_dat_scale$ahiTotal) you can use this, which turns out to be a much more powerful way of looking at the data:\n\n\nsummarise(.data = summary_all_dat_scale, mean(ahiTotal))\n\n\nThey should give the same results - check that they do. This function summarise() is more powerful because you can look at several values at the same time, e.g.:\n\n\nsummarise(.data = summary_all_dat_scale, mean(ahiTotal), sd(ahiTotal), mean(cesdTotal), sd(cesdTotal))\n\n\nWhat is the result of this command?\n\n\nBut now let’s think about what kind of patterns we’d like to investigate in the data. There are four interventions conducted in this study. Let’s look at each of these interventions and their effect of ahiTotal and cesdTotal. We can look at subgroups of data either by using the filter() function, or by using the function group_by(). The advantage of group_by() is that we can look at several groups at the same time, rather than dividing up the data file into pieces. Let’s organise by the different interventions.\n\n\nsummary_all_dat_scale_intervention &lt;- group_by(.data = summary_all_dat_scale, intervention)\n\n\nThis command takes the data summary_all_dat_scale, and then groups it according to the four interventions in the data. We can’t yet see any difference in summary_all_dat_scale_intervention but it’s in there, lurking, just waiting. Now, we can look at the means for each intervention using the summary function again. Run the summary function on summarydata_scale_intervention. What happens?\n\n\nYou can also group by several factors at the same time. We can group by intervention and get means and standard deviations, but that is not going to give us a huge amount of insight into how the interventions affect the happiness measure because we are combining the mean of ahiTotal across all occasions of testing, including testing before the intervention has been applied.\n\nSo, let’s group by intervention and occasion of testing:\n\nsummary_all_dat_intocc &lt;- group_by(.data = summary_all_dat_scale, intervention, occasion)\n\n\nNow produce the means and standard deviations of the happiness score (ahiTotal) for each intervention at each testing occasion.\n\n\nThis doesn’t print all the lines out, so you can make a new object (e.g., called sum_output) and view that, or you can filter out some of the lines so we only look at the first and second occasion of testing.\n\n\nsum_output &lt;- summarise(.data = summary_all_dat_intocc, mean(ahiTotal), sd(ahiTotal))\nView(sum_output)"
  },
  {
    "objectID": "PSYC401/Week4.html#task-6-graph-some-groups",
    "href": "PSYC401/Week4.html#task-6-graph-some-groups",
    "title": "4. Worksheet 4",
    "section": "Task 6: Graph some groups",
    "text": "Task 6: Graph some groups\n\nDraw a scatter plot of ahiTotal and cesdTotal values for the whole data set.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the ggplot() function with geom_point()\n\n\n\n\nMake it a bit more beautiful using the labs() addition.\n\n\nNow redraw the plot, but colour the points according to whether they are first, second, third, etc occasion of testing. Add in col = \"occasion\" into the aes() part of the geom_point function, so that this part looks like this: aes(x = ahiTotal, y = cesdTotal, col = occasion)"
  },
  {
    "objectID": "PSYC401/Week4.html#task-7-chi-squared-and-cramers-v",
    "href": "PSYC401/Week4.html#task-7-chi-squared-and-cramers-v",
    "title": "4. Worksheet 4",
    "section": "Task 7: Chi-squared and Cramer’s V",
    "text": "Task 7: Chi-squared and Cramer’s V\n\nLet’s now have a look at running Chi-squared and Cramer’s V tests in R. download this week’s data from here. Read titanic.csv into an object called “titanic”. View the data. It should correspond to the data in the overhead slides.\nMake a bar graph to count the numbers of survived and died by class.\nNow let’s see if there is a significant relation between class and survival using Chi-squared:\n\n\nchisq.test(x = titanic$class, y = titanic$survival)\n\n\nThe results give the chi-squared value, the number of degrees of freedom, and the p-value. P = 2.2e-16 means p = .0000000000000022. That’s highly significant. That means the observations are divided across the categories in a way that is very unlikely to be due to chance (for this number (P = 2.2e-16), it means there’s a 2 in a quadrillion chance that titanic survival was not related to class). In a report, you would write: Chi-squared(2, N= 1309) = 127.86, p &lt; .001.\n\n\nNow, let’s compute Cramer’s V. First, we need to make sure we have the package lsr.\n\n\nlibrary(lsr)\n\n\nThen run the test:\n\n\ncramersV(x = titanic$class, y = titanic$survival)\n\n\nYour next task is to run some Chi-squared and Cramer’s V tests on some of the other nominal data. Open the data “PSYC401-shipley-scores-anonymous-17_22.csv” again. Investigate the association between gender and year (are there different distributions of males and females in each of our masters’ year cohorts) using Chi-squared and Cramer’s V. Is it significant?\nWhat about the association between english_status and Gender?\nWhat about the association between english_status and academic year?"
  },
  {
    "objectID": "PSYC401/Week4.html#task-8-more-chi-squared-and-cramers-v-tests",
    "href": "PSYC401/Week4.html#task-8-more-chi-squared-and-cramers-v-tests",
    "title": "4. Worksheet 4",
    "section": "Task 8: More Chi-squared and Cramer’s V tests",
    "text": "Task 8: More Chi-squared and Cramer’s V tests\n\nLook at the “ahicesd.csv” and “participantinfo.csv” data sets from week 2 again. Which nominal measures could you look at an association between? Report the Chi-squared test and Cramer’s V results for these associations. Are these associations significant? How do you interpret the significant associations?\nHave a further browse of Psychological Science for data sets that you can download and begin to explore. Practise applying the data manipulation and graphing functions to these data sets.\n\nDescription of Woodworth, R.J., O’Brien-Malone, A., Diamond, M.R. and Schuz, B. (2018). Data from, “Web-based Positive Psychology Interventions: A Reexamination of Effectiveness”. Journal of Open Psychology Data, 6(1).\n\nIn our study we attempted a partial replication of the study of Seligman, Steen, Park, and Peterson (2005) which had suggested that the web-based delivery of positive psychology exercises could, as the consequence of containing specific, powerful therapeutic ingredients, effect greater increases in happiness and greater reductions in depression than could a placebo control. Participants (n=295) were randomly allocated to one of four intervention groups referred to, in accordance with the terminology in Seligman et al. (2005) as 1: Using Signature Strengths; 2: Three Good Things; 3: Gratitude Visit; 4: Early Memories (placebo control). At the commencement of the study, participants provided basic demongraphic information (age, sex, education, income) in addition to completing a pretest on the Authentic Happiness Inventory (AHI) and the Center for Epidemiologic Studies-Depression (CES-D) scale. Participants were asked to complete intervention-related activities during the week following the pretest. Futher measurements were then made on the AHI and CESD immediately after the intervention period (‘posttest’) and then 1 month after the posttest (day 38), 3 months after the posttest (day 98), and 6 months after the posttest (day 189). Participants were not able to to complete a follow-up questionnaire prior to the time that it was due but might have completed at either at the time that it was due, or later. We recorded the date and time at which follow-up questionnaires were completed."
  },
  {
    "objectID": "PSYC401/Week5.html",
    "href": "PSYC401/Week5.html",
    "title": "5. Worksheet 5",
    "section": "",
    "text": "You should be able to answer yes to all the following. If you can’t yet, go back to the previous workbooks and repeat your working until you can answer yes, being able to type in and run the commands without referring to your notes.\n\nI can open R-studio\nI can open new libraries using library()\nI can make an R script file\nI can input a file into an object in R-studio using read_csv()\nI can join two files together using inner_join()\nI can select certain variables from an object using select()\nI can select subsets of data using filter() (e.g., I can select participants in two conditions from a data set containing participants in four conditions)\nI can make new variables using mutate()\nI can arrange data according to subsets using group_by()\nI can change format of data from wide to long format using pivot_longer\nI can change format of data from long to wide format using pivot_wider\nI can produce summaries of means and standard deviations for subsets of data after applying group_by() using summarise()\nI can draw histograms of single variables, point plots of two ratio/interval/ordinal variables, bar plots of counts, and box plots of one categorical and one ratio/interval/ordinal variable using ggplot()\nI can run a Chi-squared test and Cramer’s V test using chisq.test() and cramersV()\nI can interpret the results of a Chi-squared test and Cramer’s V test and write up a simple report of the results.\nI can save an R script file."
  },
  {
    "objectID": "PSYC401/Week5.html#task-1-checklist-what-i-can-now-do",
    "href": "PSYC401/Week5.html#task-1-checklist-what-i-can-now-do",
    "title": "5. Worksheet 5",
    "section": "",
    "text": "You should be able to answer yes to all the following. If you can’t yet, go back to the previous workbooks and repeat your working until you can answer yes, being able to type in and run the commands without referring to your notes.\n\nI can open R-studio\nI can open new libraries using library()\nI can make an R script file\nI can input a file into an object in R-studio using read_csv()\nI can join two files together using inner_join()\nI can select certain variables from an object using select()\nI can select subsets of data using filter() (e.g., I can select participants in two conditions from a data set containing participants in four conditions)\nI can make new variables using mutate()\nI can arrange data according to subsets using group_by()\nI can change format of data from wide to long format using pivot_longer\nI can change format of data from long to wide format using pivot_wider\nI can produce summaries of means and standard deviations for subsets of data after applying group_by() using summarise()\nI can draw histograms of single variables, point plots of two ratio/interval/ordinal variables, bar plots of counts, and box plots of one categorical and one ratio/interval/ordinal variable using ggplot()\nI can run a Chi-squared test and Cramer’s V test using chisq.test() and cramersV()\nI can interpret the results of a Chi-squared test and Cramer’s V test and write up a simple report of the results.\nI can save an R script file."
  },
  {
    "objectID": "PSYC401/Week5.html#task-2-load-prepare-and-explore-the-data",
    "href": "PSYC401/Week5.html#task-2-load-prepare-and-explore-the-data",
    "title": "5. Worksheet 5",
    "section": "Task 2: Load, prepare, and explore the data",
    "text": "Task 2: Load, prepare, and explore the data\n\nClear out R using rm(list=ls())\nLoad again the data set on the Shipley and Gent vocabulary scores.\nSet the research question: do people who self-identify as male or female have different scores on the Gent vocabulary test? The research hypothesis is: “People who identify as male or female have different vocabulary scores”. What is the null hypothesis?\nTo test the research hypothesis, we will filter people who self-identify as male or female from the data set. To be inclusive, additional research questions would be part of your research project to analyse also people who self-identify as other gender. Run this command to extract a subset of the data (note that the | stands for “or”, and means Gender matches male or gender matches female:\n\n\ndat2 &lt;- filter(.data = dat, Gender == 'Male' | Gender == 'Female')\n\n\nDraw a box plot of Gent vocabulary test 1 scores by gender. For a box plot, note that we need data in “long format”, where each observation is on one line, and we have a column that indicates which condition (in this case Gender) the participant is in. Does it look like there might be a gender effect? What is the direction of the effect?\nNote that unless we had filtered the data, the box plot would contain ‘NA’ as well, which stands for missing data. In a data set it’s always a good idea to call missing data ‘NA’ rather than just leaving them blank because this could be interpreted as a zero or as an error of filling in data. Missing values make things untidy, so it’s good practice to focus only on the variables we need for the t-test and remove all other missing values. Use select() to get just the Gender and Gent_1_score variables, and put this in a new object called ‘dat3’.\nNext, in order to run a t-test we have to remove any rows of data which contain a ‘NA’ - either in the Gender or the Gent_1_score variables. We do this using drop_na(dat3), put the result in a new object called ‘dat4’. Run this command:\n\n\ndat4 &lt;- drop_na(dat3)\n\n\nNow, redraw the box plot from Step 21. Check there are just two groups.\nCompute mean and SDs for people who self-identify as male or female on Gent vocabulary test 1 scores.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse group_by() and summarise()."
  },
  {
    "objectID": "PSYC401/Week5.html#task-3-run-the-independent-t-test-and-measure-effect-size",
    "href": "PSYC401/Week5.html#task-3-run-the-independent-t-test-and-measure-effect-size",
    "title": "5. Worksheet 5",
    "section": "Task 3: Run the independent t-test and measure effect size",
    "text": "Task 3: Run the independent t-test and measure effect size\n\nConduct an independent t-test using this command:\n\n\nt.test(Gent_1_score ~ Gender, paired = FALSE, data = dat5 )\n\n\n‘Gent_1_score ~ Gender’ : the ~ can be interpreted as ‘by’, i.e., compute Gent_1_score by Gender\n‘paired = FALSE’ : this means we are doing an independent t-test (a paired t-test would have paired = TRUE)\n\n\nThe results should look like this, do yours?\n\nWelch Two Sample t-test\n\ndata:  Gent_1_score by Gender\nt = -1.7356, df = 62.409, p-value = 0.08756\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  -10.0862020   0.7105407\nsample estimates:\n  mean in group Female   mean in group Male \n57.57407             62.26190 \n\nThe key part of the results to look at is the one that has t = -1.7356, df = 62.409, p-value = 0.08756. This is the result that you report: t(62.41) = -1.74, p = .088.\n\nThe value is negative because the function includes Female before Male - and Female score is lower than Male score. What matters is how far away from zero the t-test is (either positively or negatively). The df value is slightly odd because the t.test() function figures out degrees of freedom in a technical way which takes into account differences in variance in the data between the two groups. We can just use the value that the t.test() function gives us.\n\nIs this a significant difference?\nNow we need to compute the effect size, using Cohen’s d. You need to load the library lsr then use this command:\n\n```{r}\n\ncohensD(Gent_1_score ~ Gender, method = \"unequal\", data = dat4)\n\n```\n\nIt’s pretty much the same as the t-test() command except that we use ‘method = ’unequal’ instead of ‘paired = FALSE’. For a paired t-test you would use ‘method = ’paired’\n\n\nWhat is the effect size? Make a brief report of the results - reporting means and SDs, the t-test, p-value, and Cohen’s d. Discuss your brief report in your group.\nMake sure all commands are in the source window, save them as a new R script file."
  },
  {
    "objectID": "PSYC401/Week5.html#task-4-practise-running-another-independent-t-test",
    "href": "PSYC401/Week5.html#task-4-practise-running-another-independent-t-test",
    "title": "5. Worksheet 5",
    "section": "Task 4: Practise running another independent t-test",
    "text": "Task 4: Practise running another independent t-test\n\nNext research question: do people who are native English speakers have different vocabulary scores than those who learned English as a second language? What is the research hypothesis and the null hypothesis?\nRepeat the Steps 22-30 in Tasks 2 and 3 except using english_status in place of Gender throughout.\nWrite a brief report of the results, including means and SDs for native speakers and ESL speakers, t-test, p-value, and Cohen’s d. Discuss your report in your group.\nSave your R script file.\n\nPart 3: Conducting a paired t-test\nTask 5: Conducting a paired t-test\n\nClear out R-studio before we get started again using rm(list=ls())\nWe are going to investigate again the data from this paper: Woodworth, R.J., O’Brien-Malone, A., Diamond, M.R. and Schuez, B., 2018. Data from, “Web-based Positive Psychology Interventions: A Reexamination of Effectiveness”. Journal of Open Psychology Data, 6(1).\n\nOur research question is whether happiness scores are affected by the interventions. We will look at the pre-test (occasion 0) and the first test after the intervention (occasion 1).\n\nWhat is the research hypothesis and what is the null hypothesis?\nFor a paired t-test we can only include data from people who have produced scores at both occasions of testing. So, we need a slightly different version of the data. Download the files here. Remind yourself what these data mean.\nOnce again, join the ahicesd.csv and participantinfo2.csv data in R-studio by aligning the names for the participant bumbers in these two data sets (see week 2 workbook for reminders about this).\nLet’s select only the relevant variables. Use select() to select only id, ahiTotal, and occasion variables, and save this as a new object called ‘summarydata’\nUse filter to pull out only occasion == 0 or occasion == 1 scores\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nuse occasion == 0 | occasion == 1'), save this as a new object called summarydata2\n\n\n\n\nHere is where we would usually remove all the NA values, but there aren’t any in this file (so we don’t need drop_na()).\nNow, we need to make sure occasion is treated as a categorical variable, rather than a continuous variable, so we need to convert it to a factor:\n\n```{r}\n\nsummarydata2$occasion &lt;-as.factor(summarydata2$occasion)\n\n```\n\nNow, draw a box plot of ahiTotal scores by occasion (why do we use a box plot?)\nCompute mean and SD for each occasion\nRun the paired t-test: it’s the same as for the independent t-test except that we use paired = TRUE in place of paired = FALSE:\n\n```{r}\n\nt.test(ahiTotal ~ occasion, paired = TRUE, data = summarydata2)\n\n```\nIs the result significant?\n\nBefore we run the Cohen’s d command for these data, we have to make sure we have a list of the participants in one condition, followed by the list of participants in the other condition. We can do this using the command arrange():\n\n```{r}\n\nsummarydata3 &lt;- arrange(.data = summarydata2, occasion)\n\n```\n\nNow run Cohen’s d: it’s the same as for the independent t-test except that we use ‘method = ’paired’:\n\n```{r}\n\ncohensD(ahiTotal ~ occasion, method = \"paired\", data = summarydata3)\n\n```\nWhat is the value for Cohen’s d?\n\nWrite up a brief report of the result and discuss in your group.\nSave your R script file."
  },
  {
    "objectID": "PSYC234/Week8.html",
    "href": "PSYC234/Week8.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC234/Week9.html",
    "href": "PSYC234/Week9.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC234/index.html",
    "href": "PSYC234/index.html",
    "title": "Statistics: from association to modelling causality",
    "section": "",
    "text": "Welcome\nThis is filler text introducing students to the course in general. Perhaps what’s expected of them, the course aims, and other information of use.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam in est non nisi eleifend vulputate a et magna. Mauris vulputate felis lacus, ut bibendum lectus sollicitudin vel. Integer vestibulum arcu et risus vestibulum, in ullamcorper magna consectetur. Donec ut tempus enim, at vulputate libero. Integer porta metus eget elit cursus, sit amet hendrerit ipsum facilisis. Maecenas mollis, elit gravida ornare tempus, est lectus mattis velit, et commodo nunc lectus eu enim. Aenean luctus, felis ac sodales accumsan, dolor nunc euismod lorem, vel vulputate ipsum orci quis ipsum. Quisque placerat, velit vitae dictum feugiat, lectus lorem rutrum ligula, at fringilla ex enim consectetur felis. Aliquam erat volutpat. Nunc a nisi eget ex ornare dictum.\n\n\nCourse Contacts\n\n\n\n\nEmail Address\n\n\n\n\nTom Beesley\nt.beesley at lancaster dot ac dot uk\n\n\nJohn Towse\nj.towse at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC234/Week1.html",
    "href": "PSYC234/Week1.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC234/Week2.html",
    "href": "PSYC234/Week2.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC234/Week3.html",
    "href": "PSYC234/Week3.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC234/Week7.html",
    "href": "PSYC234/Week7.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC234/Week6.html",
    "href": "PSYC234/Week6.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC234/Week4.html",
    "href": "PSYC234/Week4.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi\n\n\n\n Back to top"
  },
  {
    "objectID": "Resources/index.html",
    "href": "Resources/index.html",
    "title": "Accessing the R Server",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Warning\n\n\n\nThis website is under construction. Not all data is in place, and not all modules are filled out. Pages should be in place, but some are filler. If the pages have names in the sidebar, then you’re able to explore these as examples.\n\n\n\nWelcome\nWelcome to Lancaster University’s Department of Psychology modules on Statistics for Psychologists. In the sidebar, you will find a complete list of all the statistics modules that are currently offered. This is your one-stop shop for all the lab materials you need for working with R and statistics.\n\n\nAccessing the R Server\nYou can access the Department’s R server at psy-rstudio.lancaster.ac.uk. Remember you will need to be on campus, or access via the VPN.\nHere are detailed instructions for accessing workbench. This isn’t written yet as it’s possible we will switch to SSO before the next academic year, in which case, lorem ipsum.\n\n\nFAQs and Common Troubleshooting\n\n  \n    \n      \n        I can't access the server. It just loads and loads and then times out.\n      \n    \n    \n      Use the VPN, or be on campus. Instructions for accessing the VPN are found here.\n    \n  \n  \n    \n      \n        My code doesn't work and keeps giving me errors. How do I fix this?\n      \n    \n    \n      Welcome to programming, this is 90% of what writing code is like. Getting better at programming is understanding how to solve problems and how to effectively use resources. StackOverflow is a good example of a resource.\n    \n  \n  \n    \n      \n        Is the server down?\n      \n    \n    \n      Hopefully not. Check your internet connection first, perhaps check with others if they can access it too. If not, don't panic. Get in touch with THIS IS WHERE WE MAY NEED A CENTRALISED COMMUNICATION SERVICE LIKE A SHARED MAILBOX WITH FORWARDING SET UP?\n    \n  \n  \n    \n    \n      \n        Is there another way to use Rstudio without the R server?\n      \n    \n    \n      Yes!\n      \n      You can also install R and Rstudio onto your own computer which has its advantages and disadvantages to using the R server. You can install R and Rstudio directly from Posit [here](https://posit.co/download/rstudio-desktop/), who develop and maintain Rstudio. You will need to install both, R is the programming language and Rstudio is the interface between you and the computer that runs R.\n\nThe advantage of having your own installation of R and Rstudio (just referred to as Rstudio from here on in) is that *you* control the settings completely. You can install new packages, set defaults, use specific versions of R and its libraries. In short, you have complete control over what you can do, which will lead to you becoming a better, more well-rounded data scientist (when we use R, we are data scientists as well as psychologists. You can wear many hats of expertise!).\n\nThe disadvantage is that *you* are in control. If something goes wrong or doesn't install properly, it is up to you to resolve. As a department, we strongly recommend that you use the R server as your means of accessing Rstudio as we know that the server works, we have tested the teaching materials with the server and in labs we can help troubleshoot because we know that the versiond of R, the libraries, etc. are handled by us, we look after it so that you don't have to. A multitude of things can go wrong with a personal installation and you will need to fix this (which is part of being a well-rounded data scientist). If you ask for help with a local install, the first thing we will ask is \"does it work on the R server?\", if the answer is yes, then we can give some pointers but we cannot fix it for you (in my personal experience, I once spent 8+ hours of actively trying to fix a single issue with my Rstudio \\[okay, so it was quite complex an issue, but the point remains\\]).\n\nAs you see, the advantage and disadvantage are intrinsically linked. From a teaching perspective, we cannot try and problem solve every person's individual and unique setup of Rstudio, there simply isn't enough time in a week. Yet the benefit of having your own version means you are totally unrestricted in what you can do with R, if you can imagine it, there is probably a set of libraries and functions that can help. If there isn't, you can make them!\n\nIt is your choice for how to use Rstudio, but we recommend that you use the R server for the materials that we teach, but we provide this to complement and enhance the teaching experience. If you want to go beyond what is in the worksheets, then installing and using your own version of Rstudio is a very good first step.\n    \n  \n  \n      \n    \n      \n        Where can I learn more about R and Rstudio outside of the labs?\n      \n    \n    \n      Great question. We can recommend looking at these books, R4DS, Advanced R, etc.\n    \n  \n  \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC122/Week18.html",
    "href": "PSYC122/Week18.html",
    "title": "8. Linear Model Part 2",
    "section": "",
    "text": "Welcome to your overview of the materials you will work with in PSYC122 Week 18.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project."
  },
  {
    "objectID": "PSYC122/Week18.html#our-learning-goals",
    "href": "PSYC122/Week18.html#our-learning-goals",
    "title": "8. Linear Model Part 2",
    "section": "Our learning goals",
    "text": "Our learning goals\nIn Week 18, we aim to further develop skills in analyzing and in visualizing psychological data.\nWe will use linear models to estimate the association between predictors and outcomes. What is new, here, is that we will explore the power and flexibility of the linear model analysis method in two important aspects.\n\n\n\n\n\n\nTip\n\n\n\n\nWe will fit linear models including multiple predictors, this is why this form of analysis is also often called multiple regression. 2 We will use linear models to estimate the effects of numeric and categorical or nominal predictor variables.\n\n\n\nWhen we do these analyses, we will need to adapt how we report the results:\n\nwe need to report information about the model we specify, identifying all predictors;\nwe will need to decide if the effects of one or more predictors are significant;\nwe will report the model fit statistics (F, R-squared) as well as coefficient estimates;\nand we need to learn to write texts describing the impact of predictors.\n\nUsually, in describing the impacts of predictors, we are required to communicate:\n\nthe direction of the effect – do values of the outcome variable increase or decrease given increasing values of the predictor?\nthe size of the effect – how much do values of the outcome variable increase or decrease given increasing values of the predictor?\n\nThis task of description is enabled by producing plots of the predictions we can make:\n\nabout how we expect the outcome to change, given different values of a predictor.\n\n\n\n\n\n\n\nTip\n\n\n\nWe will aim to build skills in producing professional-looking plots for our audiences.\n\nWe can produce plots showing the effects of predictors\nAs predictions of change in outcome, given different values of the predictor variables."
  },
  {
    "objectID": "PSYC122/Week18.html#sec-w18-resources-intro",
    "href": "PSYC122/Week18.html#sec-w18-resources-intro",
    "title": "8. Linear Model Part 2",
    "section": "Your resources",
    "text": "Your resources\nYou will see – below – links to the lectures, information about the data we will analyze, and an explanation of the activities.\nAll the links to the lectures, and everything you need for your practical work class can also be found under the Week 18 resources section title, on Moodle:\nLink to Moodle"
  },
  {
    "objectID": "PSYC122/Week18.html#lectures-video-recordings",
    "href": "PSYC122/Week18.html#lectures-video-recordings",
    "title": "8. Linear Model Part 2",
    "section": "Lectures: video recordings",
    "text": "Lectures: video recordings\nThe lecture material for this week is presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. (You will need to be on campus or logged in to the university VPN to get access to the videos.)\nPart 1 of 4; about 19 minutes\nPart 2 of 4; about 13 minutes\nPart 3 of 4; about 15 minutes\nPart 4 of 4; about 13 minutes\nThe lectures have three main areas of focus.\n1. Working with the linear model with multiple predictors\nWe focus in-depth on how you code linear models, how you identify critical information in the results summaries, and how you report the results: the language and the style you can use in your reports.\n\n\n\n\n\n\nTip\n\n\n\n\nA small change to lm() coding releases power and flexibility in how you use the analysis method.\n\n\n\n2. Analyses are done in context so when we conduct analyses we must use contextual information\nThe power and flexibility of the linear model presents challenges. We must decide which predictor variables we specify in our model. This specification requires us to think about our theoretical assumptions and what they require us to include to make sense of the behaviours or the individual differences we observe when we do things like investigating what makes health information easy or difficult to understand.\n3. Developing critical thinking\nAs we develop conceptual understanding and practical skills, we must learn to reflect critically on our analyses, and learn to critically evaluate the analyses we read about when we read research reports in the scientific literature.\n\n\n\n\n\n\nTip\n\n\n\nCritical analysis can develop by considering\n\nvalidity\nmeasurement\ngeneralizability\n\n\n\nWe are always working in the broader context of uncertainty:\n\nuncertainty about the predictions we may make concerning outcomes of interest;\nuncertainty given the possibility that predicted effects may vary between individuals or groups;\nuncertainty given the influence of sources of randomness in how specific responses are produced.\n\n\nLinks to other classes\nIn the lecture, I sketch out the ways that the linear model can be extended. You will learn new and different analysis methods in the second year classes but, most of the time, you may understand that …\n\n\n\n\n\n\nImportant\n\n\n\nEverything is some kind of linear model."
  },
  {
    "objectID": "PSYC122/Week18.html#pre-lab-activity-1",
    "href": "PSYC122/Week18.html#pre-lab-activity-1",
    "title": "8. Linear Model Part 2",
    "section": "Pre-lab activity 1",
    "text": "Pre-lab activity 1\nIn weeks 16-19, we have been working together on a research project to investigate how people vary in their response to health advice.\nCompleting the project involves collecting responses from PSYC122 students.\nTo enter your responses, we invite you to complete a short survey.\nComplete the survey by clicking on the link here\n\n\n\n\n\n\nWarning\n\n\n\nIn our week 19 class activity, we will analyze the data we collect here.\n\nThis means we will close the survey by 5pm Monday 6 March (week 18)\n\n\n\n\nPre-lab activity 1 alternative\nIf you do not want to complete the survey, we invite you to read the pre-registered research plan for the PSYC122 health advice research project.\nRead the project pre-registration"
  },
  {
    "objectID": "PSYC122/Week18.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "href": "PSYC122/Week18.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "title": "8. Linear Model Part 2",
    "section": "Pre-lab activity 2: Getting ready for the lab class",
    "text": "Pre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 122-22-w18_for-students.zip files you need and upload them to your RStudio Server.\nThe folder includes the data files:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nand the code files:\n\n2022-23-PSYC122-w18-how-to.R\n2022-23-PSYC122-w18-workbook.R\n\nYou will use 2022-23-PSYC122-w18-workbook.R in the lab activity practical class.\nAlternatively, you can instead download the resources you need from the week 18 section of the Moodle page for the PSYC122 module:\nLink to Moodle\n\nWhat is in the how-to and workbook.R files?\n\n\n\n\n\n\nImportant\n\n\n\n\nOur aim is to make sure you can work with code, and write notes in the .R files.\n\n\n\nIn the workbook.R file you use for the lab activity, we identify tasks and questions, and leave you spaces where you can write code or answers.\nIn both the .R files:\n\n2022-23-PSYC122-w18-how-to.R\n2022-23-PSYC122-w18-workbook.R\n\nwe will take things step-by-step.\n\n\n\n\n\n\nTip\n\n\n\n\nMake sure you start at the top of the .R file and work your way, in order, through each task.\nComplete each task before you move on to the next task.\n\n\n\nThe how-to guide comprises an .R file 2022-23-PSYC122-w18-how-to.R with code and advice. The code in the .R file was written to work with the data file:\n\nstudy-one-general-participants.csv.\n\n\n\n\n\n\n\nTip\n\n\n\nWe show you how to do everything you need to do in the lab activity (Section 6) in the how-to guide.\n\nStart by looking at the how-to guide to understand what steps you need to follow in the lab activity."
  },
  {
    "objectID": "PSYC122/Week18.html#sec-w18-activity",
    "href": "PSYC122/Week18.html#sec-w18-activity",
    "title": "8. Linear Model Part 2",
    "section": "Lab activity",
    "text": "Lab activity\nIn the lab activity .R file 2022-23-PSYC122-w18-workbook.R, you will work with data from a study about how people respond to guidance about a variety of health topics (general topics):\n\nstudy-two-general-participants.csv\n\nThe data are similar in format to the response data we are collecting as part of the PSYC122 project.\n\nTasks\nIn the activity, we are going to work through the following tasks.\n\nEmpty the R environment – using rm(list=ls())\nLoad relevant libraries – using library()\nRead in the data files – using read_csv()\nInspect the data – using head() and summary()\nEstimate the way in which an outcome may vary, given different values in a predictor variable – using lm()\nDo this with multiple predictor variables\nBetter understand linear model predictions by comparing one outcome-predictor relation\nCreate boxplots to examine the potential association between variation in a continuous outcome variable and the differences between groups or levels in a categorical variable – using geom_boxplot()\nEstimate the effects of factors as well as numeric variables\nVisualize the model predictions – using ggpredict()\n\nThe 2022-23-PSYC122-w18-workbook.R file takes you through the tasks, one by one.\nIf you are unsure about what you need to do, check in the how-to guide: look at the advice in 2022-23-PSYC122-w18-how-to.R.\nYou will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code. You will need to change the names of the dataset or the variables to complete the tasks in the activity.\nThis process of adapting demonstration code is a process critical to data literacy and to effective problem solving in working with data in psychological science.\n\n\nWhat is in the data files\nEach of the data files we will work with has a similar structure.\nYou can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy varianble coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\nAnswers\n\n\n\n\n\n\nTip\n\n\n\nYou can now download the answers version of the activity workbook .R here.\n\n\nThe answers version presents my answers for questions, and some extra information where that is helpful."
  },
  {
    "objectID": "PSYC122/Week19.html",
    "href": "PSYC122/Week19.html",
    "title": "9. Linear Model Part 3",
    "section": "",
    "text": "Welcome to your overview of the materials and guidance you will work with in PSYC122 Week 19.\nWe have completed four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC122, PSYC123 and PSYC124.\nWe have been doing this in the context of a live research project with potential real world impacts: the Clearly understood project.\nWe will be revisiting some of the ideas and techniques you have seen introduced in previous classes. And we will be extending your development with some new ideas, to strengthen your skills.\nThe PSYC122 health comprehension survey is now closed, and we will focus our practical work in week 19 on analyzing the data we have collected."
  },
  {
    "objectID": "PSYC122/Week19.html#our-learning-goals",
    "href": "PSYC122/Week19.html#our-learning-goals",
    "title": "9. Linear Model Part 3",
    "section": "Our learning goals",
    "text": "Our learning goals\nIn Week 19, we aim to further develop skills in analyzing and in visualizing psychological data.\nWe will use linear models to estimate the association between predictors and outcomes in order to answer our research questions.\n\n\n\n\n\n\nTip\n\n\n\nWhat is new, here, is that we will compare the results from different studies to critically examine questions relating to results reproducibility:\n\nDo we see the same results when similar methods are used to collect data to address the same questions?\n\n\n\nWe shall be focusing our analysis work on response data contributed by PSYC122 students.\n\nBut we will critically examine whether the results of our analyses of PSYC122 data are or are not similar to the results of the analyses of data collected in other studies.\n\nWhen we do linear models, as you have seen, we usually need to report:\n\ninformation about the model;\nif the effects of the predictors are significant, and what the estimates of the effects tell us;\noverall, how well the model works to predict observed outcomes in our data.\n\nIn describing the impacts of predictors, we need to think about:\n\nthe direction of the effect – do values of the outcome variable increase or decrease given increasing values of the predictor?\nthe size of the effect – how much do values of the outcome variable increase or decrease given increasing values of the predictor?\n\n\n\n\n\n\n\nTip\n\n\n\nIn assessing results reproducibility, therefore, we may focus here on:\n\nWhether an effect is or is not significant in the datasets we are comparing;\nWhether the estimate of the coefficient for the slope of the effect has similar sign (positive or negative) or size (the value of the coefficient)."
  },
  {
    "objectID": "PSYC122/Week19.html#resources-for-you",
    "href": "PSYC122/Week19.html#resources-for-you",
    "title": "9. Linear Model Part 3",
    "section": "Resources for you",
    "text": "Resources for you\nYou will see – below – links to the lectures, information about the data we will analyze, and an explanation of the activities.\nAll the links to the lectures, and everything you need for your practical work class can also be found under the Week 19 resources section title, on Moodle:\nLink to Moodle"
  },
  {
    "objectID": "PSYC122/Week19.html#lectures-video-recordings",
    "href": "PSYC122/Week19.html#lectures-video-recordings",
    "title": "9. Linear Model Part 3",
    "section": "Lectures: video recordings",
    "text": "Lectures: video recordings\nThe lecture material for this week is presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. (You will need to be on campus or logged in to the university VPN to get access to the videos.)\nPart 1 of 4; about 15 minutes\nPart 2 of 4; about 13 minutes\nPart 3 of 4; about 20 minutes\nPart 4 of 4; about 20 minutes\nThe lectures have three main areas of focus.\n1. Thinking critically about how people vary, and about the robustness and generalizability of results in psychological science\nWe shall proceed to answer our research questions with the data we have collected with the help of PSYC122 students. As we do this, we should reflect on where the data come from – the fact that people vary, and results vary – and we should consider, critically, questions concerning key ideas:\n\nMethods reproducibility – Will a different researcher be able to get the same results if they analyze the same data?\nResults reproducibility – Will we get the same results if we collect new data using the same procedure?\n\nThese questions are part of our motivation for recommending open science practices.\n2. What the PSYC122 data tell us: the possible answers to our research questions\nWe have been working to develop understanding and skills using example questions, tasks and data developed within the health comprehension Clearly understood project. PSYC122 students contributed their responses to a version of the survey we have been using to collect data to find answers to our research questions. In our practical work, we will be examining the PSYC122 data but as we do so we should understand that no one study in psychological science will give us definitive answers to any interesting question about people and what they do. In the lecture, we reflect on how data from different studies – using the same methods, with the same aims – may nevertheless vary:\n\nvary in the data distributions\nand vary in the results that analyses indicate.\n\nCritically evaluating results across a series of studies, or replication attempts, is part of the process of accumulating evidence to build insight in psychological science, given measurement under uncertainty and limits in our samples.\n3. Growing in independence, working with free open R\nR is free open source software. This is critical to ensuring that when we do analyses with psychological data we can usefully share data and code in ways that help to build evidence and insight in psychological science.\nBut there is a bigger benefit: the vast, free, R knowledge ecosystem.\n\n\n\n\n\n\nTip\n\n\n\nEvery problem you ever have:\n\nsomeone has had it before\nsolved it\nand written a blog (or tweet or toot) or recorded a YouTube or TikTok about it\n\n\n\nIn the week 19 lecture, and in the practical materials, we start to look at how you can locate and use the information you need to do data analysis. But it is important that you recognize two things:\n\n\n\n\n\n\nImportant\n\n\n\n\nEverything you need is out there, you just need to learn how to find it.\nSharing knowledge about R represents a new way to share scientific information: maybe this – not books or journals – is the future.\n\n\n\n\nThe lectures: the implications of our results, the revolution in open science and being a part of the revolution\nLast, we set up questions for you to consider:\n\nWhat are the results of our analyses with the new data?\nWhat are the implications for health communication?\n\nThen we step back, and consider how we are learning to work in terms of the modern workflow, and the on-going revolution in open science, and how people now build and share knowledge."
  },
  {
    "objectID": "PSYC122/Week19.html#pre-lab-activity-1",
    "href": "PSYC122/Week19.html#pre-lab-activity-1",
    "title": "9. Linear Model Part 3",
    "section": "Pre-lab activity 1",
    "text": "Pre-lab activity 1\nIn weeks 16-19, we have been working together on a research project to investigate how people vary in their response to health advice. Completing the project involved collecting responses from PSYC122 students. In our class activities, this week, we will analyze the data we collected up to Monday 7th. March.\nThe link to the survey is here, for your information:\nhere\nThe survey is now officially closed.\nAlso or your information, you can read the pre-registered research plan for the PSYC122 health advice research project, here:\nRead the project pre-registration\nLooking at the survey or at the pre-registration are entirely optional activities."
  },
  {
    "objectID": "PSYC122/Week19.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "href": "PSYC122/Week19.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "title": "9. Linear Model Part 3",
    "section": "Pre-lab activity 2: Getting ready for the lab class",
    "text": "Pre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 122-22-w19_for-students.zip files you need and upload them to your RStudio Server.\nThe folder includes the data files:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nAs well as the data compiled from 2022-23 PSYC122 students\n\n2022-23_PSYC122-subjects.csv\n\nAs well as the code files:\n\n2022-23-PSYC122-w19-how-to.R\n2022-23-PSYC122-w19-workbook.R\n\nYou will use 2022-23-PSYC122-w19-workbook.R in the lab activity practical class.\nAlternatively, you can instead download the resources you need from the week 189 section of the Moodle page for the PSYC122 module:\nLink to Moodle\n\nWhat is in the how-to and workbook.R files?\n\n\n\n\n\n\nImportant\n\n\n\n\nOur aim is to make sure you can work with code, and write notes in the .R files.\n\n\n\nIn the workbook.R file you use for the lab activity, we identify tasks and questions, and leave you spaces where you can write code or answers.\nIn both the .R files:\n\n2022-23-PSYC122-w19-how-to.R\n2022-23-PSYC122-w19-workbook.R\n\nwe will take things step-by-step.\n\n\n\n\n\n\nTip\n\n\n\n\nMake sure you start at the top of the .R file and work your way, in order, through each task.\nComplete each task before you move on to the next task.\n\n\n\nThe how-to guide comprises an .R file 2022-23-PSYC122-w19-how-to.R with code and advice. The code in the .R file was written to work – this week – with two data files:\n\nstudy-one-general-participants.csv.\nstudy-two-general-participants.csv.\n\nThis is so that you can learn how to:\n\nanalyse data from more than one study;\ncompare the results from the analyses of data from different studies to assess the robustness or generalizability of findings.\n\n\n\n\n\n\n\nTip\n\n\n\nWe show you how to do everything you need to do in the lab activity (Section 6) in the how-to guide.\n\nStart by looking at the how-to guide to understand what steps you need to follow in the lab activity."
  },
  {
    "objectID": "PSYC122/Week19.html#sec-w19-activity",
    "href": "PSYC122/Week19.html#sec-w19-activity",
    "title": "9. Linear Model Part 3",
    "section": "Lab activity",
    "text": "Lab activity\nIn the lab activity .R file 2022-23-PSYC122-w19-workbook.R, you will work with data from two studies about how people respond to guidance about a variety of health topics (general topics):\n\nstudy-two-general-participants.csv which you have already seen before;\n2022-23_PSYC122-subjects.csv which is new, comprising the responses contributed by 2022-23 PSYC122 students.\n\n\nTasks\nIn the activity, we are going to work through the following tasks.\n\nEmpty the R environment – using rm(list=ls())\nLoad relevant libraries – using library(): notice we use some new ones here\nRead in the data files – using read_csv()\nInspect the data – using head() and summary()\nPlot the distributions of some key variables\nLearn how to produce grids of plots, showing them side-by-side, for easy comparison\nWork to produce grids of plots to consolidate skills\nProduce scatterplots to examine potential associations\nProduce grids of scatterplots, showing them side-by-side to allow comparison of potential associations\nExamine the association between accuracy of understanding and rated accuracy using cor.test()\nWork out how to locate useful online information about R functions or libraries\nEstimate the way in which an outcome may vary, given different values in a predictor variable – using lm()\nDo this analysis using the 2022-23 PSYC122 data, and evaluate the robustness of results across a series of studies\nPractise visualizing model predictions using ggpredict()\n\nThe 2022-23-PSYC122-w19-workbook.R file takes you through the tasks, one by one.\nIf you are unsure about what you need to do, check in the how-to guide: look at the advice in 2022-23-PSYC122-w19-how-to.R.\nYou will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code. You will need to change the names of the dataset or the variables to complete the tasks in the activity.\nThis process of adapting demonstration code is a process critical to data literacy and to effective problem solving in working with data in psychological science.\n\n\nWhat is in the data files\nEach of the data files we will work with has a similar structure.\nYou can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy varianble coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\nAnswers\n\n\n\n\n\n\nTip\n\n\n\nYou can now download the answers version of the activity workbook .R here.\n\n\nThe answers version presents my answers for questions, and some extra information where that is helpful."
  },
  {
    "objectID": "PSYC122/index.html",
    "href": "PSYC122/index.html",
    "title": "Statistics for Psychologists II",
    "section": "",
    "text": "Welcome\nThis is filler text introducing students to the course in general. Perhaps what’s expected of them, the course aims, and other information of use.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam in est non nisi eleifend vulputate a et magna. Mauris vulputate felis lacus, ut bibendum lectus sollicitudin vel. Integer vestibulum arcu et risus vestibulum, in ullamcorper magna consectetur. Donec ut tempus enim, at vulputate libero. Integer porta metus eget elit cursus, sit amet hendrerit ipsum facilisis. Maecenas mollis, elit gravida ornare tempus, est lectus mattis velit, et commodo nunc lectus eu enim. Aenean luctus, felis ac sodales accumsan, dolor nunc euismod lorem, vel vulputate ipsum orci quis ipsum. Quisque placerat, velit vitae dictum feugiat, lectus lorem rutrum ligula, at fringilla ex enim consectetur felis. Aliquam erat volutpat. Nunc a nisi eget ex ornare dictum.\n\n\nCourse Contacts\n\n\n\n\nEmail Address\n\n\n\n\nTom Beesley\nt.beesley at lancaster dot ac dot uk\n\n\nJohn Towse\nj.towse at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC122/Week12.html",
    "href": "PSYC122/Week12.html",
    "title": "2. Correlation Part 2",
    "section": "",
    "text": "Today we will continue a look at correlation as a measure of association between two numerical variables. We will review assumptions associated with correlation, discuss some issues important to be aware of when interpreting correlation results and finally, we’ll talk about intercorrelation."
  },
  {
    "objectID": "PSYC122/Week12.html#lectures",
    "href": "PSYC122/Week12.html#lectures",
    "title": "2. Correlation Part 2",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week is presented in two parts:\n\nCorrelation – Assumption, issues and intercorrelation – Theory\nCorrelation – Assumption, issues and intercorrelation – How to"
  },
  {
    "objectID": "PSYC122/Week12.html#reading",
    "href": "PSYC122/Week12.html#reading",
    "title": "2. Correlation Part 2",
    "section": "Reading",
    "text": "Reading\nThe reading that accompanies the lectures this week is again from chapter 9 of the core text by Howell (2017). Please note that I mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Miller and Haden.\nRougly, last week we covered the material in sections 9.1 to 9.4, as well as sections 9.8 to 9.11 and section 9.15. This week, we’ll cover the material in sections 9.5 to 9.7, and sections 9.12 to 9.13. Even if a section is not mentioned here, all of chapter 9 is relevant."
  },
  {
    "objectID": "PSYC122/Week12.html#pre-lab-activities",
    "href": "PSYC122/Week12.html#pre-lab-activities",
    "title": "2. Correlation Part 2",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures on correlation and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Online interactive tutorial to practise your data-wrangling skills\nData comes in lots of different formats. One of the most common formats is that of a two-dimensional table (the two dimensions being rows and columns). Usually, each row stands for a separate observation (e.g. a participant), and each column stands for a different variable (e.g. a response, category, or group). A key benefit of tabular data is that it allows you to store different types of data-numerical measurements, alphanumeric labels, categorical descriptors-all in one place.\nIt may surprise you to learn that scientists actually spend far more of time cleaning and preparing their data than they spend actually analysing it. This means completing tasks such as cleaning up bad values, changing the structure of tables, merging information stored in separate tables, reducing the data down to a subset of observations, and producing data summaries. Some have estimated that up to 80% of time spent on data analysis involves such data preparation tasks (Dasu & Johnson, 2003)!\nMany people seem to operate under the assumption that the only option for data cleaning is the painstaking and time-consuming cutting and pasting of data within a spreadsheet program like Excel. We have witnessed students and colleagues waste days, weeks, and even months manually transforming their data in Excel, cutting, copying, and pasting data. Fixing up your data by hand is not only a terrible use of your time, but it is error-prone and not reproducible. Additionally, in this age where we can easily collect massive datasets online, you will not be able to organise, clean, and prepare these by hand.\nIn short, you will not thrive as a psychologist if you do not learn some key data wrangling skills. Although every dataset presents unique challenges, there are some systematic principles you should follow that will make your analyses easier, less error-prone, more efficient, and more reproducible.\nIn the online tutorial, you will see how data science skills will allow you to efficiently get answers to nearly any question you might want to ask about your data. By learning how to properly make your computer do the hard and boring work for you, you can focus on the bigger issues.\nYou’ll be practising the select(), filter(), mutate(), arrange(), group_by() and summarise() functions from the dplyr package.\nYou’ve used these functions before, but if you’d like to quickly remind yourself what they do, watch the video (~10 min) on Data wrangling: dplyr and pipes. As the title suggests, I also explain in the video what a ‘pipe’ (this thing: %&gt;%) is and you’ll be practising with that as well.\nIf you’re ready to begin, go to the tutorial linked to below. There is no need to install or download anything. Each tutorial has everything you need to write and run R code, right in the tutorial.\n\nWorking with Tibbles Practise how to extract values form a table, subset tables, calculate summary statistics, and derive new variables.\n\n\n\nPre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 122_week12_forStudents.zip file and upload it into the new folder in RStudio Server you created (see last week’s Pre-lab activity 4 for instructions on how to do that."
  },
  {
    "objectID": "PSYC122/Week12.html#lab-activities",
    "href": "PSYC122/Week12.html#lab-activities",
    "title": "2. Correlation Part 2",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nconstructing and interpreting histograms and qq-plots\nconstructing and interpreting a matrix of scatterplots\nrunning intercorrelation analysis and interpret the results\ncorrect for multiple comparisons when running intercorrelation analysis\nconstructing a correlation matrix in APA format\nwhen and why to apply correlation analysis to answers questions in psychological science\n\n\nLab activity 1: Assumptions of Correlation Analysis\n\nQuestion 1\nCorrelation would be an appropriate form on analysis for researchers interested in the relationship between:\n\nDog (breed) and height (cm) of owner\nSpeed of swimming (mph) and area of tank (cm)\nNumber of cows sitting and rain fall (mm)\nTotal llama saliva (ml) expelled and gender of visitors\nb and d\nb and c\n\n\n\nQuestion 2\nWhen would you use Spearman’s rho analysis instead of Pearson’s r?\n\nWhen there are clear outliers in the data\nWhen the data is not normally distributed\nWhen the relationship between X and Y is curvilinear\na and b\n\n\n\nQuestion 3\nUsing the histograms and qq-plots below, which of these variables satisfies the normality assumption? Explain your answers.\nHistogram non-words \nQQ-plot non-words \nHistogram words \nQQ-plot words \nHistogram vocabulary \nQQ-plot non-words \n\n\nQuestion 4\nWhy should correlation analysis not be conducted on variables with a curvilinear relationship?\n\n\n\nLab activity 2: Attitudes towards vaping\nGreat work so far! Now we really want to see what you can do yourself. In this activity we’ll use real data on implicit and explicit attitudes towards vaping. You’ll need the data file VapingData.csv and the R-script 122_wk12_labAct2_template.R that you downloaded when completing Pre-lab activity 2.\n\nBackground\nExplicit attitudes were measured via a questionnaire where higher scores indicated a positive attitude towards vaping (VapingQuestionaireScore). Implicit attitudes were measured through an Implicit Association Test (IAT) using images of Vaping and Kitchen utensils and associating them with positive and negative words.\nThe IAT works on the principal that associations that go together (that are congruent, e.g. warm and sun) should be quicker to respond to than associations that do not go together (that are incongruent, e.g. warm and ice). You can read up more on the procedure on the Noba Project which has a good description of the procedure under the section “Subtle/Nonsconscious Research Methods”.\nFor this exercise, you need to know that “Block 3” in the experiment tested reaction times and accuracy towards congruent associations, pairing positive words with Kitchen utensils and negative words with Vaping. “Block 5” in the experiment tested reaction times and accuracy towards incongruent associations, pairing positive words with Vaping and negative words with Kitchen Utensils. As such, if reaction times were longer in Block 5 than in Block 3 then people are considered to hold the view that Vaping is negative (i.e. congruent associations are quicker than incongruent associations). However, if reaction times were quicker in Block 5 than in Block 3 then people are considered to hold the view that Vaping is positive (i.e. incongruent associations were quicker than congruent associations). The difference between reaction times in Block5 and Block3 is called the participants’ IAT score.\n\n\nStep 0: Clean your environment\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one. You can do this by clicking on the broom icon at the top of the environment window, or you can use the code below.\n\nTASK: Use the code snippet below to clear the environment. TIP: If you hover your mouse over the box that includes the code snippet, a ‘copy to clipboard’ icon will appear in the top right corner of the box. Click that to copy the code. Now you can easily paste it into your script. \n\n\nrm(list=ls())                            \n\n\n\nStep 1: Set your working directory\nNow, make sure your working directory is set to the folder in which you have stored the data file (VapingData.csv).\n\nTASK: Use the code snippet below to check what you working directory is currently set to. This is the folder that R will use to look for files. Is the file path that is written to the Console after you run the code snippet the one that contains the data file? You can check by nativating to the path you can see in the Console in the ‘Files’ pane on the right. Does it contain the data file (‘VapingData.csv’)?\n\n\ngetwd()\n\nIf your working directory is not set to the folder that contains the data file, navigate to folder that contains the data file in the ‘Files’ pane, click ‘More’ and then on ‘Set as working directory’.\n\n\n\nStep 2: Load packages\nBefore we can get started we need to tell R which libraries to use. For this analysis we’ll need broom, car, Hmisc, lsr and tidyverse.\n\nTASK: Load the relevant libraries. HINT: Use the library() function.\n\n\n\nStep 3: Read in the data\nThe data file we’ll be working with is VapingData.csv\n\nTASK: Read in the data file (using the read_csv() function) and store it in an object called dat. Have a look at the layout of the data and familiarise yourself with it. You have 8 columns. Reaction times and Accuracy scores for Blocks 3 and 5 as well as the Explicit Vaping Questionnaire scores, Sex and Age, for each participant.\n\nQUESTION 1: For how many participants do we have data?\n\n\nStep 4: Data wrangling\nThe data are not in a shape yet that we can actually use for our analysis. We’ll have to do some ’data wrangling to knock them into shape. We need to take the following into account:\n\nAccuracy was calculated as proportion and as such can’t go above 1.Participants entered their own data so some might have made a mistake. Remove participants who had an accuracy greater than 1 in either Block 3 or Block 5 as we are unclear on the accuracy of these values.\nWe also only want participants who were paying attention so best remove anybody whose average accuracy score across Blocks 3 and 5 was less than 80%. Note - this value is arbitrary and if you wanted, in your own experiment, you could use a more relaxed or strict cut-off based on other studies or guidance. Note that these decisions should be set out at the start of your research as part of your pre-registration or as part of your Registered Report. Finally, in this instance, remember, the values are in proportions not percentages (so 80% will be .8).\nNow that we have removed data points that were the result of data entry mistakes or came participants who didn’t pay attention during the task, we create an IAT score for participants by subtracting Block 3 reaction times (RT) from Block 5 reaction times (IAT_BLOCK5_RT - IAT_BLOCK3_RT).\n\n\nTASK: Look closely at each line of code below and check you understand what it does. Copy the code to your script and for each line add a comment to describe what it does.\n\n\ndat &lt;- dat %&gt;% \n  filter(IAT_BLOCK3_Acc &lt; 1) %&gt;%\n  filter(IAT_BLOCK5_Acc &lt; 1) %&gt;%\n  mutate(IAT_ACC = (IAT_BLOCK3_Acc + IAT_BLOCK5_Acc)/2) %&gt;%\n  filter(IAT_ACC &gt; .8) %&gt;%\n  mutate(IAT_RT = IAT_BLOCK5_RT - IAT_BLOCK3_RT)\n\nQUESTION 2: For how many participants do we have data now that we have cleaned them up?\nQUESTION 3: Use the information in the background description to understand how the scores relate to attitudes. What does a positive IAT_RT score reflect? What does a negative IAT_RT score reflect? What does a higher score on the ‘VapingQuestionnaireScore’ mean?\n\n\nStep 5: Calculating descriptive statistics\nNow that we have the variables that we need and the data cleaned up, we will create a descriptives summary of the number of people, and the means for the IAT and Vaping Questionnaire Score.\n\nTASK: Look closely at each line of code below and check you understand what it does. Copy the code to your script and for each line add a comment to describe what it does.\n\n\ndescriptives &lt;- dat %&gt;%\n  summarise(n = n(),\n            mean_IAT_ACC = mean(IAT_ACC),\n            mean_IAT_RT = mean(IAT_RT),\n            mean_VPQ = mean(VapingQuestionnaireScore, na.rm = TRUE))\n\nQUESTION 4: Why might these averages be useful? Why are averages not always useful in correlations?\n\n\nStep 6: Check the assumptions\nVariable types\nQUESTION 5: What are the variable types for the implicit (IAT_RT) and the explicit (VapingQuestionnaireScore) attitude variables?\nMissing data\n\nTASK: Remove participants with missing data. HINT: Use the filter() function and the is.na() function\n\nQUESTION 6: How many people had missing data?\nNormality\n\nTASK: Create histograms and qq-plots for the IAT_RT and VapingQuestionnaireScore variables. HINT: Use the ggplot() function with geom_histogram() and use the qqPlot() function (note the capital P)\n\nQUESTION 7: What do you conclude from the histograms and the qq-plots? Are the VapingQuestionnaireScore and the IAT_RT normally distributed?\nLinearity and homoscedasticity\n\nTASK: Plot the relationship between IAT_RT and VapingQuestionnaireScore using a scatterplot and a line of best fit. HINT: For this you’ll need the ggplot() function together with geom_point() and geom_smooth(). Make sure to give your axes some sensible labels.\n\nQUESTION 8: What do you conclude from the scatterplot in terms of the homoscedasticity of the data and the linearity, direction and strength of the relationship? What does the scatterplot tell you about possible issues (outliers, range restrictions)?\n\n\nStep 7: Conduct a correlation analysis\nQUESTION 9: Do you need to calculate Pearson’s r or Spearman’s rho?\n\nTASK: Conduct a correlation analysis. HINT: Use the cor.test() function. You may want to use the pull() and the round() functions to get the numbers out.\n\nQUESTION 10: Can you write up the results including the r, df, p-value and an interpretation?\n\n\nStep 8: Intercorrelations\nFinally, let’s check whether either implicit or explicit attitude is associated with age.\nFirst, let’s create a new data frame that only includes the relevant variables. Look closely at each line of code below and check you understand what it does. Don’t forget to copy the code below to your script and run it.\n\ndat_matrix &lt;- dat %&gt;%\n  select(Age, IAT_RT, VapingQuestionnaireScore) %&gt;%\n  as.data.frame(dat_matrix) # Make sure tell R that dat is a data frame\n\n\nTASK: Now, create a matrix of scatterplots. HINT: Use the pairs() function.\n\nQUESTION 11: What do you conclude from the scatterplots?\n\nTASK: Finally, conduct intercorrelation (multiple correlations). HINT: Use the correlate() function. Do you need Pearson’s r or Spearman’s rho? Have you adjusted for multiple comparisons?\n\nQUESTION 12: What do you conclude from the results of the correlation analysis?"
  },
  {
    "objectID": "PSYC122/Week12.html#answers",
    "href": "PSYC122/Week12.html#answers",
    "title": "2. Correlation Part 2",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\nLab activity 1\n\nCorrelation would be an appropriate form on analysis for researchers interested in the relationship between\n\n\n\nDog (breed) and height (cm) of owner\n\n\nSpeed of swimming (mph) and area of tank (cm)\n\n\nNumber of cows sitting and rain fall (mm)\n\n\nTotal llama saliva (ml) expelled and gender of visitors\n\n\nb and d\n\nf. b and c All variables in b anc c are continuous. Dog breed and gender are categorical.\n\n\nWhen would you use Spearman’s rho analysis instead of Pearson’s r?\n\n\n\nWhen there are clear outliers in the data\n\nb. When the data is not normally distributed\n\nWhen the relationship between X and Y is curvilinear\n\n\na and b\n\n\n\nUsing the histograms and qq-plots below, which of these variables satisfies the normality assumption? Explain your answers. Vocabulary. Only for vocabulary does the histogram resemble a bell curve and do the data-points in the qq-plot fall within the dashed blue lines.\nWhy should correlation analysis not be conducted on variables with a curvilinear relationship? May be subject to a type 2 error  there actually is a relationship between variables yet we reject the null hypothesis. As the relationship is not linear, correlation analysis will not identify this.\n\n\n\nLab activity 2\nYou can download the R-script that contains the code to complete lab activity 2 here: 122_wk12_labAct2.R.\n\nFor how many participants do we have data? There are 166 observations, so we have data for 166 participants. You can see this in the Environment window in the top right. This does not tell us whether any of these participants have any missing data.\nFor how many participants do we have data now that we have cleaned them up? 104 participants\nUse the information in the background description to understand how the scores relate to attitudes. What does a positive IAT_RT score reflect? People with a positive IAT_RT are considered to hold the implicit view that vaping is negative (i.e. congruent associations are quicker than incongruent associations) What does a negative IAT_RT score reflect? People with a negative IAT_RT are considered to hold the implicit view that vaping is positive (i.e. incongruent associations were quicker than congruent associations). What does a higher score on the ‘VapingQuestionnaireScore’ mean? Higher scores indicated a positive explicit attitude towards vaping.\nWhy might these averages be useful? Why are averages not always useful in correlations? It is always worth thinking about which averages are informative and which are not. Knowing the average explicit attitude towards vaping could well be informative. In contrast, if you are using an ordinal scale and people use the whole of the scale then the average may just tell you the middle of the scale you are using - which you already know and really isn’t that informative. So it is always worth thinking about what your descriptives are calculating.\nWhat are the variable types for the implicit (IAT_RT) and the explicit (VapingQuestionnaireScore) attitude variables? Both can be considered continuous variables and at least at interval level.\nHow many people had missing data? 8. Before we removed participants with missing data, we had 104 observations, now we have 96. So there must have been 8 participants without a score on one or the other variable.\nWhat do you conclude from the histograms and the qq-plots? Are the VapingQuestionnaireScore and the IAT_RT normally distributed? Yes. Both histograms resemble a normal distribution (bell curve) and the open circles in the qq-plots fall within the blue stripy lines.\nWhat do you conclude from the scatterplot in terms of the homoscedasticity of the data and the linearity, direction and strength of the relationship? What does the scatterplot tell you about possible issues (outliers, range restrictions)? The data look like a cloud without a clear direction. This suggests the relationship might might be weak. In terms of linearity, the scatterplot doesn’t suggest any curvilinear relationships. Variance seems quite constant, but there do seem to be few people with negative IAT_RT (Implicit attitude) scores, suggesting few people held the view that vaping is positive.\nDo you need to calculate Pearson’s r or Spearman’s rho? Pearson’s r because the data do meet the assumptions.\nCan you write up the results including the r, df, p-value and an interpretation? Testing the hypothesis of a relationship between implicit and explicit attitudes towards vaping, a Pearson correlation found no significant relationship between IAT reaction times (implicit attitude) and answers on a Vaping Questionnaire (explicit attitude), r(94) = -.02, p = .822. Overall this suggests that there is no direct relationship between implicit and explicit attitudes with regard to vaping and as such our hypothesis was not supported; we cannot reject the null hypothesis.\nWhat do you conclude from the scatterplots? The scatterplots with age suggest that age is highly skewed with only a few participants older than 25. For now, let’s say we’ll therefore calculate Spearman’s rho, rather than Pearson’s r. That is ok for now, but if you were analysing these data for a research project, you’d want to have a closer look at the age variable (think histogram, qq-plot, and think about either collecting more data from older participants or transforming the variable (more about that next year).\nWhat do you conclude from the results of the correlation analysis? No significant correlation with age was found."
  },
  {
    "objectID": "PSYC122/Part1.html",
    "href": "PSYC122/Part1.html",
    "title": "Introduction to Part 1",
    "section": "",
    "text": "This is a collection of tuition material written for Psychology undergraduates at Lancaster University. At the moment the content represents the “lab materials” for the PSYC122 module in first year. As was the case for PSYC121, they feature tuition of programming with R, building on the skills you developed last term.\n\n\nSome parts should be completed before you attend the lab session (watching lectures, reading chapters, pre-lab activities). All the links to the different materials and activities are also in the ‘to-do list’ for the relevant week on Moodle."
  },
  {
    "objectID": "PSYC122/Part1.html#analysis-labs-and-pre-lab-work",
    "href": "PSYC122/Part1.html#analysis-labs-and-pre-lab-work",
    "title": "Introduction to Part 1",
    "section": "",
    "text": "Some parts should be completed before you attend the lab session (watching lectures, reading chapters, pre-lab activities). All the links to the different materials and activities are also in the ‘to-do list’ for the relevant week on Moodle."
  },
  {
    "objectID": "PSYC122/Week13.html",
    "href": "PSYC122/Week13.html",
    "title": "3. The Linear Model",
    "section": "",
    "text": "This week we will focus on the linear model and simple linear regression."
  },
  {
    "objectID": "PSYC122/Week13.html#lectures",
    "href": "PSYC122/Week13.html#lectures",
    "title": "3. The Linear Model",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week is presented in two parts:\n\nThe linear model (~25 min)\nHow to build a linear model in R (~30 min) You can find the example script in this week’s zip-folder (see under Pre-lab activity 3)."
  },
  {
    "objectID": "PSYC122/Week13.html#reading",
    "href": "PSYC122/Week13.html#reading",
    "title": "3. The Linear Model",
    "section": "Reading",
    "text": "Reading\nThe reading that accompanies the lectures this week is from chapter 10 of the core text by Howell (2017). Please note that I mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Miller and Haden."
  },
  {
    "objectID": "PSYC122/Week13.html#pre-lab-activities",
    "href": "PSYC122/Week13.html#pre-lab-activities",
    "title": "3. The Linear Model",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapter you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Visualising the regression line\nHave a look at this visualisation of the regression line by Ryan Safner.\nIn this shiny app, you see a randomly-generated set of data points (within specific parameters, to keep the graph scaled properly). You can choose a slope and intercept for the regression line by using the sliders. The graph also displays the residuals as dashed red lines. Moving the slope or the intercept too much causes the generated line to create much larger residuals. The shiny app also calculates the sum of squared errors (SSE) and the standard error of the regression (SER), which calculates the average size of the error (the red numbers). These numbers reflect how well the regression line fits the data, but you don’t need to worry about those for now.\nIn the app he uses the equation Y = aX + b in which b is the intercept and a is the slope.\nThis is slightly different from the equation you saw during the lecture. There we talked about Y = b0 + b1*X + e. Same equation, just different letters. So b0 in the lecture is equivalent to b in the app and b1 in the lecture is equivalent to a in the app.\nPre-lab activity questions:\n\nChange the slider for the intercept. How does it change the regression line?\nChange the slider for the slope. How does it change the regression line?\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line?\n\n\n\nPre-lab activity 2: Data visualisation - practice with ggplot2()\nIn this week’s online tutorials, you will practise visualing data.\nIf you’re ready to begin, go to the tutorial linked to below. There is no need to install or download anything. Each tutorial has everything you need to write and run R code, right in the tutorial.\n\nVisualisation basics Practise the basics of how to create a graph, how to add variables and how to make different types of graphs.\nScatterplots This tutorial revisits scatterplots. Along the way, you will learn to build multi-layer plots and to use new coordinate systems.\n\n\n\nPre-lab activity 3: Getting ready for the lab class\n\nGet your files ready\nDownload the 122_week13_forStudents.zip file and upload it into the new folder in RStudio Server you created."
  },
  {
    "objectID": "PSYC122/Week13.html#lab-activities",
    "href": "PSYC122/Week13.html#lab-activities",
    "title": "3. The Linear Model",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nconducting simple regression in R\ninterpreting simple regression in R\nreporting the results in APA format\nwhen and why to apply simple regression to answer questions in psychological science\n\n\nLab activity 1: The regression line\n\nQuestion 1\nWhat is the regression equation as discussed during the lecture and what does each letter represent?\n\n\nQuestion 2\nWhat are residuals?\n\n\nQuestion 3\nDiscuss the answers to the pre-lab activity questions. What did you find?\n\nChange the slider for the intercept. How does it change the regression line? The value for y at x = 0 changes.\nChange the slider for the slope. How does it change the regression line? The steepness of the line changes.\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line? The distance between the fitted values (the line) and the observed values (the dots) increases. Therefore, the red dashed lines become longer suggesting that the residuals increase. The model therefore fits the data less well.\n\n\n\n\nLab activity 2: Statistics anxiety and engagement in module activities\nIn this lab, we’ll be working with real data and using regression to explore the question of whether there is a relationship between statistics anxiety and engagement in course activities. You’ll need the data files psess.csv and stars2.csv and the R-script 122_wk13_labAct2_template.R that you downloaded when completing Pre-lab activity 3.\n\nBackground\nThe hypothesis is that students who are more anxious about statistics are less likely to engage in course-related activities. This avoidance behaviour could ultimately be responsible for lower performance for these students (although we won’t be examining the assessment scores in this activity).\nWe are going to analyse data from the STARS Statistics Anxiety Survey, which was administered to students in the third-year statistics course in Psychology at the University of Glasgow. All the responses have been anonymised by associating the responses for each student with an arbitrary ID number (integer).\nThe STARS survey (Cruise, Cash, & Bolton, 1985) is a 51-item questionnaire, with each response on a 1 to 5 scale, with higher numbers indicating greater anxiety.\nCruise, R. J., Cash, R. W., & Bolton, D. L. (1985). Development and validation of an instrument to measure statistical anxiety. Proceedings of the American Statistical Association, Section on Statistical Education, Las Vegas, NV.\nExample items from the STARS survey\n\nAs a measure of engagement in the course, we will use data from Moodle usage analytics. Over the course of the term, there were eight optional weekly on-line sessions that students could attend for extra support. The variable n_weeks in the psess.csv file tells you how many (out of eight) a given student attended.\nOur hypothesis was that greater anxiety would be reflected in lower engagement. Answer the following question.\nQUESTION 1: If our hypothesis is correct, what type of correlation (if any) should we observe between students’ mean anxiety levels and the variable n_weeks?\nBefore we do anything else, let’s load the libraries that we need for this analysis.\n\nTASK: Load broom, car and tidyverse. HINT: Use library().\n\n\nTASK: Read in both files, have a look at the layout of the data and familiarise yourself with it. HINTYou can use the read_csv() and the head() function.\n\nQUESTION 2 In the stars table, what do the numbers in the first row across the three columns refer to?\nNow that we’ve read in both data files, the next step is to calculate the mean anxiety scores for each participant. At the moment we have scores on all questions separately for each participant in the stars table. Instead we need one mean anxiety score for each participant.\n\nTASK: Write the code to calculate mean anxiety scores. Remember that participant is identified by the ID variable. Store the resulting table in a variable named stars_mean. HINT: Use group_by() and summarise(). Also, remember to use na.rm = TRUE when calculating the mean scores to deal with participants who have missing data (NAs).\n\nQUESTION 3 What is the mean anxiety score for participant 3?\nOk, before we get ahead of ourselves, in order to perform the regression analysis we need to combine the data from stars (the mean anxiety scores) with the data from engage (n_weeks).\n\nTASK: Join the two tables, call the resulting table joined. HINT: Use the inner_join() function (making use of the variable that is common across both tables) to join.\n\nWe now need descriptive statistics for both variables.\n\nTASK: Calculate the mean and standard deviations for the anxiety scores and the engagement data.\n\nQUESTION 4 What are the means and standard deviation for anxiety and engagement with the statistics module?\nAs always, it is a good idea to visualise your data.Now that we have all the variables in one place, make a scatterplot of anxiety as a function of engagement.\n\nTASK: Write the code to create the scatterplot. HINT: For this you’ll need the ggplot() function together with geom_point() and geom_smooth(). Make sure to give your axes some sensible labels with the labs() function.\n\nQUESTION 5 What does the scatterplot suggest about the relationship between anxiety and engagement?\nWith all the variables in place, we’re ready now to start building the regression model.\n\nTASK: Use the lm() function to run the regression model when you model engagement (the outcome variable) as a function of anxiety (the predictor variable) and use the summary() function to look at the output. HINT: lm(outcome ~ predictor, data = my_data).\n\nQUESTION 6 What is the estimate of the y-intercept for the model, rounded to three decimal places?\nQUESTION 7 To three decimal places, if the General Linear Model for this model is Y=beta0 + beta1X + e, then beta1 is …\nQUESTION 8 To three decimal places, for each unit increase in anxiety, engagement decreases by …\nQUESTION 9 To two decimal places, what is the overall F-ratio of the model?\nQUESTION 10 Is the overall model significant?\nQUESTION 11 What proportion of the variance does the model explain?\nNow that we’ve fitted a model, let’s check whether the model meets the assumptions of linearity, normality and homoscedasticity.\n\nTASK: Write the code to check the assumptions. HINT: crPlots() to check linearity, qqPlot() to check normality of the residuals, and residualPlot() to check homoscedasticity of the residuals.\n\nQUESTION 12 Does the relationship appear to be linear?\nQUESTION 13 Do the residuals show normality?\nQUESTION 14 Do the residuals show homoscedasticity?\nFinally, it’s time to write up the results following APA guidelines.\nQUESTION 15 What would the results section look like if you wrote them up, following APA guidelines? HINT: The Purdue writing lab website is helpful for guidance on punctuating statistics."
  },
  {
    "objectID": "PSYC122/Week13.html#answers",
    "href": "PSYC122/Week13.html#answers",
    "title": "3. The Linear Model",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\nLab activity 1\n\nWhat is the regression equation as discussed during the lecture and what does each letter represent? Y = b0 + b1 * X + e Y = outcome variable b0 = intercept b1 = slope X = predictor variable e = error\nWhat are residuals? Residuals reflect the discrepancy between the observed values and the fitted values and give an indication of how well the model ‘fits’ the data.\nDiscuss the answers to the pre-lab activity questions. What did you find?\n\n\n\nChange the slider for the intercept. How does it change the regression line? The value for y at x = 0 changes.\n\n\nChange the slider for the slope. How does it change the regression line? The steepness of the line changes.\n\n\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line? The distance between the fitted values (the line) and the observed values (the dots) increases. Therefore, the red dashed lines become longer suggesting that the residuals increase. The model therefore fits the data less well.\n\n\n\n\nLab activity 2\nYou can download the R-script that contains the code to complete lab activity 2 here: 122_wk13_labAct2.R.\n\nIf our hypothesis is correct, what type of correlation (if any) should we observe between students’ mean anxiety levels and the variable n_weeks? A negative correlation\nIn the stars table, what do the numbers in the first row across the three columns refer to? ID = 3, Question = Q01 and Score = 1 shows us that participant 3 reported a score of 1 on question 1.\nWhat is the mean anxiety score for participant 3? 1.058824\nWhat are the means and standard deviation for anxiety and engagement with the statistics module? Anxiety M = 2.08, SD = 0.56; Engagement M = 4.54, SD = 2.42.\nWhat does the scatterplot suggest about the relationship between anxiety and engagement? That there might indeed be a relatively strong negative correlation between the two; students with more anxiety, engage less.\nWhat is the estimate of the y-intercept for the model, rounded to three decimal places? 9.057. Explanation: In the summary table, this is the estimate of the intercept.\nTo three decimal places, if the General Linear Model for this model is Y=beta0 + beta1X + e, then beta1 is … -2.173. Explanation: In the summary table, this is the estimate of mean_anxiety, i.e., the slope.\nTo three decimal places, for each unit increase in anxiety, engagement decreases by … 2.173. Explanation: In the summary table, this is also the estimate of mean_anxiety, the slope is how much it decreases so you just remove the - sign.\nTo two decimal places, what is the overall F-ratio of the model? 11.99. Explanation: In the summary table, the F-ratio is noted as the F-statistic.\nIs the overall model significant? Yes. Explanation: The overall model p-value is .001428 which is less than .05, therefore significant.\nWhat proportion of the variance does the model explain? 25.52%. Explanation: The variance explained is determined by R-squared, you simply multiple it by 100 to get the percent.\nDoes the relationship appear to be linear? Yes, the pink link roughly falls across the dashed blue line and looks mostly linear.\nDo the residuals show normality? Yes, in the qq-plot the open circles mostly assemble around the solid blue line, and fall mostly within the range of the dashed blue lines.\nDo the residuals show homoscedasticity? Yes, the residual plot shows that the spread of the residuals is roughly similar for different fitted values.\nWhat would the results section look like if you wrote them up, following APA guidelines? A simple linear regression was performed with engagement (M = 4.54, SD = 0.56) as the outcome variable and statistics anxiety (M = 2.08, SD = 0.56) as the predictor variable. The results of the regression indicated that the model significantly predicted course engagement (F(1, 35) = 11.99, p &lt; .001, R^2 = 0.25), accounting for 25% of the variance. Anxiety was a significant negative predictor (beta = -2.17, p &lt; 0.001): as anxiety increased, course engagement decreased."
  },
  {
    "objectID": "PSYC122/Week11.html",
    "href": "PSYC122/Week11.html",
    "title": "1. Correlation",
    "section": "",
    "text": "Today we will take a look at correlation as a measure of association between two numerical variables. We will create scatterplots to visualise correlations, we will run a correlation analysis and we will practise interpreting and reporting the results."
  },
  {
    "objectID": "PSYC122/Week11.html#lectures",
    "href": "PSYC122/Week11.html#lectures",
    "title": "1. Correlation",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week is presented in two parts:\n\nTheory Watch this part before you complete the reading and the pre-lab activities.\nHow to Watch this part either after the ‘Theory’ part of after you’ve completed the pre-lab activities 1 to 3. Definitely watch it before you come to your lab session."
  },
  {
    "objectID": "PSYC122/Week11.html#reading",
    "href": "PSYC122/Week11.html#reading",
    "title": "1. Correlation",
    "section": "Reading",
    "text": "Reading\nThe reading that accompanies the lectures this week and next week is from chapter 9 of the core text by Howell (2017). Please note that I mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Miller and Haden.\nRougly, this week we’ll cover the material in sections 9.1 to 9.4, as well as sections 9.8 to 9.11 and section 9.15. Next week, we’ll cover the material in sections 9.5 to 9.7, and sections 9.12 to 9.13. Even if a section is not mentioned here, all of chapter 9 is relevant."
  },
  {
    "objectID": "PSYC122/Week11.html#pre-lab-activities",
    "href": "PSYC122/Week11.html#pre-lab-activities",
    "title": "1. Correlation",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures on correlation and read the textbook sections you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Visualing correlations\nHave a look at this visualisation of correlations by Kristoffer Magnusson.\nAfter having read the relevant sections of Howell (2017) Chapter 9, use this visualisation page to visually replicate the scatterplots in Figures 9.1 and 9.2 - use a sample of 100. After that, visually replicate the scatterplots in Figure 9.3.\nEach time you change the correlation, pay attention to the shared variance (the overlap between the two variables) and see how this changes with the changing level of relationship between the two variables. The greater the shared variance, the stronger the relationship.\nAlso, try setting the correlation to r = .5 and then moving a single dot to see how one data point, a potential outlier, can change the stated correlation value between two variables.\n\n\nPre-lab activity 2: Guess the correlation\nNow that you are well versed in interpreting scatterplots (scattergrams) have a go at this online app on guessing the correlation.\nThis is a very basic app that allows you to see how good you are at recognising different correlation strengths from the scatterplots. We would recommend you click the “Track Performance” tab so you can keep an overview of your overall bias to underestimate or overestimate a correlation.\nIs this all just a bit of fun? Well, yes, because stats is actually fun, and no, because it serves a purpose of helping you determine if the correlations you see in your own data are real, and to help you see if correlations in published research match with what you are being told. As you will have seen from the above examples, one data point can lead to a misleading relationship and even what might be considered a medium to strong relationship may actually have only limited relevance in the real world. One only needs to mention Anscombe’s Quartet to be reminded of the importance of visualising your data, which leads us to the final pre-lab activity for this week.\n\n\nPre-lab activity 3: Anscombe’s quartet\nAnscombe (1973) showed that four sets of bivariate data (X, Y) that have the exact same means, medians, and relationships can look very different when plotted. You can read more about this here.\nAll in this is a clear example of why you should visualise your data and not to rely on just the numbers.\n\n\nPre-lab activity 4: Getting ready for the lab class\n\nRemind yourself of the basics of how to work with RStudio.\nYou might want to re-visit some of the materials that John and Tom provided in PSYC121:\n\nBasics of working with RStudio\nVideo on how to upload a zip file and import data (3.5 mins)\nVideo on basic operations in RStudio (8 mins)\nVideo on using scripts and using the console (3 mins)\n\n\n\nCreate a folder and a Project for Week 11.\nClick here for the instructions from Week 6 of PSYC121 if you are unsure.\n\n\nGet your files ready\nDownload the 122_week11_forStudents.zip file and upload it into the new folder in RStudio Server you created at the previous step. If you need them, here are the instructions from Week 2 of PSYC121."
  },
  {
    "objectID": "PSYC122/Week11.html#lab-activities",
    "href": "PSYC122/Week11.html#lab-activities",
    "title": "1. Correlation",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nconstructing and interpreting scatterplots\nrunning correlation analysis and interpret the results\nreporting the results in APA format\nconstructing a correlation matrix in APA format\nwhen and why to apply correlation analysis to answers questions in psychological science\n\n\nLab activity 1: Interpreting correlation\n\nQuestion 1\nBelow are scatterplots that show the relationship between ‘how much you know about correlation and how attractive you appear to members of the opposite (&/or same) sex’. Choose the type of correlation (strength and direction) displayed in each graph using one of the following:\n\nPerfect positive correlation\nPerfect negative correlation\nStrong positive correlation\nStrong negative correlation\nModerate positive correlation\nModerate negative correlation\nNull correlation\n\nFigure A  Figure B  Figure C  Figure D \n\n\nQuestion 2\nSuppose it was observed that there is a correlation of r = -.81 between a driver’s age and the cost of car insurance. This correlation would mean that, in general, older people pay more for car insurance.\nTRUE or FALSE? Explain why.\nNote: explain your chosen answer based on the statistic given, not on why you think the correlation may or may note make ‘logical’ sense).\n\n\nQuestion 3\nSuppose that there is a correlation of r = .87 between the length of time a person is in prison and the amount of aggression the person displays on a psychological inventory administered at release. This means that spending a longer amount of time in prison causes people to become more aggressive.\nTRUE or FALSE? Explain why.\n\n\nQuestion 4\nA significant correlation was found between having great hair and performance in correlation labs. The correlation coefficient was .7. How much variance in correlation lab performance can the ‘greatness’ of your hair explain?\n\n51%\n70%\n49%\n30%\nWho cares I’ve got great hair.\n\nWhat was the reason for your answer?\nWhat is this ‘new coefficient’ called?\n\n\n\nLab activity 2: Visualising, calculating and reporting correlations\nGoing back to the data discussed in Chapter 11 of Miller & Haden, you’ll remember it contains data from 25 8-year-old children on:\n\na standardised test of reading ability (Abil)\nintelligence (IQ)\nthe number of minutes per week spent reading in the home (Home)\nand the number of minutes per week spent watching TV (TV)\n\nIn the video on ‘How to conduct correlation analysis using R’ we looked at the correlation between reading ability and intelligence. Now, let’s look at the correlation between number of minutes per week spent reading in the home and watching TV.\nThe folder you were asked to download under ‘Pre-lab activity 4: Getting ready for the lab class’ contains the datafile (“MillerHadenData.csv”) as well as the R-script from the ‘How to …’ video (122_wk11_howtoExample.R) that you can use here and adapt.\n\nLoad the ‘broom’ and the ‘tidyverse’ libraries by running the first two lines of code.\nRead in the data. You should now see an object with 25 observations and 5 variables in the ‘Environment’. Click on it to view it.\nConstruct a scatterplot of the relationship between ‘Home’ and ‘TV’.\nWhat can you tell from the scatterplot about the direction of the relationship?\nConduct the correlation analysis.\nWhat is the correlation coefficient (Pearson’s r)?\nWhat is the p value?\nIs the correlation significant at the p &lt; .05 level?\nWhat are the degrees of freedom you need to report?\nHow much variance in ‘time spent reading’ can be accounted for by ‘time spent watching TV’? (Hint: you can use the Console in RStudio as a calculator.)\nWrite a few sentences in which you report this result, following APA guidelines.\n\n\n\nLab activity 3: More correlations\nResearchers were interested in the relationship between hazardous alcohol use and impulsivity (making unplanned, rapid decisions without thinking or ‘acting on a whim’). To investigate the relationship, 20 participants completed both the alcohol use disorder identification test (AUDIT; Saunders, Aasland, Babor, de la Fuente, & Grant, 1993) and the Barratt’s Impulsiveness Scale (BIS-11) (Patton, Stanford, & Barratt, 1995). The datafile (“alcoholUse_Impulsivity.csv”) is in the folder you were asked to download under ‘Pre-lab activity 4: Getting ready for the lab class’. Again, the R-script from the ‘How to …’ video (122_wk11_howtoExample.R) is useful here.\n\nLoad the ‘broom’ and the ‘tidyverse’ libraries by running the first two lines of code.\nRead in the data. You should now see an object containing the data in the ‘Environment’. How many variables does it have?\nConstruct a scatterplot of the relationship between ‘Hazardous Alcohol Use’ and ‘Impulsivity’.\nWhat can you tell from the scatterplot about the direction of the relationship?\nConduct the correlation analysis.\nWhat is the correlation coefficient (Pearson’s r)?\nWhat is the p value?\nIs the correlation significant at the p &lt; .05 level?\nWhat are the degrees of freedom you need to report?\nHow much variance in ‘impulsivity’ can be accounted for by ‘hazardous alcohol use’? (Hint: you can use the Console in RStudio as a calculator.)\nConstruct a correlation matrix to display the correlation coefficient in a table.\nGive three logically possible directions of causality, indicating for each direction whether it is a plausible explanation in light of the variables involved (and why). No, this is not a trick question —-I know that correlation does not infer causation, but think critically! New studies/ideas are constructed by thinking what the previous study doesn’t tell us about what could be happening with the variables of interest.\n\nJob completed — Well done!"
  },
  {
    "objectID": "PSYC122/Week11.html#answers",
    "href": "PSYC122/Week11.html#answers",
    "title": "1. Correlation",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\nLab activity 1: Interpreting correlation\n\nScatterplots\n\n\n\nstrong positive correlation\n\n\nnull correlation\n\n\nmoderate positive correlation\n\n\nperfect negative correlation\n\n\n\nFALSE Explanation: The correlation coefficient is negative and therefore infers a negative correlation. As such, older people pay less for car insurance: as age increases, car insurance costs decrease.\nFALSE Explanation: This is a bit of trick question as it has the sneaky ‘cause’ word in. The correlation coefficient is a positive number, suggesting a positive relationship between length of time in prison and aggression. However, causation cannot be inferred from correlation and therefore we cannot know whether time spent in prison CAUSES aggression, and rather we suggest a relationship between the two that as length of time in prison increases, aggression increases.\nc 49% The ‘coefficient of determination’ or ‘R-squared’ tells us the proportion or variance in one variable that can be predicted if we know the other variable. We can determine this by squaring the r. Therefore, .72 = .49, R2 = .49.\n\n\n\nLab activity 2: Constructing scatterplots and calculating correlations\nYou can download the R-script that contains the code to complete lab activities 2 and 3 here: 122_wk11_labActivities2_3.R.\n\nSee R script\nSee R script\nSee R script\nWhat can you tell from the scatterplot about the direction of the relationship? There is a negative association between ‘Home’ and ‘TV’. This means that the longer a child spends watching TV, the shorter they will read at home.\nConduct the correlation analysis. See R script\nWhat is the correlation coefficient (Pearson’s r)? r = -.65\nWhat is the p value? p &lt; .001\nIs the correlation significant at the p &lt; .05 level? Yes, because the p-value is smaller than .005\nWhat are the degrees of freedom you need to report? 23\nHow much variance in ‘time spent reading’ can be accounted for by ‘time spent watching TV’? 42%\nWrite a few sentences in which you report this result, following APA guidelines. Something along the lines of: A Pearson’s correlation coefficient was used to assess the relationship between time spent reading at home and time spent watching TV at home. There was a significant negative correlation, r(23) = -.65, p &lt; .001. As time spent watching TV at home increased, time spent reading at home decreased.\n\n\n\nLab activity 3: More correlations\n\nSee R script\nHow many variables does it have? 3\nSee R script\nWhat can you tell from the scatterplot about the direction of the relationship? There is a positive association between ‘hazardous alcohol use’ and ‘impulsivity’. This means that as a participant’s score on ‘hazardous alcohol use’ goes up, their score on ‘impulsivity’ also goes up.\nSee R script\nWhat is the correlation coefficient (Pearson’s r)? r = .54\nWhat is the p value? p = .014\nIs the correlation significant at the p &lt; .05 level? Yes\nWhat are the degrees of freedom you need to report? 18\nHow much variance in ‘impulsivity’ can be accounted for by ‘hazardous alcohol use’? (Hint: you can use the Console in RStudio as a calculator.) 29%\nConstruct a correlation matrix to display the correlation coefficient in a table.\n\n 12. Give three logically possible directions of causality, indicating for each direction whether it is a plausible explanation in light of the variables involved (and why). No, this is not a trick question —-I know that correlation does not infer causation, but think critically! New studies/ideas are constructed by thinking what the previous study doesn’t tell us about what could be happening with the variables of interest.\nJust really looking for reasoning here.\nExamples:\n\nBeing more impulsive may make people consume more alcohol.\nConsuming more alcohol may make people more impulsive.\nAn outgoing personality might influence both your level of impulsivity and you are more likely to be socialising in the pub and consuming alcohol. So the same ‘third factor’ may influence both our variables of interest."
  },
  {
    "objectID": "PSYC122/Part2.html",
    "href": "PSYC122/Part2.html",
    "title": "Introduction to Part 2",
    "section": "",
    "text": "Welcome to our overview of the materials and activities you will work with in PSYC122 Part 2.\nWe will complete a series of four classes which will run in weeks 16-19.\nThe classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124."
  },
  {
    "objectID": "PSYC122/Part2.html#a-change-in-approach",
    "href": "PSYC122/Part2.html#a-change-in-approach",
    "title": "Introduction to Part 2",
    "section": "A change in approach",
    "text": "A change in approach\nIn weeks 16-20, we are going to focus on working in research in context (see Figure Figure 1).\n\n\n\n\n\n\n\n\nG\n\n  \n\nreading\n\n reading   \n\nknowledge\n\n knowledge   \n\nreading–knowledge\n\n   \n\nconventions\n\n conventions   \n\nknowledge–conventions\n\n   \n\nconcepts\n\n concepts   \n\nknowledge–concepts\n\n   \n\npractices\n\n practices   \n\nknowledge–practices\n\n  \n\n\nFigure 1: This is a simple graphviz graph.\n\n\n\n\nYou have been introduced to R. We know that some of you are new to R so we will practice the skills you are learning. We will consolidate, revise, and extend these skills.\nWe will build your understanding of the linear model.\nThe big change is this focus on the context. The reason is that not talking about the context risks limiting how you approach, do or think about data analysis.\nIn traditional methods teaching, the schedule of classes will progress through a series of tests, one test a week, from simpler to more complex tests (e.g., from t-test to multiple regression at the undergraduate level). Textbooks often mirror this structure, presenting one test per chapter.\nIn this approach, the presentation is often brief about the context: the question the researchers are investigating; the methods they use to collect data, including the measurements; and the assumptions they make about how your reasoning can get you from the things you measure to the things you are trying to understand. In this approach, also, example data may be presented in a limited, partial, way.\nThe reasons for this are understandable: methods are complex, technical, subjects for learning, and teachers and students do not also have time, perhaps, to think about statistics and about theoretical or measurement assumptions.\nThis is risky because it presents a misleading view of the challenge in learning methods: the misleading view is that the challenge is just the (difficult enough) challenge of learning about statistical methods, or dealing with numbers. It is risky, also, because it implies that if you learn the method, and can match the textbook example – the variables, the state of the data – when it is your turn to do an analysis, all will be well.\nA more productive approach – this is the approach we will take – is to expose, and talk about some of the real challenges that anybody who handles data, or quantitative evidence, in professional life. These challenges include:\n\nThinking about the mapping from our concerns to the research questions, to the things we measure, to analysis we do, and then the conclusions we make.\nSelecting or constructing valid measures that can be assumed to measure the things they are supposed to measure.\nTaking samples of observations, and making conclusions about the population.\nMaking estimates and linking these estimates to an account that is explicit about causes."
  },
  {
    "objectID": "PSYC122/Week14.html",
    "href": "PSYC122/Week14.html",
    "title": "4. Chi-Square",
    "section": "",
    "text": "This week we will focus on Chi-square as a measure of association between categorical variables."
  },
  {
    "objectID": "PSYC122/Week14.html#lectures",
    "href": "PSYC122/Week14.html#lectures",
    "title": "4. Chi-Square",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week is presented in two parts:\n\nAssociations between categorical variables (~23 min)\nHow to do Chi-square in R (~19 min) You can find the example script in this week’s zip-folder (see under Pre-lab activity 3)."
  },
  {
    "objectID": "PSYC122/Week14.html#reading",
    "href": "PSYC122/Week14.html#reading",
    "title": "4. Chi-Square",
    "section": "Reading",
    "text": "Reading\nThe reading that accompanies the lectures this week is Chapter 19: Chi-square of the core text by Howell (2017). Please note that I might mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Greene & D’Oliveira (2006)."
  },
  {
    "objectID": "PSYC122/Week14.html#pre-lab-activities",
    "href": "PSYC122/Week14.html#pre-lab-activities",
    "title": "4. Chi-Square",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapter you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Calculating Chi-square by hand and interpreting the results\nIs there a relationship between the number of people who smoke and the number of people who drink? Please note that the question is the number of people (frequency) and not how much people drink/smoke. Alos not that these are fictitious data. You’ll need a table of critical values for Chi-square. This can be found in the ‘Week 14 – resources’ folder on Moodle.\n Pre-lab activity questions:\n\nComplete the Pearson’s Chi-square test by hand using the data above and fill in the blanks:\n\n\\(\\chi 2\\) ( , N = ) = , p\n\nCan you reject the null hypothesis?\n\n\n\nPre-lab activity 2: Data visualisation - practice with ggplot2()\nIn this week’s online tutorials, you will practise creating bar chars, a device for visualising the distribution of categorical variables.\nIf you’re ready to begin, go to the tutorial linked to below. There is no need to install or download anything. Each tutorial has everything you need to write and run R code, right in the tutorial.\n\nBar Charts In this tutorial you will learn how to make and enhance bar charts with the ggplot2 package.\n\n\n\nPre-lab activity 3: Getting ready for the lab class\n\nGet your files ready\nDownload the 122_week14_forStudents.zip file and upload it into the new folder in RStudio Server you created."
  },
  {
    "objectID": "PSYC122/Week14.html#lab-activities",
    "href": "PSYC122/Week14.html#lab-activities",
    "title": "4. Chi-Square",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nconducting Pearson’s Chi-square in R\ninterpreting Pearson’s Chi-square in R\nreporting the results in APA format\nwhen and why to apply Pearson’s Chi-square to answer questions in psychological science\n\n\nLab activity 1: Understanding the application of the Chi-square test\n\nQUESTION 1\nHow does Pearson’s chi-square differ from Pearson’s correlation?\n\n\nQUESTION 2\nChi-square test of independence would be appropriate when testing the following questions:\n\n\nWhat is the relationship between gender and soft drink preference? True or False?\n\nb.How do males and females compare in terms of wanting to be a psychologist when they leave school? True or False?\n\n\n\nQUESTION 3\nWrite the chi-square formula below.\n\n\nQUESTION 4\nWhat were your answers to the pre-lab activity 1 questions? Please compare them with other students at your table.\n+a. Complete the Pearson’s Chi-square test by hand using the data above and fill in the blanks:\n\\(\\chi 2\\) ( , N = ) = , p\n+b. Can you reject the null hypothesis?\n\n\nQUESTION 5\nWhy is it recommended to opt for multiple 2 x 2 chi-squares instead of chi-squares larger than 2 x 2?\n\n\nQUESTION 6\nHow could you ‘modify’ the contingency table below for chi-square analysis to aid subsequent interpretation of the data/results?\n\n\n\n\nLab activity 2: Reminders through association\nFor this lab, we’re going to use data from Rogers, T. & Milkman, K. L. (2016). Reminders through association. Psychological Science, 27, 973-986. You can read the full paper online but the short version is that the authors looked at how people remember to follow through with the intention of doing something.\nAlthough there are lots of potential reasons (e.g., some people may lack the self-control resources), Rogers and Milkman (2016) propose that some people fail to follow through simply because they forget about their good intentions. If this is the case, the authors argue, then having visual reminders to follow through on their intentions may help people remember to keep them. For example, a person may choose to put a sticker for their gym on their car window, so that every time they get in the car they remember to go to the gym.\nIn Study 1 by Rogers and colleagues, participants took part in an unrelated experiment but at the start of the task they were asked to return a small stack of paper clips to the reception of the building at the end of the study and if they did so the researchers would donate $1 to a charity. They were then asked if they intended to do this. Those in the reminder-through-association (RTA) condition read “Thank you! To remind you to pick up a paper clip, an elephant statuette will be sitting on the counter as you collect your payment.” This message was followed by a picture of the elephant statuette. Those in the control condition simply read “Thank you!”.\nWhat we want to do is to run a chi-square analysis to determine whether those in the RTA condition were more likely to remember to return the paper-clips than those in the control condition. Open the 122_wk14_labAct2.R script in RStudio and work your way through it. All instructions, hints and questions are contained in the script."
  },
  {
    "objectID": "PSYC122/Week14.html#answers",
    "href": "PSYC122/Week14.html#answers",
    "title": "4. Chi-Square",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\nLab activity 1\n\nHow does Pearson’s Chi-square differ from Pearson’s correlation? Pearson’s Chi-square assesses whether there is a relationship between categorical (or nominal) variables. Pearson’s correlation assesses whether there is a relationship between continuous (or interval/ratio) variables.\nChi-square test of independence would be appropriate when testing the following questions:\n\n\n\nWhat is the relationship between gender and soft drink preference? True\n\n\nHow do males and females compare in terms of wanting to be a psychologist when they leave school? True\n\n\n\nWrite the chi-square formula below. \nWhat were your answers to the pre-lab activity 1 questions? Please compare them with other students at your table.\n\n\n\nComplete the Pearson’s chi-square test by hand using the data above and fill in the blanks:\n\n\n\nFirst determine expected frequencies: Smoke/drink: (70 x 65) / 110 = 41.36 Smoke/don’t drink (70 x 45) / 110 = 28.63 Don’t smoke/drink (40 x 65) / 110 = 23.63 Don’t smoke/don’t drink (40 x 45) / 110 = 16.36\nThen calculate chi-square: Smoke/drink: (50-41.36)2 / 41.36 = 1.80 Smoke/don’t drink: (20-28.63)2 / 28.63 = 2.60 Don’t smoke/drink: (15-23.63)2 / 23.63 = 3.15 Don’t smoke/don’t drink: (25-16.36)2 / 16.36 = 4.56 1.80+2.60+3.15+4.56 = 12.11\n\\(\\chi 2\\) (1, N = 110) = 12.11, p &lt; .001\n\n\nCan you reject the null hypothesis? Yes\n\n\n\nWhy is it recommended to opt for multiple 2 x 2 chi-squares instead of chi-squares larger than 2 x 2? It is easier to interpret and such a design requires a smaller sample size.\nHow could you ‘modify’ the contingency table below for chi-square analysis to aid subsequent interpretation of the data/results?\n\n\nBy combining ‘interested’ and ‘somewhat interested’ or by partitioning (doing multiple 2 x 2 chi-squares, while using Bonferroni correction to account for running multiple tests\n\n\nLab activity 2\nPlease see the 122_wk14_labAct2_withAnswers.R script for the relevant code and answers to questions."
  },
  {
    "objectID": "PSYC122/Week17.html",
    "href": "PSYC122/Week17.html",
    "title": "7. Linear Models",
    "section": "",
    "text": "Welcome to your overview of the materials you will work with in PSYC122 Week 17.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the week 17 class, we will aim to answer the research question:\n\nWhat person attributes predict success in understanding?"
  },
  {
    "objectID": "PSYC122/Week17.html#sec-week-17-intro",
    "href": "PSYC122/Week17.html#sec-week-17-intro",
    "title": "7. Linear Models",
    "section": "",
    "text": "Welcome to your overview of the materials you will work with in PSYC122 Week 17.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the week 17 class, we will aim to answer the research question:\n\nWhat person attributes predict success in understanding?"
  },
  {
    "objectID": "PSYC122/Week17.html#our-learning-goals",
    "href": "PSYC122/Week17.html#our-learning-goals",
    "title": "7. Linear Models",
    "section": "Our learning goals",
    "text": "Our learning goals\nIn Week 17, we aim to further develop skills in analyzing and in visualizing psychological data.\n\n\n\n\n\n\nTip\n\n\n\n\nIn psychological science, research questions like our question can be examined using linear models.\n\n\n\nWhen we do these analyses, we will need to think about how we report the results:\n\nwe usually need to report information about the kind of model we specify;\nwe will need to report the nature of the association estimated in our model;\nand we usually need to decide, is the association significant?\ndoes the association reflect a positive or negative relationship between outcome and predictor?\nis the association we see in our sample data relatively strong or weak?\n\n\n\n\n\n\n\nTip\n\n\n\n\nWe will aim to build skills in producing professional-looking plots for our audiences.\n\n\n\nAt every stage, as we work, we will visualize the data to:\n\nUnderstand the shape of the relationships we may observe or predict."
  },
  {
    "objectID": "PSYC122/Week17.html#sec-w17-resources-intro",
    "href": "PSYC122/Week17.html#sec-w17-resources-intro",
    "title": "7. Linear Models",
    "section": "Your resources",
    "text": "Your resources\nYou will see – below – links to the lectures, information about the data we will analyze, and an explanation of the activities.\nAll the links to the lectures, and everything you need for your practical work class can also be found under the Week 17 resources section title, on Moodle:\nLink to Moodle"
  },
  {
    "objectID": "PSYC122/Week17.html#lectures-video-recordings",
    "href": "PSYC122/Week17.html#lectures-video-recordings",
    "title": "7. Linear Models",
    "section": "Lectures: video recordings",
    "text": "Lectures: video recordings\nThe lecture material for this week is presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. (You will need to be on campus or logged in to the university VPN to get access to the videos.)\nPart 1 of 4; about 19 minutes\nPart 2 of 4; about 11 minutes\nPart 3 of 4; about 9 minutes\nPart 4 of 4; about 15 minutes\nThe lectures have two main areas of focus.\n1. Understanding the scientific process\nI outline the steps through which a psychological scientist may progress, in logic and practice, from research questions to hypotheses to analyses.\nI rehearse some of the key points I have discussed, previously, in order to build a framework in which you can understand how we go from:\n\na set of concerns – here: What makes it easy or difficult to understand written health information?\nthrough choices about what data to collect, and how\nto specific questions, and then predictions\nthat we can test using linear models.\n\nWe are learning data analysis methods. But the key point is that we use these methods in the context of a research project with concerns, aims, methodological assumptions, and choices. This is generally true so the aim is to present a concrete example of how research works.\nAs part of the discussion, I raise questions you might want to consider. These questions – and questions you can originate for yourselves – are also part of the context for our data analysis, because they help to inform how you interpret or evaluate the results. These questions are examples of the critical evaluation that you will need to develop through your studies.\n2. The linear model\nWe look at how the linear model can be used to address research questions in the context of the Clearly understood health comprehension project. But I aim to outline some general ideas about why we use the linear model technique, and how it works. I build on work you have done with Margriet Groen in earlier PSYC122 classes, so that we can strengthen understanding, and extend skills.\n\n\n\n\n\n\nTip\n\n\n\nTo work with the recordings:\n\nWatch the video parts right through.\nUse the printable versions of the slides (provided on Moodle) to make notes.\nTry out the coding exercises in the how-to and the workbook (see Section 5.1) to see for yourself how you can construct visualizations and do analyses.\n\n\n\nIn the lecture, I talk about how we use the linear model to estimate the association between (1.) an outcome like mean accuracy of understanding and (2.) a predictor like vocabulary knowledge: estimating the association as the expected change in the average outcome given variation in the predictor.\nIn the plot on the right of Figure 1, we show the distribution curve of mean (comprehension) accuracy scores observed at each value of vocabulary. You can see that the middle – the average – of each distribution, marked by a line, increases as we go from left (low scores) to right (high scores).\n\n\n\n\n\nFigure 1: Plots showing the association between the outcome mean accuracy of understanding a predictor, vocabulary knowledge. Both plots show the same data. The plot on the right is modified to show how accuracy of understanding varies between individuals in the sample with the sample vocabulary test scores\n\n\n\n\nIn the lecture, I talk about how the information we get from a linear model allows us to predict the way in which outcome values may vary (increase or decrease), given different values in the predictor variable.\nWe could form a prediction line anywhere but the linear model helps us to estimate the prediction (“best fit”) line that minimizes the differences between observed and predicted outcomes: the residuals, as shown in Figure 2.\n\n\n\n\n\nFigure 2: Plot showing the prediction of mean accuracy of understanding, given information about participant vocabulary knowledge, with lines drawn to show the difference between observed outcomes (shown in orange-red) and predicted outcomes (shown as black circles on the blue line) for each vocabulary test score value in our sample\n\n\n\n\nThe lectures end with a discussion of the critical information you must identify and extract when you view the summary of a linear model results.\nI then show you how to report the results. I give you examples of the conventional language you can use to report your results.\n\n\n\n\n\n\nTip\n\n\n\nIn reporting linear model results, we need to explain what they tell us about the association between outcome and predictor variables.\n\n\nWe can use visualization to help us to interpret the model estimates. In the how-to guide and in the lab activity workbook (see Section 5.1), we look at how you can draw prediction plots, given model estimates.\n\nLinks to other classes\nWe do not provide further reading for this class but you will find it helpful to revise some of the key ideas you have been learning about, in PSYC122 and in other modules.\n\nThe lectures in PSYC123 on: (week 2) reliability and validity; (week 3) experimental design, especially between-subjects studies and individual differences; and (week 9) precise hypotheses.\nThe lecture in PSYC122 (week 13) introducing the linear model.\n\n\n\nPre-lab activity 1\nIn weeks 16-19, we will be working together on a research project to investigate how people vary in their response to health advice.\nCompleting the project involves collecting responses from PSYC122 students.\nTo enter your responses, we invite you to complete a short survey.\nComplete the survey by clicking on the link here\n\n\n\n\n\n\nTip\n\n\n\nIn our week 19 class activity, we will analyze the data we collect here.\n\n\n\nSurvey information\nThe survey asks you to:\n\ncomplete some questions about who you are;\nand then answer some questions about what you know about some English words, about what you know about health matters, and about how you approach reading.\n\nThe survey then asks you to:\n\nread five short extracts from patient information leaflets about different kinds of health issue;\nrespond to some multiple choice questions about each extract;\nand rate how well you think you understand the advice.\n\nThe survey should take about 20 minutes to complete. Some people will take less time, and some people might take a little more time.\nTaking part in the survey is completely voluntary. You can stop at any time without completing the survey if you do not want to finish it. If you do not want to do the survey, you can do an alternative activity (see below).\nAll responses will be recorded completely anonymously.\n\n\n\nPre-lab activity 1 alternative\nIf you do not want to complete the survey, we invite you to read the pre-registered research plan for the PSYC122 health advice research project.\nRead the project pre-registration"
  },
  {
    "objectID": "PSYC122/Week17.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "href": "PSYC122/Week17.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "title": "7. Linear Models",
    "section": "Pre-lab activity 2: Getting ready for the lab class",
    "text": "Pre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 122-22-w17_for-students.zip files you need and upload them to your RStudio Server.\nThe folder includes the data files:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nand the code files:\n\n2022-23-PSYC122-w17-how-to.R\n2022-23-PSYC122-w17-workbook.R\n\nYou will use 2022-23-PSYC122-w17-workbook.R in the lab activity practical class.\nAlternatively, you can instead download the resources you need from the week 17 section of the Moodle page for the PSYC122 module:\nLink to Moodle\n\nWhat is in the how-to and workbook.R files?\n\n\n\n\n\n\nImportant\n\n\n\n\nOur aim is to make sure you can work with code, and write notes in the .R files.\n\n\n\nIn the workbook.R file you use for the lab activity, we identify tasks and questions, and leave you spaces where you can write code or answers.\nIn both the .R files:\n\n2022-23-PSYC122-w17-how-to.R\n2022-23-PSYC122-w17-workbook.R\n\nwe will take things step-by-step.\n\n\n\n\n\n\nTip\n\n\n\n\nMake sure you start at the top of the .R file and work your way, in order, through each task.\nComplete each task before you move on to the next task.\n\n\n\nThe how-to guide comprises an .R file 2022-23-PSYC122-w17-how-to.R with code and advice. The code in the .R file was written to work with the data file:\n\nstudy-one-general-participants.csv.\n\n\n\n\n\n\n\nTip\n\n\n\nWe show you how to do everything you need to do in the lab activity (Section 6) in the how-to guide.\n\nStart by looking at the how-to guide to understand what steps you need to follow in the lab activity."
  },
  {
    "objectID": "PSYC122/Week17.html#sec-w17-activity",
    "href": "PSYC122/Week17.html#sec-w17-activity",
    "title": "7. Linear Models",
    "section": "Lab activity",
    "text": "Lab activity\nIn the lab activity .R file 2022-23-PSYC122-w17-workbook.R, you will work with data from a study about how people respond to guidance about a variety of health topics (general topics):\n\nstudy-two-general-participants.csv\n\nThe data are similar in format to the response data we are collecting as part of the PSYC122 project.\n\nTasks\nIn the activity, we are going to work through the following tasks.\n\nEmpty the R environment – using rm(list=ls())\nLoad relevant libraries – using library()\nRead in the data file – using read_csv()\nInspect the data – using head() and summary()\nVisualize the distribution of values – using geom_histogram()\nEdit the appearance of the histogram plots, using binwidth, theme_bw(), labs(), xlim() and geom_vline()\nVisualize the potential association between the values of two variables by producing a scatterplot using geom_point()\nEdit the appearance of the scatterplots by using the geom_point() arguments alpha, size, colour, and shape, and by changing the axis limits using xlim() and ylim().\nRevise how you test the associations between pairs of variables through correlation analyses using cor.test()\nExamine the predictive relation between outcome and predictor variables using lm()\nVisualize the model predictions – using geom_abline() and information from the model results\n\nThe activity 2022-23-PSYC122-w17-workbook.R file takes you through the tasks, one by one.\n\n\n\n\n\n\nTip\n\n\n\nIf you are unsure about what you need to do, look in 2022-23-PSYC122-w17-how-to.R.\n\n\nIn the how-to, you will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code. You will need to change the names of the dataset or the variables to complete the tasks in the activity.\n\n\nWhat is in the data files\nEach of the data files we will work with has a similar structure.\nYou can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy varianble coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\nAnswers\n\n\n\n\n\n\nTip\n\n\n\nYou can now download the answers version of the activity workbook .R here.\n\n\nThe answers version presents my answers for questions, and some extra information where that is helpful."
  },
  {
    "objectID": "PSYC122/Week16.html",
    "href": "PSYC122/Week16.html",
    "title": "6. Data Visualisation",
    "section": "",
    "text": "Welcome to your overview of the materials and guidance you will work with in PSYC122 Week 16.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\nWe encounter written health information all the time: in warnings signs, on medication labels, in clinics when we go to see the doctor, and online when we research things we are worried about. It is not always easy to understand this information. The problem is that it is unclear how health information should be communicated. As psychologists, we can help to improve health communication.\n\n\n\n\n\n\nImportant\n\n\n\nIn these classes, we will complete a research project to answer the research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\n\n\nWe will present our PSYC122 lessons in the context of this research project because we think that this context will help you to make sense of the data, and to see why we ask you to practice the skills we are teaching.\nWe will be revisiting some of the ideas and techniques you have seen introduced in previous classes. This is to give you the opportunity to revise and consolidate your learning. We will extend your development with some new ideas, to strengthen your skills.\nUltimately, we aim to contribute new findings from the data we will collect together. These new findings will, we hope, help to make the provision of health advice a bit more useful in future.\n\n\nIn Week 16, we will ask you to do three things.\n\n\n\n\n\n\nNote\n\n\n\nFirst, we will ask you to do a pre-lab activity that involves completing a survey.\n\n\n\nCompleting the survey will help you to make sense of the numbers you will be working with in the activities.\nCompleting the pre-lab activity will help to teach you about the challenges of measurement, a key aspect of the scientific thinking skills we will help you to develop.\n\n\n\n\n\n\n\nNote\n\n\n\nSecond, we will ask you to do a set of practical tasks in the lab activity that are designed to consolidate your learning on data visualization.\n\n\n\nWe will be:\n\n\nUsing histograms to examine the distributions of variables;\nLearning to edit the histograms to present them professionally.\n\n\n\n\n\n\nFigure 1: How-to guide example of a histogram showing observed mean accuracy of understanding of health information\n\n\n\n\n\nWe will be:\n\n\nUsing scatterplots to examine the association of variables;\nLearning to edit the plots to present them professionally.\n\n\n\n\n\n\nFigure 2: How-to guide example of a scatterplot representing the potential association between values of health literacy (HLVA) and mean self-rated accuracy of understanding of health information.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThird, we will ask you to think critically about predictions about potential associations between measures of participant attributes and measures of understanding of health information.\n\n\n\nWe will use correlations to test predictions about associations and thus answer research questions."
  },
  {
    "objectID": "PSYC122/Week16.html#sec-week-16-intro",
    "href": "PSYC122/Week16.html#sec-week-16-intro",
    "title": "6. Data Visualisation",
    "section": "",
    "text": "Welcome to your overview of the materials and guidance you will work with in PSYC122 Week 16.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\nWe encounter written health information all the time: in warnings signs, on medication labels, in clinics when we go to see the doctor, and online when we research things we are worried about. It is not always easy to understand this information. The problem is that it is unclear how health information should be communicated. As psychologists, we can help to improve health communication.\n\n\n\n\n\n\nImportant\n\n\n\nIn these classes, we will complete a research project to answer the research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\n\n\nWe will present our PSYC122 lessons in the context of this research project because we think that this context will help you to make sense of the data, and to see why we ask you to practice the skills we are teaching.\nWe will be revisiting some of the ideas and techniques you have seen introduced in previous classes. This is to give you the opportunity to revise and consolidate your learning. We will extend your development with some new ideas, to strengthen your skills.\nUltimately, we aim to contribute new findings from the data we will collect together. These new findings will, we hope, help to make the provision of health advice a bit more useful in future.\n\n\nIn Week 16, we will ask you to do three things.\n\n\n\n\n\n\nNote\n\n\n\nFirst, we will ask you to do a pre-lab activity that involves completing a survey.\n\n\n\nCompleting the survey will help you to make sense of the numbers you will be working with in the activities.\nCompleting the pre-lab activity will help to teach you about the challenges of measurement, a key aspect of the scientific thinking skills we will help you to develop.\n\n\n\n\n\n\n\nNote\n\n\n\nSecond, we will ask you to do a set of practical tasks in the lab activity that are designed to consolidate your learning on data visualization.\n\n\n\nWe will be:\n\n\nUsing histograms to examine the distributions of variables;\nLearning to edit the histograms to present them professionally.\n\n\n\n\n\n\nFigure 1: How-to guide example of a histogram showing observed mean accuracy of understanding of health information\n\n\n\n\n\nWe will be:\n\n\nUsing scatterplots to examine the association of variables;\nLearning to edit the plots to present them professionally.\n\n\n\n\n\n\nFigure 2: How-to guide example of a scatterplot representing the potential association between values of health literacy (HLVA) and mean self-rated accuracy of understanding of health information.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThird, we will ask you to think critically about predictions about potential associations between measures of participant attributes and measures of understanding of health information.\n\n\n\nWe will use correlations to test predictions about associations and thus answer research questions."
  },
  {
    "objectID": "PSYC122/Week16.html#sec-resources-intro",
    "href": "PSYC122/Week16.html#sec-resources-intro",
    "title": "6. Data Visualisation",
    "section": "Your resources",
    "text": "Your resources\nYou will see – below – links to the lectures, information about the data we will analyze, and an explanation of the activities.\nAll the links to the lectures, and everything you need for your practical work class can also be found in the Week 16 files folder on Moodle, here:\nLink to Moodle\n\nLectures: video recordings\nThe lecture material for this week is presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. (You will need to be on campus or logged in to the university VPN to get access to the videos.)\nPart 1 of 4; about 15 minutes\nPart 2 of 4; about 11 minutes\nPart 3 of 4; about 22 minutes\nPart 4 of 4; about 11 minutes\nThe lecture is designed to give you an overview of:\n\nWhat we are doing in weeks 16-20, and how and why you will develop your critical thinking skills;\nA summary of the health communication project, and the ideas we assume to develop our hypotheses;\nHow we visualize and think about distributions and associations;\nAnd how we use R to estimate and test correlations.\n\n\n\n\n\n\n\nTip\n\n\n\nTo work with the recordings:\n\nWatch the video parts right through.\nUse the printable versions of the slides (provided on Moodle) to make notes.\nTry out the coding exercises in the how-to and the workbook (see Section 2.5.1) to see for yourself how you can construct visualizations and do analyses.\n\n\n\n\n\nLinks to other classes\nWe do not provide further reading for this class but you will find it helpful to revise some of the key ideas you have been learning about PSYC122 and in other modules.\n\nThe lectures in PSYC123 on: (week 1) the scientific method; (week 2) reliability and validity; (week 3) experimental design, especially between-subjects studies; (week 6) hypothesis testing; and (week 9) precise hypotheses.\nThe lecture in PSYC122 on (week 11) correlation.\n\n\n\nPre-lab activity 1\nIn weeks 16-19, we will be working together on a research project to investigate how people vary in their response to health advice.\nCompleting the project involves collecting responses from PSYC122 students.\nTo enter your responses, we invite you to complete a short survey.\nComplete the survey by clicking on the link here\n\n\n\n\n\n\nTip\n\n\n\nIn our week 19 class activity, we will analyze the data we collect here.\n\n\n\nSurvey information\nThe survey asks you to:\n\ncomplete some questions about who you are;\nand then answer some questions about what you know about some English words, about what you know about health matters, and about how you approach reading.\n\nThe survey then asks you to:\n\nread five short extracts from patient information leaflets about different kinds of health issue;\nrespond to some multiple choice questions about each extract;\nand rate how well you think you understand the advice.\n\nThe survey should take about 20 minutes to complete. Some people will take less time, and some people might take a little more time.\nTaking part in the survey is completely voluntary. You can stop at any time without completing the survey if you do not want to finish it. If you do not want to do the survey, you can do an alternative activity (see below).\nAll responses will be recorded completely anonymously.\n\n\n\nPre-lab activity 1 alternative\nIf you do not want to complete the survey, we invite you to read the pre-registered research plan for the PSYC122 health advice research project.\nRead the project pre-registration\n\n\nPre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 122-22-w16_for-students.zip files you need and upload them to your RStudio Server.\nThe folder includes data files:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nand the code files:\n\n2022-23-PSYC122-w16-how-to.R\n2022-23-PSYC122-w16-workbook.R\n\nAlternatively, you can instead download the resources you need from the module Moodle page for PSYC122:\nLink to Moodle\n\n\nWhat is in the how-to and workbook.R files?\n\n\n\n\n\n\nImportant\n\n\n\nYou have been getting used to working with .R script files.\n\nNow our aim is to make sure you can work with code, and write notes in the files.\nIn the workbook you use for the lab activity, we identify tasks and questions, and leave you spaces where you can write code or answers.\n\n\n\nIn both the .R files:\n\n2022-23-PSYC122-w16-how-to.R\n2022-23-PSYC122-w16-workbook.R\n\nwe will take things step-by-step.\nWe split .R scripts into parts, tasks and questions:\n\ndifferent parts for different phases of the analysis process;\ndifferent tasks for different steps in each phase;\ndifferent questions to examine different ideas or coding steps.\n\n\n\n\n\n\n\nTip\n\n\n\n\nMake sure you start at the top of the .R file and work your way, in order, through each task.\nComplete each task before you move on to the next task.\n\n\n\n\n\n\nReview the how-to guide\nThe how-to guide comprises an .R file 2022-23-PSYC122-w16-how-to.R with code and advice. The code in the .R file was written to work with the data file:\n\nstudy-one-general-participants.csv.\n\n\n\n\n\n\n\nTip\n\n\n\nWe show you how to do everything you need to do in the lab activity (Section 2.7) in the how-to guide.\n\nStart by looking at the how-to guide to understand what steps you need to follow in the lab activity.\n\n\n\n\n\nLab activity\nIn the lab activity .R file 2022-23-PSYC122-w16-workbook.R, you will work with data from a study about how people respond to guidance about a variety of health topics (general topics):\n\nstudy-two-general-participants.csv\n\nThe data are similar in format to the response data we are collecting as part of the PSYC122 project.\n\nTasks\nIn the activity workbook, we are going to work through the following tasks.\n\nEmpty the R environment – using rm(list=ls())\nLoad relevant libraries – using library()\nRead in the data file – using read_csv()\nInspect the data – using head() and summary()\nChange the type classification of a variable in the data – using as.factor()\nDraw histograms to examine the distributions of variables – using ggplot() and geom_histogram()\nEdit the appearance of one variable histogram plot step-by-step\nDraw scatterplots to examine the associations between some variables – using ggplot() and geom_point()\nDraw scatterplots to examine different variables\nEdit the appearance of a plot step-by-step\nExamine associations between variables using correlation.\n\nThe activity 2022-23-PSYC122-w16-workbook.R file takes you through the tasks, one by one.\n\n\n\n\n\n\nTip\n\n\n\nIf you are unsure about what you need to do, look at the advice in 2022-23-PSYC122-w16-how-to.R on what steps you have to follow, and examples on how to write the code.\n\n\nYou will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code.\n\nDon’t forget: You will need to change the names of the dataset or the variables to complete the tasks in the activity.\n\nThis process of adapting demonstration code is a process critical to data literacy and to effective problem solving in working with data in psychological science.\n\n\nWhat is in the data files\nEach of the data files we will work with has a similar structure, as you can see in this extract.\n\n\n\n\n\nparticipant_ID\nmean.acc\nmean.self\nstudy\nAGE\nSHIPLEY\nHLVA\nFACTOR3\nQRITOTAL\nGENDER\nEDUCATION\nETHNICITY\n\n\n\n\nstudytwo.1\n0.4107143\n6.071429\nstudytwo\n26\n27\n6\n50\n9\nFemale\nHigher\nAsian\n\n\nstudytwo.10\n0.6071429\n8.500000\nstudytwo\n38\n24\n9\n58\n15\nFemale\nSecondary\nWhite\n\n\nstudytwo.100\n0.8750000\n8.928571\nstudytwo\n66\n40\n13\n60\n20\nFemale\nHigher\nWhite\n\n\nstudytwo.101\n0.9642857\n8.500000\nstudytwo\n21\n31\n11\n59\n14\nFemale\nHigher\nWhite\n\n\n\n\n\n\n\nYou can use the scroll bar at the bottom of the data window to view different columns.\nYou can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy varianble coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\n\n\n\n\nTip\n\n\n\nIt is always a good idea to view the dataset – click on the name of the dataset in the R-Studio Environment window, and check out the columns, scroll through the rows – to get a sense of what you are working with.\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nTip\n\n\n\nYou can now download the answers version of the activity workbook .R here.\n\n\nThe answers version presents my answers for questions, and some extra information where that is helpful."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/knowledge-ecosystem.html",
    "href": "PSYC402/extra_ToBeOrganised/knowledge-ecosystem.html",
    "title": "R knowledge",
    "section": "",
    "text": "R knowledge\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/intro.html",
    "href": "PSYC402/extra_ToBeOrganised/intro.html",
    "title": "Introduction: the why",
    "section": "",
    "text": "The research report assignment requires students to locate, access, analyse and report previously collected data. This introduction is intended to answer the first question anybody might ask.\n\nWhy: what is the motivation for the assignment?\n\nIn following materials, I will answer the questions.\n\nHow can the assignment be done?\nWhat do we expect students to do?\n\nIt is going to appear, at first, that I am going a long way away from telling you what you need to do for the assignment. I hope you will agree that the discussion that follows is worth your time in reading it. It will help you to understand why we are asking you to do the assignment, and why we are looking for what we are looking for. It will help you to understand how this work will aid your development. And it will help to show how doing the assignment furnishes the opportunity for research experience that will help you later in your working life.\nFor those who are more eager to start the work, here are the links to the what information in ?@sec-what and to the how information ?@sec-how.\n\n\nThere are two ideas motivating our approach. It will be helpful to you if I sketch them out early, here. We can demonstrate the usefulness of these ideas as we progress through our work.\nThe first key idea is expressed clearly in sociological discussions of science. This is that there is a difference between science “…being done, science in the making, and science already done, a finished product …” [@bourdieu2004; p.2]. The awareness we want to develop is that there are two things: there is the story that may be presented in a textbook or in a lecture about scientific work or scientific claims; and there is the work we do in practice, as we develop graduate skills, and as we exercise those skills professionally in the workplace.\nThe second key idea connects to the first. This idea is that reported analyses are not necessary or sufficient to the data or the question. What does this mean? It means that the same data can reasonably be analysed in different ways. There is no necessary way to analyse some data though there may be conventions or normal practices [@kuhn1970]. It means that it is unlikely that any one analysis will do all the work that could be done (a sufficiency) to get you from your data to useful or reasonable answers to your questions.\nThese ideas may be unsettling but they are realistic. Stating them will better prepare you for professional work. In the workplace, the accuracy of these ideas will emerge when you see how a team in any sector (health, marketing …) gets from its data to its product. If we talk about the ideas now, we can get you ready for dealing with the practical and the ethical concerns you will confront when that happens.\nWe will begin by discussing psychological research, and research about psychological research, to answer the question: Why: what is the motivation for the assignment? We will then move to answering the what and the how questions.\n\n\n\n\n\nWe are here because we are interested in humans and human behaviour, and because we are interested in scientific methods of making sense of these things. Some of us are aware that science (including psychological science) has undergone a rolling series of crises: the replicability or replication crisis [@Pashler2012a; @Pashler2012b]; the statistical crisis [@gelman2014a]; and the generalizability crisis [@yarkoni2022]. And that science is undergoing a response to these crises, evidenced in the advocacy of pre-registration [@nosek2018; @nosek2019prereg], and of registered reports [@nosek2014], the use of open science badges (e.g., for the journal Psychological Science), the completion of large-scale replication studies [@aarts2015], and the identification of open science principles [@munafò2017]. We may usefully refer, collectively, to the crises and the responses, as the credibility revolution [@vazire2018]\nWe could teach a course on this (in Lancaster, we do) but I must be brief, here, and invite you to follow the references, if you are interested. Before going on, I want to call your attention to the fact that important elements of the hard work in trying to make science work better has been led by PhD students and by junior researchers [e.g., @herndon2014]. Graduate students may, at first, assume that the fact that a research article has been published in a journal means the findings that are reported must be true. Most of the time, some educated skepticism is more appropriate. An important driver of the realization that there are problems evident in the literature, and that there are changes we can make to improve practice, comes from independent post-publication review work exposing the problems in published work (see, e.g., this account by Andrew Gelman)\n\n\n\n\n\n\nTip\n\n\n\n\nAllow yourself to feel skeptical about the reports you read then work with the motivation this feeling provides.\n\n\n\nIn brief, then, most practicing scientists now understand or should understand that many of the claims we encounter in the published scientific literature are unlikely to be supported by the evidence [@Ioannidis2005], whether we are looking at the evidence of the results in the reports themselves, or evidence in later attempts to find the same results [e.g., @aarts2015]. We suspect that this may result from a number of causes. We understand that researchers may engage in questionable research practices [@john2012]. We understand that researchers may exploit the potential for flexibility in doing and reporting analyses [@Simmons2011a]. We understand that there are problems in how psychologists use or talk about the measurement of psychological constructs [@flake2020]. We understand that there are problems in how psychologists sample people for their studies, both in where we recruit [@bornstein2013; @wild2022; @Henrich2010], and in how many we recruit [@button2013; @cohen1962; @sedlmeier1989; @vankov2014]. We understand that there are problems in how psychologists specify or think about their hypotheses or predictions [@meehl1967; @scheel2022]. And we understand that there are problems in how scientists do, or rather do not, comply with good practice recommendations designed to fix these problems (discussed further in the following).\nThis discussion could (again) be unsettling. This list of problems could make you angry or sad. I, like others, think it is exciting. It is exciting because these problems have probably existed for a long time [e.g., @cohen1962; @meehl1967] and now, having identified the problems, we can hope to do something about it. It is exciting because if you care about people, the study of people, or the applications in clinical, education and other domains of the results of the study of people, then you might hope to see better, more useful, science in the future [@vazire2018].\nAs someone who teaches graduate and undergraduate students, I want to help you to be the change you want to see in the world 1. We cannot solve every problem but we can try to do better those things that are within our reach. I am going to end this introduction with a brief discussion of some ideas we can use to guide our better practices.\n\n\n\nIn this course, for this assignment, we are going to focus on:\n\nmultiverse analyses\nkinds of reproducibility\nthe current state of the match between open science ideas and practices\n\nIn the classes on the linear model, we will discuss:\n\nthe links between theory, prediction and analysis\npsychological measurement\nsamples\nvariation in results\n\n\n\n\n\n\nI am going to link this discussion to a metaphor (see Figure Figure 1) or a description you will find useful: the data analysis pipeline or workflow.\n\n\n\n\n\n\n\n\nQ\n\n \n\ncluster_R\n\n   \n\nnd_1\n\n Get raw data   \n\nnd_2\n\n Tidy data   \n\nnd_1-&gt;nd_2\n\n    \n\nnd_3_l\n\n Visualize   \n\nnd_2-&gt;nd_3_l\n\n    \n\nnd_3\n\n Analyze   \n\nnd_2-&gt;nd_3\n\n    \n\nnd_3_r\n\n Explore   \n\nnd_2-&gt;nd_3_r\n\n    \n\nnd_3_a\n\n Assumptions   \n\nnd_3_a-&gt;nd_3_l\n\n    \n\nnd_3_a-&gt;nd_3\n\n    \n\nnd_3_a-&gt;nd_3_r\n\n    \n\nnd_3_l-&gt;nd_3\n\n   \n\nnd_4\n\n Present   \n\nnd_3_l-&gt;nd_4\n\n    \n\nnd_3-&gt;nd_4\n\n   \n\n\nFigure 1: The data analysis pipeline or workflow\n\n\n\n\nThis metaphor or way of thinking is very common (take a look at the diagram in Wickham and Grolemund’s 2017 book “R for Data Science) and you may see the words “data pipeline” used in job descriptions, or you may benefit from saying, in a job application, something like: I am skilled in designing and implementing each stage of the quantitative data analysis pipeline, from data tidying to results presentation. I say this because scientists I have mentored got their jobs because they can do these things – and successfully explained that they can do these things – in sectors like educational testing, behavioural analysis, or public policy research.\nThe reason this metaphor is useful is that it helps us to organize our thinking, and to manage what we do when we do data analysis, we:\n\nget some data;\nprocess or tidy the data;\nexplore, visualize, and analyze the data;\npresent or report our findings.\n\nWe introduce the idea that your analysis work will flow through the stages of a pipeline from getting the data to presenting your findings because, next, we will examine how pipelines can multiply.\n\n\n\n\n\n\nTip\n\n\n\n\nAs you practice your data analysis work, try to identify the elements and the order of your work, as the parts of a workflow.\n\n\n\n\n\n\nWhat researchers have come to realize: because we started looking … The open secret that has been well kept [@bourdieu2004]: because everybody who does science knows about it, yet we may not teach it; and because we do not write textbooks revealing it … Is that at each stage in the analysis workflow, we can and do make choices where multiple alternative choices are possible. @gelman2014 capture this insight as the “garden of forking paths”2 (see Figure 2).\nThe general idea is that it is possible to have multiple potential different paths from the data to the results. The results will vary, depending on the path we take. In an analysis, we could take multiple different paths simply because at point A we decide to do B1, B2 or B3, maybe we choose B1, and then at point B1, we may decide to do C1, C2 or C3. Here, maybe we have our raw data at point A. Maybe we could do one of two different things when we tidy the data: action B1 or B2. Then, when we have our tidy data, maybe we can choose to do our analysis in one of six ways. Where we are at each step depends on the choices we made at the previous steps.\n\n\n\n\n\n\n\n\nD\n\n  \n\nA\n\n A   \n\nB1\n\n B1   \n\nA-&gt;B1\n\n    \n\nB2\n\n B2   \n\nA-&gt;B2\n\n    \n\nC1\n\n C1   \n\nB1-&gt;C1\n\n    \n\nC2\n\n C2   \n\nB1-&gt;C2\n\n    \n\nC3\n\n C3   \n\nB1-&gt;C3\n\n    \n\nC4\n\n C4   \n\nB2-&gt;C4\n\n    \n\nC5\n\n C5   \n\nB2-&gt;C5\n\n    \n\nC6\n\n C6   \n\nB2-&gt;C6\n\n   \n\n\nFigure 2: Forking paths in data analysis\n\n\n\n\nIn the end, it may appear to us that we took one path or that only one path was possible. When we report our analysis, in a dissertation or in a published journal article, we may report the analysis as if only one analysis path had been considered. But, critically, our findings may depend on the choices we made and this variation in results may be hidden from view.\nI am talking about forking paths because the multiplicity of paths has consequences, and we discuss these next.\n\n\n\n\n\n\nTip\n\n\n\n\nIt is about here, I hope, that you can start to see why it would makes sense to access data from a published study and to examine if you can get the same results as the study authors.\n\n\n\n\n\n\n\nI am going to discuss, now, what are commonly called multiverse analyses. Psychologists use this term, having been introduced to it in an influential paper by @steegen2016, but it comes from theoretical physics (take a look at wikipedia).\nI explain this because I do not want you to worry. The ideas themselves are within your grasp whatever your background in psychology or elsewhere. It is the implications for our data analysis practices that are challenging. They are challenging because what we discuss should increase your skepticism about the results you encounter in published papers. And they are challenging because they reveal your freedom to question whether published authors could have done their analysis in a different way.\nWe are going to look at:\n\ndataset construction\nanalysis choices\n\n\n\nIn first discussing the wider context (of crisis and revolution), then discussing the specific context (of multiverses and, in the following, of reproducibility), I should be clear about the link between the two things. The finding that some results may not be supported by the evidence is probably due to a mix of causes. But one of those causes will be the combination of uncertainty over data processing or the uncertainty over analysis methods revealed in multiverse analyses, as we see next, combined with the limitations of data and code sharing, and the incompleteness of results reporting (as we see later).\n\n\n\nWhen you collect or access data for a research study, the complete raw dataset you receive is almost never the complete dataset you analyze or whose analysis you report. This is not a story about deliberately cheating. It is a story about the normal practice of science [@kuhn1970].\nPicture some common scenarios. You did a survey, you got responses from a 100 participants on 10 questions, and you asked people to report their education, ethnicity and gender. You did an experimental study, you tested two groups of 50 people each in 100 trials (imagine a common task like the Stroop test), and you observed the accuracy and the timing of their responses. You tested 100 children, 20 children in each of five different schools, on a range of educational ability measures.\nIn these scenarios, the psychologist or the analyst of behavioural data must process their data. In doing so, you will ask yourself a series of questions like:\n\nhow do we code for gender, ethnicity, education?\nwhat do we about reaction times that are very short, e.g., \\(RT &lt; 200ms\\) or very long, e.g., \\(RT &gt; 1500ms\\))?\nif we present multiple questions measuring broadly the same thing (e.g. how confident are you that you understand what you have read? how easy did you find what you read?) how do we summarize the scores on those questions? do we combine scores?\nwhat do we do about people who may not appear to have understood the task instructions?\n\nTypically, the answers to these questions will be given to you by your supervisor, a colleague or a textbook example. For example, we might say:\n\n“We excluded all reaction times greater than 1500ms before analysis.”\n\nTypically, the explanation for these answers are rarely explained. We might say:\n\n“Consistent with common practice in this field, we excluded all reaction times greater than 1500ms before analysis.”\n\nBut the reader of a journal article typically will not see an explanation for why, as in the example, we exclude reaction times greater than 1500ms and not 2000ms or 3000ms, etc. We typically do not see an explanation for why we exclude all reaction times greater than 1500ms but other researchers exclude all reaction times greater than 2000ms. (I do not pick this example at random: there are serious concerns about the impact on analyses of exclusions like this [@Ulrich1994a].)\nWhat @steegen2016 showed is that a dataset can be processed for analysis in multiple different ways, with a number of reasonable alternate choices that can be applied, for each choice point: construction choices about classifying people or about excluding participants given their responses. If a different dataset is constructed for each combination of alternatives then many different datasets can be produced, all starting from the same raw data. (For their example study, @steegen2016 found they could construct 120 or 210 different datasets, based on the choice combinations.) Critically, for us, @steegen2016 showed that if we apply the same analysis method to the different datasets then our results will vary.\nLet me spell this out, bit by bit:\n\nwe approach our study with the same research question, and the same verbal prediction;\nwe begin with the exact same data;\nwe then construct different datasets depending on different but equally reasonable processing choices;\nwe then apply the same analysis analysis, to test the same prediction, using each different dataset;\nwe will see different results for the analyses of the different datasets.\n\nAlternate constructions of the same data may cause variation in the results of statistical tests. Some kinds of data processing choices may be more influential on results than others. It seems unlikely that we can identify, in advance, which choices matter more.\n@steegen2016 suggest that we can deflate (shrink) the multiverse in different ways. I want to state their suggestions, here, because we will come back to these ideas in the classes on the linear model.\n\nDevelop better theories and improved measurement of the constructs of interest.\nDevelop more complete and precise theory for why some processing options are better than others.\n\nBut you will be asking yourself: what do I need to think about, for the assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you read a psychological research report, identify where the researchers talk about how they process their data: classification, coding, exclusion, transformation, etc.\nIf you can access the raw data, ask yourself: could different choices change the results of the same analysis?\n\n\n\n\n\n\nEven if we begin with the same research question and, critically, the same dataset, the results of a series of studies show that different researchers will often (reasonably) make different choices about the analysis they do to answer the research question. We often call these studies (analysis or model) multiverse studies. In these studies, we see variation in analysis and this variation is also associated with variation in results.\nAn influential example, in psychology, is reported by Silberzahn and colleagues [@silberzahn2015; @silberzahn2017] who asked 29 teams of researchers to answer the same question (“Are (soccer) referees more likely to give red cards to players with dark skin than to players with light skin?”) with the same dataset (data about referee decisions in football league games). The teams made their own decisions about how to answer the question in doing the analysis. The teams shared their plans, and commented on each others’ ideas. The discussion did not lead to a consensus about what analysis approach is best. In the end, the different teams did different analyses and, critically, the different analyses had different results. The results varied in whether the test of the effect of players skin colour (on whether red cards were given) was significant or not, and on the strength of the estimated association between the darkness of skin colour (lighter to darker) and the chances (low to high) of getting a red card.\nThere have now been a series of multiverse or multi-analyst studies which demonstrate that, under certain conditions, different researchers may adopt different analysis approaches – which will have different results – in answering the same research question with the same data. This demonstration has been repeated in studies in health, medicine, psychology, neuoscience, and sociology, among other research fields (e.g., @parsons; @breznau2022; @klau; @klau2021; @wessel2020multiverse; @poline2006; @maier-hein2017; @starns2019; @fillard2011; @dutilh2019; @salganik2020; @bastiaansen2020; @botvinik-nezer2020; @schweinsberg2021; @patel2015; see, for reviews, and some helpful guidance, @aczel2021; @delgiudice2021; @hoffmann; @wagenmakers2022).\nIn these studies, we typically see variation in how psychological constructs are operationalized (e.g., how do we measure or code for social status?), how data are processed or datasets constructed (as in @steegen2016a), plus variation in what statistical techniques are used, and in how those techniques are used. This variation can be understood to reflect kinds of uncertainty [@klau; @klau2021]: uncertainty about how to process data, and uncertainty about the model or methods we should use to test or estimate effects. Further research makes it clear that we should be aware, if we are not already, of the variation in results that can be expected because different researchers may choose to design studies, and construct stimulus materials, in different ways given the same research hypothesis information [@landy2020a].\nBut you will be asking yourself: what do I need to think about, for the assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you read a psychological research report, identify where the researchers talk about how they analyse their data: the hypothesis or prediction they test; the method; their assumptions; the variables they include; the checks or the alternate analyses they did or did not do.\nIf you can access the data and analysis code, ask yourself: could different methods change the results of the same analysis?\n\n\n\n\n\n\nThis is a good place to look at what we have discussed, and present an evaluation of the story so far.\nThis is not a story where everybody or nobody is right or where everything or nothing is true 3. Instead, we can be guided by the advice [@meehl1967; @scheel2022; @steegen2016] that we should (1.) seek better and more complete theorizing about the constructs of interest and how we measure them, and (2.) seek more complete and more precise theory so that some options are theoretically superior than others, and should be preferred, when constructing datasets or specifying analysis methods.\nNot all research questions and not all hypothesis information will allow an equally wide variety of potential reasonable approaches to the analysis. As Paul Meehl argued a long time ago [@meehl1967; @meehl1978], and researchers like Anne Scheel [@scheel2021; @scheel2022] argued more recently, the complexity of the thing we study – people, and what they do – and the still early development of our understanding of this thing, mean that what we want but what we do not see, in psychology, are scientifically productive tests of falsifiable theories. (See, consistent with this perspective, discussions by @auspurg2021 and by @delgiudice2021 about the range of analysis possibilities that may or may not be allowed, in multiverse analyses, by more or less clear research questions or well-developed causal theories.) Our concern should not so much be with being able to do statistical analysis, or with finding significant or not significant results. It would be more useful to do analyses to test concrete, inflexible, precise predictions that can be wrong.\nNor is this a story, I think, about the potential for cheating. While we may refer to subjective choices or to researcher flexibility, the differences that we see do not resemble the researcher degrees of freedom [@Simmons2011] some may exploit, consciously or unconsciously, to change results to suit their aims. Instead, the multiverse results show us the impact of the reasonable differences in approach that different researchers may sensibly choose to take when they try to answer a research question with data.\nNot all alternates, at a given point of choosing, in the data analysis workflow, will have equal impact. Work by Young [@young2017; @young2018] indicates that if we deliberately examine the impact of method or model uncertainty, over different sets of possible choices — about what variables or what observations we include in an analysis, for example — we may find that some results are robust to an array of different options, while other results are highly susceptible to different choices. This work suggests another way in which uncertainty about methods or variation in results can be turned into progress in understanding the phenomena that interest us: through systematic, informed, interrogation of the ways that results can vary.\nIn general, in science, the acceptance of research findings must always be negotiated [@bourdieu2004]. Here, we see that the grounds of negotiation should often include an analysis of the impact on the value of evidence of the different analysis approaches that researchers can or do apply to the data that underly that evidence.\nBut you will be asking yourself: what do I need to think about, for the assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nThe results of multiverse analyses show us that if we see one analysis reported in a paper, or one workflow, that does not mean that only one analysis can reasonably be applied.\nIf you read the methods or results section of a paper, you should reflect: what other analysis methods could be used here? How could variation in analysis method — in what or how you do the analysis — influence the results?\n\n\n\nMaking you aware of the potential for analysis choices is useful because developing researchers, including graduate students, are often not aware of the room for choice in the data analysis workflow. Developing researchers — you — may be instructed that “this is how we do things” or “you should follow what researchers did previously”. Following convention is not necessarily a bad thing: it is a feature of the normal practice of science [@kuhn1970]. However, you can now see, perhaps, that there likely will be alternative ways to process or to analyse data than the approach a supervisor, lab or field normally adopts.\nThis understanding or awareness has three implications for practice, it means:\n\nWhen we talk about the analysis we do, we should explain our choices.\nWe should check, or enable others to check, what impact making different choices would have on our results.\nMost importantly: we can allow ourselves the freedom to critically evaluate the choices researchers make, even the choices researchers make in published articles.\n\n\n\n\n\nMultiverse analyses and post-publication analyses, in general, show that we can and should question or critically evaluate the analyses we encounter in the literature. This work can usefully detect problems in original published analyses [e.g., @Gelman2009c; @herndon2014; @Wagenmakers2011]. It can demonstrate where original published claims are or are not robust to variation of analysis method or approach.\nGiven these lessons, and the implications we have identified, we should expect or hope to see open science practices [@munafò2017; @nosek2022]:\n\nshare data and code;\npublish research reports in ways that enable others to check or query analyses.\n\nAs we discuss, following, these practices are now common but the quality of practice can sometimes be questioned. This matters for you because it makes it more challenging – in specific identifiable locations – to locate, access, analyse and report previously collected data.\nThe discussion of current practices identifies where or how the assignment may be more challenging, but also identifies some of the exact places where the assignment provides a real opportunity to do original research work.\nFirst, I am going to introduce some ideas that will help you to think about what you are doing when you do this work. We focus on the concept of reproducibility.\n@gilmore2017 [following @goodman2016] present three kinds of reproducibility:\n\nmethods reproducibility\nresults reproducibility\ninferential reproducibility\n\nIn looking at reproducibility, here, we are considering how much, or in what ways, the results or the claims that are made in a published study can be found or repeated by someone else.\n\n\nAs @gilmore2017 discuss, methods reproducibility means that another researcher should be able to get the same results if they use the same tools and analysis methods to analyse the same dataset [some researchers also refer to analytic reproducibility or computational reproducibility; see e.g. @crüwell; @hardwicke2018; @hardwicke; @laurinavichyute2022; @minocher].\nIn neuroimaging, the multiplicity of possible implementations of the data analysis pipeline [@carp2012plurality], and the fact that important elements or information about the pipeline deployed by researchers may be missing from published reports [@carp2012secret], can make it challenging to identify how results can be reproduced.\nIn psychological science, in evaluating reports of results from analyses of behavioural data collected through survey or experimental work, in principle, we should expect to be able to access the data collected by the study authors, follow the description of their analysis method, and reproduce the results they report.\n\n\n\n\n\n\nTip\n\n\n\n\nFor an assignment in which we ask students to locate, access, analyse and report previously collected data, we are directly concerned with methods reproducibility.\n\n\n\n\n\n\nResults reproducibility means that if another researcher completes a new study with new data they are able to get the same results as the results reported following an original study: this often referred to as replication. The replication studies that have been reported [e.g., @aarts2015], and continue to be reported (see, for example, the studies discussed by @nosek2022), in the last several years, present attempts to examine the results reproducibility of published findings.\nIn the classes on the linear model, we will examine if similar or different results are observed in a series of studies using the same procedure and the same materials. We shall discuss, in those classes, in more depth, what results reproducibility (or study replication) can or cannot tell us about the behaviours that interest us.\n\n\n\nInferential reproducibility means that if a researcher repeats a study (aiming for results reproducibility) or re-analyzes an original dataset (aiming for methods reproducibility) then they can come to the same or similar conclusions as the authors of the report of an original study.\nHow is inferential reproducibility not methods or results reproducibility? @goodman2016 explain that researchers can make the same conclusions from different sets of results and can reach different conclusions from the same set of results.\nHow is it possible to reach different conclusions from the same results? We can imagine two scenarios.\nFirst, we have to think about the wider research field, the research context, within which we consider a set of results. It may be that two different researchers will come to look at the same results with different expectations about what the results could tell us (in Bayesian terms, with different prior expectations). Given different expectations, it is easy to imagine different researchers looking at the same results and, for example, one researcher being more skeptical than another about what conclusion can be taken from those results. (In the class on graduate writing skills, I discuss in some depth the importance of reviewing a research literature in order to get an understanding of the assumptions, conventions or expectations that may be shared by the researchers working in the field.)\nSecond, imagine two different researchers looking at the same results — picture the original authors of a published study, and someone doing a post-publication re-analysis of their data — you can expect that the re-analysis or the reproducibility analysis could identify reasons to value the evidence differently, or to reach more skeptical conclusions, through critical evaluation of:\n\ndata processing choices;\nthe choice of the method used to do analysis;\nchoices in how the analysis method is used.\n\nWhere that critical evaluation involves an analysis of the choices the original researchers made, perhaps involving an analysis of other choices they could have made, perhaps reflecting on how effectively the analyses address a given research question or test a given prediction.\n\n\n\n\n\n\nTip\n\n\n\n\nWe can think about the work we do, when we analyse previously reported data, in terms of the need to identify the reproducibility of results, methods and inferences.\nIn psychological science, determining that someone can get the same results, by analyzing the same data, or will reach the same conclusions from the same results, are important – potentially, original – research contributions.\n\n\n\n\n\n\n\nI have said that we should expect or hope to see open science practices [@munafò2017; @nosek2022] where researchers:\n\nshare data and code;\npublish research reports in ways that enable others to check or query analyses.\n\nThis raises an important question: What exactly do we see, when we look at current practices? The question is important because answering it helps to identify where the challenges are located when you complete your work to locate, access, analyse and report previously collected data.\nI break the discussion of what we see into two parts. Firstly, I look at the results of audits of data and code sharing (see Section 1.2.6.2): are data shared and can we access the data? Secondly, I discuss analyses of methods reproducibility, and shared data and code usability (see Section 1.2.6.3): can others reproduce the results reported in published articles, given shared data? can others access and run shared analysis code? can others use the shared code to reproduce the reported results? Again, I need to be brief but reference sources that you can follow-up.\n\n\nI should be clear, before we go on, about the link between the credibility revolution in science, and the effort to examine reproducibility of results. Many elements of the credibility revolution emerged out of the observation that it has often been difficult to repeat the results of published studies when we conduct new studies (replication studies or results reproducibility; e.g., @aarts2015). However, it is clearly difficult to know what to replicate or reproduce if we cannot reproduce the results presented in a study report (methods reproducibility), given the study data [@artner2021; @laurinavichyute2022; @minocher].\n\n\n\nResearch on data and code sharing practices suggest that practices have improved, from earlier low levels.\nIn an important early report, @wicherts2006 observed that it was very difficult to obtain data reported in psychological research articles from the authors of the articles. They asked for data from the lead authors of 141 articles published in four leading psychology journals, for about 25% of the studies. This low response rate was found despite the fact that authors in these journals must agree to the principle that data can be shared with others wishing to verify claims.\nPractice has changed: how?\nOne change to practice has involved the use of open science badges. In journals like Psychological Science authors of articles may be awarded badges — Open Data, Open Materials, Preregistration badges — by the editorial team. Authors can apply for and earn the badges by providing information about open practices, and journal articles are published with the badges displayed near the front of the articles.\nIn theory, initiatives like encouraging authors to earn open science badges should mean that data sharing practices improve, enabling access to data and code for those, like you, who would like to re-analyze previously published data. In theory, all you should need to do — to locate and access data — is just search articles in the journal Psychological Science for studies with open data badges, and follow links from the published articles to then access study data at an open repository like the Open Science Framework (OSF) What do we see in practice?\nAnalyses reported by @kidwell2016 as well as analyses reviewed by @nosek2022 indicate that more articles have claimed to make data available in the time since badges were introduced. When they did their analysis, @kidwell2016 found that a substantial proportion, but not all, of the articles in Psychological Science can be found to actually provide access to shared data. However, critically, many but not all the articles with open data badges provide access to data available through an open repository, data that are correct, complete and usable [@kidwell2016]. In their later report, the analyses reviewed by @nosek2022 suggest that the use of repositories like OSF for data sharing may be accelerating but that, over the last few years, the rate at which open science practices like sharing data, overall, appears to be substantial but not yet reported or observed in a majority of the work of researchers.\nMany journals now require the authors of articles to include a Data Availability Statement to locate their data. Analyses by @federer2022 indicate that Data Availability Statements for articles published in the open access 4 journal PLOS ONE often, helpfully, include Digital Object Identifiers (DOIs) or Universal resource locators (URLs) enabling direct access to shared data (i.e., without having to contact authors). Of those DOIs or URLs, most appeared to be associated with resources that could successfully be retrieved. In contrast, analyses reported by @gabelica2022 that where article authors state that “data sets are available on reasonable request” (the most common availability statement), most of the time, the authors did not respond or declined to share the data [see similar findings, across fields, by @tedersoo2021]. Clearly, in the analyses of open science practices we have seen so far, data sharing is more effective where sharing does not have to work through authors.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you are looking for a study in order to get data that you can then reanalyze, it makes sense to look, first, for studies focusing on research questions that interest you.\nWhen you are looking for published reports where the authors share data, look for articles with open science badges or where you can see a Data Availability Statement.\nChoose articles where the authors provide a direct link to their data, where the data are located on an open repository like the Open Science Framework (there are other repositories).\n\n\n\n\n\n\nResearch on data and code sharing practices suggest that practices have improved but that there are concerns about the quality of the sharing. Here, the critical concern relates to the word enable in the objective: that we should publish research reports in ways that enable others to check or query analyses.\nJohn Towse and colleagues [@towse2021] examined the quality of open datasets to assess their quality in terms of their completeness and reusability [see also @roche2015].\n\ncompleteness: are all the data and the data descriptors supporting a study’s findings publicly available?\nreusability: how readily can the data be accessed and understood by others?\n\nFor a sample of datasets, they found that about half were incomplete, and about two-thirds were shared in a way that made them difficult to use. Practices tended to be slightly better in more recent publications. (Broadly similar results are reported by [@hardwicke2018].)\nWhere data were found to be incomplete, this appeared to be, in part, because participants were excluded in the processing of the data for analysis but this information was not in the report, or because data were shared without a guide or “readme” file or data dictionary (or codebook) explaining the structure, coding or composition of the shared data.\nPotentially important for future open science practices, [@towse2021; also @roche2015] found that sharing data as Supplementary materials may appear to carry risks that, in the long term, mean that data may become inaccessible.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you locate open data you can access, look for a guide, “readme” file, codebook or data dictionary explaining the data: you need to be able to understand what the variables are, what the observations relate to (observations per person, per trial?) and how variables are coded.\nLocate and examine carefully the parts of the published report, or the data guide, where the authors explain how they processed their data.\n\n\n\nA number of studies have been conducted to examine whether shared data and analysis code can be reused by others to reproduce the results reported in papers [e.g., @artner2021; @crüwell; @hardwicke2018; @hardwicke; @laurinavichyute2022; @minocher; @obels2020; see @artner2021 for a review of reproducibility studies]. In critical respects, the researchers doing this work are doing work similar to the work we are helping students to do, locating, accessing, and analyzing previously collected data. In these studies, typically, the researchers progressed through a series of steps.\n\nSearched the articles published in a journal (e.g., Cognition, the Journal of Memory and Language, Psychological Science), published in a topic area across multiple journals (e.g., social learning, psychological research), or associated with a specific practice (e.g., registered reports.\nSelected a subset of articles where it was identified that data could be accessed.\nIdentify a target result or outcome to reproduce, for each article. In their analyses, Hardwicke and colleagues [@hardwicke2018; @hardwicke] focused on attempting to reproduce primary or straightforward and substantive outcomes: substantive – if emphasized in the abstract, or presented in a table or figure; straightforward – if the outcome could be calculated using the kind of test one would learn in an introductory psychology course (e.g., t-test, correlation).\nAttempted to reproduce the results reported in the article, using the description of the data analysis presented in the article, and the analysis code (if provided), in some cases asking for information from the original study authors, in other cases working independently of original authors.\n\nWhat the reproducibility studies appear to show is that, for many published reports, if data are shared and if the shared data are accessible and reusable then, most of the time, the researchers could reproduce the results presented by the original study authors [@hardwicke2018; @hardwicke; @laurinavichyute2022; @minocher; @obels2020; but see @crüwell]. This is great. But what is interesting, for us, is where the reproducibility researchers encountered challenges. You may encounter the same or similar challenges.\nI list some challenges that the researchers describe, following. Before you look at the list, I want to assure you: you will not find all these challenges present for any one article you look at. Most likely, you will find one or two challenges. Obviously, some challenges will be more difficult than others.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you find a study you are interested in, with open data and maybe open analysis code, your main challenge will often be to identify exactly what analysis the original study authors did to answer their research question.\nLocate and examine carefully the parts of the published report where the authors explain how they did the analysis that gave them their key result. Usually that key result should be identified in the abstract or in the conclusion.\n\n\n\n\n\n\nData Availability Statements or open science badges indicate data are shared but data are not directly accessible through a link to an open repository.\nThe data are shared and accessible but there is missing or incorrect information about the data. The documentation, codebook or data dictionary is missing or incomplete. There is unclear or missing information about the variables or the observations, or about the coding of variable values, responses.\nOriginal study authors may share raw and processed data or just processed or just raw data. It may not be clear how raw data were processed to construct the data analysed for the report. It may not be clear how variables were transformed or calculated or processed.\nThere may be mismatches between the variables referred to in the report and the variables named in the data file. It may be unclear how a data file corresponds to a study described in a report, where there are multiple studies and multiple data files.\n\n\n\n\n\nThe original report includes a description of the analysis but the description of the analysis procedure is incomplete or ambiguous.\nThere may be a mismatch, in the report, between a hypothesis, and the analysis specified to test the hypothesis (maybe in the Methods section), compared to a long sequence of results reported in the Results section. This makes it difficult to identify the key analysis.\nIt is easier to reproduce results if both data and code are shared because the presentation of the analysis code usually (not always) makes clear what analysis was done to get the results presented in the report.\nSometimes, analysis code is shared but it is difficult to use because it requires proprietary software (e.g., SPSS) or because it requires function libraries that are no longer publicly available.\nSometimes, there are errors in the analysis. Sometimes, there are errors in the presentation of the results, where results have been incorrectly copied into reports from analysis outputs.\n\n\n\n\n\n\n\nThe research report assignment requires students to locate, access, analyse and report previously collected data. At the start of the introduction, I said I would explain the answer to the question:\n\nWhy: what is the motivation for the assignment?\n\nI summarize, following, the main points of the answer I have given. When you review these points, I want you to think about two things, returning to the ideas of @bourdieu2004 and @kuhn1970 I sketched at the start.\nOften what we do in science is guided by convention, the assumptions and habits of normal practice [@kuhn1970]. These conventions can work in our minds so that if we encounter an anomaly or discrepancy between what we expect and what we find, in our work, we may usually blame ourselves: it was something wrong that we did or failed to do. It can cause us anxiety if we do not reproduce a result we think we should be able to reproduce [@lubega]. But I want you to understand, from the start, that sometimes, if you think you have found an error or a problem in a published analysis or a shared dataset, you may be right.\nIf there is anything we have learned, through the findings of replication studies, multiverse analyses, and reproducibility audits it is that people make mistakes, different choices are often reasonable, and we always need to check the evidence.\n\n\n\nWe are in the middle of a credibility revolution. The lessons we have learned so far oblige us to think about and to teach good open science practices that safeguard the value of evidence in psychology.\nThis matters, even if we do not care about scientific methods, because if we care about the translation into policy or practice – in clinical psychology, in education, health, marketing and other fields – what we do will depend on the value of the research evidence that informs policy ideas or practice guides.\nFocusing on data analysis, it is useful to think about the whole data pipeline in analysis, the workflow that takes us from data collection to raw data to data processing to analysis to the presentation of results.\nAt every stage of the data pipeline, there are choices about what to do. There are not always reasons why we make one choice instead of another. Sometimes, we are guided by convention, example or instruction.\nThe existence of choices means the path we take, when we do data analysis, can be one path among multiple different forking paths.\nFor some parts of the pipeline – dataset construction, data analysis choices – reasonable people might make different decisions to sensibly answer the same research question, given the same data. This variation between pathways can be more or less important in influencing the results we see.\nIf results tend to stay similar across different ways of doing analysis, we might conclude that the results are reasonably robust across contexts, choices, or other variation in methods.\nTo enable others to see what we did (versus what we could have done), to see how we got to our results from our data, it is important to share our data and code.\nEveryone makes mistakes and we should make it easy for others, and ourselves, to find those mistakes by sharing our data and code in accessible, clear, usable ways.\nWe need to teach and learn how to share effectively the data and the code that we used to answer our research questions.\n\nIn constructing the assignment – in asking and supporting students to locate, access, analyse and report previously collected data – we are presenting an opportunity to really investigate and evaluate existing practices.\nYou may find that this work is challenging, in some of the places that reproducibility research has identified there can be challenges. Where the challenges cannot be fixed – if you have found an interesting study but the study data are inaccessible or unusable – we will advise you to move on to another study. Where the challenges can be fixed – if data require processing, or if analysis information requires clarification – we will provide you with help or enabling information so that you fix the problems yourself.\n\n\n\n\n\n\nTip\n\n\n\n\nMaybe the main lesson from this exercise is a reminder of the Golden rule: treat others as you would like to be treated.\nIf it is frustrating when it is difficult to understand information about an analysis or about data, or when it is difficult to access and reuse shared data and code.\nWhen it is your turn, do better, reflecting on what frustrated you.\n\n\n\nOne last question: why not just do less demanding or challenging tasks? Because this is part of what makes graduate degree valuable, what will make you more skilled in the workplace. Most of the time, we work in teams, we inherit problems or data analysis tasks, or are given results with partial information. The lessons you learn here will help you to effectively navigate those situations."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/intro.html#the-key-ideas",
    "href": "PSYC402/extra_ToBeOrganised/intro.html#the-key-ideas",
    "title": "Introduction: the why",
    "section": "",
    "text": "There are two ideas motivating our approach. It will be helpful to you if I sketch them out early, here. We can demonstrate the usefulness of these ideas as we progress through our work.\nThe first key idea is expressed clearly in sociological discussions of science. This is that there is a difference between science “…being done, science in the making, and science already done, a finished product …” [@bourdieu2004; p.2]. The awareness we want to develop is that there are two things: there is the story that may be presented in a textbook or in a lecture about scientific work or scientific claims; and there is the work we do in practice, as we develop graduate skills, and as we exercise those skills professionally in the workplace.\nThe second key idea connects to the first. This idea is that reported analyses are not necessary or sufficient to the data or the question. What does this mean? It means that the same data can reasonably be analysed in different ways. There is no necessary way to analyse some data though there may be conventions or normal practices [@kuhn1970]. It means that it is unlikely that any one analysis will do all the work that could be done (a sufficiency) to get you from your data to useful or reasonable answers to your questions.\nThese ideas may be unsettling but they are realistic. Stating them will better prepare you for professional work. In the workplace, the accuracy of these ideas will emerge when you see how a team in any sector (health, marketing …) gets from its data to its product. If we talk about the ideas now, we can get you ready for dealing with the practical and the ethical concerns you will confront when that happens.\nWe will begin by discussing psychological research, and research about psychological research, to answer the question: Why: what is the motivation for the assignment? We will then move to answering the what and the how questions."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/intro.html#why-what-is-the-motivation-for-the-assignment",
    "href": "PSYC402/extra_ToBeOrganised/intro.html#why-what-is-the-motivation-for-the-assignment",
    "title": "Introduction: the why",
    "section": "",
    "text": "We are here because we are interested in humans and human behaviour, and because we are interested in scientific methods of making sense of these things. Some of us are aware that science (including psychological science) has undergone a rolling series of crises: the replicability or replication crisis [@Pashler2012a; @Pashler2012b]; the statistical crisis [@gelman2014a]; and the generalizability crisis [@yarkoni2022]. And that science is undergoing a response to these crises, evidenced in the advocacy of pre-registration [@nosek2018; @nosek2019prereg], and of registered reports [@nosek2014], the use of open science badges (e.g., for the journal Psychological Science), the completion of large-scale replication studies [@aarts2015], and the identification of open science principles [@munafò2017]. We may usefully refer, collectively, to the crises and the responses, as the credibility revolution [@vazire2018]\nWe could teach a course on this (in Lancaster, we do) but I must be brief, here, and invite you to follow the references, if you are interested. Before going on, I want to call your attention to the fact that important elements of the hard work in trying to make science work better has been led by PhD students and by junior researchers [e.g., @herndon2014]. Graduate students may, at first, assume that the fact that a research article has been published in a journal means the findings that are reported must be true. Most of the time, some educated skepticism is more appropriate. An important driver of the realization that there are problems evident in the literature, and that there are changes we can make to improve practice, comes from independent post-publication review work exposing the problems in published work (see, e.g., this account by Andrew Gelman)\n\n\n\n\n\n\nTip\n\n\n\n\nAllow yourself to feel skeptical about the reports you read then work with the motivation this feeling provides.\n\n\n\nIn brief, then, most practicing scientists now understand or should understand that many of the claims we encounter in the published scientific literature are unlikely to be supported by the evidence [@Ioannidis2005], whether we are looking at the evidence of the results in the reports themselves, or evidence in later attempts to find the same results [e.g., @aarts2015]. We suspect that this may result from a number of causes. We understand that researchers may engage in questionable research practices [@john2012]. We understand that researchers may exploit the potential for flexibility in doing and reporting analyses [@Simmons2011a]. We understand that there are problems in how psychologists use or talk about the measurement of psychological constructs [@flake2020]. We understand that there are problems in how psychologists sample people for their studies, both in where we recruit [@bornstein2013; @wild2022; @Henrich2010], and in how many we recruit [@button2013; @cohen1962; @sedlmeier1989; @vankov2014]. We understand that there are problems in how psychologists specify or think about their hypotheses or predictions [@meehl1967; @scheel2022]. And we understand that there are problems in how scientists do, or rather do not, comply with good practice recommendations designed to fix these problems (discussed further in the following).\nThis discussion could (again) be unsettling. This list of problems could make you angry or sad. I, like others, think it is exciting. It is exciting because these problems have probably existed for a long time [e.g., @cohen1962; @meehl1967] and now, having identified the problems, we can hope to do something about it. It is exciting because if you care about people, the study of people, or the applications in clinical, education and other domains of the results of the study of people, then you might hope to see better, more useful, science in the future [@vazire2018].\nAs someone who teaches graduate and undergraduate students, I want to help you to be the change you want to see in the world 1. We cannot solve every problem but we can try to do better those things that are within our reach. I am going to end this introduction with a brief discussion of some ideas we can use to guide our better practices.\n\n\n\nIn this course, for this assignment, we are going to focus on:\n\nmultiverse analyses\nkinds of reproducibility\nthe current state of the match between open science ideas and practices\n\nIn the classes on the linear model, we will discuss:\n\nthe links between theory, prediction and analysis\npsychological measurement\nsamples\nvariation in results\n\n\n\n\n\n\nI am going to link this discussion to a metaphor (see Figure Figure 1) or a description you will find useful: the data analysis pipeline or workflow.\n\n\n\n\n\n\n\n\nQ\n\n \n\ncluster_R\n\n   \n\nnd_1\n\n Get raw data   \n\nnd_2\n\n Tidy data   \n\nnd_1-&gt;nd_2\n\n    \n\nnd_3_l\n\n Visualize   \n\nnd_2-&gt;nd_3_l\n\n    \n\nnd_3\n\n Analyze   \n\nnd_2-&gt;nd_3\n\n    \n\nnd_3_r\n\n Explore   \n\nnd_2-&gt;nd_3_r\n\n    \n\nnd_3_a\n\n Assumptions   \n\nnd_3_a-&gt;nd_3_l\n\n    \n\nnd_3_a-&gt;nd_3\n\n    \n\nnd_3_a-&gt;nd_3_r\n\n    \n\nnd_3_l-&gt;nd_3\n\n   \n\nnd_4\n\n Present   \n\nnd_3_l-&gt;nd_4\n\n    \n\nnd_3-&gt;nd_4\n\n   \n\n\nFigure 1: The data analysis pipeline or workflow\n\n\n\n\nThis metaphor or way of thinking is very common (take a look at the diagram in Wickham and Grolemund’s 2017 book “R for Data Science) and you may see the words “data pipeline” used in job descriptions, or you may benefit from saying, in a job application, something like: I am skilled in designing and implementing each stage of the quantitative data analysis pipeline, from data tidying to results presentation. I say this because scientists I have mentored got their jobs because they can do these things – and successfully explained that they can do these things – in sectors like educational testing, behavioural analysis, or public policy research.\nThe reason this metaphor is useful is that it helps us to organize our thinking, and to manage what we do when we do data analysis, we:\n\nget some data;\nprocess or tidy the data;\nexplore, visualize, and analyze the data;\npresent or report our findings.\n\nWe introduce the idea that your analysis work will flow through the stages of a pipeline from getting the data to presenting your findings because, next, we will examine how pipelines can multiply.\n\n\n\n\n\n\nTip\n\n\n\n\nAs you practice your data analysis work, try to identify the elements and the order of your work, as the parts of a workflow.\n\n\n\n\n\n\nWhat researchers have come to realize: because we started looking … The open secret that has been well kept [@bourdieu2004]: because everybody who does science knows about it, yet we may not teach it; and because we do not write textbooks revealing it … Is that at each stage in the analysis workflow, we can and do make choices where multiple alternative choices are possible. @gelman2014 capture this insight as the “garden of forking paths”2 (see Figure 2).\nThe general idea is that it is possible to have multiple potential different paths from the data to the results. The results will vary, depending on the path we take. In an analysis, we could take multiple different paths simply because at point A we decide to do B1, B2 or B3, maybe we choose B1, and then at point B1, we may decide to do C1, C2 or C3. Here, maybe we have our raw data at point A. Maybe we could do one of two different things when we tidy the data: action B1 or B2. Then, when we have our tidy data, maybe we can choose to do our analysis in one of six ways. Where we are at each step depends on the choices we made at the previous steps.\n\n\n\n\n\n\n\n\nD\n\n  \n\nA\n\n A   \n\nB1\n\n B1   \n\nA-&gt;B1\n\n    \n\nB2\n\n B2   \n\nA-&gt;B2\n\n    \n\nC1\n\n C1   \n\nB1-&gt;C1\n\n    \n\nC2\n\n C2   \n\nB1-&gt;C2\n\n    \n\nC3\n\n C3   \n\nB1-&gt;C3\n\n    \n\nC4\n\n C4   \n\nB2-&gt;C4\n\n    \n\nC5\n\n C5   \n\nB2-&gt;C5\n\n    \n\nC6\n\n C6   \n\nB2-&gt;C6\n\n   \n\n\nFigure 2: Forking paths in data analysis\n\n\n\n\nIn the end, it may appear to us that we took one path or that only one path was possible. When we report our analysis, in a dissertation or in a published journal article, we may report the analysis as if only one analysis path had been considered. But, critically, our findings may depend on the choices we made and this variation in results may be hidden from view.\nI am talking about forking paths because the multiplicity of paths has consequences, and we discuss these next.\n\n\n\n\n\n\nTip\n\n\n\n\nIt is about here, I hope, that you can start to see why it would makes sense to access data from a published study and to examine if you can get the same results as the study authors.\n\n\n\n\n\n\n\nI am going to discuss, now, what are commonly called multiverse analyses. Psychologists use this term, having been introduced to it in an influential paper by @steegen2016, but it comes from theoretical physics (take a look at wikipedia).\nI explain this because I do not want you to worry. The ideas themselves are within your grasp whatever your background in psychology or elsewhere. It is the implications for our data analysis practices that are challenging. They are challenging because what we discuss should increase your skepticism about the results you encounter in published papers. And they are challenging because they reveal your freedom to question whether published authors could have done their analysis in a different way.\nWe are going to look at:\n\ndataset construction\nanalysis choices\n\n\n\nIn first discussing the wider context (of crisis and revolution), then discussing the specific context (of multiverses and, in the following, of reproducibility), I should be clear about the link between the two things. The finding that some results may not be supported by the evidence is probably due to a mix of causes. But one of those causes will be the combination of uncertainty over data processing or the uncertainty over analysis methods revealed in multiverse analyses, as we see next, combined with the limitations of data and code sharing, and the incompleteness of results reporting (as we see later).\n\n\n\nWhen you collect or access data for a research study, the complete raw dataset you receive is almost never the complete dataset you analyze or whose analysis you report. This is not a story about deliberately cheating. It is a story about the normal practice of science [@kuhn1970].\nPicture some common scenarios. You did a survey, you got responses from a 100 participants on 10 questions, and you asked people to report their education, ethnicity and gender. You did an experimental study, you tested two groups of 50 people each in 100 trials (imagine a common task like the Stroop test), and you observed the accuracy and the timing of their responses. You tested 100 children, 20 children in each of five different schools, on a range of educational ability measures.\nIn these scenarios, the psychologist or the analyst of behavioural data must process their data. In doing so, you will ask yourself a series of questions like:\n\nhow do we code for gender, ethnicity, education?\nwhat do we about reaction times that are very short, e.g., \\(RT &lt; 200ms\\) or very long, e.g., \\(RT &gt; 1500ms\\))?\nif we present multiple questions measuring broadly the same thing (e.g. how confident are you that you understand what you have read? how easy did you find what you read?) how do we summarize the scores on those questions? do we combine scores?\nwhat do we do about people who may not appear to have understood the task instructions?\n\nTypically, the answers to these questions will be given to you by your supervisor, a colleague or a textbook example. For example, we might say:\n\n“We excluded all reaction times greater than 1500ms before analysis.”\n\nTypically, the explanation for these answers are rarely explained. We might say:\n\n“Consistent with common practice in this field, we excluded all reaction times greater than 1500ms before analysis.”\n\nBut the reader of a journal article typically will not see an explanation for why, as in the example, we exclude reaction times greater than 1500ms and not 2000ms or 3000ms, etc. We typically do not see an explanation for why we exclude all reaction times greater than 1500ms but other researchers exclude all reaction times greater than 2000ms. (I do not pick this example at random: there are serious concerns about the impact on analyses of exclusions like this [@Ulrich1994a].)\nWhat @steegen2016 showed is that a dataset can be processed for analysis in multiple different ways, with a number of reasonable alternate choices that can be applied, for each choice point: construction choices about classifying people or about excluding participants given their responses. If a different dataset is constructed for each combination of alternatives then many different datasets can be produced, all starting from the same raw data. (For their example study, @steegen2016 found they could construct 120 or 210 different datasets, based on the choice combinations.) Critically, for us, @steegen2016 showed that if we apply the same analysis method to the different datasets then our results will vary.\nLet me spell this out, bit by bit:\n\nwe approach our study with the same research question, and the same verbal prediction;\nwe begin with the exact same data;\nwe then construct different datasets depending on different but equally reasonable processing choices;\nwe then apply the same analysis analysis, to test the same prediction, using each different dataset;\nwe will see different results for the analyses of the different datasets.\n\nAlternate constructions of the same data may cause variation in the results of statistical tests. Some kinds of data processing choices may be more influential on results than others. It seems unlikely that we can identify, in advance, which choices matter more.\n@steegen2016 suggest that we can deflate (shrink) the multiverse in different ways. I want to state their suggestions, here, because we will come back to these ideas in the classes on the linear model.\n\nDevelop better theories and improved measurement of the constructs of interest.\nDevelop more complete and precise theory for why some processing options are better than others.\n\nBut you will be asking yourself: what do I need to think about, for the assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you read a psychological research report, identify where the researchers talk about how they process their data: classification, coding, exclusion, transformation, etc.\nIf you can access the raw data, ask yourself: could different choices change the results of the same analysis?\n\n\n\n\n\n\nEven if we begin with the same research question and, critically, the same dataset, the results of a series of studies show that different researchers will often (reasonably) make different choices about the analysis they do to answer the research question. We often call these studies (analysis or model) multiverse studies. In these studies, we see variation in analysis and this variation is also associated with variation in results.\nAn influential example, in psychology, is reported by Silberzahn and colleagues [@silberzahn2015; @silberzahn2017] who asked 29 teams of researchers to answer the same question (“Are (soccer) referees more likely to give red cards to players with dark skin than to players with light skin?”) with the same dataset (data about referee decisions in football league games). The teams made their own decisions about how to answer the question in doing the analysis. The teams shared their plans, and commented on each others’ ideas. The discussion did not lead to a consensus about what analysis approach is best. In the end, the different teams did different analyses and, critically, the different analyses had different results. The results varied in whether the test of the effect of players skin colour (on whether red cards were given) was significant or not, and on the strength of the estimated association between the darkness of skin colour (lighter to darker) and the chances (low to high) of getting a red card.\nThere have now been a series of multiverse or multi-analyst studies which demonstrate that, under certain conditions, different researchers may adopt different analysis approaches – which will have different results – in answering the same research question with the same data. This demonstration has been repeated in studies in health, medicine, psychology, neuoscience, and sociology, among other research fields (e.g., @parsons; @breznau2022; @klau; @klau2021; @wessel2020multiverse; @poline2006; @maier-hein2017; @starns2019; @fillard2011; @dutilh2019; @salganik2020; @bastiaansen2020; @botvinik-nezer2020; @schweinsberg2021; @patel2015; see, for reviews, and some helpful guidance, @aczel2021; @delgiudice2021; @hoffmann; @wagenmakers2022).\nIn these studies, we typically see variation in how psychological constructs are operationalized (e.g., how do we measure or code for social status?), how data are processed or datasets constructed (as in @steegen2016a), plus variation in what statistical techniques are used, and in how those techniques are used. This variation can be understood to reflect kinds of uncertainty [@klau; @klau2021]: uncertainty about how to process data, and uncertainty about the model or methods we should use to test or estimate effects. Further research makes it clear that we should be aware, if we are not already, of the variation in results that can be expected because different researchers may choose to design studies, and construct stimulus materials, in different ways given the same research hypothesis information [@landy2020a].\nBut you will be asking yourself: what do I need to think about, for the assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you read a psychological research report, identify where the researchers talk about how they analyse their data: the hypothesis or prediction they test; the method; their assumptions; the variables they include; the checks or the alternate analyses they did or did not do.\nIf you can access the data and analysis code, ask yourself: could different methods change the results of the same analysis?\n\n\n\n\n\n\nThis is a good place to look at what we have discussed, and present an evaluation of the story so far.\nThis is not a story where everybody or nobody is right or where everything or nothing is true 3. Instead, we can be guided by the advice [@meehl1967; @scheel2022; @steegen2016] that we should (1.) seek better and more complete theorizing about the constructs of interest and how we measure them, and (2.) seek more complete and more precise theory so that some options are theoretically superior than others, and should be preferred, when constructing datasets or specifying analysis methods.\nNot all research questions and not all hypothesis information will allow an equally wide variety of potential reasonable approaches to the analysis. As Paul Meehl argued a long time ago [@meehl1967; @meehl1978], and researchers like Anne Scheel [@scheel2021; @scheel2022] argued more recently, the complexity of the thing we study – people, and what they do – and the still early development of our understanding of this thing, mean that what we want but what we do not see, in psychology, are scientifically productive tests of falsifiable theories. (See, consistent with this perspective, discussions by @auspurg2021 and by @delgiudice2021 about the range of analysis possibilities that may or may not be allowed, in multiverse analyses, by more or less clear research questions or well-developed causal theories.) Our concern should not so much be with being able to do statistical analysis, or with finding significant or not significant results. It would be more useful to do analyses to test concrete, inflexible, precise predictions that can be wrong.\nNor is this a story, I think, about the potential for cheating. While we may refer to subjective choices or to researcher flexibility, the differences that we see do not resemble the researcher degrees of freedom [@Simmons2011] some may exploit, consciously or unconsciously, to change results to suit their aims. Instead, the multiverse results show us the impact of the reasonable differences in approach that different researchers may sensibly choose to take when they try to answer a research question with data.\nNot all alternates, at a given point of choosing, in the data analysis workflow, will have equal impact. Work by Young [@young2017; @young2018] indicates that if we deliberately examine the impact of method or model uncertainty, over different sets of possible choices — about what variables or what observations we include in an analysis, for example — we may find that some results are robust to an array of different options, while other results are highly susceptible to different choices. This work suggests another way in which uncertainty about methods or variation in results can be turned into progress in understanding the phenomena that interest us: through systematic, informed, interrogation of the ways that results can vary.\nIn general, in science, the acceptance of research findings must always be negotiated [@bourdieu2004]. Here, we see that the grounds of negotiation should often include an analysis of the impact on the value of evidence of the different analysis approaches that researchers can or do apply to the data that underly that evidence.\nBut you will be asking yourself: what do I need to think about, for the assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nThe results of multiverse analyses show us that if we see one analysis reported in a paper, or one workflow, that does not mean that only one analysis can reasonably be applied.\nIf you read the methods or results section of a paper, you should reflect: what other analysis methods could be used here? How could variation in analysis method — in what or how you do the analysis — influence the results?\n\n\n\nMaking you aware of the potential for analysis choices is useful because developing researchers, including graduate students, are often not aware of the room for choice in the data analysis workflow. Developing researchers — you — may be instructed that “this is how we do things” or “you should follow what researchers did previously”. Following convention is not necessarily a bad thing: it is a feature of the normal practice of science [@kuhn1970]. However, you can now see, perhaps, that there likely will be alternative ways to process or to analyse data than the approach a supervisor, lab or field normally adopts.\nThis understanding or awareness has three implications for practice, it means:\n\nWhen we talk about the analysis we do, we should explain our choices.\nWe should check, or enable others to check, what impact making different choices would have on our results.\nMost importantly: we can allow ourselves the freedom to critically evaluate the choices researchers make, even the choices researchers make in published articles.\n\n\n\n\n\nMultiverse analyses and post-publication analyses, in general, show that we can and should question or critically evaluate the analyses we encounter in the literature. This work can usefully detect problems in original published analyses [e.g., @Gelman2009c; @herndon2014; @Wagenmakers2011]. It can demonstrate where original published claims are or are not robust to variation of analysis method or approach.\nGiven these lessons, and the implications we have identified, we should expect or hope to see open science practices [@munafò2017; @nosek2022]:\n\nshare data and code;\npublish research reports in ways that enable others to check or query analyses.\n\nAs we discuss, following, these practices are now common but the quality of practice can sometimes be questioned. This matters for you because it makes it more challenging – in specific identifiable locations – to locate, access, analyse and report previously collected data.\nThe discussion of current practices identifies where or how the assignment may be more challenging, but also identifies some of the exact places where the assignment provides a real opportunity to do original research work.\nFirst, I am going to introduce some ideas that will help you to think about what you are doing when you do this work. We focus on the concept of reproducibility.\n@gilmore2017 [following @goodman2016] present three kinds of reproducibility:\n\nmethods reproducibility\nresults reproducibility\ninferential reproducibility\n\nIn looking at reproducibility, here, we are considering how much, or in what ways, the results or the claims that are made in a published study can be found or repeated by someone else.\n\n\nAs @gilmore2017 discuss, methods reproducibility means that another researcher should be able to get the same results if they use the same tools and analysis methods to analyse the same dataset [some researchers also refer to analytic reproducibility or computational reproducibility; see e.g. @crüwell; @hardwicke2018; @hardwicke; @laurinavichyute2022; @minocher].\nIn neuroimaging, the multiplicity of possible implementations of the data analysis pipeline [@carp2012plurality], and the fact that important elements or information about the pipeline deployed by researchers may be missing from published reports [@carp2012secret], can make it challenging to identify how results can be reproduced.\nIn psychological science, in evaluating reports of results from analyses of behavioural data collected through survey or experimental work, in principle, we should expect to be able to access the data collected by the study authors, follow the description of their analysis method, and reproduce the results they report.\n\n\n\n\n\n\nTip\n\n\n\n\nFor an assignment in which we ask students to locate, access, analyse and report previously collected data, we are directly concerned with methods reproducibility.\n\n\n\n\n\n\nResults reproducibility means that if another researcher completes a new study with new data they are able to get the same results as the results reported following an original study: this often referred to as replication. The replication studies that have been reported [e.g., @aarts2015], and continue to be reported (see, for example, the studies discussed by @nosek2022), in the last several years, present attempts to examine the results reproducibility of published findings.\nIn the classes on the linear model, we will examine if similar or different results are observed in a series of studies using the same procedure and the same materials. We shall discuss, in those classes, in more depth, what results reproducibility (or study replication) can or cannot tell us about the behaviours that interest us.\n\n\n\nInferential reproducibility means that if a researcher repeats a study (aiming for results reproducibility) or re-analyzes an original dataset (aiming for methods reproducibility) then they can come to the same or similar conclusions as the authors of the report of an original study.\nHow is inferential reproducibility not methods or results reproducibility? @goodman2016 explain that researchers can make the same conclusions from different sets of results and can reach different conclusions from the same set of results.\nHow is it possible to reach different conclusions from the same results? We can imagine two scenarios.\nFirst, we have to think about the wider research field, the research context, within which we consider a set of results. It may be that two different researchers will come to look at the same results with different expectations about what the results could tell us (in Bayesian terms, with different prior expectations). Given different expectations, it is easy to imagine different researchers looking at the same results and, for example, one researcher being more skeptical than another about what conclusion can be taken from those results. (In the class on graduate writing skills, I discuss in some depth the importance of reviewing a research literature in order to get an understanding of the assumptions, conventions or expectations that may be shared by the researchers working in the field.)\nSecond, imagine two different researchers looking at the same results — picture the original authors of a published study, and someone doing a post-publication re-analysis of their data — you can expect that the re-analysis or the reproducibility analysis could identify reasons to value the evidence differently, or to reach more skeptical conclusions, through critical evaluation of:\n\ndata processing choices;\nthe choice of the method used to do analysis;\nchoices in how the analysis method is used.\n\nWhere that critical evaluation involves an analysis of the choices the original researchers made, perhaps involving an analysis of other choices they could have made, perhaps reflecting on how effectively the analyses address a given research question or test a given prediction.\n\n\n\n\n\n\nTip\n\n\n\n\nWe can think about the work we do, when we analyse previously reported data, in terms of the need to identify the reproducibility of results, methods and inferences.\nIn psychological science, determining that someone can get the same results, by analyzing the same data, or will reach the same conclusions from the same results, are important – potentially, original – research contributions.\n\n\n\n\n\n\n\nI have said that we should expect or hope to see open science practices [@munafò2017; @nosek2022] where researchers:\n\nshare data and code;\npublish research reports in ways that enable others to check or query analyses.\n\nThis raises an important question: What exactly do we see, when we look at current practices? The question is important because answering it helps to identify where the challenges are located when you complete your work to locate, access, analyse and report previously collected data.\nI break the discussion of what we see into two parts. Firstly, I look at the results of audits of data and code sharing (see Section 1.2.6.2): are data shared and can we access the data? Secondly, I discuss analyses of methods reproducibility, and shared data and code usability (see Section 1.2.6.3): can others reproduce the results reported in published articles, given shared data? can others access and run shared analysis code? can others use the shared code to reproduce the reported results? Again, I need to be brief but reference sources that you can follow-up.\n\n\nI should be clear, before we go on, about the link between the credibility revolution in science, and the effort to examine reproducibility of results. Many elements of the credibility revolution emerged out of the observation that it has often been difficult to repeat the results of published studies when we conduct new studies (replication studies or results reproducibility; e.g., @aarts2015). However, it is clearly difficult to know what to replicate or reproduce if we cannot reproduce the results presented in a study report (methods reproducibility), given the study data [@artner2021; @laurinavichyute2022; @minocher].\n\n\n\nResearch on data and code sharing practices suggest that practices have improved, from earlier low levels.\nIn an important early report, @wicherts2006 observed that it was very difficult to obtain data reported in psychological research articles from the authors of the articles. They asked for data from the lead authors of 141 articles published in four leading psychology journals, for about 25% of the studies. This low response rate was found despite the fact that authors in these journals must agree to the principle that data can be shared with others wishing to verify claims.\nPractice has changed: how?\nOne change to practice has involved the use of open science badges. In journals like Psychological Science authors of articles may be awarded badges — Open Data, Open Materials, Preregistration badges — by the editorial team. Authors can apply for and earn the badges by providing information about open practices, and journal articles are published with the badges displayed near the front of the articles.\nIn theory, initiatives like encouraging authors to earn open science badges should mean that data sharing practices improve, enabling access to data and code for those, like you, who would like to re-analyze previously published data. In theory, all you should need to do — to locate and access data — is just search articles in the journal Psychological Science for studies with open data badges, and follow links from the published articles to then access study data at an open repository like the Open Science Framework (OSF) What do we see in practice?\nAnalyses reported by @kidwell2016 as well as analyses reviewed by @nosek2022 indicate that more articles have claimed to make data available in the time since badges were introduced. When they did their analysis, @kidwell2016 found that a substantial proportion, but not all, of the articles in Psychological Science can be found to actually provide access to shared data. However, critically, many but not all the articles with open data badges provide access to data available through an open repository, data that are correct, complete and usable [@kidwell2016]. In their later report, the analyses reviewed by @nosek2022 suggest that the use of repositories like OSF for data sharing may be accelerating but that, over the last few years, the rate at which open science practices like sharing data, overall, appears to be substantial but not yet reported or observed in a majority of the work of researchers.\nMany journals now require the authors of articles to include a Data Availability Statement to locate their data. Analyses by @federer2022 indicate that Data Availability Statements for articles published in the open access 4 journal PLOS ONE often, helpfully, include Digital Object Identifiers (DOIs) or Universal resource locators (URLs) enabling direct access to shared data (i.e., without having to contact authors). Of those DOIs or URLs, most appeared to be associated with resources that could successfully be retrieved. In contrast, analyses reported by @gabelica2022 that where article authors state that “data sets are available on reasonable request” (the most common availability statement), most of the time, the authors did not respond or declined to share the data [see similar findings, across fields, by @tedersoo2021]. Clearly, in the analyses of open science practices we have seen so far, data sharing is more effective where sharing does not have to work through authors.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you are looking for a study in order to get data that you can then reanalyze, it makes sense to look, first, for studies focusing on research questions that interest you.\nWhen you are looking for published reports where the authors share data, look for articles with open science badges or where you can see a Data Availability Statement.\nChoose articles where the authors provide a direct link to their data, where the data are located on an open repository like the Open Science Framework (there are other repositories).\n\n\n\n\n\n\nResearch on data and code sharing practices suggest that practices have improved but that there are concerns about the quality of the sharing. Here, the critical concern relates to the word enable in the objective: that we should publish research reports in ways that enable others to check or query analyses.\nJohn Towse and colleagues [@towse2021] examined the quality of open datasets to assess their quality in terms of their completeness and reusability [see also @roche2015].\n\ncompleteness: are all the data and the data descriptors supporting a study’s findings publicly available?\nreusability: how readily can the data be accessed and understood by others?\n\nFor a sample of datasets, they found that about half were incomplete, and about two-thirds were shared in a way that made them difficult to use. Practices tended to be slightly better in more recent publications. (Broadly similar results are reported by [@hardwicke2018].)\nWhere data were found to be incomplete, this appeared to be, in part, because participants were excluded in the processing of the data for analysis but this information was not in the report, or because data were shared without a guide or “readme” file or data dictionary (or codebook) explaining the structure, coding or composition of the shared data.\nPotentially important for future open science practices, [@towse2021; also @roche2015] found that sharing data as Supplementary materials may appear to carry risks that, in the long term, mean that data may become inaccessible.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you locate open data you can access, look for a guide, “readme” file, codebook or data dictionary explaining the data: you need to be able to understand what the variables are, what the observations relate to (observations per person, per trial?) and how variables are coded.\nLocate and examine carefully the parts of the published report, or the data guide, where the authors explain how they processed their data.\n\n\n\nA number of studies have been conducted to examine whether shared data and analysis code can be reused by others to reproduce the results reported in papers [e.g., @artner2021; @crüwell; @hardwicke2018; @hardwicke; @laurinavichyute2022; @minocher; @obels2020; see @artner2021 for a review of reproducibility studies]. In critical respects, the researchers doing this work are doing work similar to the work we are helping students to do, locating, accessing, and analyzing previously collected data. In these studies, typically, the researchers progressed through a series of steps.\n\nSearched the articles published in a journal (e.g., Cognition, the Journal of Memory and Language, Psychological Science), published in a topic area across multiple journals (e.g., social learning, psychological research), or associated with a specific practice (e.g., registered reports.\nSelected a subset of articles where it was identified that data could be accessed.\nIdentify a target result or outcome to reproduce, for each article. In their analyses, Hardwicke and colleagues [@hardwicke2018; @hardwicke] focused on attempting to reproduce primary or straightforward and substantive outcomes: substantive – if emphasized in the abstract, or presented in a table or figure; straightforward – if the outcome could be calculated using the kind of test one would learn in an introductory psychology course (e.g., t-test, correlation).\nAttempted to reproduce the results reported in the article, using the description of the data analysis presented in the article, and the analysis code (if provided), in some cases asking for information from the original study authors, in other cases working independently of original authors.\n\nWhat the reproducibility studies appear to show is that, for many published reports, if data are shared and if the shared data are accessible and reusable then, most of the time, the researchers could reproduce the results presented by the original study authors [@hardwicke2018; @hardwicke; @laurinavichyute2022; @minocher; @obels2020; but see @crüwell]. This is great. But what is interesting, for us, is where the reproducibility researchers encountered challenges. You may encounter the same or similar challenges.\nI list some challenges that the researchers describe, following. Before you look at the list, I want to assure you: you will not find all these challenges present for any one article you look at. Most likely, you will find one or two challenges. Obviously, some challenges will be more difficult than others.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you find a study you are interested in, with open data and maybe open analysis code, your main challenge will often be to identify exactly what analysis the original study authors did to answer their research question.\nLocate and examine carefully the parts of the published report where the authors explain how they did the analysis that gave them their key result. Usually that key result should be identified in the abstract or in the conclusion.\n\n\n\n\n\n\nData Availability Statements or open science badges indicate data are shared but data are not directly accessible through a link to an open repository.\nThe data are shared and accessible but there is missing or incorrect information about the data. The documentation, codebook or data dictionary is missing or incomplete. There is unclear or missing information about the variables or the observations, or about the coding of variable values, responses.\nOriginal study authors may share raw and processed data or just processed or just raw data. It may not be clear how raw data were processed to construct the data analysed for the report. It may not be clear how variables were transformed or calculated or processed.\nThere may be mismatches between the variables referred to in the report and the variables named in the data file. It may be unclear how a data file corresponds to a study described in a report, where there are multiple studies and multiple data files.\n\n\n\n\n\nThe original report includes a description of the analysis but the description of the analysis procedure is incomplete or ambiguous.\nThere may be a mismatch, in the report, between a hypothesis, and the analysis specified to test the hypothesis (maybe in the Methods section), compared to a long sequence of results reported in the Results section. This makes it difficult to identify the key analysis.\nIt is easier to reproduce results if both data and code are shared because the presentation of the analysis code usually (not always) makes clear what analysis was done to get the results presented in the report.\nSometimes, analysis code is shared but it is difficult to use because it requires proprietary software (e.g., SPSS) or because it requires function libraries that are no longer publicly available.\nSometimes, there are errors in the analysis. Sometimes, there are errors in the presentation of the results, where results have been incorrectly copied into reports from analysis outputs."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/intro.html#this-is-why",
    "href": "PSYC402/extra_ToBeOrganised/intro.html#this-is-why",
    "title": "Introduction: the why",
    "section": "",
    "text": "The research report assignment requires students to locate, access, analyse and report previously collected data. At the start of the introduction, I said I would explain the answer to the question:\n\nWhy: what is the motivation for the assignment?\n\nI summarize, following, the main points of the answer I have given. When you review these points, I want you to think about two things, returning to the ideas of @bourdieu2004 and @kuhn1970 I sketched at the start.\nOften what we do in science is guided by convention, the assumptions and habits of normal practice [@kuhn1970]. These conventions can work in our minds so that if we encounter an anomaly or discrepancy between what we expect and what we find, in our work, we may usually blame ourselves: it was something wrong that we did or failed to do. It can cause us anxiety if we do not reproduce a result we think we should be able to reproduce [@lubega]. But I want you to understand, from the start, that sometimes, if you think you have found an error or a problem in a published analysis or a shared dataset, you may be right.\nIf there is anything we have learned, through the findings of replication studies, multiverse analyses, and reproducibility audits it is that people make mistakes, different choices are often reasonable, and we always need to check the evidence.\n\n\n\nWe are in the middle of a credibility revolution. The lessons we have learned so far oblige us to think about and to teach good open science practices that safeguard the value of evidence in psychology.\nThis matters, even if we do not care about scientific methods, because if we care about the translation into policy or practice – in clinical psychology, in education, health, marketing and other fields – what we do will depend on the value of the research evidence that informs policy ideas or practice guides.\nFocusing on data analysis, it is useful to think about the whole data pipeline in analysis, the workflow that takes us from data collection to raw data to data processing to analysis to the presentation of results.\nAt every stage of the data pipeline, there are choices about what to do. There are not always reasons why we make one choice instead of another. Sometimes, we are guided by convention, example or instruction.\nThe existence of choices means the path we take, when we do data analysis, can be one path among multiple different forking paths.\nFor some parts of the pipeline – dataset construction, data analysis choices – reasonable people might make different decisions to sensibly answer the same research question, given the same data. This variation between pathways can be more or less important in influencing the results we see.\nIf results tend to stay similar across different ways of doing analysis, we might conclude that the results are reasonably robust across contexts, choices, or other variation in methods.\nTo enable others to see what we did (versus what we could have done), to see how we got to our results from our data, it is important to share our data and code.\nEveryone makes mistakes and we should make it easy for others, and ourselves, to find those mistakes by sharing our data and code in accessible, clear, usable ways.\nWe need to teach and learn how to share effectively the data and the code that we used to answer our research questions.\n\nIn constructing the assignment – in asking and supporting students to locate, access, analyse and report previously collected data – we are presenting an opportunity to really investigate and evaluate existing practices.\nYou may find that this work is challenging, in some of the places that reproducibility research has identified there can be challenges. Where the challenges cannot be fixed – if you have found an interesting study but the study data are inaccessible or unusable – we will advise you to move on to another study. Where the challenges can be fixed – if data require processing, or if analysis information requires clarification – we will provide you with help or enabling information so that you fix the problems yourself.\n\n\n\n\n\n\nTip\n\n\n\n\nMaybe the main lesson from this exercise is a reminder of the Golden rule: treat others as you would like to be treated.\nIf it is frustrating when it is difficult to understand information about an analysis or about data, or when it is difficult to access and reuse shared data and code.\nWhen it is your turn, do better, reflecting on what frustrated you.\n\n\n\nOne last question: why not just do less demanding or challenging tasks? Because this is part of what makes graduate degree valuable, what will make you more skilled in the workplace. Most of the time, we work in teams, we inherit problems or data analysis tasks, or are given results with partial information. The lessons you learn here will help you to effectively navigate those situations."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/intro.html#footnotes",
    "href": "PSYC402/extra_ToBeOrganised/intro.html#footnotes",
    "title": "Introduction: the why",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis encouragement is often attributed to Gandhi but is attributed ((here)) to a Brooklyn school teacher, Ms Arleen Lorrance, who led a transformative school project in the 1970s.↩︎\nThe term is taken from the name of a short story by Jorge Luis Borges, “El jardin de senderos que se bifurcan”.↩︎\nThere could be a story where the hero (us) ultimately learns to reject binary (present, absent; significant, non-significant) choices, and embrace variation, or embrace uncertainty [@Gelman2015; @vasishth2021].↩︎\nOpen access journals publish articles that are free to read or download.↩︎"
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/index.html",
    "href": "PSYC402/extra_ToBeOrganised/index.html",
    "title": "Preface",
    "section": "",
    "text": "We can, here, explain a development in the approach we take in teaching this course. Naturally, this development in approach will require a parallel development in your approach to learning.\nWe are going to focus on working in research in context (see Figure Figure 1).\n\n\n\n\n\n\n\n\nG\n\n  \n\nreading\n\n reading   \n\nknowledge\n\n knowledge   \n\nreading–knowledge\n\n   \n\nconventions\n\n conventions   \n\nknowledge–conventions\n\n   \n\nconcepts\n\n concepts   \n\nknowledge–concepts\n\n   \n\npractices\n\n practices   \n\nknowledge–practices\n\n  \n\n\nFigure 1: This is a simple graphviz graph.\n\n\n\n\nYou have been introduced to R. We know that some of you are new to R so we will practice the skills you are learning. We will consolidate, revise, and extend these skills.\nWe will encounter — some, for the first time – the linear model also known as regression analysis, multiple regression.\nBut the big change is this focus on the context. The reason is that not talking about the context has a dangerous impact on how you approach, do or think about data analysis.\nIn traditional methods teaching, the schedule of classes will progress through a series of tests, one test a week, from simpler to more complex tests (e.g., from t-test to multiple regression at the undergraduate level). Textbooks often mirror this structure, presenting one test per chapter. In this approach, the presentation is often brief about the context: the question the researchers are investigating; the methods they use to collect data, including the measurements; and the assumptions they make about how your reasoning can get you from the things you measure to the things you are trying to understand. In this approach, also, example data may be presented in a limited, partial, way.\nThe reasons for this are understandable: methods are complex, technical, subjects for learning, and teachers and students do not also have time, perhaps, to think about statistics and about theoretical or measurement assumptions. This is a mistake because it presents a misleading view of the challenge in learning methods: the challenge is just the (difficult enough) challenge of learning about statistical methods, or dealing with numbers. It is a mistake, also, because it implies that if you learn the method, and can match the textbook example – the variables, the state of the data – when it is your turn to do an analysis, all will be well.\nMaybe. I think a more productive approach – this is the approach we will take – is to expose, and talk about some of the real challenges that anybody who handles data, or quantitative evidence, in professional life. These challenges include:\n\nThinking about the mapping from our concerns to the research questions, to the things we measure, to analysis we do, and then the conclusions we make.\nSelecting or constructing valid measures that can be assumed to measure the things they are supposed to measure.\nTaking samples of observations, and making conclusions about the population.\nMaking estimates and linking these estimates to an account that is explicit about causes."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/index.html#a-change-in-approach",
    "href": "PSYC402/extra_ToBeOrganised/index.html#a-change-in-approach",
    "title": "Preface",
    "section": "",
    "text": "We can, here, explain a development in the approach we take in teaching this course. Naturally, this development in approach will require a parallel development in your approach to learning.\nWe are going to focus on working in research in context (see Figure Figure 1).\n\n\n\n\n\n\n\n\nG\n\n  \n\nreading\n\n reading   \n\nknowledge\n\n knowledge   \n\nreading–knowledge\n\n   \n\nconventions\n\n conventions   \n\nknowledge–conventions\n\n   \n\nconcepts\n\n concepts   \n\nknowledge–concepts\n\n   \n\npractices\n\n practices   \n\nknowledge–practices\n\n  \n\n\nFigure 1: This is a simple graphviz graph.\n\n\n\n\nYou have been introduced to R. We know that some of you are new to R so we will practice the skills you are learning. We will consolidate, revise, and extend these skills.\nWe will encounter — some, for the first time – the linear model also known as regression analysis, multiple regression.\nBut the big change is this focus on the context. The reason is that not talking about the context has a dangerous impact on how you approach, do or think about data analysis.\nIn traditional methods teaching, the schedule of classes will progress through a series of tests, one test a week, from simpler to more complex tests (e.g., from t-test to multiple regression at the undergraduate level). Textbooks often mirror this structure, presenting one test per chapter. In this approach, the presentation is often brief about the context: the question the researchers are investigating; the methods they use to collect data, including the measurements; and the assumptions they make about how your reasoning can get you from the things you measure to the things you are trying to understand. In this approach, also, example data may be presented in a limited, partial, way.\nThe reasons for this are understandable: methods are complex, technical, subjects for learning, and teachers and students do not also have time, perhaps, to think about statistics and about theoretical or measurement assumptions. This is a mistake because it presents a misleading view of the challenge in learning methods: the challenge is just the (difficult enough) challenge of learning about statistical methods, or dealing with numbers. It is a mistake, also, because it implies that if you learn the method, and can match the textbook example – the variables, the state of the data – when it is your turn to do an analysis, all will be well.\nMaybe. I think a more productive approach – this is the approach we will take – is to expose, and talk about some of the real challenges that anybody who handles data, or quantitative evidence, in professional life. These challenges include:\n\nThinking about the mapping from our concerns to the research questions, to the things we measure, to analysis we do, and then the conclusions we make.\nSelecting or constructing valid measures that can be assumed to measure the things they are supposed to measure.\nTaking samples of observations, and making conclusions about the population.\nMaking estimates and linking these estimates to an account that is explicit about causes."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/summary.html",
    "href": "PSYC402/extra_ToBeOrganised/summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nTo complete when book is completed.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/how.html",
    "href": "PSYC402/extra_ToBeOrganised/how.html",
    "title": "How",
    "section": "",
    "text": "The research report assignment requires students to locate, access, analyse and report previously collected data. Here, we answer the question:\n\nHow can the assignment be done?\n\nWe outline the workflow you can follow, proceeding through a series of steps to complete the essential tasks. Look at this outline, make a plan, and then follow the advice, taking it one step at a time.\n\n\nStudents have taken a variety of approaches to the assignment.\n\nSome students choose to complete an analysis of a publicly available dataset, analyzed previously, data for which the report has been published in a journal article.\nSome students choose to complete an analysis of a publicly available dataset that has been made available (for a report published as a data journal) but has not been analysed previously.\nSome students choose to complete an analysis of one of the data-sets used for practical exercises in class: the example or demonstration data we collect together as the curated data.\n\nAsk in class or on the discussion forum for advice about any one of these approaches.\nHere, I offer guidance on what to do if you want to locate, access, and analyse previously collected data where those data are presented in a journal article. I consider, first, working with datasets where an analysis of the data has been presented in the article (see Section 1.2). I then look at working with datasets where the data are presented without an analysis (see Section 1.3). Our advice on working with datasets presented without an analysis will overlap in key respects with our advice on working with curated data.\n\n\n\nIn the following, I split our guidance into two parts. I look next at the task of locating, accessing and checking the data (Section 1.2.1). Then I look at the task of figuring out what analysis you can do with the data (see Section 1.2.2). Obviously, you cannot consider an analysis if you cannot be sure that you can work with the data [@minocher].\n\n\nAt the start of your work on the assignment, you will need to (1.) locate then (2.) access data for analysis, and then you will need to (3.) check that the data are usable. I set out advice on doing each step, following. Work through the steps: one step at a time.\n\n\nIt is usually helpful to find a dataset where the data have been collected in a study within a topic area you care about, or could be interested in. It is helpful because you will need to work with the data and it will be motivating if you are interested in what the data concern. And it is helpful because, often, you will need to do a bit of reading on related research to learn about the context for the data collection, and you will usually want to read research sources that interest you.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nDo a search: look for an article with usable data in a topic area that interests you.\n\n\n\nThere are at least two ways you can do this. Both should be reasonably quick methods to get to a usable dataset.\n\nDo a search on Google scholar).\nDo a search on the webpages of a journal.\n\nMost psychological research is published in journals like Psychological Science. If you want, you can look at a list of psychology journals here.\nIn a journal like Psychological Science you can look through lists of previously published articles (in issues, volumes, by year) on the journal webpage. Here is the list of issues for Psychological Science..\n\n\nIn both methods, you are looking for an article associated with data (and maybe analysis code) you can access and that you are sure you can use. In both methods, you need to first think about some key words to use in your search. Ask yourself:\n\nWhat are you interested in? What population, intervention or effect, comparison, or outcome?\n\nThen:\n\nWhat words do people use, in articles you have seen, when they talk about this thing?\n\nYou can use these words, and maybe consider alternate terms. For example, I am interested in reading comprehension or development reading comprehension but researchers working on reading development might also refer to children reading comprehension.\nYou want to be as efficient as possible so combine your search for articles in an interesting topic area with your search for accessible data. We can learn from the research we discussed on data sharing practices (?@sec-sharing) by looking for specific markers that data associated with an article should be accessible.\nIf you are doing a search (1.) on Google scholar), I would use the key words related to your topic plus words like: open data badge; open science badge. So, I would do a search for the words: reading comprehension open data badge. I have done this: you can try it. The search results will list articles related to the topic of reading comprehension, where the authors claim to have earned the open data badge because they have made data available.\nIf you are doing a search (2.) in a journal list of articles, then what you are looking for are articles that interest you and which are listed with open data badges. In the listing for Psychological Science (here)) a quick read of the journal issue articles index shows that article titles are listed together with symbols representing the open science badges that authors have claimed.\nIn other journals (e.g., PLOS ONE, PeerJ, Collabra), you may be looking for interesting articles with the words Data Availability Statement, Data Accessibility Statement, Supplementary data or Supplementary materials in the article webpage somewhere. Journals like PeerJ or Collabra, in particular, make it easy to locate data associated with published articles on their web pages.\nIn Collabra, you can find published articles through the journal webpage (here). If you click on the title of any article, and look at the article webpage, then on the left of the article text, you can see an index of article contents and that index lists the Data Availability Statement. Click on that and you are often taken to a link to a data repository.\n\n\n\n\nIf you have located an interesting article with evidence (an open data badge or a data accessibility statement) that the authors have shared their data, you need to check that you can access the data. Most of the time, now, you are looking for a link you can use to go directly to the shared data. The link is often presented as a hyperlink on a webpage, associated with Digital Object Identifiers (DOIs) or Universal resource locators (URLs). Or, increasingly, you are looking for a link to a data repository on a site like the Open Science Framework (OSF).\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nAccess the data associated with the article you have found.\n\n\n\nHere are some recent examples from my work that you can check, to give you a sense of where or how to find the accessible link to the shared data.\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\nRodríguez-Ferreiro, J., Aguilera, M., & Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. PeerJ, 8, e9511. https://doi.org/10.7717/peerj.9511\nThese are both open access articles.\nIf you look at the webpage for, @rodríguez-ferreiro2020, (here)), you can do a search in the article text for the keyword OSF (on the article webpage, use keys CMD-F plus OSF). You are checking to see if you can click on the link and and if clicking on the link takes you to a repository listing the data for the article. The @rodríguez-ferreiro2020 article is associated with a data plus analysis code repository (OSF))\nNotice that on the repository webpage, you can see a description of the project plus .pdf files and a folder Dataset and Code. If you can click through to the folders, and download the datafiles, you have accessed the data successfully.\nI have guided you, here, through to the @rodríguez-ferreiro2020 data repository, can you find the data for the @ricketts2021 repository?\n\n\n\nIf you have located an interesting article with data that you can access, and if you have read the introductory notes (?@sec-checkanalyses), then you will know that you need to make sure that you can use the data.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nCheck the data and the data documentation to make sure you can understand what you have got and whether you can use it.\n\n\n\nWhat make data usable are:\n\nInformation in the article, or in the data repository documentation, on the study design and data collection methods: you need to be able to understand where the data came from, how they were collected, and why.\nClear data documentation: you need to find information on the variables, the observations, the scoring, the coding, and whether and how the data were processed to get them from raw data state to the data ready for analysis.\n\nData documentation is often presented as a note or a wiki page or a miniature paper and may be called a codebook, data dictionary, guide to materials or something similar. You will need to check that you can find information on (examples shown are from the @rodríguez-ferreiro2020 OSF guide to materials):\n\nwhat the data files are called e.g. PrimDir-111019.csv;\nhow the named data files correspond to the studies presented in the report;\nwhat the data file columns are called and what variables the column data represent e.g. relation, coding for prime-target relatedness condition ...;\nhow scores or responses in columns were collected or calculated e.g. age, giving the age in years ...;\nhow coding was done, if coding was used e.g. biling, giving the bilingualism status;\nwhether data were processed, how missing values were coded, whether participants or observations were excluded before analysis e.g. Missing values in the rt column ... coded as NA\n\nIf these information are not presented, or are not clear: walk away.\n\n\n\n\nAfter you have found an interesting article, and have confirmed that you can use the associated data, you will need to plan what analysis you want to do.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nIdentify and understand the analysis in the article.\nWork out what analysis you want to do.\n\n\n\nStudents have taken a variety of approaches to the assignment.\n\nSome students choose to complete a reanalysis of the data, in an attempt to reproduce the results presented in the article (see Section 1.2).\nSome students choose to complete an alternate analysis of the data, varying elements of the analysis (?@sec-multiverse).\n\nEither way, you will want to first make sure you can identify exactly what the authors of the original study did, how they did it, and why they did it.\nYou can process the key article information efficiently using the QALMRI method we discussed in the class on graduate writing skills [@brosowsky; @kosslyn2005]. You are first aiming to locate information on the broad and the specific question the study addresses, the methods the study authors used to collect data, the results they report, and the conclusions they present given the results. Can you find these bits of information?\n\n\nFollowing Hardwicke and colleagues [@hardwicke2018; @hardwicke] it would be sensible to focus on identifying the primary or substantive result for a study in an article.\n\nSubstantive if emphasized in the abstract, or presented in a table or figure.\n\nAs we discussed in the class on graduate writing skills, the article authors should signal what they consider to be the primary result for a study by telling you that a result is critical or key or that a result is the or an answer to their research question.\n\n\n\n\n\n\nTip\n\n\n\n\nAn article may present multiple studies: focus on one.\nThe results section of an article, for a study, may list multiple results: identify the primary or substantive result.\n\n\n\nIf you are, then you will want to identify a result that is both substantive and straightforward [@hardwicke2018; @hardwicke].\n\nstraightforward if the outcome could be calculated using the kind of test you have been learning about or will learn about (e.g., t-test, correlation, the linear model)\n\nPsychological science researchers use a variety of data analysis methods and not all the analyses that you read about will be analyses done using methods that you know about. The use of the methods we teach — t-test, correlation, and the linear model — are very very common; that is why we teach them. But you may also see reports of analyses done using methods like ANOVA, and multilevel or (increasingly) linear mixed-effects models [@meteyard2020].\nIn the research on the reproducibility of results in the literature (?@sec-checkanalyses), the researchers attempting to reproduce results often focused on answering the research question the original authors stated using the data the original authors shared. This does not mean that they always tried to exactly reproduce an analysis or an analysis result. Sometimes, that was not possible.\nSometimes, you will encounter an article and a dataset you are interested in but the analysis presented in the article looks a bit complicated, or more complex than the methods you have learned would allow you to do. In this situation, don’t give up. What you can do – maybe with our advice – is identify a part of the primary result that you can try to reproduce. For example, what if the original study authors report a linear mixed-effects analysis of the effects of both prime relatedness and schizotypy score on response reaction time [@rodríguez-ferreiro2020]? Maybe you have not learned about mixed-effects models, or you have not learned about analysing the effects of two variables but you have (you will) learn about analysing the effect of one variable using the linear model method: OK then, do an analysis of the shared data using the method you know.\nYou may be helped, here, by knowing about two good-enough (mostly true) insights from statistical analysis:\n\nMany of the common analysis methods you see used in psychological science can be coded as a linear model.\nMore advanced common analysis methods — (Generalized) Linear Mixed-effects Models (GLMMs) — can be understood as more sophisticated versions of the linear model. (Conversely, the linear model can be understood as an approximation of a GLMM.)\n\nThere is a nice discussion of the idea that common statistical tests are linear models here.\n\n\n\n\n\n\nTip\n\n\n\n\nIdentify the analysis method used to get the result you are interested in.\nIf it is complex or unfamiliar, discuss whether a simpler method can be used.\nIf the result is complex, discuss whether you can attempt to reproduce a part or a simpler result.\n\n\n\n\n\n\nIt can be interesting and important work to complete a simpler analysis of shared data. Sometimes, we learn that a simpler analysis is as good account of the behaviour we observe as other more complex analyses. This can happen if, for example, our theory predicts that two effects should work together but an analysis shows that we can explain behaviour in an account in which the two effects are independent. For example, @ricketts2021 predicted that children should learn words more effectively if they were shown the spellings of the words and they were told they would be helped by seeing the spelling but, in our data, we found that just seeing the spellings was enough to explain the learning we observed.\nIn completing analyses that vary from original analyses, we are engaging in the kind of work people do when they do multiverse analyses or robustness checks (?@sec-multiverse).\n\n\n\n\n\n\nTip\n\n\n\nIn planning an alternate or multiverse analysis, do not suppose that you need to do multiple analyses: you do not.\n\n\nIn planning an alternate or multiverse analysis, you will want to begin by critically evaluating the analysis you see described in the published article. I talk about how to do this, next.\nBefore we go on, note that I previously discussed an example of how to critically evaluate the results of published research in the context of @rodríguez-ferreiro2020. Take a look at the Introduction of that article. There, we summarised the analyses researchers did previously and used the information about the analyses to explain inconsistencies in the research literature. We found limitations in the analyses that people did that had (negative) consequences for the strength of the conclusions we can take from the data.\n\n\nIf you revisit our discussion of multiverse analyses, you will see that we discussed two things: (1.) analyses of the impact on results of varying how you construct datasets for analysis (?@sec-multiversedata) and (2.) analyses of the impact on results of varying what analysis method you use, or how you use the method (?@sec-multiverseanalysis). These are both good ways to approach thinking about the description of the analysis you see in a published article.\nAs we noted in ?@sec-multiversedata, you almost always have to process the data you collect (in an experiment or a survey) before you can analyze the data. Often, this means you need to code for responses to survey questions e.g. asking people to self-report their gender, or you need to identify and code for people making errors when they try to do the experimental task you set them, or you need to process the data to exclude participants who took too long to do the task (if taking too long is a problem). Not all of these processing steps will have an impact on the results but some might. This is why you can sometimes do useful and sometimes original research work in reanalyzing previously published data.\nYou can begin your analysis planning work by first identifying exactly what data processing the original study authors did then identifying what different data processing they could have done. Remember the research we discussed in relation to reproducibility studies, you need to be prepared for the possibility that it is challenging to identify what researchers did to process their data for analysis ?@sec-datachallenges. To identify the information you need, look for keywords like code, exclude, process, tidy, transform in the text of the article, or look for words like this in the documentation you find in the data repository.\nWhen you have identified this information, you can then consider three questions:\n\nWhat data processing steps were completed before analysis?\nWhat were the reasons given explaining why these processing steps were completed?\nWhat could happen to the results if different choices were made?\n\nWorking through these questions can then get you to a good plan for an analysis of the data. For example, a simple but useful analysis you can do is to check what happens to the results if you do an analysis with data from all the participants tested, if participants are excluded (for some reason) in the data processing step. Obviously, if the original study authors only share processed data, you cannot do this kind of work. Another simple but useful analysis you can do is to check what happens to the results if you change the coding of variables. Sometimes different coding of categorical variables (e.g., ethnicity) are reasonable. For example, you can ask: what happens if you analyze the impact of the variable given a different coding? (In case you are reading these notes and thinking about recoding a factor, there are some useful functions you can use; read about them here.)\n\n\n\n\n\n\nTip\n\n\n\n\nDo you want to check the impact of varying data processing choices: check, do you need and have access to the raw data? can you see how to recode variables?\n\n\n\nAs we noted in ?@sec-multiverseanalysis, when we consider how to answer a research question with a dataset, it is often possible to imagine multiple different analysis methods: reasonable alternatives. Most often, this is most clearly apparent when we are looking at an observational dataset or data collected given a cross-sectional study design.\nIn cross-sectional or observational studies, we typically are not manipulating experimental conditions, and we are often analyzed data using some kind of linear model. We often collect data or have access to data on a number of different variables relevant to our interests. For example, in studies I have done on how people read [@davies2013; @davies2017], we wanted to know what factors would predict or influence how people do basic reading tasks like reading aloud. We collected information on many different kinds of word properties and on the attributes of the participants we tested. (Note: the papers are associated with data repositories in Supplementary Materials.) It is an open question which variables should be included in a prediction model of the observed outcome (reading response reaction times). Therefore, if you are interested in a study like this, and can access usable data from the study, it will often be true that you are able to sensibly motivate a different analysis of the study data using a different choice of variables.\nAs discussed in a number of interesting analyses, over the years [e.g., @patel2015], researchers may be interested in the specific impact of one particular predictor variable (e.g., we may be interested in whether it is easier to read words we learned early in life), but will need to include in their analysis that variable plus other variables known to affect the outcome. In that situation, the effect of the variable of interest may appear to be different depending on what other variables are also analyzed. This makes it interesting and useful to check the impact of different analysis choices.\nWe will look at data like these, for analyses involving the linear model, in our classes on this method.\n\n\n\n\n\n\nTip\n\n\n\n\nDo you want to check the impact of different analysis choices: check, do you need and have access to a choice of variables?\nCan you think of some reasons to justify using a different choice of variables in your analysis.\n\n\n\n\n\n\n\n\nHere’s a quick summary of the advice we have discussed so far.\n\nAt the start of your work, you will need to (1.) locate then (2.) access data for analysis, and then you will need to (3.) check that the data are usable.\nOnce you have confirmed you have found interesting data you can use, you should plan your analysis.\nStudents do a variety of kinds of analysis. Whatever your interest, you first will want to first make sure you can identify exactly what the authors of the original study did, how they did it, and why they did it.\nIf you are interested in attempting a methods reproducibility test (can you repeat a result, given shared data?) you will perhaps benefit from focusing a result that is both substantive and straightforward.\nIf you are interested in doing an alternate analysis, you can critically evaluate the data processing and the data analysis choices that the original study authors made. You can consider whether other choices would be appropriate, and might sensibly motivate a (limited) investigation of the impact of a different analysis pipeline choice on the results.\n\nWhat if you access interesting data that were shared without a previous analysis? We talk about that situation, next.\n\n\n\n\nA number of datasets have been published online with information about the data but with no analysis. You can look for data that may be interest you in a number of different places, now, but I would focus on one. I talk about that next. Then I offer some guidance on how you might approach analyzing such data Section 1.3.2.\n\n\nWicherts and colleagues set up the Journal of Open Psychology Data (JOPD) to make it easier for Psychologists to share experimental data. A link to the journal webpage is here) Usually, a data paper reports a study and provides a link to a downloadable dataset.\nSome datasets that I have looked at in JOPD and other places include the following.\n\n\nWicherts did what he recommended and put a large dataset online here\nYou can analyse these data in a number of different interesting ways. You can explore relationships between gender, intelligence and personality differences.\nThe data file and an explanatory document are located at the end of the article. Read the article, it’s worth your time. Wicherts reports:\n\nThe file includes data from our freshman-testing program called “Testweek” ( Busato et al., 2000, Smits et al., 2011 and Wicherts and Vorst, 2010) in which 537 students (age: M = 21.0, SD = 4.3) took the Advanced Progressive Matrices ( Raven, Court, & Raven, 1996), a test of Arithmetic, a Number Series test, a Hidden Figures Test, a test of Vocabulary, a test of Verbal Analogies, and a Logical Reasoning test ( Elshout, 1976).\nAlso included are data from a Dutch big five personality inventory (Elshout & Akkerman, 1975), the NEO-PI-R ( Hoekstra, Ormel, & Fruyt, 1996), scales of social desirability and impression management (based on work by Paulhus, 1984 and Wicherts, 2002), sex of the participants, and grade point averages of the freshmen’s first trimester that may act as outcome variable.\n\n\n\n\nSmits and colleagues (including Wicherts) put an even larger dataset online at the Journal of Open Psychology Data here)\nYou will need to register to be able to download the data but the process is simple.\nThe Smits dataset includes Big-5 personality scores for several thousand individuals recorded over a series of years. You can analyse these data in interesting ways including examining changes in personality scores among students over different years.\n\n\n\nTjew A Sin and colleagues shared a dataset at the Journal of Open Psychology Data on an interesting study they did to test the idea that interpersonal touch or simulated interpersonal touch can relieve existential concerns (fear of death) among individuals with low self-esteem. The data can be found here)\nThe Tjew A Sin can be downloaded from a link to a repository location, given at the end of the article. You will likely need to register to download the data. Note that the spreadsheets holding the study data include 999 values to code for missing data. Note also that the data spreadsheets include (in different columns) scores per participant for various measures e.g. mortality anxiety or self-esteem. The measures are explained in the paper. To use the data, you will need to work out the simple process of how to sum the scores across items to get e.g. a measure of self-esteem for each person.\n\n\n\nBerger and Anaki shared data on the disgust sensitivity of a large sample of individuals. The data are from the administration of the Disgust Scale to a set of Hebrew speakers. They can be found here)\nThe experimenters collected data on participants’ characteristics so that analyses of the way in which sensitivity varies in relation to demographic attributes is possible. You will see that the disgust scale is explained in the paper. The different disgust scores, for each item in the disgust scale, can be found in different columns. The disgust scores, for person, are calculated overall as values: Mean_general_ds, Mean_core, Mean_Animal_reminder, Mean_Contamination\nWhen you download the dataset, you may need to change the file name, adding a suffix: .txt (for the tab delimited file), to be opened in Excel, or .sav (for the SPSS data file), to be opened in SPSS – to the file name to allow you to open it in the appropriate application.\n\n\n\n\nThe availability of rich, curated, clearly usable datasets with many variables can make it challenging to decide what to do.\nI would advise beginning with an exploratory analysis of the data you have accessed. You will want to begin by using the data visualization skills we have taught you to examine:\n\nThe distributions of the variables that interest you using histograms, density plots or bar charts.\nThe potential relationship between variables using scatterplots.\n\nIn such Exploratory Data Analyses, you are interested in what the data visualization tells you about the nature of the dataset you have accessed. The papers associated with the datasets can sometimes offer only outline information: how the data were collected, coded, and processed. You may need to satisfy yourself that there is nothing odd or surprising about the distributions of scores. This stage can help you to identify problems like survey responses with implausible scores.\nThe work you do in exploring, and summarizing, the data variables that interest you will often constitute a substantial element of the work you can do and present for your report. You may discuss, for advice, what parts of this work will be interesting or useful to present.\nThen, our advice is simple.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen working with open datasets, consider keeping the analysis simple.\n\n\n\nNote that simple is relative. Do what interests you. Work with the methods you have learned or will learn (the linear model).\nIn practice, you will find that part of the challenge is located not in using the data or in running an analysis like a linear model, it is in (1.) justifying or motivating the analysis and (2.) explaining the implications of your findings.\nWorking on the thinking you must develop to motivate an analysis or to explain implications requires you to do some (limited) reading of relevant research. (Relevant sources will be cited in data papers, as part of their outline of the background for their data collection.) If you consider the advice we discussed in the graduate class on developing writing skills, you will see that there I talked about how you might extract data from a set of relevant sources (papers) to get an understanding of the questions people ask, the assumptions they make. That is the kind of process you can follow to develop your thinking around the analysis you will do. What you are looking for is information you can use so that you can say something brief about, for example, why it might be interesting to analyze, say, whether personality (measured using the Big-5) varies given differences in gender or differences between population cohorts. The reading and the conceptual development should be fairly limited, not extensive, but should be sufficient that you can write something sensible when you introduce and then when you discuss your analysis results.\n\n\n\n\nIn this chapter, I have outlined some advice on how you might approach the task of locating, accessing, and analyzing previously collected data. The main advice is to think about your workflow in stages, then progress through the work one step at a time.\nYou will need to begin by assuring yourself that you can find a dataset that interests you, and that you can access and use the data. The usability of data will require clear, understandable, descriptions in the published article (if any) about the research question and hypothesis, the study design, the data collection methods, the data processing steps, and the data analysis (if any). Sometimes, useful information about data processing and data analysis can be found in detail in repository documentation (e.g., in guides to materials) but only referenced in the text of the article.\nIf you know you can locate, access and have checked data as usable, you will want to think about what analysis you want to do the data. The approach you take depending on what aims you would like to pursue.\nIf you are interested in attempting a methods reproducibility test (i.e. checking if you can repeat presented results, given shared data), then you will first need to identify a substantive and straightforward result to try to reproduce. If you identify a primary result to examine, you will want to check that you can work with the data that have been shared, and then that you can use the analysis methods you have learned to reproduce some or all of the result that interests you.\nIf you are interested in doing an alternate or a different analysis (from what may be presented), you may need to consider the information you can locate on data processing and on data analysis choices. Did the original study authors process the data before sharing it, how? are the raw data available? What analyses did the authors do and why? When you consider this information, you may critically evaluate the choices made. In the context of this critical evaluation, you may find good reasons to justify doing a different analysis, whether to examine the impact of making different data processing choices, or to examine the impact of using a different analysis method, or of applying the same method differently (e.g., by including different variables).\nIn considering an analysis of data shared without a published set of results, you may want to keep your approach simple. Focus on what analysis you can do using the methods you have learned. And think about the understanding you will need to develop, to justify the analysis you do, and to make sense, in the discussion of your report of the analysis results you will present.\nIt is always a good idea to explore your data using visualization techniques throughout your workflow.\n\n\n\n\n\n\nTip\n\n\n\n\nYou can always get advice, do not hesitate to ask.\nWe are happy to discuss your thinking, especially in class."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/how.html#the-variety-of-things-students-do",
    "href": "PSYC402/extra_ToBeOrganised/how.html#the-variety-of-things-students-do",
    "title": "How",
    "section": "",
    "text": "Students have taken a variety of approaches to the assignment.\n\nSome students choose to complete an analysis of a publicly available dataset, analyzed previously, data for which the report has been published in a journal article.\nSome students choose to complete an analysis of a publicly available dataset that has been made available (for a report published as a data journal) but has not been analysed previously.\nSome students choose to complete an analysis of one of the data-sets used for practical exercises in class: the example or demonstration data we collect together as the curated data.\n\nAsk in class or on the discussion forum for advice about any one of these approaches.\nHere, I offer guidance on what to do if you want to locate, access, and analyse previously collected data where those data are presented in a journal article. I consider, first, working with datasets where an analysis of the data has been presented in the article (see Section 1.2). I then look at working with datasets where the data are presented without an analysis (see Section 1.3). Our advice on working with datasets presented without an analysis will overlap in key respects with our advice on working with curated data."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/how.html#sec-publishedanalysis",
    "href": "PSYC402/extra_ToBeOrganised/how.html#sec-publishedanalysis",
    "title": "How",
    "section": "",
    "text": "In the following, I split our guidance into two parts. I look next at the task of locating, accessing and checking the data (Section 1.2.1). Then I look at the task of figuring out what analysis you can do with the data (see Section 1.2.2). Obviously, you cannot consider an analysis if you cannot be sure that you can work with the data [@minocher].\n\n\nAt the start of your work on the assignment, you will need to (1.) locate then (2.) access data for analysis, and then you will need to (3.) check that the data are usable. I set out advice on doing each step, following. Work through the steps: one step at a time.\n\n\nIt is usually helpful to find a dataset where the data have been collected in a study within a topic area you care about, or could be interested in. It is helpful because you will need to work with the data and it will be motivating if you are interested in what the data concern. And it is helpful because, often, you will need to do a bit of reading on related research to learn about the context for the data collection, and you will usually want to read research sources that interest you.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nDo a search: look for an article with usable data in a topic area that interests you.\n\n\n\nThere are at least two ways you can do this. Both should be reasonably quick methods to get to a usable dataset.\n\nDo a search on Google scholar).\nDo a search on the webpages of a journal.\n\nMost psychological research is published in journals like Psychological Science. If you want, you can look at a list of psychology journals here.\nIn a journal like Psychological Science you can look through lists of previously published articles (in issues, volumes, by year) on the journal webpage. Here is the list of issues for Psychological Science..\n\n\nIn both methods, you are looking for an article associated with data (and maybe analysis code) you can access and that you are sure you can use. In both methods, you need to first think about some key words to use in your search. Ask yourself:\n\nWhat are you interested in? What population, intervention or effect, comparison, or outcome?\n\nThen:\n\nWhat words do people use, in articles you have seen, when they talk about this thing?\n\nYou can use these words, and maybe consider alternate terms. For example, I am interested in reading comprehension or development reading comprehension but researchers working on reading development might also refer to children reading comprehension.\nYou want to be as efficient as possible so combine your search for articles in an interesting topic area with your search for accessible data. We can learn from the research we discussed on data sharing practices (?@sec-sharing) by looking for specific markers that data associated with an article should be accessible.\nIf you are doing a search (1.) on Google scholar), I would use the key words related to your topic plus words like: open data badge; open science badge. So, I would do a search for the words: reading comprehension open data badge. I have done this: you can try it. The search results will list articles related to the topic of reading comprehension, where the authors claim to have earned the open data badge because they have made data available.\nIf you are doing a search (2.) in a journal list of articles, then what you are looking for are articles that interest you and which are listed with open data badges. In the listing for Psychological Science (here)) a quick read of the journal issue articles index shows that article titles are listed together with symbols representing the open science badges that authors have claimed.\nIn other journals (e.g., PLOS ONE, PeerJ, Collabra), you may be looking for interesting articles with the words Data Availability Statement, Data Accessibility Statement, Supplementary data or Supplementary materials in the article webpage somewhere. Journals like PeerJ or Collabra, in particular, make it easy to locate data associated with published articles on their web pages.\nIn Collabra, you can find published articles through the journal webpage (here). If you click on the title of any article, and look at the article webpage, then on the left of the article text, you can see an index of article contents and that index lists the Data Availability Statement. Click on that and you are often taken to a link to a data repository.\n\n\n\n\nIf you have located an interesting article with evidence (an open data badge or a data accessibility statement) that the authors have shared their data, you need to check that you can access the data. Most of the time, now, you are looking for a link you can use to go directly to the shared data. The link is often presented as a hyperlink on a webpage, associated with Digital Object Identifiers (DOIs) or Universal resource locators (URLs). Or, increasingly, you are looking for a link to a data repository on a site like the Open Science Framework (OSF).\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nAccess the data associated with the article you have found.\n\n\n\nHere are some recent examples from my work that you can check, to give you a sense of where or how to find the accessible link to the shared data.\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\nRodríguez-Ferreiro, J., Aguilera, M., & Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. PeerJ, 8, e9511. https://doi.org/10.7717/peerj.9511\nThese are both open access articles.\nIf you look at the webpage for, @rodríguez-ferreiro2020, (here)), you can do a search in the article text for the keyword OSF (on the article webpage, use keys CMD-F plus OSF). You are checking to see if you can click on the link and and if clicking on the link takes you to a repository listing the data for the article. The @rodríguez-ferreiro2020 article is associated with a data plus analysis code repository (OSF))\nNotice that on the repository webpage, you can see a description of the project plus .pdf files and a folder Dataset and Code. If you can click through to the folders, and download the datafiles, you have accessed the data successfully.\nI have guided you, here, through to the @rodríguez-ferreiro2020 data repository, can you find the data for the @ricketts2021 repository?\n\n\n\nIf you have located an interesting article with data that you can access, and if you have read the introductory notes (?@sec-checkanalyses), then you will know that you need to make sure that you can use the data.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nCheck the data and the data documentation to make sure you can understand what you have got and whether you can use it.\n\n\n\nWhat make data usable are:\n\nInformation in the article, or in the data repository documentation, on the study design and data collection methods: you need to be able to understand where the data came from, how they were collected, and why.\nClear data documentation: you need to find information on the variables, the observations, the scoring, the coding, and whether and how the data were processed to get them from raw data state to the data ready for analysis.\n\nData documentation is often presented as a note or a wiki page or a miniature paper and may be called a codebook, data dictionary, guide to materials or something similar. You will need to check that you can find information on (examples shown are from the @rodríguez-ferreiro2020 OSF guide to materials):\n\nwhat the data files are called e.g. PrimDir-111019.csv;\nhow the named data files correspond to the studies presented in the report;\nwhat the data file columns are called and what variables the column data represent e.g. relation, coding for prime-target relatedness condition ...;\nhow scores or responses in columns were collected or calculated e.g. age, giving the age in years ...;\nhow coding was done, if coding was used e.g. biling, giving the bilingualism status;\nwhether data were processed, how missing values were coded, whether participants or observations were excluded before analysis e.g. Missing values in the rt column ... coded as NA\n\nIf these information are not presented, or are not clear: walk away.\n\n\n\n\nAfter you have found an interesting article, and have confirmed that you can use the associated data, you will need to plan what analysis you want to do.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nIdentify and understand the analysis in the article.\nWork out what analysis you want to do.\n\n\n\nStudents have taken a variety of approaches to the assignment.\n\nSome students choose to complete a reanalysis of the data, in an attempt to reproduce the results presented in the article (see Section 1.2).\nSome students choose to complete an alternate analysis of the data, varying elements of the analysis (?@sec-multiverse).\n\nEither way, you will want to first make sure you can identify exactly what the authors of the original study did, how they did it, and why they did it.\nYou can process the key article information efficiently using the QALMRI method we discussed in the class on graduate writing skills [@brosowsky; @kosslyn2005]. You are first aiming to locate information on the broad and the specific question the study addresses, the methods the study authors used to collect data, the results they report, and the conclusions they present given the results. Can you find these bits of information?\n\n\nFollowing Hardwicke and colleagues [@hardwicke2018; @hardwicke] it would be sensible to focus on identifying the primary or substantive result for a study in an article.\n\nSubstantive if emphasized in the abstract, or presented in a table or figure.\n\nAs we discussed in the class on graduate writing skills, the article authors should signal what they consider to be the primary result for a study by telling you that a result is critical or key or that a result is the or an answer to their research question.\n\n\n\n\n\n\nTip\n\n\n\n\nAn article may present multiple studies: focus on one.\nThe results section of an article, for a study, may list multiple results: identify the primary or substantive result.\n\n\n\nIf you are, then you will want to identify a result that is both substantive and straightforward [@hardwicke2018; @hardwicke].\n\nstraightforward if the outcome could be calculated using the kind of test you have been learning about or will learn about (e.g., t-test, correlation, the linear model)\n\nPsychological science researchers use a variety of data analysis methods and not all the analyses that you read about will be analyses done using methods that you know about. The use of the methods we teach — t-test, correlation, and the linear model — are very very common; that is why we teach them. But you may also see reports of analyses done using methods like ANOVA, and multilevel or (increasingly) linear mixed-effects models [@meteyard2020].\nIn the research on the reproducibility of results in the literature (?@sec-checkanalyses), the researchers attempting to reproduce results often focused on answering the research question the original authors stated using the data the original authors shared. This does not mean that they always tried to exactly reproduce an analysis or an analysis result. Sometimes, that was not possible.\nSometimes, you will encounter an article and a dataset you are interested in but the analysis presented in the article looks a bit complicated, or more complex than the methods you have learned would allow you to do. In this situation, don’t give up. What you can do – maybe with our advice – is identify a part of the primary result that you can try to reproduce. For example, what if the original study authors report a linear mixed-effects analysis of the effects of both prime relatedness and schizotypy score on response reaction time [@rodríguez-ferreiro2020]? Maybe you have not learned about mixed-effects models, or you have not learned about analysing the effects of two variables but you have (you will) learn about analysing the effect of one variable using the linear model method: OK then, do an analysis of the shared data using the method you know.\nYou may be helped, here, by knowing about two good-enough (mostly true) insights from statistical analysis:\n\nMany of the common analysis methods you see used in psychological science can be coded as a linear model.\nMore advanced common analysis methods — (Generalized) Linear Mixed-effects Models (GLMMs) — can be understood as more sophisticated versions of the linear model. (Conversely, the linear model can be understood as an approximation of a GLMM.)\n\nThere is a nice discussion of the idea that common statistical tests are linear models here.\n\n\n\n\n\n\nTip\n\n\n\n\nIdentify the analysis method used to get the result you are interested in.\nIf it is complex or unfamiliar, discuss whether a simpler method can be used.\nIf the result is complex, discuss whether you can attempt to reproduce a part or a simpler result.\n\n\n\n\n\n\nIt can be interesting and important work to complete a simpler analysis of shared data. Sometimes, we learn that a simpler analysis is as good account of the behaviour we observe as other more complex analyses. This can happen if, for example, our theory predicts that two effects should work together but an analysis shows that we can explain behaviour in an account in which the two effects are independent. For example, @ricketts2021 predicted that children should learn words more effectively if they were shown the spellings of the words and they were told they would be helped by seeing the spelling but, in our data, we found that just seeing the spellings was enough to explain the learning we observed.\nIn completing analyses that vary from original analyses, we are engaging in the kind of work people do when they do multiverse analyses or robustness checks (?@sec-multiverse).\n\n\n\n\n\n\nTip\n\n\n\nIn planning an alternate or multiverse analysis, do not suppose that you need to do multiple analyses: you do not.\n\n\nIn planning an alternate or multiverse analysis, you will want to begin by critically evaluating the analysis you see described in the published article. I talk about how to do this, next.\nBefore we go on, note that I previously discussed an example of how to critically evaluate the results of published research in the context of @rodríguez-ferreiro2020. Take a look at the Introduction of that article. There, we summarised the analyses researchers did previously and used the information about the analyses to explain inconsistencies in the research literature. We found limitations in the analyses that people did that had (negative) consequences for the strength of the conclusions we can take from the data.\n\n\nIf you revisit our discussion of multiverse analyses, you will see that we discussed two things: (1.) analyses of the impact on results of varying how you construct datasets for analysis (?@sec-multiversedata) and (2.) analyses of the impact on results of varying what analysis method you use, or how you use the method (?@sec-multiverseanalysis). These are both good ways to approach thinking about the description of the analysis you see in a published article.\nAs we noted in ?@sec-multiversedata, you almost always have to process the data you collect (in an experiment or a survey) before you can analyze the data. Often, this means you need to code for responses to survey questions e.g. asking people to self-report their gender, or you need to identify and code for people making errors when they try to do the experimental task you set them, or you need to process the data to exclude participants who took too long to do the task (if taking too long is a problem). Not all of these processing steps will have an impact on the results but some might. This is why you can sometimes do useful and sometimes original research work in reanalyzing previously published data.\nYou can begin your analysis planning work by first identifying exactly what data processing the original study authors did then identifying what different data processing they could have done. Remember the research we discussed in relation to reproducibility studies, you need to be prepared for the possibility that it is challenging to identify what researchers did to process their data for analysis ?@sec-datachallenges. To identify the information you need, look for keywords like code, exclude, process, tidy, transform in the text of the article, or look for words like this in the documentation you find in the data repository.\nWhen you have identified this information, you can then consider three questions:\n\nWhat data processing steps were completed before analysis?\nWhat were the reasons given explaining why these processing steps were completed?\nWhat could happen to the results if different choices were made?\n\nWorking through these questions can then get you to a good plan for an analysis of the data. For example, a simple but useful analysis you can do is to check what happens to the results if you do an analysis with data from all the participants tested, if participants are excluded (for some reason) in the data processing step. Obviously, if the original study authors only share processed data, you cannot do this kind of work. Another simple but useful analysis you can do is to check what happens to the results if you change the coding of variables. Sometimes different coding of categorical variables (e.g., ethnicity) are reasonable. For example, you can ask: what happens if you analyze the impact of the variable given a different coding? (In case you are reading these notes and thinking about recoding a factor, there are some useful functions you can use; read about them here.)\n\n\n\n\n\n\nTip\n\n\n\n\nDo you want to check the impact of varying data processing choices: check, do you need and have access to the raw data? can you see how to recode variables?\n\n\n\nAs we noted in ?@sec-multiverseanalysis, when we consider how to answer a research question with a dataset, it is often possible to imagine multiple different analysis methods: reasonable alternatives. Most often, this is most clearly apparent when we are looking at an observational dataset or data collected given a cross-sectional study design.\nIn cross-sectional or observational studies, we typically are not manipulating experimental conditions, and we are often analyzed data using some kind of linear model. We often collect data or have access to data on a number of different variables relevant to our interests. For example, in studies I have done on how people read [@davies2013; @davies2017], we wanted to know what factors would predict or influence how people do basic reading tasks like reading aloud. We collected information on many different kinds of word properties and on the attributes of the participants we tested. (Note: the papers are associated with data repositories in Supplementary Materials.) It is an open question which variables should be included in a prediction model of the observed outcome (reading response reaction times). Therefore, if you are interested in a study like this, and can access usable data from the study, it will often be true that you are able to sensibly motivate a different analysis of the study data using a different choice of variables.\nAs discussed in a number of interesting analyses, over the years [e.g., @patel2015], researchers may be interested in the specific impact of one particular predictor variable (e.g., we may be interested in whether it is easier to read words we learned early in life), but will need to include in their analysis that variable plus other variables known to affect the outcome. In that situation, the effect of the variable of interest may appear to be different depending on what other variables are also analyzed. This makes it interesting and useful to check the impact of different analysis choices.\nWe will look at data like these, for analyses involving the linear model, in our classes on this method.\n\n\n\n\n\n\nTip\n\n\n\n\nDo you want to check the impact of different analysis choices: check, do you need and have access to a choice of variables?\nCan you think of some reasons to justify using a different choice of variables in your analysis.\n\n\n\n\n\n\n\n\nHere’s a quick summary of the advice we have discussed so far.\n\nAt the start of your work, you will need to (1.) locate then (2.) access data for analysis, and then you will need to (3.) check that the data are usable.\nOnce you have confirmed you have found interesting data you can use, you should plan your analysis.\nStudents do a variety of kinds of analysis. Whatever your interest, you first will want to first make sure you can identify exactly what the authors of the original study did, how they did it, and why they did it.\nIf you are interested in attempting a methods reproducibility test (can you repeat a result, given shared data?) you will perhaps benefit from focusing a result that is both substantive and straightforward.\nIf you are interested in doing an alternate analysis, you can critically evaluate the data processing and the data analysis choices that the original study authors made. You can consider whether other choices would be appropriate, and might sensibly motivate a (limited) investigation of the impact of a different analysis pipeline choice on the results.\n\nWhat if you access interesting data that were shared without a previous analysis? We talk about that situation, next."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/how.html#sec-noanalysis",
    "href": "PSYC402/extra_ToBeOrganised/how.html#sec-noanalysis",
    "title": "How",
    "section": "",
    "text": "A number of datasets have been published online with information about the data but with no analysis. You can look for data that may be interest you in a number of different places, now, but I would focus on one. I talk about that next. Then I offer some guidance on how you might approach analyzing such data Section 1.3.2.\n\n\nWicherts and colleagues set up the Journal of Open Psychology Data (JOPD) to make it easier for Psychologists to share experimental data. A link to the journal webpage is here) Usually, a data paper reports a study and provides a link to a downloadable dataset.\nSome datasets that I have looked at in JOPD and other places include the following.\n\n\nWicherts did what he recommended and put a large dataset online here\nYou can analyse these data in a number of different interesting ways. You can explore relationships between gender, intelligence and personality differences.\nThe data file and an explanatory document are located at the end of the article. Read the article, it’s worth your time. Wicherts reports:\n\nThe file includes data from our freshman-testing program called “Testweek” ( Busato et al., 2000, Smits et al., 2011 and Wicherts and Vorst, 2010) in which 537 students (age: M = 21.0, SD = 4.3) took the Advanced Progressive Matrices ( Raven, Court, & Raven, 1996), a test of Arithmetic, a Number Series test, a Hidden Figures Test, a test of Vocabulary, a test of Verbal Analogies, and a Logical Reasoning test ( Elshout, 1976).\nAlso included are data from a Dutch big five personality inventory (Elshout & Akkerman, 1975), the NEO-PI-R ( Hoekstra, Ormel, & Fruyt, 1996), scales of social desirability and impression management (based on work by Paulhus, 1984 and Wicherts, 2002), sex of the participants, and grade point averages of the freshmen’s first trimester that may act as outcome variable.\n\n\n\n\nSmits and colleagues (including Wicherts) put an even larger dataset online at the Journal of Open Psychology Data here)\nYou will need to register to be able to download the data but the process is simple.\nThe Smits dataset includes Big-5 personality scores for several thousand individuals recorded over a series of years. You can analyse these data in interesting ways including examining changes in personality scores among students over different years.\n\n\n\nTjew A Sin and colleagues shared a dataset at the Journal of Open Psychology Data on an interesting study they did to test the idea that interpersonal touch or simulated interpersonal touch can relieve existential concerns (fear of death) among individuals with low self-esteem. The data can be found here)\nThe Tjew A Sin can be downloaded from a link to a repository location, given at the end of the article. You will likely need to register to download the data. Note that the spreadsheets holding the study data include 999 values to code for missing data. Note also that the data spreadsheets include (in different columns) scores per participant for various measures e.g. mortality anxiety or self-esteem. The measures are explained in the paper. To use the data, you will need to work out the simple process of how to sum the scores across items to get e.g. a measure of self-esteem for each person.\n\n\n\nBerger and Anaki shared data on the disgust sensitivity of a large sample of individuals. The data are from the administration of the Disgust Scale to a set of Hebrew speakers. They can be found here)\nThe experimenters collected data on participants’ characteristics so that analyses of the way in which sensitivity varies in relation to demographic attributes is possible. You will see that the disgust scale is explained in the paper. The different disgust scores, for each item in the disgust scale, can be found in different columns. The disgust scores, for person, are calculated overall as values: Mean_general_ds, Mean_core, Mean_Animal_reminder, Mean_Contamination\nWhen you download the dataset, you may need to change the file name, adding a suffix: .txt (for the tab delimited file), to be opened in Excel, or .sav (for the SPSS data file), to be opened in SPSS – to the file name to allow you to open it in the appropriate application.\n\n\n\n\nThe availability of rich, curated, clearly usable datasets with many variables can make it challenging to decide what to do.\nI would advise beginning with an exploratory analysis of the data you have accessed. You will want to begin by using the data visualization skills we have taught you to examine:\n\nThe distributions of the variables that interest you using histograms, density plots or bar charts.\nThe potential relationship between variables using scatterplots.\n\nIn such Exploratory Data Analyses, you are interested in what the data visualization tells you about the nature of the dataset you have accessed. The papers associated with the datasets can sometimes offer only outline information: how the data were collected, coded, and processed. You may need to satisfy yourself that there is nothing odd or surprising about the distributions of scores. This stage can help you to identify problems like survey responses with implausible scores.\nThe work you do in exploring, and summarizing, the data variables that interest you will often constitute a substantial element of the work you can do and present for your report. You may discuss, for advice, what parts of this work will be interesting or useful to present.\nThen, our advice is simple.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen working with open datasets, consider keeping the analysis simple.\n\n\n\nNote that simple is relative. Do what interests you. Work with the methods you have learned or will learn (the linear model).\nIn practice, you will find that part of the challenge is located not in using the data or in running an analysis like a linear model, it is in (1.) justifying or motivating the analysis and (2.) explaining the implications of your findings.\nWorking on the thinking you must develop to motivate an analysis or to explain implications requires you to do some (limited) reading of relevant research. (Relevant sources will be cited in data papers, as part of their outline of the background for their data collection.) If you consider the advice we discussed in the graduate class on developing writing skills, you will see that there I talked about how you might extract data from a set of relevant sources (papers) to get an understanding of the questions people ask, the assumptions they make. That is the kind of process you can follow to develop your thinking around the analysis you will do. What you are looking for is information you can use so that you can say something brief about, for example, why it might be interesting to analyze, say, whether personality (measured using the Big-5) varies given differences in gender or differences between population cohorts. The reading and the conceptual development should be fairly limited, not extensive, but should be sufficient that you can write something sensible when you introduce and then when you discuss your analysis results."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/how.html#summary-how",
    "href": "PSYC402/extra_ToBeOrganised/how.html#summary-how",
    "title": "How",
    "section": "",
    "text": "In this chapter, I have outlined some advice on how you might approach the task of locating, accessing, and analyzing previously collected data. The main advice is to think about your workflow in stages, then progress through the work one step at a time.\nYou will need to begin by assuring yourself that you can find a dataset that interests you, and that you can access and use the data. The usability of data will require clear, understandable, descriptions in the published article (if any) about the research question and hypothesis, the study design, the data collection methods, the data processing steps, and the data analysis (if any). Sometimes, useful information about data processing and data analysis can be found in detail in repository documentation (e.g., in guides to materials) but only referenced in the text of the article.\nIf you know you can locate, access and have checked data as usable, you will want to think about what analysis you want to do the data. The approach you take depending on what aims you would like to pursue.\nIf you are interested in attempting a methods reproducibility test (i.e. checking if you can repeat presented results, given shared data), then you will first need to identify a substantive and straightforward result to try to reproduce. If you identify a primary result to examine, you will want to check that you can work with the data that have been shared, and then that you can use the analysis methods you have learned to reproduce some or all of the result that interests you.\nIf you are interested in doing an alternate or a different analysis (from what may be presented), you may need to consider the information you can locate on data processing and on data analysis choices. Did the original study authors process the data before sharing it, how? are the raw data available? What analyses did the authors do and why? When you consider this information, you may critically evaluate the choices made. In the context of this critical evaluation, you may find good reasons to justify doing a different analysis, whether to examine the impact of making different data processing choices, or to examine the impact of using a different analysis method, or of applying the same method differently (e.g., by including different variables).\nIn considering an analysis of data shared without a published set of results, you may want to keep your approach simple. Focus on what analysis you can do using the methods you have learned. And think about the understanding you will need to develop, to justify the analysis you do, and to make sense, in the discussion of your report of the analysis results you will present.\nIt is always a good idea to explore your data using visualization techniques throughout your workflow.\n\n\n\n\n\n\nTip\n\n\n\n\nYou can always get advice, do not hesitate to ask.\nWe are happy to discuss your thinking, especially in class."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html",
    "title": "Data visualization",
    "section": "",
    "text": "In writing this chapter, I have two aims.\n\nThe first aim for this chapter is to expose students to an outline summary of some key ideas and techniques for data visualization in psychological science.\n\nThere is an extensive experimental and theoretical literature concerning data visualization, what choices we can or should make, and how these choices have more or less impact, in different circumstances or for different audiences. Here, we can only give you a flavour of the on-going discussion. If you are interested, you can follow-up the references in the cited articles. But, using this chapter, I hope that you will gain a sense of the reasons how or why we may choose to do different things when we produce visualizations.\n\nThe second aim is to provide materials, and to show visualizations, to raise an awareness of what results come from making different choices. This is because we hope to encourage students to make choices based on reasons and it is hard to know what choices count without first seeing what the results might look like.\n\nIn my experience, knowing that there are choices is the first step. In proprietary software packages like Excel and SPSS there are plenty of choices but these are limited by the menu systems to certain combinations of elements. Here, in using R to produce visualizations, there is much more freedom, and much more capacity to control what a plot shows and how it looks, but knowing where to start has to begin with seeing examples of what some of the choices result in.\nAt the end of the chapter, I highlight some resources you can use in independent learning for further development, see Section 1.9.\nSo, we are aiming to (1.) start to build insight into the choices we make and (2.) provide resources to enable making those choices in data visualization.\n\n\n\nData visualization is important. Building skills in visualization matters to you because, even if you do not go on to professional work in which you produce visualizations you will certainly be working in fields in which you need to work with, or read or evaluate, visualizations.\nYou have already been doing this: our cultural or visual environment is awash in visualizations, from weather maps to charts on the television news. It will empower you if you know a bit about how or why these visualizations are produced in the ways that they are produced. That is a complex development trajectory but we can get started here.\nIn the context of the research report exercise, see ?@sec-pipeline, I mention data visualization in relation to stages of the data analysis pipeline or workflow. But the reality is that, most of the time, visualization is useful and used at every stage of data analysis workflow.\n\n\n\n\n\n\n\n\nQ\n\n \n\ncluster_R\n\n   \n\nnd_1\n\n Get raw data   \n\nnd_2\n\n Tidy data   \n\nnd_1-&gt;nd_2\n\n    \n\nnd_3_l\n\n Visualize   \n\nnd_2-&gt;nd_3_l\n\n    \n\nnd_3\n\n Analyze   \n\nnd_2-&gt;nd_3\n\n    \n\nnd_3_r\n\n Explore   \n\nnd_2-&gt;nd_3_r\n\n    \n\nnd_3_a\n\n Assumptions   \n\nnd_3_a-&gt;nd_3_l\n\n    \n\nnd_3_a-&gt;nd_3\n\n    \n\nnd_3_a-&gt;nd_3_r\n\n    \n\nnd_3_l-&gt;nd_3\n\n   \n\nnd_4\n\n Present   \n\nnd_3_l-&gt;nd_4\n\n    \n\nnd_3-&gt;nd_4\n\n   \n\n\nFigure 1: The data analysis pipeline or workflow\n\n\n\n\n\n\n\nI write this chapter with three kinds of honesty in mind.\n\nI will expose some of the process involved in thinking about and preparing for the production of plots.\n\n\nI can assure you that when a professional data analysis worker produces plots in R they will be looking for information about what to do, and how to do it, online. I will provide links to the information I used, when I wrote this chapter, in order to figure out the coding to produce the plots.\nI won’t pretend that I got the plots “right first time” or that I know all the coding steps by memory. Neither is true for me and they would not be true for most professionals if they were to write a chapter like this. Looking things up online is something we all do so showing you where the information can be found will help you grow your skills.\n\n\nI will show how we often prepare for the production of plots by processing the data that we must use to inform the plots.\n\n\nWe almost always have to process the data we collected or gathered together from our exerimental work or our observations.\nIn this chapter, some of the coding steps I will outline are done in advance of producing a plot, to give the plotting code something to work with.\nKnowing about these processing steps will ensure you have more flexibility or power in getting your plots ready.\n\n\nI am going to expose variation, as often as I can, in observations.\n\n\nWe typically collect data about or from people, about their responses to things we may present (stimuli) or, given tasks, under different conditions, or concerning individual differences on an array of dimensions.\nSources of variation will be everywhere in our data, even though we often work with statistical analyses (like the t-test) that focus our attention on the average participant or the average response.\nModern analysis methods (like mixed-effects models) enable us to account for sources of variation systematically, so it is good to begin thinking about, say, how people vary in their response to different experimental conditions from early in your development.\n\n\n\n\nThe approach we will take is to focus on step-by-step guides to coding. I will show plots and I will walk through the coding steps, explaining my reasons for the choices I make.\nWe will be working with plotting functions like ggplot() provided in libraries like ggplot2 [@R-ggplot2] which is part of the tidyverse [@R-tidyverse] collection of libraries.\nYou can access information about the tidyverse collection here.\n\n\nThe gg in ggplot stands for the “Grammar of Graphics”, and the ideas motivating the development of the ggplot2 library of functions are grounded in the ideas concerning the grammar of graphics, set out in the book of that name [@wilkinson2013].\nWhat is helpful to us, here, is the insight that the code elements (and how they result in visual elements) can be identified as building blocks, or layers, that we can add and adjust piece by piece when we are producing a visualization.\nA plot represents information and, critically, every time we write ggplot code we must specify somewhere the ways that our plot links data to something we see. In terms of ggplot, we specify aesthetic mappings using the aes() code to tell R what variables should be mapped e.g. to x-axis or y-axis location, to colour, or to group assignments. We then add elements to instruct R how to represent the aesthetic mappings as visual objects or attributes: geometric objects like a scatter of points geom_point() or a collection of bars geom_bar(); or visual features like colour, shape or size e.g. aes(colour = group). We can add visual elements in a series of layers, as shall see in the practical demonstrations of plot construction. We can adjust how scaling works. And we can add annotation, labels, and other elements to guide and inform the attention of the audience.\nYou can read more about mastering the grammar here.\n\n\n\nWe know that (some of) you want to see more use of pipes (represented as %&gt;% or |&gt;) in coding. There will be plenty of pipes in this chapter.\nIn using pipes in the code, I am structuring the code so that it works — and is presented — in a sequence of steps. There are different ways to write code but I find this way easier to work with and to read and I think you will too.\nLet’s take a small example:\n\nsleepstudy %&gt;%\n  group_by(Subject) %&gt;%\n  summarise(average = mean(Reaction)) %&gt;%\n  ggplot(aes(x = average)) + \n  geom_histogram()\n\n\n\n\nHere, we work through a series of steps:\n\nsleepstudy %&gt;% we first tell R we want to work with the dataset called sleepstudy and the %&gt;% pipe symbol at the end of the line tells R that we want it to pass that dataset on to the next step for what happens next.\ngroup_by(Subject) %&gt;% tells R that we want it to do something, here, group the rows of data according to the Subject (participant identity) coding variable, and pass the grouped data on to the next step for what happens following.\nsummarise(average = mean(Reaction)) %&gt;% tells R to take the grouped variable and calculate a summary, the mean Reaction score, for each group of observations for each participant. The %&gt;% pipe at the end of the line tells R to pass the summary dataset of mean Reaction scores on to the next process.\nggplot(aes(x = average)) + tells R that we want it to take these summary average Reaction scores and make a plot out of them.\ngeom_histogram() tells R that we want a histogram plot.\n\nWhat you can see is that each line ending in a %&gt; pipe passes something on to the next line. A following line takes the output of the process coded in the preceding line, and works with it.\nEach step is executed in turn, in strict sequence. This means that if I delete line 3 summarise(average = mean(Reaction)) %&gt;% then the following lines cannot work because the ggplot() function will be looking for a variable average that does not yet exist.\n\n\n\n\n\n\nWarning\n\n\n\n\nYou can see that in the data processing part of the code, successive steps in data processing end in a pipe %&gt;%.\nIn contrast, successive steps of the plotting code add ggplot elements line by line with each line (except the last) ending in a +.\n\n\n\nNotice that none of the processing steps actually changes the dataset called sleepstudy. The results of the process exist and can be used only within the sequence of steps that I have coded. If you want to keep the results of processing steps, you need to assign an object name to hold them, and I show how to do this, in the following.\nYou can read a clear explanation of pipes here.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the code you see:\n\nEach chunk of code is highlighted in the chapter.\nIf you hover a cursor over the highlighted code a little clipboard symbol appears in the top right of the code chunk.\nClick on the clipboard symbol to copy the code, paste it into your own R-Studio instance.\nThen experiment: try out things like removing or commmenting out lines, or changing lines, to see what effect that has.\nBreaking things, or changing things, helps to show what each bit of code does.\n\n\n\n\n\n\n\nData visualization is not really about coding, as about thinking.\n\nWhat are our goals?\nWhy do we make some choices instead of others?\n\n\n\n@gelman2013 outline the goals we may contemplate when we produce or evaluate visual data displays. In general, they argue, we are doing one or both of two things.\n\nDiscovery\nCommunication\n\nIn practice, this may involve the following (I paraphrase them, here).\n\nDiscovery goals\n\n\nGetting a sense of what is in a dataset, checking assumptions, confirming expectations, and looking for distinct patterns.\nMaking sense of the scale and complexity of the dataset.\nExploring the data to reveal unexpected aspects. As we will see, using small multiples (grids of plots) can often help with this.\n\n\nCommunication goals\n\n\nWe communicate about our data to ourselves and to others. The process of constructing and evaluating a plot is often one way we speak to ourselves about own data, developing an understanding of what we have got. Once we have done this for ourselves, we can better figure out how to do it to benefit the understanding of an audience.\nWe often use a plot to tell a story: the story of our study, our data, or our insight and how we get to it.\nWe can use visualizations to attract attention and stimulate interest. Often, in presenting data to an audience through a talk or a report we need to use effective visualizations to ensure we get attention and that we locate the attention of our audience in the right places.\n\n\n\n\nYou will see a rich variety of data visualizations in media and in the research literature. You will know that some choices, in the production of those visualizations, appear to work better than others.\nSome of the reasons why some choices work better will relate to what we can understand in terms of the psychological science of how visual data communication works. A useful recent review of relevant research is presented by @franconeri2021.\n@franconeri2021 provide a reason for working on visualizations: they allow us humans to process an array of information at once, often faster than if we were reading about the information, bit by bit. Effective visualization, then, is about harnessing the power of the human visual system, or visual cognition, for quick, efficient, information processing. Critically for science, in addition, visualizations can be more effective for discovering or communicating the critical features of data than summary statistics, as we shall see.\nIn producing visualizations, we often work with a vocabulary or palette of objects or visual elements. @franconeri2021 discuss how visualizations rely on visual channels to transform numbers into images that we can process visually.\n\nDot plots and scatterplots represent values as position.\nBar graphs represent values as position (the heights of the tops of bars) but also as lengths.\nAngles are presented when we connect points to form a line, allowing us to encode the differences between points.\nIntensity can be presented through variation in luminance contrast or colour saturation.\n\nThese channels can be ordered by how precisely they have been found to communicate different numeric values to the viewer. Your audience may more accurately perceive the difference between two quantities if you communicate that difference through the difference in the location of two points than if you ask your audience to compare the angles of two lines or the intensity of two colour spots.\nIn constructing data visualizations, we often work with conventions, established through common practice in a research tradition. For example, if you are producing a scatterplot, then most of the time your audience will expect to see the outcome (or dependent variable) represented by the vertical height (on the y-axis) of points. And your audience will expect that higher points represent larger quantities of the y-axis variable.\nIn constructing visualizations, we need to be aware of the cognitive work that we require the audience to do. Comparisons are harder, requiring more processing and imposing more load on working memory. You can help your reader by guiding their attention, by grouping or ordering visual elements to identify the most important comparisons. We can vary colour and shape to group or distinguish visual elements. We can add annotation or elements like lines or arrows to guide attention.\nVisualizations are presented in context, whether in presentations or in reports. This context should be provided, by you the producer, with the intention to support the communication of your key messages. A visual representation, a plot, will be presented with a title, maybe a title note, maybe with annotation in the plot, and maybe with accompanying text. You should use these textual elements to lead your audience, to help them make sense of what they are looking at.\nThe diversity of audiences means that we should habitually add alt text for data visualizations to help those who use screen readers by providing a summary description of what images show. This chapter has been written using Quarto and rendered to .html with alt text included along with all images. Please do let me know if you are using a screen reader and the alt text description is or is not so helpful.\nYou can read a helpful explanation of alt text here.\nIf you use colour in images then we should use colour bind colour palettes.\nYou can read about using colour blind palettes here or here.\nIn the following practical exercises, we work with many of the insights in our construction of visualizations.\n\n\n\n\nWe can get started before we understand in depth the key ideas or the coding steps. This will help to show where we are going. We will work with the sleepstudy dataset.\nI will model the process, to give you an example workflow:\n\nthe data, where they come from — what we can find out;\nhow we approach the data — what we expect to see;\nhow we visualize the data — discovery, communication.\n\n\n\nWhen we work with R, we usually work with functions like ggplot() provided in libraries like ggplot2 [@R-ggplot2]. These libraries typically provide not only functions but also datasets that we can use for demonstration and learning.\nThe lme4 library [@R-lme4] provides the sleepstudy dataset and we will take a look at these data to offer a taste of what we can learn to do. Usually, information about the R libraries we use will be located on the Comprehensive R Archive Network (CRAN) web pages, and we can find the technical reference information for lme4 in the CRAN reference manual for the library, where we see that the sleepstudy data are from a study reported by [@belenky2003]. The manual says that the sleepstudy dataset comprises:\n\nA data frame with 180 observations on the following 3 variables. [1.] Reaction – Average reaction time (ms) [2.] Days – Number of days of sleep deprivation [3.] Subject – Subject number on which the observation was made.\n\nWe can take a look at the first few rows of the dataset.\n\nsleepstudy %&gt;%\n    head(n = 4)\n\n  Reaction Days Subject\n1 249.5600    0     308\n2 258.7047    1     308\n3 250.8006    2     308\n4 321.4398    3     308\n\n\nWhat we are looking at are:\n\nThe average reaction time per day (in milliseconds) for subjects in a sleep deprivation study. Days 0-1 were adaptation and training (T1/T2), day 2 was baseline (B); sleep deprivation started after day 2.\n\nThe abstract for @belenky2003 tells us that participants were deprived of sleep and the impact of relative deprivation was tested using a cognitive vigilance task for which the reaction times of responses were recorded.\nSo, we can expect to find:\n\nA set of rows corresponding to multiple observations for each participant (Subject)\nA reaction time value for each participant (Reaction)\nRecorded on each Day\n\n\n\n\nIn data analysis work, we often begin with the objective to understand the structure or the nature of the data we are working with.\nYou can call this the discovery phase:\n\nwhat have we got?\ndoes it match our expectations?\n\nIf these are reaction time data (collected in an cognitive experiment) do they look like cognitive reaction time data should look? We would expect to see a skewed distribution of observed reaction times distributed around an average located somewhere in the range 200-700ms.\nFigure 2 represents the distribution of reaction times in the sleepstudy dataset.\nI provide notes on the code steps that result in the plot. Click on the Notes tab to see them. Later, I will discuss some of these elements.\n\nPlotNotes\n\n\n\nsleepstudy %&gt;%\n  ggplot(aes(x = Reaction)) +\n  geom_histogram(binwidth = 15) +\n  geom_vline(xintercept = mean(sleepstudy$Reaction), \n             colour = \"red\", linetype = 'dashed', size = 1.5) +\n  annotate(\"text\", x = 370, y =20, \n                    colour = \"red\", \n                    label = \"Average value shown in red\") +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 2: Figure showing a histogram of sleepstudy reaction time data\n\n\n\n\n\n\nThe plotting code pipes the data into the plotting code steps to produce the plot. You can see some elements that will be familiar to you and some new elements.\n\nsleepstudy %&gt;%\n  ggplot(aes(x = Reaction)) +\n  geom_histogram(binwidth = 15) +\n  geom_vline(xintercept = mean(sleepstudy$Reaction), \n             colour = \"red\", linetype = 'dashed', size = 1.5) +\n  annotate(\"text\", x = 370, y =20, \n                    colour = \"red\", \n                    label = \"Average value shown in red\") +\n  theme_bw()\n\nLet’s go through the code step-by-step:\n\nsleepstudy %&gt;% asks R to take the sleepstudy dataset and %&gt;% pipe it to the next steps for processing.\nggplot(aes(x = Reaction)) + takes the sleepstudy data and asks R to use the ggplot() function to produce a plot.\naes(x = Reaction) tells R that in the plot we want it to map the Reaction variable values to locations on the x-axis: this is the aesthetic mapping.\ngeom_histogram(binwidth = 15) + tells R to produce a histogram then add a step.\ngeom_vline(...) + tells R we want to draw vertical line.\nxintercept = mean(sleepstudy$Reaction), ... tells R to draw the vertical line at the mean value of the variable Reaction in the sleepstudy dataset.\ncolour = \"red\", linetype = 'dashed', size = 1.5 tells R we want the vertical line to be red, dashed and 1.5 times the usual size.\nannotate(\"text\", ...) tells R we want to add a text note.\nx = 370, y =20, ... tells R we want the note added at the x,y coordinates given.\ncolour = \"red\", ..; and we want the text in red.\n...label = \"Average value shown in red\") + tells R we want the text note to say that this is where the average is.\ntheme_bw() lastly, we change the theme.\n\n\n\n\nFigure 2 shows a distribution of reaction times, ranging from about 200ms to 500ms. The distribution has a peak around 300ms. The location of the mean is shown with a dashed red line. The distribution includes a long tail of longer times. This is pretty much what we would expect to see.\nWe may wish to communicate the information we gain through using this histogram, in a presentation or in a report.\n\n\n\nLet us imagine that it is our study. (Here, we shall not concern ourselves too much — with apologies — with understanding what the original study authors actually did.)\nIf we are looking at the impact of sleep deprivation on cognitive performance, we might predict that reaction times got longer (responses slowed) as the study progressed. Is that what we see?\nTo examine the association between two variables, we often use scatterplots. Figure 3 is a scatterplot indicating the possible association between reaction time and days in the sleepstudy data. Points are ordered on x-axis from 0 to 9 days, on y-axis from 200 to 500 ms reaction time.\nI provide notes on the code steps that result in the plot. Click on the Notes tab to see them. Later, I will discuss some of these elements.\n\nPlotNotes\n\n\n\nsleepstudy %&gt;%\n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point(size = 1.5, alpha = .5) + \n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  theme_bw()\n\n\n\n\nFigure 3: Figure showing a scatterplot of the relation between reaction time and days in the sleepstudy data\n\n\n\n\n\n\nNotice the numbered steps in producing this plot.\n\nsleepstudy %&gt;% \n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point() + \n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  theme_bw()\n\n\nName the dataset: the dataset is called sleepstudy in the lme4 library which makes it available therefore we use this name to specify it.\nsleepstudy %&gt;% uses the %&gt;% pipe operator to pass this dataset to ggplot() to work with, in creating the plot. Because ggplot() now knows about the sleepstudy data, we can next specify what aesthetic mappings we need to use.\nggplot(aes(x = Days, y = Reaction)) + tells R that we want to map Days information to x-axis position and Reaction (response time) information to y-axis position.\ngeom_point() + tells R that we want to locate points – creating a scatterplot – at the paired x-axis and y-xis coordinates.\nscale_x_continuous(breaks = c(0, 3, 6, 9)) + is new: we tell R that we want the x-axis tick labels – the numbers R shows as labels on the x-axis – at the values 0, 3, 6, 9 only.\ntheme_bw() requires R to make the plot background white and the foreground plot elements black.\n\nYou can find more information on scale_ functions in the ggplot2 reference information.\nhttps://ggplot2.tidyverse.org/reference/scale_continuous.html\n\n\n\nThe plot suggests that reaction time increases with increasing number of days.\nIn producing this plot, we are both (1.) engaged in discovery and, potentially, (2.) able to do communication.\n\nDiscovery: is the relation between variables what we should expect, given our assumptions?\nCommunication: to ourselves and others, what relation do we observe, given our sample?\n\nAt this time, we have used and discussed scatterplots before, why we use them, how we write code to produce them, and how we read them.\nWith two additional steps we can significantly increase the power of the visualization. Figure 4 is a grid of scatterplots indicating the possible association between reaction time and days separately for each participant.\nAgain, I hide an explanation of the coding steps in the Notes tab: the interested reader can click on the tab to view the step-by-step guide to what is happening.\n\nPlotNotes\n\n\n\nsleepstudy %&gt;%\n  group_by(Subject) %&gt;%\n  mutate(average = mean(Reaction)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Subject = fct_reorder(Subject, average)) %&gt;%\n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  facet_wrap(~ Subject) +\n  theme_bw()\n\n\n\n\nFigure 4: Figure showing a scatterplot of the relation between reaction time and days: here, we plot the data for each participant separately\n\n\n\n\n\n\nNotice the numbered steps in producing this plot.\n\nsleepstudy %&gt;%\n  group_by(Subject) %&gt;%\n  mutate(average = mean(Reaction)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Subject = fct_reorder(Subject, average)) %&gt;%\n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  facet_wrap(~ Subject) +\n  theme_bw()\n\nYou can see that the block of code combines data processing and data plotting steps. Let’s look at the data processing steps then the plotting steps in order.\nFirst: why are we doing this? My aim is to produce a plot in which I show the association between Days and Reaction for each Subject individually. I suspect that the association between Days and Reaction may be stronger – so the trend will be steeper – for participants who are slower overall. I suspect this because, given experience, I know that slower, less accurate, participants tend to show larger effects.\nSo: in order to get a grid of plots, one plot for each Subject, in order of the average Reaction for each individual Subject, I need to first calculate the average Reaction then order the dataset rows by those averages. I do that in steps, using pipes to feed information from one step to the next step, as follows.\n\nsleepstudy %&gt;% tells R what data I want to use, and pipe it to the next step.\ngroup_by(Subject) tells R I want it to work with data (rows) grouped by Subject identity code, %&gt;% piping the grouped form of the data forward to the next step\nmutate(average = mean(Reaction)) uses mutate() to create a new variable average which I calculate as the mean() of Reaction, piping the data with this additional variable %&gt;% forward to the next step.\nungroup() %&gt;% tells R I want it to go back to working with the data in rows not grouped rows, and pipe the now ungrouped form of the data to the next step.\nmutate(Subject = fct_reorder(Subject, average)) tells R I want it to sort the rows of the whole sleepstudy dataset in order, moving groups of rows identified by Subject so that data for Subject codes associated with faster times are located near the top of the dataset.\n\nThese data, ordered by Subject by the average Reaction for each participant, are then %&gt;% piped to ggplot to create a plot.\n\nggplot(aes(x = Days, y = Reaction)) + specifies the aesthetic mappings, as before.\ngeom_point() + asks R to locate points at the x-axis, y-axis coordinates, creating a scatterplot, as before.\ngeom_line() + is new: I want R to connect the points, showing the trend in the association between Days and Reaction for each person.\nscale_x_continuous(breaks = c(0, 3, 6, 9)) + fixes the x-axis labels, as before.\nfacet_wrap(~ Subject) + is the big new step: I ask R to plot a separate scatterplot for the data for each individual Subject.\n\nYou can see more information about facetting here:\nhttps://ggplot2.tidyverse.org/reference/facet_wrap.html\nIn short, with the facet_wrap(~ .) function, we are asking R to subset the data by a grouping variable, specified (~ .) by replacing the dot with the name of the variable.\nNotice that I use %&gt;% pipes to move the data processing forward, step by step. But I use + to add plot elements, layer by layer.\n\n\n\nFigure Figure 4 is a grid or lattice of scatterplots revealing how the possible association between reaction time and days varies quite substantially between the participants in the sleepstudy data. Most plots indicate that reaction time increases with increasing number of days. However, different participants show this trend to differing extents.\nWhat are the two additions I made to the conventional scatterplot code?\n\nI calculated the average reaction time per participant, and I ordered the data by those averages.\nI facetted the plots, breaking them out into separate scatterplots per participant.\n\nWhy would you do this? Variation between people or groups, in effects or in average outcomes, are often to be found in psychological data [@vasishth2021]. The variation between people that we see in these data — in the average response reaction time, and in how days affects times — would motivate the use of linear mixed-effects models to analyze the way that sleep patterns affect responses in the sleep study [@Pinheiro2000a].\n\n\n\n\n\n\nTip\n\n\n\nThe data processing and plotting functions in the tidyverse collection of libraries enable us to discover and to communicate variation in behaviours that should strengthen our and others’ scientific understanding.\n\n\n\n\n\nWhat we have seen, so far, is that we can make dramatic changes to the appearance of visualizations (e.g., through faceting) and also that we can exert fine control over the details (e.g., adjusting scale labels). What we need to stop and consider are what we want to do (and why), in what order.\nWe have seen how we can feed a data process into a plot to first prepare then produce the plot in a sequence of steps. In processing the data, we can take some original data and extract or calculate information that we can use for our plotting e.g. calculating the mean of a distribution in order to then highlight where that mean is located.\nWe have also seen the use of plots, and the editing of their appearance, to represent information visually. We can verbalize the thought process behind the production of these plots through a series of questions.\n\nAre we looking at the distribution of one variable (if yes: consider a histogram) or are we comparing the distributions of two or more variables (if yes: consider a scatterplot)?\nIs there a salient feature of the plot we want to draw the attention of the audience to? We can add a visual element (like a line) and annotation text to guide the audience.\nAre we interested in variation between sub-sets of the data? We can facet the plot to examine variation between sub-sets (facets) enabling the comparison of trends.\n\n\n\n\n\nIn this guide, we illustrate some of the ideas about visualization we discussed at the start, working with practical coding examples. We will be working with real data from a published research project. We are going to focus the practical coding examples on the data collected for the analysis reported by @ricketts2021.\n\nWe will focus on working with the data from one of the tasks, in one of the studies reported by @ricketts2021.\n\nThis means that you can consolidate your learning by applying the same code moves to data from the other task in the same study, or to data from the other study.\nIn applying code to other data, you will need to be aware of differences in, say, the way that some things like the outcome response variable are coded.\n\nYou can then further extend your development by trying out the coding moves for yourself using the data collected by @rodríguez-ferreiro2020.\n\nThese data are from a quite distinct kind of investigation, on a different research topic than the topic we will be exploring through our working examples.\nHowever, some aspects of the data structure are similar.\nCritically, the data are provided with comprehensive documentation.\n\n\n\n\nTo do our practical work, we will need functions and data. We get these at the start of our workflow.\n\n\nWe are going to need the lme4, patchwork, psych and tidyverse libraries of functions and data.\n\nlibrary(ggeffects)\nlibrary(patchwork)\nlibrary(psych)\nlibrary(tidyverse)\n\n\n\n\nYou can access the data we are going to use in two different ways.\n\n\nThe data associated with both [@ricketts2021] and [@rodríguez-ferreiro2020] are freely available through project repositories on the Open Science Framework web pages.\nYou can get the data from the @ricketts2021 paper through the repository located here.\nYou can get the data from the @rodríguez-ferreiro2020 paper through the repository located here.\nThese data are associated with full explanations of data collection methods, materials, data processing and data analysis code. You can review the papers and the repository material guides for further information.\nIn the following, I am going to abstract summary information about the @ricketts2021 study and data. I shall leave you to do the same for the @rodríguez-ferreiro2020 study.\n\n\n\nDownload the data-visualization.zip files folder and upload the files to RStudio Server.\nThe folder includes the @ricketts2021 data files:\n\nconcurrent.orth_2020-08-11.csv\nconcurrent.sem_2020-08-11.csv\nlong.orth_2020-08-11.csv\nlong.sem_2020-08-11.csv\n\nThe folder also includes the @rodríguez-ferreiro2020 data files:\n\nPrimDir-111019_English.csv\nPrimInd-111019_English.csv\n\n\n\n\n\n\n\nWarning\n\n\n\n\nThese data files are collected together in a folder for download, for your convenience, but the version of record for the data for each study comprise the files located on the OSF repositories associated with the original articles.\n\n\n\n\n\n\n\n\n@ricketts2021 conducted an investigation of word learning in school-aged children. They taught children 16 novel words in a study with a 2 x 2 factorial design. In this investigation, they tested whether word learning is helped by presenting targets for word learning with their spellings, and whether learning is helped by telling children that they would benefit from the presence of those spellings.\nThe presence of orthography (the word spelling) was manipulated within participants (orthography absent vs. orthography present): for all children, eight of the words were taught with orthography present and eight with orthography absent. Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\nA pre-test was conducted to establish participants’ knowledge of the stimuli. Then, each child was seen for three 45-minute sessions to complete training (Sessions 1 and 2) and post-tests (Session 3). @ricketts2021 completed two studies: Study 1 and Study 2. All children, in both studies 1 and 2 completed the Session 3 post-tests.\nIn Study 1, longitudinal post-test data were collected because children were tested at two time points. Children were administered post-tests in Session 3, as noted: Time 1. Post-tests were then re-administered approximately eight months later at Time 2 (\\(M = 241.58\\) days from Session 3, \\(SD = 6.10\\)). In Study 2, the Study 1 sample was combined with an older sample of children. The additional Study 2 children were not tested at Time 2, and the analysis of Study 2 data did not incorporate test time as a factor.\nThe outcome data for both studies consisted of performance on post-tests.\nThe semantic post-test assessed knowledge for the meanings of newly trained words using a dynamic or sequential testing approach. I will not explain this approach in more detail, here, because the practical visualization exercises focus on the orthographic knowledge (spelling knowledge) post-test, explained next.\nThe orthographic post-test was included to ascertain the extent of orthographic knowledge after training. Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure indexing the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. The maximum score is 0, with higher scores indicating less accurate responses.\nFor the Study 1 analysis, the files are:\n\nlong.orth_2020-08-11.csv\nlong.sem_2020-08-11.csv\n\nWhere long indicates the longitudinal nature of the data-set.\nFor the Study 2 analysis, the files are:\n\nconcurrent.orth_2020-08-11.csv\nconcurrent.sem_2020-08-11.csv\n\nWhere concurrent indicates the inclusion of concurrent (younger and older) child participant samples.\nEach column in each data-set corresponds to a variable and each row corresponds to an observation (i.e., the data are tidy). Because the design of the study involves the collection of repeated observations, the data can be understood to be in a long format.\nEach child was asked to respond to 16 words and, for each of the 16 words, we collected post-test responses from multiple children. All words were presented to all children.\nWe explain what you will find when you inspect the .csv files, next.\n\n\nThe variables included in .csv files are listed, following, with information about value coding or calculation.\n\nParticipant — Participant identity codes were used to anonymize participation. Children included in studies 1 and 2 – participants in the longitudinal data collection – were coded “EOF[number]”. Children included in Study 2 only (i.e., the older, additional, sample) were coded “ND[number]”.\nTime — Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\nStudy — Observations taken for children included in studies 1 and 2 – participants in the longitudinal data collection – were coded “Study1&2”. Children included in Study 2 only (i.e., the older, additional, sample) were coded “Study2”.\nInstructions — Variable coding for whether participants undertook training in the explicit or incidental conditions.\nVersion — Experiment administration coding\nWord — Letter string values show the words presented as stimuli to children.\nConsistency_H — Calculated orthography-to-phonology consistency value for each word.\nOrthography — Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.\nMeasure — Variable coding for the post-test measure: Sem_all if the semantic post-test; Orth_sp if the orthographic post-test.\nScore — Variable coding for response category.\n\nFor the semantic (sequential or dynamic) post-test, responses were scored as corresponding to:\n\n3 – correct response in the definition task\n2 – correct response in the cued definition task\n1 – correct response in the recognition task\n0 – if the item wasn’t correctly defined or recognised\n\nFor the orthographic post-test, responses were scored as:\n\n1 – correct, if the target spelling was produced in full\n0 – incorrect\n\nHowever, the analysis reported by @ricketts2021 focused on the more sensitive Levenshtein distance measure (see following).\n\nWASImRS — Raw score – Matrix Reasoning subtest of the Wechsler Abbreviated Scale of Intelligence\nTOWREsweRS — Raw score – Sight Word Efficiency (SWE) subtest of the Test of Word Reading Efficiency; number of words read correctly in 45 seconds\nTOWREpdeRS — Raw score – Phonemic Decoding Efficiency (PDE) subtest of the Test of Word Reading Efficiency; number of nonwords read correctly in 45 seconds\nCC2regRS — Raw score – Castles and Coltheart Test 2; number of regular words read correctly\nCC2irregRS — Raw score – Castles and Coltheart Test 2; number of irregular words read correctly\nCC2nwRS — Raw score – Castles and Coltheart Test 2; number of nonwords read correctly\nWASIvRS — Raw score – vocabulary knowledge indexed by the Vocabulary subtest of the WASI-II\nBPVSRS — Raw score – vocabulary knowledge indexed by the British Picture Vocabulary Scale – Third Edition\nSpelling.transcription — Transcription of the spelling response produced by children in the orthographic post-test\nLevenshtein.Score — Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure indexing the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. For example, the response ‘epegram’ for target ‘epigram’ attracts a Levenshtein score of 1 (one substitution). Thus, this score gives credit for partially correct responses, as well as entirely correct responses. The maximum score is 0, with higher scores indicating less accurate responses.\n\n(Notice that, for the sake of brevity, I do not list the z_ variables but these are explained in the study OSF repository materials.)\n\n\n\n\n\n\nWarning\n\n\n\nLevenshtein distance scores are higher if a child makes more errors in producing the letters in a spelling response.\n\nThis means that if we want to see what factors help a child to learn a word, including its spelling, then we want to see that helpful factors are associated with lower Levenshtein scores.\n\n\n\nTo demonstrate some of the processes we can enact to process and visualize data, and some of the benefits of doing so, we are going to work with the concurrent.orth_2020-08-11.csv dataset. These are data corresponding to the @ricketts2021 Study 2. concurrent refers to the analysis (a concurrent comparison) of data from younger and older children.\n\n\n\n\nAssuming you have downloaded the data files, we first read the dataset into the R environment: concurrent.orth_2020-08-11.csv. We do the data read in a bit differently than you have seen it done before; we will come back to what is going on (in Section 1.7.4.1).\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\",\n\n                      col_types = cols(\n\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n\n                      ))\n\nWe can inspect these data using summary().\n\nsummary(conc.orth)\n\n  Participant   Time          Study         Instructions Version\n EOF001 :  16   1:1167   Study1&2:655   explicit  :592   a:543  \n EOF002 :  16            Study2  :512   incidental:575   b:624  \n EOF004 :  16                                                   \n EOF006 :  16                                                   \n EOF007 :  16                                                   \n EOF008 :  16                                                   \n (Other):1071                                                   \n         Word     Consistency_H     Orthography     Measure    \n Accolade  : 73   Min.   :0.9048   absent :583   Orth_sp:1167  \n Cataclysm : 73   1st Qu.:1.5043   present:584                 \n Contrition: 73   Median :1.9142                               \n Debacle   : 73   Mean   :2.3253                               \n Dormancy  : 73   3rd Qu.:3.0436                               \n Epigram   : 73   Max.   :3.9681                               \n (Other)   :729                                                \n     Score           WASImRS     TOWREsweRS      TOWREpdeRS       CC2regRS    \n Min.   :0.0000   Min.   : 5   Min.   :51.00   Min.   :19.00   Min.   :28.00  \n 1st Qu.:0.0000   1st Qu.:13   1st Qu.:69.00   1st Qu.:35.00   1st Qu.:36.00  \n Median :0.0000   Median :17   Median :74.00   Median :41.00   Median :38.00  \n Mean   :0.2913   Mean   :16   Mean   :74.23   Mean   :41.59   Mean   :36.91  \n 3rd Qu.:1.0000   3rd Qu.:19   3rd Qu.:80.00   3rd Qu.:50.00   3rd Qu.:39.00  \n Max.   :1.0000   Max.   :25   Max.   :93.00   Max.   :59.00   Max.   :40.00  \n                                                                              \n   CC2irregRS       CC2nwRS         WASIvRS          BPVSRS     \n Min.   :17.00   Min.   :13.00   Min.   :16.00   Min.   :103.0  \n 1st Qu.:23.00   1st Qu.:29.00   1st Qu.:25.00   1st Qu.:119.0  \n Median :25.00   Median :33.00   Median :29.00   Median :133.0  \n Mean   :25.24   Mean   :32.01   Mean   :29.12   Mean   :130.9  \n 3rd Qu.:27.00   3rd Qu.:37.00   3rd Qu.:33.00   3rd Qu.:142.0  \n Max.   :35.00   Max.   :40.00   Max.   :39.00   Max.   :158.0  \n                                                                \n Spelling.transcription Levenshtein.Score  zTOWREsweRS        zTOWREpdeRS      \n Epigram   : 57         Min.   :0.000     Min.   :-2.67807   Min.   :-2.33900  \n Platitude : 43         1st Qu.:0.000     1st Qu.:-0.60283   1st Qu.:-0.68243  \n Contrition: 42         Median :1.000     Median :-0.02638   Median :-0.06122  \n fracar    : 39         Mean   :1.374     Mean   : 0.00000   Mean   : 0.00000  \n Nonentity : 39         3rd Qu.:2.000     3rd Qu.: 0.66537   3rd Qu.: 0.87061  \n raconter  : 35         Max.   :7.000     Max.   : 2.16415   Max.   : 1.80243  \n (Other)   :912                                                                \n   zCC2regRS        zCC2irregRS          zCC2nwRS          zWASIvRS       \n Min.   :-3.3636   Min.   :-2.22727   Min.   :-3.1053   Min.   :-2.63031  \n 1st Qu.:-0.3435   1st Qu.:-0.60461   1st Qu.:-0.4920   1st Qu.:-0.82633  \n Median : 0.4115   Median :-0.06373   Median : 0.1614   Median :-0.02456  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.: 0.7890   3rd Qu.: 0.47716   3rd Qu.: 0.8147   3rd Qu.: 0.77721  \n Max.   : 1.1665   Max.   : 2.64070   Max.   : 1.3047   Max.   : 1.97986  \n                                                                          \n    zBPVSRS         mean_z_vocab       mean_z_read       zConsistency_H   \n Min.   :-1.9946   Min.   :-2.06910   Min.   :-2.39045   Min.   :-1.4153  \n 1st Qu.:-0.8495   1st Qu.:-0.85941   1st Qu.:-0.43321   1st Qu.:-0.8181  \n Median : 0.1525   Median :-0.01483   Median : 0.08829   Median :-0.4096  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.7967   3rd Qu.: 0.72964   3rd Qu.: 0.68438   3rd Qu.: 0.7157  \n Max.   : 1.9418   Max.   : 1.96083   Max.   : 1.52690   Max.   : 1.6368  \n                                                                          \n\n\nYou should notice one key bit of information in the summary. Focus on the summary for what is in the Participant column. You can see that we have a number of participants in this dataset, listed by Participant identity code in the summary() view e.g. EOF001. For each participant, we have 16 rows of data.\nWhen we ask R for a summary of a nominal variable or factor it will show us the levels of each factor (i.e., each category or class of objects encoded by the categorical variable), and a count for the number of observations for each level.\nTake a look at the rows of data for EOF001.\n\n\n\n\n\nParticipant\nTime\nStudy\nInstructions\nVersion\nWord\nConsistency_H\nOrthography\nMeasure\nScore\nWASImRS\nTOWREsweRS\nTOWREpdeRS\nCC2regRS\nCC2irregRS\nCC2nwRS\nWASIvRS\nBPVSRS\nSpelling.transcription\nLevenshtein.Score\nzTOWREsweRS\nzTOWREpdeRS\nzCC2regRS\nzCC2irregRS\nzCC2nwRS\nzWASIvRS\nzBPVSRS\nmean_z_vocab\nmean_z_read\nzConsistency_H\n\n\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nAccolade\n1.9142393\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nacalade\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.4095955\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nCataclysm\n3.5060075\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nCataclysm\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n1.1763372\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nContrition\n1.7486898\nabsent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nContrition\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.5745381\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nDebacle\n2.9008386\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\ndibarcle\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.5733869\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nDormancy\n1.6263089\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\ndoormensy\n3\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.6964704\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nEpigram\n1.3822337\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nEpigram\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.9396508\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nFoible\n2.7051987\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nFoible\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.3784641\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nFracas\n3.1443345\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nfracar\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.8159901\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nLassitude\n0.9048202\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nlacitude\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-1.4153141\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nLuminary\n1.0985931\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nloomenery\n4\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-1.2222516\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nNonentity\n3.9681391\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nnonenterty\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n1.6367746\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nPlatitude\n0.9048202\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nPlatitude\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-1.4153141\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nPropensity\n1.6861898\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\npropencity\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.6368090\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nRaconteur\n3.8245334\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nraconter\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n1.4936954\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nSyncopation\n3.0436450\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nsincipation\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.7156697\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nVeracity\n2.8693837\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nvaracity\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.5420473\n\n\n\n\n\n\n\nYou can see that for EOF001, as for every participant, we have information on the conditions under which we observed their responses (Instructions, Orthography), as well as information about the stimuli that we asked participants to respond to (e.g., Word, Consistency_H), information about the responses or outcomes we recorded (Measure, Score, Spelling.transcription,  Levenshtein.Score), and information about the participants themselves (e.g., TOWREsweRS, TOWREpdeRS).\n\n\n\nWe almost always need to process data in order to render the information ready for discovery or communication data visualization.\n\n\nYou will have seen that data processing began when we first read the data in for use. Let’s go back and take a look at the code steps.\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\",\n\n                      col_types = cols(\n\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n\n                        )\n                      )\n\nThe chunk of code is doing two things: first, we tell R what .csv file we want to read into the environment, and what we want to call the dataset; and then we tell R how we want to classify the data variable columns.\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\" first reads the named .csv file, creating an object I will call conc.orth: a dataset or tibble we can now work with in R.\n\n\nYou have been using the read.csv() function to read in data files.\nThe read_csv() function is the more modern tidyverse form of the function you were introduced to.\nBoth versions work in similar ways but read_csv() is a bit more efficient, and it allows us to do what we do next.\n\n\ncol_types = cols( ... ) tells R how to interpret some of the columns in the .csv.\n\n\nThe read_csv() function is excellent at working out what types of data are held in each column but sometimes we have to tell it what to do.\nHere, I am specifying with e.g. Participant = col_factor() that the Participant column should be treated as a categorical or nominal variable, a factor.\n\nUsing the col_types = cols( ... ) argument saves me from having to first read the data in then using code like the following to require, technically, coerce R into recognizing the nominal nature of variables like Participant with code like\n\nconc.orth$Participant &lt;- as.factor(conc.orth$Participant)\n\n\n\nI do not have to do step 2 of the read-in process, here. What happens if we use just read_csv()? Try it.\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\")\n\n\n\n\nYou can read more about read_csv() here\nYou can read more about col_types = cols() here\n\n\n\n\nThe @ricketts2021 dataset orth.conc is a moderately sized and rich dataset with several observations, on multiple variables, for each of many participants. Sometimes, we want to extract information from a more complex dataset because we want to understand or present a part of it, or a relatively simple account of it. We look at an example of how you might do that now.\nAs you saw when you looked at the summary of the orth.conc dataset, we have multiple rows of data for each participant. Recall the design of the study. For each participant, we recorded their response to a stimulus word, in a test of word learning, for 16 words.\nFor each participant, we have a separate row for each response the participant made to each word. But you will have noticed that information about the participant is repeated. So, for participant EOF001, we have data about their performance e.g. on the BPVSRS vocabulary test (they scored 126). Notice that that score is repeated: the same value is copied for each row, for this participant, in the BPVSRS column. The reason the data are structured like this are not relevant here 1 but it does require us to do some data processing, as I explain next.\nIt is a very common task to want to present a summary of the attributes of your participants or stimuli when you are reporting data in a report of a psychological research project. We could get a summary of the participant attributes using the psych library describe function as follows.\n\nconc.orth %&gt;%\n  select(WASImRS:BPVSRS) %&gt;%\n  describe(ranges = FALSE, skew = FALSE)\n\n           vars    n   mean    sd   se\nWASImRS       1 1167  16.00  4.30 0.13\nTOWREsweRS    2 1167  74.23  8.67 0.25\nTOWREpdeRS    3 1167  41.59  9.66 0.28\nCC2regRS      4 1167  36.91  2.65 0.08\nCC2irregRS    5 1167  25.24  3.70 0.11\nCC2nwRS       6 1167  32.01  6.12 0.18\nWASIvRS       7 1167  29.12  4.99 0.15\nBPVSRS        8 1167 130.87 13.97 0.41\n\n\nBut you can see that part of the information in the summary does not appear to make sense at first glance. We do not have 1167 participants in this dataset, as @ricketts2021 report.\nHow do we extract the participant attribute variable data for each unique participant code for the participants in our dataset?\n\nconc.orth.subjs &lt;- conc.orth %&gt;%\n  group_by(Participant) %&gt;%\n  mutate(mean.score = mean(Levenshtein.Score)) %&gt;%\n  ungroup() %&gt;%\n  distinct(Participant, .keep_all = TRUE) %&gt;%\n  select(WASImRS:BPVSRS, mean.score, Participant)\n\nWe create a new dataset conc.orth.subjs by taking conc.orth and piping it through a series of processing steps. As part of the process, we want to extract the data for each unique unique Participant identity code using distinct(). Along the way, we want to calculate the mean accuracy of response on the outcome measure (Score), that is, the average number of edits separating a child’s spelling of a target word from the correct spelling.\nThis is how we do it.\n\nconc.orth.subjs &lt;- ... tells R to create a new dataset conc.orth.subjs.\nconc.orth %&gt;% ... we do this by telling R to take conc.orth and pipe it through the following steps.\ngroup_by(Participant) %&gt;% first we group the data by Participant identity code.\nmutate(mean.score = mean(Score)) %&gt;% then we use mutate() to create the new variable mean.score by calculating the mean() of the Score variable values (i.e. the average score) for each participant. We then pipe to the next step.\nungroup() %&gt;% we tell R to ungroup the data because we want to work with all rows for what comes next, and we then pipe to the next step.\ndistinct(Participant, .keep_all = TRUE) %&gt;% requires R to extract from the full orth.conc dataset the set of (here, 16) data rows we have for each distinct (uniquely identified) Participant. We use the argument .keep_all = TRUE to tell R that we want to keep all columns. This requires the next step, so we tell R to pipe %&gt;% the data.\nselect(WASImRS:BPVSRS, mean.score, Participant) then tells R to select just the columns with information about participant attributes. (WASImRS:BPVSRS tells R to select every column between WASImRS and BPVSRS inclusive. mean.score, Participant tells R we also want those columns, specified by name, including the mean.score column of average response scores we calculated just earlier.\n\nWe can now get a sensible summary of the descriptive statistics for the participants in Study 2 of the @ricketts2021 investigation.\n\nconc.orth.subjs %&gt;%\n  select(-Participant) %&gt;%\n  describe(ranges = FALSE, skew = FALSE)\n\n           vars  n   mean    sd   se\nWASImRS       1 73  16.00  4.33 0.51\nTOWREsweRS    2 73  74.22  8.73 1.02\nTOWREpdeRS    3 73  41.58  9.73 1.14\nCC2regRS      4 73  36.90  2.67 0.31\nCC2irregRS    5 73  25.23  3.72 0.44\nCC2nwRS       6 73  32.00  6.17 0.72\nWASIvRS       7 73  29.12  5.02 0.59\nBPVSRS        8 73 130.88 14.06 1.65\nmean.score    9 73   1.38  0.62 0.07\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is exactly the kind of tabled summary of descriptive statistics we would expect to produce in a report, in a presentation of the participant characteristics for a study sample (in e.g., the Methods section).\nNotice:\n\nThe table has not yet been formatted according to APA rules.\nWe would prefer to use real words for row name labels instead of dataset variable column labels, e.g, replace TOWREsweRS with: “TOWRE word reading score”.\n\n\n\n\n\nIn these bits of demonstration code, we extract information relating just to participants. However, in this study, we recorded the responses participants made to 16 stimulus words, and we include in the dataset information about the word properties Consistency_H.\n\nCan you adapt the code you see here in order to calculate a mean score for each word, and then extract the word-level information for each distinct stimulus word identity?\n\n\n\n\nYou can read more about the psych library, which is often useful, here. You can read more about the distinct() function here.\n\n\n\n\n\nIt has taken us a while but now we are ready to examine the data using visualizations. Remember, we are engaging in visualization to (1.) do discovery, to get a sense of our data, and maybe reveal unexpected aspects, and (2.) potentially to communicate to ourselves and others what we have observed or perhaps what insights we can gain.\nWe have been learning to use histograms, in other classes, so let’s start there.\n\n\n\nWe can use histograms to visualize the distribution of observed values for a numeric variable. Let’s start simple, and then explore how to elaborate the plotting code, in a series of edits, to polish the plot presentation.\n\nggplot(data = conc.orth.subjs, aes(x = WASImRS)) +\n  geom_histogram()\n\n\n\n\nFigure 5: Distribution of WASImRS intelligence scores\n\n\n\n\nThis is how the code works.\n\nggplot(data = conc.orth.subjs, ... tells R what function to use ggplot() and what data to work with data = conc.orth.subjs.\naes(x = WASImRS) tells R what aesthetic mapping to use: we want to map values on the WASImRS variable (small to large) to locations on the x-axis (left to right).\ngeom_histogram() tells R to construct a histogram, presenting a statistical summary of the distribution of intelligence scores.\n\nWith histograms, we are visualizing the distribution of a single continuous variable by dividing the variable values into bins (i.e. subsets) and counting the number of observations in each bin. Histograms display the counts with bars.\nYou can see more information about geom_histogram here.\nFigure 5 shows how intelligence (WASImRS) scores vary in the Ricketts Study 2 dataset. Scores peak around 17, with a long tail of lower scores towards 5, and a maximum around 25.\n\nWhere I use the word “peak” I am talking about the tallest bar in the plot (or, later the highest point in a density curve). At this point, we have the most observations of the value under the bar. Here, we observed the score WASImRS \\(= 17\\) for the most children in this sample.\n\nA primary function of discovery visualization is to assess whether the distribution of scores on a variable is consistent with expectations, granted assumptions about a sample (e.g., that the children are typically developing). We would normally use research area knowledge to assess whether this distribution fits expectations for a sample of typically developing school-aged children in the UK. However, I shall leave that concern aside, here, so that we can focus on enriching the plot presentation, next.\nThere are two main problems with the plot:\n\nThe bars are “gappy” in the histogram, suggesting we have not grouped observed values in sufficiently wide subsets (bins). This is a problem because it weakens our ability to gain or communicate a visual sense of the distribution of scores.\nThe axis labeling uses the dataset variable name WASImRS but if we were to present the plot to others we could not expect them to know what that means.\n\nWe can fix both these problems, and polish the plot for presentation, through the following code steps.\n\nggplot(data = conc.orth.subjs, aes(x = WASImRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"Scores on the Wechsler Abbreviated Scale of Intelligence\") +\n  theme_bw()\n\n\n\n\nFigure 6: Distribution of WASImRS intelligence scores\n\n\n\n\nFigure 6 shows the same data, and furnishes us with the same picture of the distribution of intelligence scores but it is a bit easier to read. We achieve this by making three edits.\n\ngeom_histogram(binwidth = 2) + we change the binwidth.\n\n\nThis is so that more different observed values of the data variable are included in bins (subsets corresponding to bars) so that the bars correspond to information about a wider range of values.\nThis makes the bars bigger, wider, and closes the gaps.\nAnd this means we can focus the eyes of the audience for our plot on the visual impression we wish to communicate: the skewed distribution of intelligence scores.\n\n\nlabs(x = \"Scores on the Wechsler Abbreviated Scale of Intelligence\") + changes the label to something that should be understandable by people, in our audience, who do not have access to variable information (as we do) about the dataset.\ntheme_bw() we change the overall appearance of the plot by changing the theme.\n\n\n\nWe could, if we wanted, add a line and annotation to indicate the mean value, as you saw in Figure 2.\n\nCan you add the necessary code to indicate the mean value of WASI scores, for this plot?\n\nWe can, of course, plot histograms to indicate the distributions of other variables.\n\nCan you apply the histogram code to plot histograms of other variables?\n\n\n\n\n\nWe may wish to discover or communicate how values vary on dataset variables in two different ways. Sometimes, we need to examine how values vary on different variables. And sometimes, we need to examine how values vary on the same variable but in different groups of participants (or stimuli) or under different conditions. We look at this next. We begin by looking at how you might compare how values vary on different variables.\n\n\nIt can be useful to compare the distributions of different variables. Why?\nConsider the @ricketts2021 investigation dataset. Like many developmental investigations (see also clinical investigations), we tested children and recorded their scores on a series of standardized measures, here, measures of ability on a range of dimensions. We did this, in part, to establish that the children in our sample are operating at about the level one might expect for typically developing children in cognitive ability dimensions of interest: dimensions like intelligence, reading ability or spelling ability. So, one of the aspects of the data we are considering is whether scores on these dimensions are higher or lower than typical threshold levels. But we also want to examine the distributions of scores because we want to find out:\n\nif participants are varied in ability (wide distribution) or if maybe they are all similar (narrow distribution) as would be the case if the ability measures are too easy (so all scores are at ceiling) or too hard (so all scores are at floor);\nif there are subgroups within the sample, maybe reflected by two or more peaks;\nif there are unusual scores, maybe reflected by small peaks at very low or very high scores.\n\nWe could look at each variable, one plot at a time. Instead, next, I will show you how to produce a set of histogram plots, and present them all as a single grid of plots.\n\n\n\n\n\n\nWarning\n\n\n\nI have to warn you that the way I write the code is not good practice. The code is written with repeats of the ggplot() block of code to produce each plot. This repetition is inefficient and leaves the coding vulnerable to errors because it is hard to spot a mistake in more code. What I should do is encapsulate the code as a function (see here). The reason I do not, here, is because I want to focus our attention on just the plotting.\n\n\nFigure 7 presents a grid of plots showing how scores vary for each ability test measure, for the children in the @ricketts2021 investigation dataset. We need to go through the code steps, next, and discuss what the plots show us (discovery and communication).\n\np.WASImRS &lt;- ggplot(data = conc.orth.subjs, aes(x = WASImRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"WASI matrix\") +\n  theme_bw()\n\np.TOWREsweRS &lt;- ggplot(data = conc.orth.subjs, aes(x = TOWREsweRS)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"TOWRE words\") +\n  theme_bw()\n\np.TOWREpdeRS &lt;- ggplot(data = conc.orth.subjs, aes(x = TOWREpdeRS)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"TOWRE phonemic\") +\n  theme_bw()\n\np.CC2regRS &lt;- ggplot(data = conc.orth.subjs, aes(x = CC2regRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"CC regular words\") +\n  theme_bw()\n\np.CC2irregRS &lt;- ggplot(data = conc.orth.subjs, aes(x = CC2irregRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"CC irregular words\") +\n  theme_bw()\n\np.CC2nwRS &lt;- ggplot(data = conc.orth.subjs, aes(x = CC2nwRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"CC nonwords\") +\n  theme_bw()\n\np.WASIvRS &lt;- ggplot(data = conc.orth.subjs, aes(x = WASIvRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"WASI vocabulary\") +\n  theme_bw()\n\np.BPVSRS &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS)) +\n  geom_histogram(binwidth = 3) +\n  labs(x = \"BPVS vocabulary\") +\n  theme_bw()\n\np.mean.score &lt;- ggplot(data = conc.orth.subjs, aes(x = mean.score)) +\n  geom_histogram(binwidth = .25) +\n  labs(x = \"Mean orthographic test score\") +\n  theme_bw()\n\np.mean.score + p.BPVSRS + p.WASIvRS + p.WASImRS +\n  p.CC2nwRS + p.CC2irregRS + p.CC2regRS + \n  p.TOWREpdeRS + p.TOWREsweRS + plot_layout(ncol = 3)\n\n\n\n\nFigure 7: Distribution of childrens’ scores on ability measures\n\n\n\n\nThis is how the code works, step by step:\n\np.WASImRS &lt;- ggplot(...) first creates a plot object, which we call p.WASImRS.\nggplot(data = conc.orth.subjs, aes(x = WASImRS)) + tells R what data to use, and what aesthetic mapping to work with mapping the variable WASImRS here to the x-axis location.\ngeom_histogram(binwidth = 2) + tells R to sort the values of WASImRS scores into bins and create a histogram to show how many children in the sample present scores of different sizes.\nlabs(x = \"WASI matrix\") + changes the x-axis label to make it more informative.\ntheme_bw() changes the theme to make it a bit cleaner looking.\n\nWe do this bit of code separately for each variable. We change the plot object name, the x = variable specification, and the axis label text for each variable. We adjust the binwidth where it appears to be necessary.\nWe then use the following plot code to put all the plots together in a single grid.\n\np.mean.score + p.BPVSRS + p.WASIvRS + p.WASImRS +\n  p.CC2nwRS + p.CC2irregRS + p.CC2regRS + \n  p.TOWREpdeRS + p.TOWREsweRS + plot_layout(ncol = 3)\n\n\nIn the code, we add a series of plots together e.g. p.mean.score + p.BPVSRS + p.WASIvRS ...\nand then specify we want a grid of plots with a layout of three columns plot_layout(ncol = 3).\n\nThis syntax requires the library(patchwork) and more information about this very useful library can be found here.\nWhat do the plots show us?\nFigure 7 shows a grid of 9 histogram plots. Each plot presents the distribution of scores for the @ricketts2021 Study 2 participant sample on a separate ability measure, including scores on the BPVS vocabulary, WASI vocabulary, TOWRE words and TOWRE nonwords reading tests, as well as scores on the Castles and Coltheart regular words, irregular words and nonwords reading tests, and the mean Levenshtein distance (spelling score) outcome measure of performance for the experimental word learning post-test.\nTake a look, you may notice the following features.\n\nThe mean orthographic test score suggests that many children produced spellings to the words they learned in the @ricketts2021 study that, on average, were correct (0 edits) or were one or two edits (e.g., a letter deletion or replacement) away from the target word spelling. The children were learning the words, and most of the time, they learned the spellings of the words effectively. However, one or two children tended to produce spellings that were 2-3 edits distant from the target spelling.\n\n\nWe can see these features because we can see that the histogram peaks around 1 (at Levenshtein distance score \\(= 1\\)) but that there is a small bar of scores at around 3.\n\n\nWe can see that there are two peaks on the BPVS and WASI measures of vocabulary. What is going on there?\n\n\nIs it the case that we have two sub-groups of children within the overall sample? For example, on the BPVS test, maybe one sub-group of children has a distribution of vocabulary scores with a peak around 120 (the peak shows where most children have scores) while another sub-group of children has a distribution of vocabulary scores with a peak around 140.\n\n\nIf we look at the CC nonwords and CC regular words tests of reading ability, we may notice that while most children present relatively high scores on these tests (CC nonwords peak around 35, CC regular words peak around 37) there is a skewed distribution. Many of the children’s scores are piled up towards the maximum value in the data on the measures. But we can also see that, on both measures, there are long tails in the distributions because relatively small numbers of children have substantially lower scores.\n\n\nDevelopmental samples are often highly varied (just like clinical samples). Are all the children in the sample at the same developmental stage, or are they all typically developing?\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that in presenting a grid of plots like this, we offer a compact visual way to present the same summary information we might otherwise present using a table of descriptive statistics. In some ways, this grid of plots is more informative than the descriptive statistics because the mean and SD values do not tell you what you can see:\n\nthe characteristics of the variation in values, like the presence of two peaks;\nor the presence of unusually high or low scores (for this sample).\n\n\n\nGrids of plots like this can be helpful to inspect the distributions of variables in a concise approach. They are not really too useful for comparing the distributions because they require your eyes to move between plots, repeatedly, to do the comparison.\nHere is a more compact way to code the grid of histograms using the library(ggridges) function geom_density_ridges(). I do not discuss it in detail because I want to focus your attention on core tidyverse functions (I show you more information in the Notes tab).\nNotice that if you produce all the plots so that the are in line in the same column with a shared x-axis it becomes much easier to compare the distributions of scores. You lose some of the fine detail, discussed in relation to Figure 7, but this style allows you to gain an impression, quickly, of how for distributions of scores compare between measures. For example, we can see that within the Castles and Coltheart (CC) measures of reading ability, children do better on regular words than on nonwords, and on nonwords better than on irregular words.\n\nPlotNotes\n\n\n\nlibrary(ggridges)\nconc.orth.subjs %&gt;%\n  pivot_longer(names_to = \"task\", values_to = \"score\", cols = WASImRS:mean.score) %&gt;% \n  ggplot(aes(y = task, x = score)) +\n  geom_density_ridges(stat = \"binline\", bins = 20, scale = 0.95, draw_baseline = FALSE) +\n  theme_ridges()\n\n\n\n\nFigure 8: Distribution of childrens’ scores on ability measures\n\n\n\n\n\n\n\nlibrary(ggridges) get the library we need.\nconc.orth.subjs %&gt;% pipe the dataset for processing.\npivot_longer(names_to = \"task\", values_to = \"score\", cols = WASImRS:mean.score) %&gt;% pivot the data so all test scores are in the same column, “scores” wwith coding for “task” name, and pipe to the next step for plotting.\nggplot(aes(y = task, x = score)) + create a plot for the scores on each task.\ngeom_density_ridges(stat = \"binline\", bins = 20, scale = 0.95, draw_baseline = FALSE) + show the plots as histograms.\ntheme_ridges() change the theme to the specific theme suitable for showing a grid of ridges.\n\nYou can find more information on ggridges here.\n\n\n\n\n\n\nWe will often want to compare the distributions of variable values between groups or between conditions. This need may appear when, for example, we are conducting a between-groups manipulation of some condition and we want to check that the groups are approximately matched on dimensions that are potentially linked to outcomes (i.e., on potential confounds). The need may appear when, alternatively, we have recruited or selected participant (or stimulus) samples and we want to check that the sample sub-groups are approximately matched or detectably different on one or more dimensions of interest or of concern.\nAs a demonstration of the visualization work we can do in such contexts, let’s pick up on an observation we made earlier, that there are two peaks on the BPVS and WASI measures of vocabulary. I asked: Is it the case that we have two sub-groups of children within the overall sample? Actually, we know the answer to that question because @ricketts2021 state that they recruited one set of children for their Study 1 and then, for Study 2:\n\nThirty-three children from an additional three socially mixed schools in the South-East of England were added to the Study 1 sample (total N = 74). These additional children were older (\\(M_{age}\\) = 12.57, SD = 0.29, 17 female)\n\nDo the younger (Study 1) children differ in any way from the older (additional) children?\nWe can check this through data visualization. Our aim is to present the distributions of variables side-by-side or superimposed to ensure easy comparison. We can do this in different ways, so I will demonstrate one approach with an outline explanation of the actions, and offer suggestions for further approaches.\nI am going to process the data before I do the plotting. I will re-use the code I used before (see Section 1.7.4.2) with one additional change. I will add a line to create a group coding variable. This addition shows you how to do an action that is very often useful in the data processing part of your workflow.\n\n\nYou have seen that the @ricketts2021 report states that an additional group of children was recruited for the investigation’s second study. How do we know who they are? If you recall the summary view of the complete dataset, there is one variable we can use to code group identity.\n\nsummary(conc.orth$Study)\n\nStudy1&2   Study2 \n     655      512 \n\n\nThis summary tells us that we have 512 observations concerning the additional group of children recruited for Study 2, and 655 observations for the (younger) children whose data were analyzed for both Study 1 and Study 2 (i.e., coded as Study1&2 in the Study variable column). We can use this information to create a coding variable. (If we had age data, we could use that instead but we do not.) This is how we do that.\n\nconc.orth.subjs &lt;- conc.orth %&gt;%\n  group_by(Participant) %&gt;%\n  mutate(mean.score = mean(Levenshtein.Score)) %&gt;%\n  ungroup() %&gt;%\n  distinct(Participant, .keep_all = TRUE) %&gt;%\n  mutate(age.group = fct_recode(Study,\n    \n    \"young\" = \"Study1&2\",\n    \"old\" = \"Study2\"\n    \n  )) %&gt;%\n  select(WASImRS:BPVSRS, mean.score, Participant, age.group)\n\nThe code block is mostly the same as the code I used in Section Section 1.7.4.2 to extract the data for each participant, with two changes:\n\nFirst, mutate(age.group = fct_recode(...) tells R that I want to create a new variable age.group through the process of recoding, with fct_recode(...) the variable I specify next, in the way that I specify.\nfct_recode(Study, ...) tells R I want to recode the variable Study.\n\"young\" = \"Study1&2\", \"old\" = \"Study2\" specifies what I want recoded.\n\n\nI am telling R to look in the Study column and (a.) whenever it finds the value Study1&2 replace it with young whereas (b.) whenever it finds the value Study2 replace it with old.\nNotice that the syntax in recoding is fct_recode: “new name” = “old name”.\nHaving done that, I tell R to pipe the data, including the recoded variable, to the next step.\n\n\nselect(WASImRS:BPVSRS, mean.score, Participant, age.group) where I add the new recoded variable to the selection of variables I want to include in the new dataset conc.orth.subjs.\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that R handles categorical or nominal variables like Study (or, in other data, variables e.g. gender, education or ethnicity) as factors.\n\nWithin a classification scheme like education, we may have different classes or categories or groups e.g. “further, higher, school”. We can code these different classes with numbers (e.g. \\(school = 1\\)) or with words “further, higher, school”. Whatever we use, the different classes or groups are referred to as levels and each level has a name.\nIn factor recoding, we are changing level names while keeping the underlying data the same.\n\n\n\nThe tidyverse collection includes the forcats library of functions for working with categorical variables (forcats = factors). These functions are often very useful and you can read more about them here.\nChanging factors level coding by hand is, for many, a common task, and the fct_recode() function makes it easy. You can find the technical information on the function, with further examples, here.\n\n\n\nThere are different ways to examine the distributions of variables so that we can compare the distributions of the same variable between groups.\nFigure 9 presents some alternatives as a grid of 4 different kinds of plots designed to enable the same comparison. Each plot presents the distribution of scores for the @ricketts2021 Study 2 participant sample on the BPVS vocabulary measure so that we can compare the distribution of vocabulary scores between age groups.\nThe plots differ in method using:\n\nfacetted histograms showing the distribution of vocabulary scores, separately for each group, in side-by-side histograms for comparison;\nboxplots, showing the distribution of scores for each group, indicated by the y-axis locations of the edges of the boxes (25% and 75% quartiles) and the middle lines (medians);\nsuperimposed histograms, where the histograms for the separate groups are laid on top of each other but given different colours to allow comparison; and\nsuperimposed density plots where the densities for the separate groups are laid on top of each other but given different colours to allow comparison.\n\n\n\n\n\n\n\nTip\n\n\n\nThere is one thing you should notice about all these plots.\n\nIt looks like the BPVS vocabulary scores have their peak – most children show this value – at around 120 for the young group and at around 140 for the old group.\n\nWe return to this shortly.\n\n\n\nI am going to hide the coding and the explanation of the coding behind the Notes tab. Click on the tab to get a step-by-step explanation. Of these alternatives, I focus on one which I explain in more depth, following: d. Superimposed density plots.\n\nPlotNotes\n\n\n\n\n\n\n\nFigure 9: Distribution of childrens’ scores on the BPVS vocabulary measure: distributions are compared between the younger and older age groups\n\n\n\n\n\n\n\np.facet.hist &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"BPVS vocabulary score\", title = \"a. Faceted histograms\") +\n  facet_wrap(~ age.group) +\n  theme_bw()\n\np.colour.boxplot &lt;- ggplot(data = conc.orth.subjs, aes(y = BPVSRS, colour = age.group)) +\n  geom_boxplot() +\n  labs(x = \"BPVS vocabulary score\", title = \"b. Boxplots\") +\n  theme_bw()\n\np.colour.hist &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS, colour = age.group, fill = age.group)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"BPVS vocabulary score\", title = \"c. Superimposed histograms\") +\n  theme_bw()\n\np.colour.density &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS, colour = age.group, fill = age.group)) +\n  geom_density(alpha = .5, size = 1.5) +\n  labs(x = \"BPVS vocabulary score\", title = \"d. Superimposed density plots\") +\n  theme_bw()\n\np.facet.hist + p.colour.boxplot + p.colour.hist + p.colour.density\n\n\nIn plot “a. Faceted histograms”, we use the code to construct a histogram but the difference is we use:\n\n\nfacet_wrap(~ age.group) to tell R to split the data by age.group then present the histograms indicating vocabulary score distributions separately for each group.\n\n\nIn plot “b. Boxplots”, we use the geom_boxplot() code to construct a boxplot to summarize the distributions of vocabulary scores – as you have seen previously – but the difference is we use:\n\n\naes(y = BPVSRS, colour = age.group) to tell R to assign different colours to different levels of age.group to help distinguish the data from each group.\n\n\nIn plot “c. Superimposed histograms”, we use the code to construct a histogram but the difference is we use:\n\n\naes(x = BPVSRS, colour = age.group, fill = age.group) to tell R to assign different colours to different levels of age.group to help distinguish the data from each group.\nNotice that the fill gives the colour inside the bars and colour gives the colour of the outline edges of the bars.\n\n\nIn plot “d. Superimposed density plots”, we use the code geom_density(...) to construct what is called a density plot.\n\n\nA density plot presents a smoothed histogram to show the distribution of variable values.\nWe add arguments in geom_density(alpha = .5, size = 1.5) to adjust the thickness of the line (size = 1.5) drawn to show the shape of the distribution and adjust the transparency of the colour fill inside the line alpha = .5).\nWe useaes(x = BPVSRS, colour = age.group, fill = age.group) to tell R to assign different colours to different levels of age.group to help distinguish the data from each group.\nNotice that the fill gives the colour inside the density plots and colour gives the colour of the outline edges of the densities.\n\n\n\n\nDensity plots can be helpful when we wish to compare distributions. This is because we can superimpose distribution plots on top of each other, enabling us or our audience to directly compare the distributions: directly because the distributions are shown on the same scale, in the same image.\nWe can (roughly) understand a density plot as working like a smoothed version of the histogram. Imagine how the heights of the bars in the histogram represent how many observations we have of the values in a particular bin. If we draw a smooth curving line through the tops of the bars then we are representing the chances that an observation in our sample has a value (the value under the curve) at any specific location on the x-axis. You can see that in Figure 10.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nFigure 10: Distribution of childrens’ scores on the BPVS vocabulary measure. The figure shows the histogram versus density plot representation of the same data distribution\n\n\n\n\nYou can find the ggplot2 reference information on the geom_density() function, with further examples, here. You can find technical information on density functions here and here.\nWe can develop the density plot to enrich the information we can discover or communicate through the plot. Figure 11 shows the distribution of scores on both the BPVS and WASI vocabulary knowledge measures.\n\np.BPVSRS.density &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS, colour = age.group, fill = age.group)) +\n  geom_density(alpha = .5, size = 1.5) +\n  geom_rug(alpha = .5) +\n  geom_vline(xintercept = 120, linetype = \"dashed\") +\n  geom_vline(xintercept = 140, linetype = \"dotted\") +\n  labs(x = \"BPVSRS vocabulary score\") +\n  theme_bw()\n\np.WASIvRS.density &lt;- ggplot(data = conc.orth.subjs, aes(x = WASIvRS, colour = age.group, fill = age.group)) +\n  geom_density(alpha = .5, size = 1.5) +\n  geom_rug(alpha = .5) +\n  labs(x = \"WASI vocabulary score\") +\n  theme_bw()\n\np.BPVSRS.density + p.WASIvRS.density + plot_layout(guides = 'collect')\n\n\n\n\nFigure 11: Distribution of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\nHere is what the code does:\n\np.BPVRS.density &lt;- ggplot(...) creates a plot object called p.BPVRS.density.\ndata = conc.orth.subjs, ... says we use the conc.orth.subjs dataset to do this.\naes(x = BPVRS, colour = age.group, fill = age.group)) + says we want to map BPVRS scores to x-axis location, and age.group level coding (young, old) to both colour and fill.\ngeom_density(alpha = .5, size = 1.5) + draws a density plot; note that we said earlier what we want for colour and fill but here we also say that:\n\n\nalpha = .5 we want the fill to be transparent;\nsize = 1.5 we want the density curve line to be thicker than usual.\n\n\ngeom_rug(alpha = .5) + adds a one-dimensional plot, a series of tick marks, to show where we have observations of BPVRS scores for specific children. We ask R to make the tick marks semi-transparent.\ngeom_vline(xintercept = 120, linetype = \"dashed\") + draws a vertical dashed line where BPVRS = 120.\ngeom_vline(xintercept = 140, linetype = \"dotted\") + draws a vertical dotted line where BPVRS = 140.\nlabs(x = \"BPVS vocabulary score\") + makes the x-axis label something understandable to someone who does not know about the study.\ntheme_bw() changes the theme.\n\n\n\n\nAs we work with visualization, we should aim to develop skills in reading plots, so:\n\nWhat do we see?\n\nWhen we look at Figure 11, we can see that the younger and older children in the @ricketts2021 sample have broadly overlapping distributions of vocabulary scores. However, as we have noticed previously, the peak of the distribution is a bit lower for the younger children compared to the older children. This appears to be the case whether we are looking at the BPVS or at the WASI measures of vocabulary, suggesting that the observation does not depend on the particular vocabulary test. Is this observation unexpected? Probably not, as we should hope to see vocabulary knowledge increase as children get older. Is this observation a problem for our analysis? You need to read the paper to find out what we decided.\n\n\n\nIn the demonstration examples, I focused on comparing age groups on vocabulary, what about the other measures?\nI used superimposed density plots: are other plotting styles more effective, for you? Try using boxplots or superimposed or faceted histograms instead.\n\n\n\n\n\nSo far, we have looked at how and why we may examine the distributions of numeric variables. We have used histograms to visualize the distribution of variable values. We have explored the construction of grids of plots to enable the quick examination or concise communication of information about the distributions of multiple variables at the same time. And we have used histograms, boxplots and density plots to examine how the distributions of variables may differ between groups.\nThe comparison of the distributions of variable values in different groups (or, similarly, between different conditions) may be the kind of work we would need to do, in data visualization, as part of an analysis ending in, for example, a t-test comparison of mean values.\nWhile boxplots, density plots and histograms are typically used to examine how the values of a numeric variable vary, scatterplots are typically used when we wish to examine, to make sense of or communicate potential associations or relations between two (or more) numeric variables. We turn to scatterplots, next.\n\n\n\nMany of us start learning about scatterplots in high school math classes. Using the modern tools made available to us through the ggplot2 library (as part of tidyverse), we can produce effective, nice-looking, scatterplots for a range of discovery or communication scenarios.\nWe continue working with the @ricketts2021 dataset. In the context of the @ricketts2021 investigation, there is interest in how children vary in the reading, spelling and vocabulary abilities that may influence the capacity of children to learn new words. So, in this context, we can begin to progress our development in visualization skills by usefully considering the potential association between participant attributes in the Study 2 sample.\nLater on, we will look at more advanced plots that help us to communicate the impact of the experimental manipulations implemented by @ricketts2021, and also to discover the ways that these impacts may vary between children.\n\n\nWe can begin by asking a simple research question we can guess the answer to:\n\nDo vocabulary knowledge scores on two alternative measures, the BPVS and the WASI, relate to each other?\n\nIf two measurement instruments or tests are intended to measure individual differences in the same psychological attribute, here, vocabulary knowledge, then we would reasonably expect that scores on one test should covary with scores on the second test.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point() +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  theme_bw()\n\n\n\n\nFigure 12: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\nWhat does the plot show us?\nAs a reminder of how scatterplots work, we can recall that they present integrated information. Each point, for the @ricketts2021 data, represents information about both the BPVS and the WASI score for each child.\n\nThe vertical height of a point tells us the BPVS score recorded for a child: higher points represent higher scores.\nThe left-to-right horizontal position of the same point tells us the WASI score for the same child: points located more on the right represent higher scores.\n\nFigure 12 is a scatterplot comparing variation in childrens’ scores on the BPVS and WASI vocabulary measures: variation in BPVS scores are shown on the y-axis and variation in WASI scores are shown on the x-axis. Critically, the scientific insight the plot gives us is this: higher WASI scores are associated with higher BPVS scores.\nHow does the code work? We have seen scatterplots before but, to ensure we are comfortable with the coding, we can go through them step by step.\n\nggplot(data = conc.orth.subjs...) + tells R we want to produce a plot using ggplot() with the conc.orth.subjs dataset.\naes(x = WASIvRS, y = BPVSRS) tells R that, in the plot, WASIvRS values are mapped to x-axis (horizontal) position and BPVSRS values are mapped to y-axis (vertical) position.\ngeom_point() + constructs a scatterplot, using these data and these position mappings.\nlabs(x = \"WASI vocabulary score\", ... fixes the x-axis label.\ny = \"BPVSRS vocabulary score\",... fixes the y-axis label.\ntitle = \"Are WASI and BPVS vocabulary scores associated?\") + fixes the title.\ntheme_bw() changes the theme.\n\n\n\n\nFor this pair of variables in this dataset, the potential association in the variation of scores is quite obvious. However, sometimes it is helpful to guide the audience by imposing a smoother. There are different ways to do this, for different objectives and in different contexts. Here, we look at two different approaches. In addition, as we go, we examine how to adjust the appearance of the plot to address different potential discovery or communication needs.\nWe begin by adding what is called a LOESS smoother.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  theme_bw()\n\n\n\n\nFigure 13: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\nThe only coding difference between this plot Figure 13 and the previous plot Figure 12 appears at line 3:\n\ngeom_smooth()\n\nThe addition of this bit of code results in the addition of the curving line you see in Figure 13. The blue line is curving, and visually suggests that the relation between BPVS and WASI scores is different – sometimes more sometimes less steep – for different values of WASI vocabulary score.\nThis line is generated by the geom_smooth() code, by default, in an approach in which the dataset is effectively split into sub-sets, dividing the data up into sub-sets from the lowest to the highest WASI scores, and the predicted association between the y-axis variable (here, BPVS score) and the x-axis variable (here, WASI score) is calculated bit by bit, in a series of regression analyses, working in order through sub-sets of the data. This calculation of what is called the LOESS (locally estimated scatterplot smoothing) trend is done by ggplot for us. And this approach to visualizing the trend in a potential association between variables is often a helpful way to discover curved or non-linear relations.\nYou can find technical information on geom_smooth() here and an explanation of LOESS here.\nFor us, this default visualization is not helpful for two reasons:\n\nWe have not yet learned about linear models, so learning about LOESS comes a bit early in our development.\nIt is hard to look at Figure 13 and identify a convincing curvilinear relation between the two variables. A lot of the curve for low WASI scores appears to be linked to the presence of a small number of data points.\n\nAt this stage, it is more helpful to adjust the addition of the smoother. We can do that by adding an argument to the geom_smooth() function code.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point() +\n  geom_smooth(method = 'lm') +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  theme_bw()\n\n\n\n\nFigure 14: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\nNotice the difference between Figure 13 and Figure 14:\n\ngeom_smooth(method = 'lm') tells R to draw a trend line, a smoother, using the lm method.\n\nThe lm method requires R to estimate the association between the two variables, here, BPVS and WASI, assuming a linear model. Of course, we are going to learn about linear models but, in short, right now, what we need to know is that we assume a “straight line” relationship between the variables. This assumption requires that for any interval of WASI scores – e.g., whether we are talking about WASI scores between 20-25 or about WASI scores between 30-35 – the relation between BPVS and WASI scores has the same shape: the direction and steepness of the slope of the line is the same.\n\n\n\n\nDeveloping skill in working with data visualizations is not just about developing coding skills, it is also about developing skills in reading, and critically evaluating, the information the plots we produce show us.\n\nStop and take a good look at the scatterplot in Figure 14. Use the visual representation of data to critically evaluate the potential association between the BPVS and WASI variables. What can you see?\nYou can train your critical evaluation by asking yourself questions like the following:\n\nHow does variation in the x-axis variable relate to variation in values of the y-axis variable?\n\n\nWe can see, here, that higher WASI scores are associated with higher BPVS scores.\n\n\nHow strong is the relation?\n\n\nThe strength of the relation can be indicated by the steepness of the trend indicated by the smoother, here, the blue line.\nIf you track the position of the line, you can see, for example, that going from a WASI score of 20 to a WASI score of 40 is associated with going from a BPVS score of a little over 110 to a BPVS score of about a 150.\nThat seems like a big difference.\n\n\nHow well does the trend we are looking at capture the data in our sample?\n\n\nHere, we are concerned with how close the points are to the trend line.\nIf the trend line represents a set of predictions about how the BPVS scores vary (in height) given variation in WASI scores, we can see that in places the prediction is not very good.\nTake a look at the points located at WASI 25. We can see that there there are points indicating that different children have the same WASI score of 25 but BPVS scores ranging from about 115 to 140.\n\n\n\n\nFigure 14 presents a satisfactory looking plot but it is worth checking what edits we can make to the appearance of the plot, to indicate some of the ways that you can exercise choice in determining what a plot looks like. This will be helpful to you when you are constructing plots for presentation and report and you want to ensure the plots are as effective as possible.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point(alpha = .5, size = 2) +\n  geom_smooth(method = 'lm', colour = \"red\", size = 1.5) +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  xlim(0, 40) + ylim(0, 160) +\n  theme_bw()\n\n\n\n\nFigure 15: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\nIf you inspect the code, you can see that I have made three changes:\n\ngeom_point(alpha = .5, size = 2 changes the size of the points and their transparency (using alpha).\ngeom_smooth(method = 'lm', colour = \"red\", size = 1.5) change the colour of the smoother line, and the thickness (size) of the line.\nxlim(0, 40) + ylim(0, 160) changes the axis limits.\n\nThe last step — changing the axis limits — reveals how the sample data can be understood in the context of possible scores on these ability measures. Children could get BPVS scores of 0 or WASI scores of 0. By showing the start of the axes we get a more realistic sense of how our sample compares to the possible ranges of scores we could see in the wider population of children. This perhaps offers a more honest or realistic visualization of the potential association between BPVS and WASI vocabulary scores.\n\n\n\nAs we have seen previously, we can construct a series of plots and present them all at once in a grid or lattice. Figure 16 presents just such a grid: of scatterplots, indicating a series of potential associations.\nLet’s suppose that we are primarily interested in what factors influence the extent to which children in the @ricketts2021 word learning experiment are able to correctly spell the target words they were given to learn. As explained earlier, in Section 1.7.2, @ricketts2021 examined the spellings produced by participant children in response to target words, counting how many string edits (i.e., letter deletions etc.) separated the spelling each child produced from the target spelling they should have produced.\nWe can calculate the mean spelling accuracy score for each child, over all the target words we observed their response to. We can identify mean spelling score as the outcome variable. We can then examine whether the outcome spelling scores are or are not influenced by participant attributes like vocabulary knowledge.\nFigure 16 presents a grid of scatterplots indicating the potential association between mean spelling score and each of the variables we have in the conc.orth dataset, including the Castles and Coltheart (CC) and TOWRE measures of word or nonword reading skill, WASI and BPVS measures of vocabulary knowledge, and the WASI matrix measure of intelligence, as well as (our newly coded) age group factor.\nI hide an explanation of the coding behind the Notes tab, because we have seen how to produce grids of plots, but you can take a look if you want to learn how the plot is produced.\n\nPlotNotes\n\n\n\n\n\n\n\nFigure 16: Grid of scatterplots showing the potential association between mean spelling score, for each child, and variation in the Castles and Coltheart (CC) and TOWRE measures of word or nonword reading skill, WASI and BPVS measures of vocabulary knowledge, the WASI matrix measure of intelligence, and age group factor\n\n\n\n\n\n\nThe code to produce the figure is set out as follows.\n\np.wordsvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = TOWREsweRS, \n                              y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"Word reading\", \n       y = \"Spelling score\",\n       title = \"(a.)\") +\n  theme_bw()\n\np.nonwordsvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = TOWREsweRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"Nonword reading\", \n       y = \"Spelling score\",\n       title = \"(b.)\") +\n  theme_bw()\n\np.WASIvRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = WASIvRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"WASI vocabulary\", \n       y = \"Spelling score\",\n       title = \"(c.)\") +\n  theme_bw()\n\np.BPVSRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = BPVSRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"BPVS vocabulary score\", \n       y = \"Spelling score\",\n       title = \"(d.)\") +\n  theme_bw()\n\np.WASImRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = WASImRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"WASI matrix\", \n       y = \"Spelling score\",\n       title = \"(e.)\") +\n  theme_bw()\n\np.CC2regRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = CC2regRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"CC regular words\", \n       y = \"Spelling score\",\n       title = \"(f.)\") +\n  theme_bw()\n\np.CC2irregRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = CC2irregRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"CC irregular words\", \n       y = \"Spelling score\",\n       title = \"(g.)\") +\n  theme_bw()\n\np.CC2nwRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = CC2nwRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"CC nonwords\", \n       y = \"Spelling score\",\n       title = \"(h.)\") +\n  theme_bw()\n\np.age.groupvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = age.group, \n                                  y = mean.score)) +\n  geom_boxplot() +\n  labs(x = \"Age group\", \n       y = \"Spelling score\",\n       title = \"(i.)\") +\n  theme_bw()\n\np.wordsvsmean.score + p.nonwordsvsmean.score + p.WASIvRSvsmean.score +\n  p.BPVSRSvsmean.score + p.WASImRSvsmean.score + p.CC2regRSvsmean.score +\n  p.CC2irregRSvsmean.score + p.CC2nwRSvsmean.score + p.age.groupvsmean.score\n\n\nTo produce the grid of plots, we first create a series of plot objects using code like that shown in the chunk.\n\n\np.wordsvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = TOWREsweRS, \n                              y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"Word reading\", \n       y = \"Spelling score\",\n       title = \"(a.)\") +\n  theme_bw()\n\n\np.wordsvsmean.score &lt;- ggplot(...) creates the plot.\ndata = conc.orth.subjs tells R what data to work with.\naes(x = TOWREsweRS, y = mean.score) specifies the aesthetic data mappings.\ngeom_point(alpha = .5, size = 3) tells R to produce a scatterplot, specifying the size and transparency of the points.\ngeom_smooth(method = 'lm', size = 1.5) tells R to add a smoother, specifying the method and the thickness of the line.\nlabs(x = \"Word reading\", y = \"Spelling score\", title = \"(a.)\") fixes the labels.\ntheme_bw() adjusts the theme.\n\n\nWe then put the plots together, using the patchwork syntax where we list the plot objects by name, separating each name by a +.\n\n\np.BPVSRSvsmean.score + p.WASImRSvsmean.score + p.CC2regRSvsmean.score +\n  p.CC2irregRSvsmean.score + p.CC2nwRSvsmean.score + p.age.groupvsmean.score\n\n\n\n\nFigure 16 allows us to visually represent the potential association between an outcome measure, the average spelling score, and a series of other variables that may or may not have an influence on that outcome. Using a grid in this fashion allows us to compare the extent to which different variables appear to have an influence on the outcome. We can see, for example, that measures of variation in word reading skill appear to have stronger association (the trend lines are more steeply slowed) than measures of vocabulary knowledge or intelligence, or age group.\nUsing grids of plots like this allow us to compactly communicate these potential associations in a single figure.\n\n\n\n\n\n\nWarning\n\n\n\nLevenshtein distance scores are higher if a child makes more errors in producing the letters in a spelling response.\n\nThis means that if we want to see what factors help a child to learn a word, including its spelling, then we want to see that helpful factors are associated with lower Levenshtein scores.\n\n\n\n\n\n\n\nAs explained in Section 1.7.2, in the @ricketts2021 study, we taught children taught 16 novel words in a study with a 2 x 2 factorial design. The presence of orthography (orthography absent vs. orthography present) was manipulated within participants: for all children, eight of the words were taught with orthography (the word spelling) present and eight with orthography absent. Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not. The @ricketts2021 investigation was primarily concerned with the effects on word learning of presenting words for learning with or without showing the words with their spellings, with or without instructing students explicitly that they would be helped by the presence of the spellings.\nWe can analyze the effects of orthography and instruction using a linear model.\n\nmodel &lt;- lm(Levenshtein.Score ~ Instructions*Orthography, data = conc.orth)\n\nThe model code estimates variation in spelling score (values of the Levenshtein.Score) variable, given variation in the levels of the Instructions and Orthography factors, and their interaction.\nThis model is a limited approximation of the analysis we would need to do with these data to estimate the effects of orthography and instruction; see @ricketts2021 for more information on what analysis is required (in our view). However, it is good enough as a basis for exploring the kind of data visualization work — in terms of both discovery and communication — that you can do when you are working with data from an experimental study.\nWe can get a summary of the model results which presents the estimated effect of each experimental factor. These estimates represent the predicted change in spelling score, given variation in Orthography (present, absent) or Instruction (explicit, incidental), and given the possibility that the effect of the presence of orthography is different for different levels of instruction.\nNotice that some of the p-values are incorrectly shown as 0.000. This is a result of using functions to automatically take a model summary and generate a table. I am going to leave this error with a warning because our focus is on visualization, next.\n\n\n\nModel summary\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.584\n0.072\n21.857\n0.000\n\n\nInstructionsincidental\n-0.041\n0.103\n-0.396\n0.692\n\n\nOrthographypresent\n-0.409\n0.103\n-3.987\n0.000\n\n\nInstructionsincidental:Orthographypresent\n0.060\n0.146\n0.409\n0.683\n\n\n\n\n\n\n\nVery often, when we complete a statistical analysis of outcome data, in which we estimate or test the effects on outcomes of variation in some variables or of variation in experimental conditions, then we present a table summary of the analysis results. However, these estimates are typically difficult to interpret (it gets easier with practice) and talk about. Take a look at the summary table. We are often to focus on whether effects are significant or not significant. But, really, what we should consider is how much the outcome changes given the different experimental conditions.\nHow do we get that information from the analysis results? We can communicate results — to ourselves or to an audience — by constructing plots from the model information. The ggeffects library extends ggplot2 to enable us to do this quite efficiently.\nWhen we write code to fit a linear model like:\n\nmodel &lt;- lm(Levenshtein.Score ~ Instructions*Orthography, data = conc.orth)\n\nWe record the results as an object called model because we specify model &lt;- lm(...). We can take these results and ask R to create a plot showing predicted change in outcome (spelling) given our model. We can then present the effects of the variables, as shown in Figure 17.\n\ndat &lt;- ggpredict(model, terms = c(\"Instructions\", \"Orthography\"))\nplot(dat, facet = TRUE) + ylim(0, 3)\n\n\n\n\nFigure 17: Dot and whisker plots showing the predicted effect on outcome spelling (Levenshtein) score, given different experimental conditions: Orthography (present, absent) x Instruction (explicit, incidental).\n\n\n\n\nThe code works as follows:\n\ndat &lt;- ggpredict(model, terms = c(\"Instructions\", \"Orthography\")) tells R to calculate predicted outcomes, given our model information, for the factors \"Instructions\", \"Orthography\".\nplot(dat, facet = TRUE) plot the effects, given the predictions, showing the effect of different instruction conditions in different plot facets (the left and right panels).\nylim(0, 3) fix the y-axis to show a more honest indication of the effect on outcomes, given the potential range of spelling scores can start at 0.\n\nIn Figure 17, the dots represent the linear model estimates of outcome spelling, predicted under different conditions. The plots indicate that spelling scores are predicted to be lower when orthography is present. There appears to be little or no effect associated with different kinds of instruction.\nThe vertical lines (often termed “whiskers”) indicate the 95% confidence interval about these estimates. Confidence intervals (CIs) are often mis-interpreted so I will give the quick definition outlined by @Hoekstra2014 here:\n\nA CI is a numerical interval constructed around the estimate of a parameter [i.e. the model estimate of the effect]. Such an interval does not, however, directly indicate a property of the parameter; instead, it indicates a property of the procedure, as is typical for a frequentist technique. Specifically, we may find that a particular procedure, when used repeatedly across a series of hypothetical data sets (i.e., the sample space), yields intervals that contain the true parameter value in 95 % of the cases.\n\nIn short, the interval shows us the range of values within which we can expect to capture the effects of interest, in the long run, if we were to run our experiment over and over again.\nGiven our data and our model, these intervals indicate where the outcome might be expected to vary, given different conditions, and that is quite useful information. If you look at Figure 17, you can see that the presence of orthography (present versus absent) appears to shift outcome spelling, on average, by about a quarter of a letter edit: from over 1.5 to about 1.25. This is about one quarter of the difference, on average, between getting a target spelling correct and getting it wrong by one letter (e.g., the response ‘epegram’ for the target ‘epigram’). This is a relatively small effect but we may consider how such small effects add up, over a child’s development, cumulatively, in making the difference between wrong or nearly right spellings to correct spellings.\nIn the @ricketts2021 paper, we conducted Bayesian analyses which allow us to plot the estimated effects of experimental conditions along with what are called credible intervals indicating our uncertainty about the estimates. In a Bayesian analysis, we can indicate the probable or plausible effect of conditions, or range of plausible effects, given our data and our model. (This intuitive sense of the probable location of effects is, sometimes, what researchers and students mis-interpret confidence intervals as showing; @Hoekstra2014.) Accounting for our uncertainty is a productive approach to considering how much we learn from the evidence we collect in experiments.\nBut this gets ahead of where we are now in our development of skills and understanding. There is another way to discover how uncertain we may be about the results of our analysis. This is an approach we have already experienced: plotting trends or estimates together with the observed data points. We present an example in Figure 18.\n\nplot(dat, add.data = TRUE)\n\n\n\n\nFigure 18: Dot and whisker plots showing the predicted effect on outcome spelling (Levenshtein) score, given different experimental conditions: Orthography (present, absent) x Instruction (explicit, incidental). The estimates are shown as dot-whisker points. In addition, the plot shows as points the spelling score observed for each child for each response recorded in the conc.orth dataset.\n\n\n\n\nFigure 18 reveals the usefulness of plotting model estimates of effects alongside the raw observed outcomes. We can make two critical observations.\n\nWe can see that the observed scores clearly cluster around outcome spelling values of 0, 1, 2, 3, 4, and 5.\n\n\nThis is not a surprise because @ricketts2021 scored each response in their test of spelling knowledge by counting the number of letter edits (letter deletions, additions etc.) separating a spelling response from a target response.\nBut the plot does suggest that the linear model is missing something about the outcome data because there is no recognition in the model or the results of this bunching or clustering around whole number values of the outcome variable. (This is why @ricketts2021 use a different analysis approach.)\n\n\nWe can also see that it is actually quite difficult to distinguish the effects of the experimental condition differences on the observed spelling responses. There is a lot of variation in the responses.\n\nHow can we make sense of this variation?\nAnother approach we can take to experimental data is to examine visually how the effects of experimental conditions vary between individual participants. Usually, in teaching, learning and doing foundation or introductory statistical analyses we think about the average impact on outcomes of the experimental conditions or some set of predictor variables. It often makes sense, also, or instead, to consider the ways that the impact on outcomes vary between individuals.\nHere, it might be worthwhile to look at the effect of the conditions for each child. We can do that in different ways. In the following, we will look at a couple of approaches that are often useful. We will focus on the effect of variation in the Orthography condition (present, absent)\nTo begin our work, we first calculate the average outcome (Levenshtein.Score) spelling score for each child in each of the experimental conditions (Orthography, present versus absent):\nWe do this in a series of steps.\n\nscore.by.subj &lt;- conc.orth %&gt;%\n  group_by(Participant, Orthography) %&gt;%\n  summarise(mean.score = mean(Levenshtein.Score))\n\n\nscore.by.subj &lt;- conc.orth %&gt;% create a new dataset score.by.subj by taking the original data conc.orth and piping it through a series of processing steps, to follow.\ngroup_by(Participant, Orthography) %&gt;% first group the rows of the original dataset and piped the grouped data to the next bit. We group the data by participant identity code and by Orthography condition\nsummarise(mean.score = mean(Levenshtein.Score)) then calculate the mean Levenshtein.Score for each participant, for their responses in the Orthography present and in the Orthography absent conditions.\n\nThis first step produces a summary version of the original dataset, with two mean outcome spelling scores for each child, for their responses in the Orthography present and in the Orthography absent conditions. This arranges the summary mean scores in rows, with two rows per child: one for the absent, one for the present condition. You can see what we get in the extract from the dataset, shown next.\n\n\n\n\n\nParticipant\nOrthography\nmean.score\n\n\n\n\nEOF001\nabsent\n1.750\n\n\nEOF001\npresent\n0.875\n\n\nEOF002\nabsent\n1.375\n\n\nEOF002\npresent\n2.125\n\n\nEOF004\nabsent\n1.625\n\n\nEOF004\npresent\n1.000\n\n\nEOF006\nabsent\n0.750\n\n\nEOF006\npresent\n0.500\n\n\nEOF007\nabsent\n1.500\n\n\nEOF007\npresent\n0.625\n\n\n\n\n\n\n\nIn the second step, we also calculate the difference between spelling scores in the different Orthography conditions. We do this because @ricketts2021 were interested in whether spelling responses were different in the different conditions.\n\nscore.by.subj.diff &lt;- score.by.subj %&gt;%\n  pivot_wider(names_from = Orthography, values_from = mean.score) %&gt;%\n  mutate(difference.score = absent - present) %&gt;%\n  pivot_longer(cols = c(absent, present), \n               names_to = 'Orthography',\n               values_to = 'mean.score') \n\n\nscore.by.subj.diff &lt;- score.by.subj %&gt;% creates a new version of the summary dataset from the dataset we just produced.\npivot_wider(names_from = Orthography, values_from = mean.score) %&gt;% re-arranges the dataset so that the absent, present mean scores are side-by-side, in different columns, for each child.\nmutate(difference.score = absent - present) %&gt;% calculates the difference between the absent, present mean scores, creating a new variable, difference.score.\npivot_longer(cols = c(absent, present) ...) re-arranges the data back again so that the dataset is in tidy format, with one column of mean spelling scores, with two rows for each participant for the absent, present mean scores.\n\nThis code arranges the summary mean scores in rows, with two rows per child: one for the absent, one for the present condition — plus a difference score.\n\n\n\n\n\nParticipant\ndifference.score\nOrthography\nmean.score\n\n\n\n\nEOF001\n0.875\nabsent\n1.750\n\n\nEOF001\n0.875\npresent\n0.875\n\n\nEOF002\n-0.750\nabsent\n1.375\n\n\nEOF002\n-0.750\npresent\n2.125\n\n\nEOF004\n0.625\nabsent\n1.625\n\n\nEOF004\n0.625\npresent\n1.000\n\n\nEOF006\n0.250\nabsent\n0.750\n\n\nEOF006\n0.250\npresent\n0.500\n\n\nEOF007\n0.875\nabsent\n1.500\n\n\nEOF007\n0.875\npresent\n0.625\n\n\n\n\n\n\n\nNow we can use these data to consider how the impact of the experimental condition (Orthography: present versus absent) varies between individual participants. We do this by showing the mean outcome spelling score, separately for each participant, in each condition.\nFigure 19 shows dot plots indicating the different outcome spelling (Levenshtein) scores, for each participant, in the different experimental conditions: Orthography (present, absent). Plots are ordered, from top left to bottom right, by the difference between mean spelling scores in the absent versus present conditions. The plots indicate that some children show higher spelling scores in the present than in the absent condition (top left plots), some children show little difference between conditions (middle rows), while some children show higher spelling scores in the absent than in the present condition (bottom rows).\n\nggplot(data = score.by.subj.diff, \n       aes(x = Orthography, y = mean.score,\n           colour = Orthography)) +\n  geom_point() +\n  facet_wrap(~ reorder(Participant, difference.score)) +\n  theme(axis.text.x = element_blank())\n\n\n\n\nFigure 19: Dot plots showing the different outcome spelling (Levenshtein) scores, for each participant, in the different experimental conditions: Orthography (present, absent). Plots are ordered, from top left to bottom right, by the difference between mean spelling scores in the absent versus present conditions.\n\n\n\n\nOnce we have done the data processing in preparation, the code to produce the plot is fairly compact.\n\nggplot(data = score.by.subj.diff ... tells R to produce a plot, using ggplot() and the newly created score.by.subj.diff dataset.\naes(x = Orthography, y = mean.score,... specifies the aesthetic mappings: we tell R to locate mean.score on the y-axis and Orthography condition on the x-axis/\naes(...colour = Orthography)) + specifies a further aesthetic mapping: we tell R to map different Orthography conditions to different colours.\ngeom_point() + tells R to take the data and produce a scatterplot, given our mapping specifications.\nfacet_wrap(...) + tells to split the dataset into sub-sets (facets).\nfacet_wrap(~ reorder(Participant, difference.score)) tells R that we want the sub-sets to be organized by Participant, and we want the facets to be ordered by the difference.score calculated for each participant.\ntheme(axis.text.x = element_blank()) removes the x-axis labels because it is too crowded with the axis labels left in, and the information is already present in the colour guide legend shown on the right of the plot.\n\n\n\n\nVisualizing associations between variables encompasses a wide range of the things we have to do, in terms of both discovery and communication, when we work with data from psychological experiments.\nThe conventional method to visualize how the distribution of values in one variable covaries with the distribution of values in another variable is through using a scatterplot. However, the construction of a scatterplot can be elaborated in various ways to enrich the information we present or communicate to our audiences, or to ourselves.\n\nWe can add elements like smoothers to indicate trends.\nWe can add annotation, as with the histograms, to highlight specific thresholds.\nWe can facet the plots to indicate how trends may vary between sub-sets of the data.\n\nIn the final phases of our practical work, we started by presenting model-based predictions of the effects of experimental manipulations. However, you will have noticed that presenting plots of effects is not where we stop when we engage with a dataset. Further plotting indicates quite marked variation between participants in the effects of the conditions. This kind of insight is something we can and should seek to reveal through our visualization work.\n\n\n\n\nTo take your development further, take a look at the resources listed in Section 1.9.\nIn my experience, the most productive way to learn about visualization and about coding the production of plots, is by doing. And this work is most interesting if you have a dataset you care about: for your research report, or for your dissertation study.\nAs you have the alternate datasets described in Section 1.7.1.2.1, you can start with the data from the other task or the other study in @ricketts2021. @ricketts2021 recorded children’s responses in two different outcome tasks, the orthographic spelling task we have looked at, and a semantic or meaning-based task. It would be a fairly short step to adapt the code you see in the example code chunks to work with the semantic datasets.\nAlternatively, you can look at the data reported by @rodríguez-ferreiro2020. @rodríguez-ferreiro2020 present both measures of individual differences (on schizotypyal traits) and experimental manipulations (of semantic priming) so you can do similar things with those data as we have explored here.\n\n\n\n\n\n\nWe typically use the ggplot library (part of the tidyverse) to produce plots. Clear technical information, with useful examples you can copy and run, can be found in the reference webpages:\n\nhttps://ggplot2.tidyverse.org/reference/index.html\n\nA source of inspiration can be found here:\n\nhttps://r-graph-gallery.com\nIf you are trying to work out how to do things by searching for information online, you often find yourself at tutorial webpages. You will develop a sense of quality and usefulness with experience. Most often, what you are looking for is a tutorial that provides some explanation, and example code you can adapt for your own purposes. Here are some examples.\n\nCedric Scherer on producing raincloud plots:\n\nhttps://www.cedricscherer.com/2021/06/06/visualizing-distributions-with-raincloud-plots-and-how-to-create-them-with-ggplot2/\n\nWinston Chang on colours and colour blind palettes:\n\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\n\nThomas Lin Pedersen (and others) on putting together plots into a single presentation using the patchwork library functions:\n\nhttps://patchwork.data-imaginist.com/articles/patchwork.html\n\n\n\n\nThe book “R for Data Science” [@wickham2016] will guide you through the data analysis workflow, including data visualization, and the latest version can be accessed in an online free version here:\n\nhttps://r4ds.hadley.nz\n\nThe “ggplot2: Elegant Graphics for Data Analysis” book [@R-ggplot2] corresponding to the ggplot library was written by Hadley Wickham in its first edition, it is now in its third edition (as a work in progress, co-authored by Wickham, Danielle Navarro and Thomas Lin Pedersen) and this latest version can be accessed in an online free version here:\n\nhttps://ggplot2-book.org/index.html\n\nThe “R graphics cookbook” [@Chang2013a], and the latest version can be accessed in an online free version here:\n\nhttps://r-graphics.org\n\nThe book “Fundamentals of Data Visualization” [@wilke] is about different aspects of visualization, and can be accessed in an online free version here:\n\nhttps://clauswilke.com/dataviz/"
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html#sec-aims",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html#sec-aims",
    "title": "Data visualization",
    "section": "",
    "text": "In writing this chapter, I have two aims.\n\nThe first aim for this chapter is to expose students to an outline summary of some key ideas and techniques for data visualization in psychological science.\n\nThere is an extensive experimental and theoretical literature concerning data visualization, what choices we can or should make, and how these choices have more or less impact, in different circumstances or for different audiences. Here, we can only give you a flavour of the on-going discussion. If you are interested, you can follow-up the references in the cited articles. But, using this chapter, I hope that you will gain a sense of the reasons how or why we may choose to do different things when we produce visualizations.\n\nThe second aim is to provide materials, and to show visualizations, to raise an awareness of what results come from making different choices. This is because we hope to encourage students to make choices based on reasons and it is hard to know what choices count without first seeing what the results might look like.\n\nIn my experience, knowing that there are choices is the first step. In proprietary software packages like Excel and SPSS there are plenty of choices but these are limited by the menu systems to certain combinations of elements. Here, in using R to produce visualizations, there is much more freedom, and much more capacity to control what a plot shows and how it looks, but knowing where to start has to begin with seeing examples of what some of the choices result in.\nAt the end of the chapter, I highlight some resources you can use in independent learning for further development, see Section 1.9.\nSo, we are aiming to (1.) start to build insight into the choices we make and (2.) provide resources to enable making those choices in data visualization."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html#sec-why-visualization-matters",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html#sec-why-visualization-matters",
    "title": "Data visualization",
    "section": "",
    "text": "Data visualization is important. Building skills in visualization matters to you because, even if you do not go on to professional work in which you produce visualizations you will certainly be working in fields in which you need to work with, or read or evaluate, visualizations.\nYou have already been doing this: our cultural or visual environment is awash in visualizations, from weather maps to charts on the television news. It will empower you if you know a bit about how or why these visualizations are produced in the ways that they are produced. That is a complex development trajectory but we can get started here.\nIn the context of the research report exercise, see ?@sec-pipeline, I mention data visualization in relation to stages of the data analysis pipeline or workflow. But the reality is that, most of the time, visualization is useful and used at every stage of data analysis workflow.\n\n\n\n\n\n\n\n\nQ\n\n \n\ncluster_R\n\n   \n\nnd_1\n\n Get raw data   \n\nnd_2\n\n Tidy data   \n\nnd_1-&gt;nd_2\n\n    \n\nnd_3_l\n\n Visualize   \n\nnd_2-&gt;nd_3_l\n\n    \n\nnd_3\n\n Analyze   \n\nnd_2-&gt;nd_3\n\n    \n\nnd_3_r\n\n Explore   \n\nnd_2-&gt;nd_3_r\n\n    \n\nnd_3_a\n\n Assumptions   \n\nnd_3_a-&gt;nd_3_l\n\n    \n\nnd_3_a-&gt;nd_3\n\n    \n\nnd_3_a-&gt;nd_3_r\n\n    \n\nnd_3_l-&gt;nd_3\n\n   \n\nnd_4\n\n Present   \n\nnd_3_l-&gt;nd_4\n\n    \n\nnd_3-&gt;nd_4\n\n   \n\n\nFigure 1: The data analysis pipeline or workflow"
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html#sec-honesty",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html#sec-honesty",
    "title": "Data visualization",
    "section": "",
    "text": "I write this chapter with three kinds of honesty in mind.\n\nI will expose some of the process involved in thinking about and preparing for the production of plots.\n\n\nI can assure you that when a professional data analysis worker produces plots in R they will be looking for information about what to do, and how to do it, online. I will provide links to the information I used, when I wrote this chapter, in order to figure out the coding to produce the plots.\nI won’t pretend that I got the plots “right first time” or that I know all the coding steps by memory. Neither is true for me and they would not be true for most professionals if they were to write a chapter like this. Looking things up online is something we all do so showing you where the information can be found will help you grow your skills.\n\n\nI will show how we often prepare for the production of plots by processing the data that we must use to inform the plots.\n\n\nWe almost always have to process the data we collected or gathered together from our exerimental work or our observations.\nIn this chapter, some of the coding steps I will outline are done in advance of producing a plot, to give the plotting code something to work with.\nKnowing about these processing steps will ensure you have more flexibility or power in getting your plots ready.\n\n\nI am going to expose variation, as often as I can, in observations.\n\n\nWe typically collect data about or from people, about their responses to things we may present (stimuli) or, given tasks, under different conditions, or concerning individual differences on an array of dimensions.\nSources of variation will be everywhere in our data, even though we often work with statistical analyses (like the t-test) that focus our attention on the average participant or the average response.\nModern analysis methods (like mixed-effects models) enable us to account for sources of variation systematically, so it is good to begin thinking about, say, how people vary in their response to different experimental conditions from early in your development."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html#sec-tidyverse",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html#sec-tidyverse",
    "title": "Data visualization",
    "section": "",
    "text": "The approach we will take is to focus on step-by-step guides to coding. I will show plots and I will walk through the coding steps, explaining my reasons for the choices I make.\nWe will be working with plotting functions like ggplot() provided in libraries like ggplot2 [@R-ggplot2] which is part of the tidyverse [@R-tidyverse] collection of libraries.\nYou can access information about the tidyverse collection here.\n\n\nThe gg in ggplot stands for the “Grammar of Graphics”, and the ideas motivating the development of the ggplot2 library of functions are grounded in the ideas concerning the grammar of graphics, set out in the book of that name [@wilkinson2013].\nWhat is helpful to us, here, is the insight that the code elements (and how they result in visual elements) can be identified as building blocks, or layers, that we can add and adjust piece by piece when we are producing a visualization.\nA plot represents information and, critically, every time we write ggplot code we must specify somewhere the ways that our plot links data to something we see. In terms of ggplot, we specify aesthetic mappings using the aes() code to tell R what variables should be mapped e.g. to x-axis or y-axis location, to colour, or to group assignments. We then add elements to instruct R how to represent the aesthetic mappings as visual objects or attributes: geometric objects like a scatter of points geom_point() or a collection of bars geom_bar(); or visual features like colour, shape or size e.g. aes(colour = group). We can add visual elements in a series of layers, as shall see in the practical demonstrations of plot construction. We can adjust how scaling works. And we can add annotation, labels, and other elements to guide and inform the attention of the audience.\nYou can read more about mastering the grammar here.\n\n\n\nWe know that (some of) you want to see more use of pipes (represented as %&gt;% or |&gt;) in coding. There will be plenty of pipes in this chapter.\nIn using pipes in the code, I am structuring the code so that it works — and is presented — in a sequence of steps. There are different ways to write code but I find this way easier to work with and to read and I think you will too.\nLet’s take a small example:\n\nsleepstudy %&gt;%\n  group_by(Subject) %&gt;%\n  summarise(average = mean(Reaction)) %&gt;%\n  ggplot(aes(x = average)) + \n  geom_histogram()\n\n\n\n\nHere, we work through a series of steps:\n\nsleepstudy %&gt;% we first tell R we want to work with the dataset called sleepstudy and the %&gt;% pipe symbol at the end of the line tells R that we want it to pass that dataset on to the next step for what happens next.\ngroup_by(Subject) %&gt;% tells R that we want it to do something, here, group the rows of data according to the Subject (participant identity) coding variable, and pass the grouped data on to the next step for what happens following.\nsummarise(average = mean(Reaction)) %&gt;% tells R to take the grouped variable and calculate a summary, the mean Reaction score, for each group of observations for each participant. The %&gt;% pipe at the end of the line tells R to pass the summary dataset of mean Reaction scores on to the next process.\nggplot(aes(x = average)) + tells R that we want it to take these summary average Reaction scores and make a plot out of them.\ngeom_histogram() tells R that we want a histogram plot.\n\nWhat you can see is that each line ending in a %&gt; pipe passes something on to the next line. A following line takes the output of the process coded in the preceding line, and works with it.\nEach step is executed in turn, in strict sequence. This means that if I delete line 3 summarise(average = mean(Reaction)) %&gt;% then the following lines cannot work because the ggplot() function will be looking for a variable average that does not yet exist.\n\n\n\n\n\n\nWarning\n\n\n\n\nYou can see that in the data processing part of the code, successive steps in data processing end in a pipe %&gt;%.\nIn contrast, successive steps of the plotting code add ggplot elements line by line with each line (except the last) ending in a +.\n\n\n\nNotice that none of the processing steps actually changes the dataset called sleepstudy. The results of the process exist and can be used only within the sequence of steps that I have coded. If you want to keep the results of processing steps, you need to assign an object name to hold them, and I show how to do this, in the following.\nYou can read a clear explanation of pipes here.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the code you see:\n\nEach chunk of code is highlighted in the chapter.\nIf you hover a cursor over the highlighted code a little clipboard symbol appears in the top right of the code chunk.\nClick on the clipboard symbol to copy the code, paste it into your own R-Studio instance.\nThen experiment: try out things like removing or commmenting out lines, or changing lines, to see what effect that has.\nBreaking things, or changing things, helps to show what each bit of code does."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html#sec-ideas",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html#sec-ideas",
    "title": "Data visualization",
    "section": "",
    "text": "Data visualization is not really about coding, as about thinking.\n\nWhat are our goals?\nWhy do we make some choices instead of others?\n\n\n\n@gelman2013 outline the goals we may contemplate when we produce or evaluate visual data displays. In general, they argue, we are doing one or both of two things.\n\nDiscovery\nCommunication\n\nIn practice, this may involve the following (I paraphrase them, here).\n\nDiscovery goals\n\n\nGetting a sense of what is in a dataset, checking assumptions, confirming expectations, and looking for distinct patterns.\nMaking sense of the scale and complexity of the dataset.\nExploring the data to reveal unexpected aspects. As we will see, using small multiples (grids of plots) can often help with this.\n\n\nCommunication goals\n\n\nWe communicate about our data to ourselves and to others. The process of constructing and evaluating a plot is often one way we speak to ourselves about own data, developing an understanding of what we have got. Once we have done this for ourselves, we can better figure out how to do it to benefit the understanding of an audience.\nWe often use a plot to tell a story: the story of our study, our data, or our insight and how we get to it.\nWe can use visualizations to attract attention and stimulate interest. Often, in presenting data to an audience through a talk or a report we need to use effective visualizations to ensure we get attention and that we locate the attention of our audience in the right places.\n\n\n\n\nYou will see a rich variety of data visualizations in media and in the research literature. You will know that some choices, in the production of those visualizations, appear to work better than others.\nSome of the reasons why some choices work better will relate to what we can understand in terms of the psychological science of how visual data communication works. A useful recent review of relevant research is presented by @franconeri2021.\n@franconeri2021 provide a reason for working on visualizations: they allow us humans to process an array of information at once, often faster than if we were reading about the information, bit by bit. Effective visualization, then, is about harnessing the power of the human visual system, or visual cognition, for quick, efficient, information processing. Critically for science, in addition, visualizations can be more effective for discovering or communicating the critical features of data than summary statistics, as we shall see.\nIn producing visualizations, we often work with a vocabulary or palette of objects or visual elements. @franconeri2021 discuss how visualizations rely on visual channels to transform numbers into images that we can process visually.\n\nDot plots and scatterplots represent values as position.\nBar graphs represent values as position (the heights of the tops of bars) but also as lengths.\nAngles are presented when we connect points to form a line, allowing us to encode the differences between points.\nIntensity can be presented through variation in luminance contrast or colour saturation.\n\nThese channels can be ordered by how precisely they have been found to communicate different numeric values to the viewer. Your audience may more accurately perceive the difference between two quantities if you communicate that difference through the difference in the location of two points than if you ask your audience to compare the angles of two lines or the intensity of two colour spots.\nIn constructing data visualizations, we often work with conventions, established through common practice in a research tradition. For example, if you are producing a scatterplot, then most of the time your audience will expect to see the outcome (or dependent variable) represented by the vertical height (on the y-axis) of points. And your audience will expect that higher points represent larger quantities of the y-axis variable.\nIn constructing visualizations, we need to be aware of the cognitive work that we require the audience to do. Comparisons are harder, requiring more processing and imposing more load on working memory. You can help your reader by guiding their attention, by grouping or ordering visual elements to identify the most important comparisons. We can vary colour and shape to group or distinguish visual elements. We can add annotation or elements like lines or arrows to guide attention.\nVisualizations are presented in context, whether in presentations or in reports. This context should be provided, by you the producer, with the intention to support the communication of your key messages. A visual representation, a plot, will be presented with a title, maybe a title note, maybe with annotation in the plot, and maybe with accompanying text. You should use these textual elements to lead your audience, to help them make sense of what they are looking at.\nThe diversity of audiences means that we should habitually add alt text for data visualizations to help those who use screen readers by providing a summary description of what images show. This chapter has been written using Quarto and rendered to .html with alt text included along with all images. Please do let me know if you are using a screen reader and the alt text description is or is not so helpful.\nYou can read a helpful explanation of alt text here.\nIf you use colour in images then we should use colour bind colour palettes.\nYou can read about using colour blind palettes here or here.\nIn the following practical exercises, we work with many of the insights in our construction of visualizations."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html#sec-quick-start",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html#sec-quick-start",
    "title": "Data visualization",
    "section": "",
    "text": "We can get started before we understand in depth the key ideas or the coding steps. This will help to show where we are going. We will work with the sleepstudy dataset.\nI will model the process, to give you an example workflow:\n\nthe data, where they come from — what we can find out;\nhow we approach the data — what we expect to see;\nhow we visualize the data — discovery, communication.\n\n\n\nWhen we work with R, we usually work with functions like ggplot() provided in libraries like ggplot2 [@R-ggplot2]. These libraries typically provide not only functions but also datasets that we can use for demonstration and learning.\nThe lme4 library [@R-lme4] provides the sleepstudy dataset and we will take a look at these data to offer a taste of what we can learn to do. Usually, information about the R libraries we use will be located on the Comprehensive R Archive Network (CRAN) web pages, and we can find the technical reference information for lme4 in the CRAN reference manual for the library, where we see that the sleepstudy data are from a study reported by [@belenky2003]. The manual says that the sleepstudy dataset comprises:\n\nA data frame with 180 observations on the following 3 variables. [1.] Reaction – Average reaction time (ms) [2.] Days – Number of days of sleep deprivation [3.] Subject – Subject number on which the observation was made.\n\nWe can take a look at the first few rows of the dataset.\n\nsleepstudy %&gt;%\n    head(n = 4)\n\n  Reaction Days Subject\n1 249.5600    0     308\n2 258.7047    1     308\n3 250.8006    2     308\n4 321.4398    3     308\n\n\nWhat we are looking at are:\n\nThe average reaction time per day (in milliseconds) for subjects in a sleep deprivation study. Days 0-1 were adaptation and training (T1/T2), day 2 was baseline (B); sleep deprivation started after day 2.\n\nThe abstract for @belenky2003 tells us that participants were deprived of sleep and the impact of relative deprivation was tested using a cognitive vigilance task for which the reaction times of responses were recorded.\nSo, we can expect to find:\n\nA set of rows corresponding to multiple observations for each participant (Subject)\nA reaction time value for each participant (Reaction)\nRecorded on each Day\n\n\n\n\nIn data analysis work, we often begin with the objective to understand the structure or the nature of the data we are working with.\nYou can call this the discovery phase:\n\nwhat have we got?\ndoes it match our expectations?\n\nIf these are reaction time data (collected in an cognitive experiment) do they look like cognitive reaction time data should look? We would expect to see a skewed distribution of observed reaction times distributed around an average located somewhere in the range 200-700ms.\nFigure 2 represents the distribution of reaction times in the sleepstudy dataset.\nI provide notes on the code steps that result in the plot. Click on the Notes tab to see them. Later, I will discuss some of these elements.\n\nPlotNotes\n\n\n\nsleepstudy %&gt;%\n  ggplot(aes(x = Reaction)) +\n  geom_histogram(binwidth = 15) +\n  geom_vline(xintercept = mean(sleepstudy$Reaction), \n             colour = \"red\", linetype = 'dashed', size = 1.5) +\n  annotate(\"text\", x = 370, y =20, \n                    colour = \"red\", \n                    label = \"Average value shown in red\") +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 2: Figure showing a histogram of sleepstudy reaction time data\n\n\n\n\n\n\nThe plotting code pipes the data into the plotting code steps to produce the plot. You can see some elements that will be familiar to you and some new elements.\n\nsleepstudy %&gt;%\n  ggplot(aes(x = Reaction)) +\n  geom_histogram(binwidth = 15) +\n  geom_vline(xintercept = mean(sleepstudy$Reaction), \n             colour = \"red\", linetype = 'dashed', size = 1.5) +\n  annotate(\"text\", x = 370, y =20, \n                    colour = \"red\", \n                    label = \"Average value shown in red\") +\n  theme_bw()\n\nLet’s go through the code step-by-step:\n\nsleepstudy %&gt;% asks R to take the sleepstudy dataset and %&gt;% pipe it to the next steps for processing.\nggplot(aes(x = Reaction)) + takes the sleepstudy data and asks R to use the ggplot() function to produce a plot.\naes(x = Reaction) tells R that in the plot we want it to map the Reaction variable values to locations on the x-axis: this is the aesthetic mapping.\ngeom_histogram(binwidth = 15) + tells R to produce a histogram then add a step.\ngeom_vline(...) + tells R we want to draw vertical line.\nxintercept = mean(sleepstudy$Reaction), ... tells R to draw the vertical line at the mean value of the variable Reaction in the sleepstudy dataset.\ncolour = \"red\", linetype = 'dashed', size = 1.5 tells R we want the vertical line to be red, dashed and 1.5 times the usual size.\nannotate(\"text\", ...) tells R we want to add a text note.\nx = 370, y =20, ... tells R we want the note added at the x,y coordinates given.\ncolour = \"red\", ..; and we want the text in red.\n...label = \"Average value shown in red\") + tells R we want the text note to say that this is where the average is.\ntheme_bw() lastly, we change the theme.\n\n\n\n\nFigure 2 shows a distribution of reaction times, ranging from about 200ms to 500ms. The distribution has a peak around 300ms. The location of the mean is shown with a dashed red line. The distribution includes a long tail of longer times. This is pretty much what we would expect to see.\nWe may wish to communicate the information we gain through using this histogram, in a presentation or in a report.\n\n\n\nLet us imagine that it is our study. (Here, we shall not concern ourselves too much — with apologies — with understanding what the original study authors actually did.)\nIf we are looking at the impact of sleep deprivation on cognitive performance, we might predict that reaction times got longer (responses slowed) as the study progressed. Is that what we see?\nTo examine the association between two variables, we often use scatterplots. Figure 3 is a scatterplot indicating the possible association between reaction time and days in the sleepstudy data. Points are ordered on x-axis from 0 to 9 days, on y-axis from 200 to 500 ms reaction time.\nI provide notes on the code steps that result in the plot. Click on the Notes tab to see them. Later, I will discuss some of these elements.\n\nPlotNotes\n\n\n\nsleepstudy %&gt;%\n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point(size = 1.5, alpha = .5) + \n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  theme_bw()\n\n\n\n\nFigure 3: Figure showing a scatterplot of the relation between reaction time and days in the sleepstudy data\n\n\n\n\n\n\nNotice the numbered steps in producing this plot.\n\nsleepstudy %&gt;% \n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point() + \n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  theme_bw()\n\n\nName the dataset: the dataset is called sleepstudy in the lme4 library which makes it available therefore we use this name to specify it.\nsleepstudy %&gt;% uses the %&gt;% pipe operator to pass this dataset to ggplot() to work with, in creating the plot. Because ggplot() now knows about the sleepstudy data, we can next specify what aesthetic mappings we need to use.\nggplot(aes(x = Days, y = Reaction)) + tells R that we want to map Days information to x-axis position and Reaction (response time) information to y-axis position.\ngeom_point() + tells R that we want to locate points – creating a scatterplot – at the paired x-axis and y-xis coordinates.\nscale_x_continuous(breaks = c(0, 3, 6, 9)) + is new: we tell R that we want the x-axis tick labels – the numbers R shows as labels on the x-axis – at the values 0, 3, 6, 9 only.\ntheme_bw() requires R to make the plot background white and the foreground plot elements black.\n\nYou can find more information on scale_ functions in the ggplot2 reference information.\nhttps://ggplot2.tidyverse.org/reference/scale_continuous.html\n\n\n\nThe plot suggests that reaction time increases with increasing number of days.\nIn producing this plot, we are both (1.) engaged in discovery and, potentially, (2.) able to do communication.\n\nDiscovery: is the relation between variables what we should expect, given our assumptions?\nCommunication: to ourselves and others, what relation do we observe, given our sample?\n\nAt this time, we have used and discussed scatterplots before, why we use them, how we write code to produce them, and how we read them.\nWith two additional steps we can significantly increase the power of the visualization. Figure 4 is a grid of scatterplots indicating the possible association between reaction time and days separately for each participant.\nAgain, I hide an explanation of the coding steps in the Notes tab: the interested reader can click on the tab to view the step-by-step guide to what is happening.\n\nPlotNotes\n\n\n\nsleepstudy %&gt;%\n  group_by(Subject) %&gt;%\n  mutate(average = mean(Reaction)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Subject = fct_reorder(Subject, average)) %&gt;%\n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  facet_wrap(~ Subject) +\n  theme_bw()\n\n\n\n\nFigure 4: Figure showing a scatterplot of the relation between reaction time and days: here, we plot the data for each participant separately\n\n\n\n\n\n\nNotice the numbered steps in producing this plot.\n\nsleepstudy %&gt;%\n  group_by(Subject) %&gt;%\n  mutate(average = mean(Reaction)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Subject = fct_reorder(Subject, average)) %&gt;%\n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  facet_wrap(~ Subject) +\n  theme_bw()\n\nYou can see that the block of code combines data processing and data plotting steps. Let’s look at the data processing steps then the plotting steps in order.\nFirst: why are we doing this? My aim is to produce a plot in which I show the association between Days and Reaction for each Subject individually. I suspect that the association between Days and Reaction may be stronger – so the trend will be steeper – for participants who are slower overall. I suspect this because, given experience, I know that slower, less accurate, participants tend to show larger effects.\nSo: in order to get a grid of plots, one plot for each Subject, in order of the average Reaction for each individual Subject, I need to first calculate the average Reaction then order the dataset rows by those averages. I do that in steps, using pipes to feed information from one step to the next step, as follows.\n\nsleepstudy %&gt;% tells R what data I want to use, and pipe it to the next step.\ngroup_by(Subject) tells R I want it to work with data (rows) grouped by Subject identity code, %&gt;% piping the grouped form of the data forward to the next step\nmutate(average = mean(Reaction)) uses mutate() to create a new variable average which I calculate as the mean() of Reaction, piping the data with this additional variable %&gt;% forward to the next step.\nungroup() %&gt;% tells R I want it to go back to working with the data in rows not grouped rows, and pipe the now ungrouped form of the data to the next step.\nmutate(Subject = fct_reorder(Subject, average)) tells R I want it to sort the rows of the whole sleepstudy dataset in order, moving groups of rows identified by Subject so that data for Subject codes associated with faster times are located near the top of the dataset.\n\nThese data, ordered by Subject by the average Reaction for each participant, are then %&gt;% piped to ggplot to create a plot.\n\nggplot(aes(x = Days, y = Reaction)) + specifies the aesthetic mappings, as before.\ngeom_point() + asks R to locate points at the x-axis, y-axis coordinates, creating a scatterplot, as before.\ngeom_line() + is new: I want R to connect the points, showing the trend in the association between Days and Reaction for each person.\nscale_x_continuous(breaks = c(0, 3, 6, 9)) + fixes the x-axis labels, as before.\nfacet_wrap(~ Subject) + is the big new step: I ask R to plot a separate scatterplot for the data for each individual Subject.\n\nYou can see more information about facetting here:\nhttps://ggplot2.tidyverse.org/reference/facet_wrap.html\nIn short, with the facet_wrap(~ .) function, we are asking R to subset the data by a grouping variable, specified (~ .) by replacing the dot with the name of the variable.\nNotice that I use %&gt;% pipes to move the data processing forward, step by step. But I use + to add plot elements, layer by layer.\n\n\n\nFigure Figure 4 is a grid or lattice of scatterplots revealing how the possible association between reaction time and days varies quite substantially between the participants in the sleepstudy data. Most plots indicate that reaction time increases with increasing number of days. However, different participants show this trend to differing extents.\nWhat are the two additions I made to the conventional scatterplot code?\n\nI calculated the average reaction time per participant, and I ordered the data by those averages.\nI facetted the plots, breaking them out into separate scatterplots per participant.\n\nWhy would you do this? Variation between people or groups, in effects or in average outcomes, are often to be found in psychological data [@vasishth2021]. The variation between people that we see in these data — in the average response reaction time, and in how days affects times — would motivate the use of linear mixed-effects models to analyze the way that sleep patterns affect responses in the sleep study [@Pinheiro2000a].\n\n\n\n\n\n\nTip\n\n\n\nThe data processing and plotting functions in the tidyverse collection of libraries enable us to discover and to communicate variation in behaviours that should strengthen our and others’ scientific understanding.\n\n\n\n\n\nWhat we have seen, so far, is that we can make dramatic changes to the appearance of visualizations (e.g., through faceting) and also that we can exert fine control over the details (e.g., adjusting scale labels). What we need to stop and consider are what we want to do (and why), in what order.\nWe have seen how we can feed a data process into a plot to first prepare then produce the plot in a sequence of steps. In processing the data, we can take some original data and extract or calculate information that we can use for our plotting e.g. calculating the mean of a distribution in order to then highlight where that mean is located.\nWe have also seen the use of plots, and the editing of their appearance, to represent information visually. We can verbalize the thought process behind the production of these plots through a series of questions.\n\nAre we looking at the distribution of one variable (if yes: consider a histogram) or are we comparing the distributions of two or more variables (if yes: consider a scatterplot)?\nIs there a salient feature of the plot we want to draw the attention of the audience to? We can add a visual element (like a line) and annotation text to guide the audience.\nAre we interested in variation between sub-sets of the data? We can facet the plot to examine variation between sub-sets (facets) enabling the comparison of trends."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html#sec-practical-visualization",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html#sec-practical-visualization",
    "title": "Data visualization",
    "section": "",
    "text": "In this guide, we illustrate some of the ideas about visualization we discussed at the start, working with practical coding examples. We will be working with real data from a published research project. We are going to focus the practical coding examples on the data collected for the analysis reported by @ricketts2021.\n\nWe will focus on working with the data from one of the tasks, in one of the studies reported by @ricketts2021.\n\nThis means that you can consolidate your learning by applying the same code moves to data from the other task in the same study, or to data from the other study.\nIn applying code to other data, you will need to be aware of differences in, say, the way that some things like the outcome response variable are coded.\n\nYou can then further extend your development by trying out the coding moves for yourself using the data collected by @rodríguez-ferreiro2020.\n\nThese data are from a quite distinct kind of investigation, on a different research topic than the topic we will be exploring through our working examples.\nHowever, some aspects of the data structure are similar.\nCritically, the data are provided with comprehensive documentation.\n\n\n\n\nTo do our practical work, we will need functions and data. We get these at the start of our workflow.\n\n\nWe are going to need the lme4, patchwork, psych and tidyverse libraries of functions and data.\n\nlibrary(ggeffects)\nlibrary(patchwork)\nlibrary(psych)\nlibrary(tidyverse)\n\n\n\n\nYou can access the data we are going to use in two different ways.\n\n\nThe data associated with both [@ricketts2021] and [@rodríguez-ferreiro2020] are freely available through project repositories on the Open Science Framework web pages.\nYou can get the data from the @ricketts2021 paper through the repository located here.\nYou can get the data from the @rodríguez-ferreiro2020 paper through the repository located here.\nThese data are associated with full explanations of data collection methods, materials, data processing and data analysis code. You can review the papers and the repository material guides for further information.\nIn the following, I am going to abstract summary information about the @ricketts2021 study and data. I shall leave you to do the same for the @rodríguez-ferreiro2020 study.\n\n\n\nDownload the data-visualization.zip files folder and upload the files to RStudio Server.\nThe folder includes the @ricketts2021 data files:\n\nconcurrent.orth_2020-08-11.csv\nconcurrent.sem_2020-08-11.csv\nlong.orth_2020-08-11.csv\nlong.sem_2020-08-11.csv\n\nThe folder also includes the @rodríguez-ferreiro2020 data files:\n\nPrimDir-111019_English.csv\nPrimInd-111019_English.csv\n\n\n\n\n\n\n\nWarning\n\n\n\n\nThese data files are collected together in a folder for download, for your convenience, but the version of record for the data for each study comprise the files located on the OSF repositories associated with the original articles.\n\n\n\n\n\n\n\n\n@ricketts2021 conducted an investigation of word learning in school-aged children. They taught children 16 novel words in a study with a 2 x 2 factorial design. In this investigation, they tested whether word learning is helped by presenting targets for word learning with their spellings, and whether learning is helped by telling children that they would benefit from the presence of those spellings.\nThe presence of orthography (the word spelling) was manipulated within participants (orthography absent vs. orthography present): for all children, eight of the words were taught with orthography present and eight with orthography absent. Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\nA pre-test was conducted to establish participants’ knowledge of the stimuli. Then, each child was seen for three 45-minute sessions to complete training (Sessions 1 and 2) and post-tests (Session 3). @ricketts2021 completed two studies: Study 1 and Study 2. All children, in both studies 1 and 2 completed the Session 3 post-tests.\nIn Study 1, longitudinal post-test data were collected because children were tested at two time points. Children were administered post-tests in Session 3, as noted: Time 1. Post-tests were then re-administered approximately eight months later at Time 2 (\\(M = 241.58\\) days from Session 3, \\(SD = 6.10\\)). In Study 2, the Study 1 sample was combined with an older sample of children. The additional Study 2 children were not tested at Time 2, and the analysis of Study 2 data did not incorporate test time as a factor.\nThe outcome data for both studies consisted of performance on post-tests.\nThe semantic post-test assessed knowledge for the meanings of newly trained words using a dynamic or sequential testing approach. I will not explain this approach in more detail, here, because the practical visualization exercises focus on the orthographic knowledge (spelling knowledge) post-test, explained next.\nThe orthographic post-test was included to ascertain the extent of orthographic knowledge after training. Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure indexing the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. The maximum score is 0, with higher scores indicating less accurate responses.\nFor the Study 1 analysis, the files are:\n\nlong.orth_2020-08-11.csv\nlong.sem_2020-08-11.csv\n\nWhere long indicates the longitudinal nature of the data-set.\nFor the Study 2 analysis, the files are:\n\nconcurrent.orth_2020-08-11.csv\nconcurrent.sem_2020-08-11.csv\n\nWhere concurrent indicates the inclusion of concurrent (younger and older) child participant samples.\nEach column in each data-set corresponds to a variable and each row corresponds to an observation (i.e., the data are tidy). Because the design of the study involves the collection of repeated observations, the data can be understood to be in a long format.\nEach child was asked to respond to 16 words and, for each of the 16 words, we collected post-test responses from multiple children. All words were presented to all children.\nWe explain what you will find when you inspect the .csv files, next.\n\n\nThe variables included in .csv files are listed, following, with information about value coding or calculation.\n\nParticipant — Participant identity codes were used to anonymize participation. Children included in studies 1 and 2 – participants in the longitudinal data collection – were coded “EOF[number]”. Children included in Study 2 only (i.e., the older, additional, sample) were coded “ND[number]”.\nTime — Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\nStudy — Observations taken for children included in studies 1 and 2 – participants in the longitudinal data collection – were coded “Study1&2”. Children included in Study 2 only (i.e., the older, additional, sample) were coded “Study2”.\nInstructions — Variable coding for whether participants undertook training in the explicit or incidental conditions.\nVersion — Experiment administration coding\nWord — Letter string values show the words presented as stimuli to children.\nConsistency_H — Calculated orthography-to-phonology consistency value for each word.\nOrthography — Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.\nMeasure — Variable coding for the post-test measure: Sem_all if the semantic post-test; Orth_sp if the orthographic post-test.\nScore — Variable coding for response category.\n\nFor the semantic (sequential or dynamic) post-test, responses were scored as corresponding to:\n\n3 – correct response in the definition task\n2 – correct response in the cued definition task\n1 – correct response in the recognition task\n0 – if the item wasn’t correctly defined or recognised\n\nFor the orthographic post-test, responses were scored as:\n\n1 – correct, if the target spelling was produced in full\n0 – incorrect\n\nHowever, the analysis reported by @ricketts2021 focused on the more sensitive Levenshtein distance measure (see following).\n\nWASImRS — Raw score – Matrix Reasoning subtest of the Wechsler Abbreviated Scale of Intelligence\nTOWREsweRS — Raw score – Sight Word Efficiency (SWE) subtest of the Test of Word Reading Efficiency; number of words read correctly in 45 seconds\nTOWREpdeRS — Raw score – Phonemic Decoding Efficiency (PDE) subtest of the Test of Word Reading Efficiency; number of nonwords read correctly in 45 seconds\nCC2regRS — Raw score – Castles and Coltheart Test 2; number of regular words read correctly\nCC2irregRS — Raw score – Castles and Coltheart Test 2; number of irregular words read correctly\nCC2nwRS — Raw score – Castles and Coltheart Test 2; number of nonwords read correctly\nWASIvRS — Raw score – vocabulary knowledge indexed by the Vocabulary subtest of the WASI-II\nBPVSRS — Raw score – vocabulary knowledge indexed by the British Picture Vocabulary Scale – Third Edition\nSpelling.transcription — Transcription of the spelling response produced by children in the orthographic post-test\nLevenshtein.Score — Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure indexing the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. For example, the response ‘epegram’ for target ‘epigram’ attracts a Levenshtein score of 1 (one substitution). Thus, this score gives credit for partially correct responses, as well as entirely correct responses. The maximum score is 0, with higher scores indicating less accurate responses.\n\n(Notice that, for the sake of brevity, I do not list the z_ variables but these are explained in the study OSF repository materials.)\n\n\n\n\n\n\nWarning\n\n\n\nLevenshtein distance scores are higher if a child makes more errors in producing the letters in a spelling response.\n\nThis means that if we want to see what factors help a child to learn a word, including its spelling, then we want to see that helpful factors are associated with lower Levenshtein scores.\n\n\n\nTo demonstrate some of the processes we can enact to process and visualize data, and some of the benefits of doing so, we are going to work with the concurrent.orth_2020-08-11.csv dataset. These are data corresponding to the @ricketts2021 Study 2. concurrent refers to the analysis (a concurrent comparison) of data from younger and older children.\n\n\n\n\nAssuming you have downloaded the data files, we first read the dataset into the R environment: concurrent.orth_2020-08-11.csv. We do the data read in a bit differently than you have seen it done before; we will come back to what is going on (in Section 1.7.4.1).\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\",\n\n                      col_types = cols(\n\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n\n                      ))\n\nWe can inspect these data using summary().\n\nsummary(conc.orth)\n\n  Participant   Time          Study         Instructions Version\n EOF001 :  16   1:1167   Study1&2:655   explicit  :592   a:543  \n EOF002 :  16            Study2  :512   incidental:575   b:624  \n EOF004 :  16                                                   \n EOF006 :  16                                                   \n EOF007 :  16                                                   \n EOF008 :  16                                                   \n (Other):1071                                                   \n         Word     Consistency_H     Orthography     Measure    \n Accolade  : 73   Min.   :0.9048   absent :583   Orth_sp:1167  \n Cataclysm : 73   1st Qu.:1.5043   present:584                 \n Contrition: 73   Median :1.9142                               \n Debacle   : 73   Mean   :2.3253                               \n Dormancy  : 73   3rd Qu.:3.0436                               \n Epigram   : 73   Max.   :3.9681                               \n (Other)   :729                                                \n     Score           WASImRS     TOWREsweRS      TOWREpdeRS       CC2regRS    \n Min.   :0.0000   Min.   : 5   Min.   :51.00   Min.   :19.00   Min.   :28.00  \n 1st Qu.:0.0000   1st Qu.:13   1st Qu.:69.00   1st Qu.:35.00   1st Qu.:36.00  \n Median :0.0000   Median :17   Median :74.00   Median :41.00   Median :38.00  \n Mean   :0.2913   Mean   :16   Mean   :74.23   Mean   :41.59   Mean   :36.91  \n 3rd Qu.:1.0000   3rd Qu.:19   3rd Qu.:80.00   3rd Qu.:50.00   3rd Qu.:39.00  \n Max.   :1.0000   Max.   :25   Max.   :93.00   Max.   :59.00   Max.   :40.00  \n                                                                              \n   CC2irregRS       CC2nwRS         WASIvRS          BPVSRS     \n Min.   :17.00   Min.   :13.00   Min.   :16.00   Min.   :103.0  \n 1st Qu.:23.00   1st Qu.:29.00   1st Qu.:25.00   1st Qu.:119.0  \n Median :25.00   Median :33.00   Median :29.00   Median :133.0  \n Mean   :25.24   Mean   :32.01   Mean   :29.12   Mean   :130.9  \n 3rd Qu.:27.00   3rd Qu.:37.00   3rd Qu.:33.00   3rd Qu.:142.0  \n Max.   :35.00   Max.   :40.00   Max.   :39.00   Max.   :158.0  \n                                                                \n Spelling.transcription Levenshtein.Score  zTOWREsweRS        zTOWREpdeRS      \n Epigram   : 57         Min.   :0.000     Min.   :-2.67807   Min.   :-2.33900  \n Platitude : 43         1st Qu.:0.000     1st Qu.:-0.60283   1st Qu.:-0.68243  \n Contrition: 42         Median :1.000     Median :-0.02638   Median :-0.06122  \n fracar    : 39         Mean   :1.374     Mean   : 0.00000   Mean   : 0.00000  \n Nonentity : 39         3rd Qu.:2.000     3rd Qu.: 0.66537   3rd Qu.: 0.87061  \n raconter  : 35         Max.   :7.000     Max.   : 2.16415   Max.   : 1.80243  \n (Other)   :912                                                                \n   zCC2regRS        zCC2irregRS          zCC2nwRS          zWASIvRS       \n Min.   :-3.3636   Min.   :-2.22727   Min.   :-3.1053   Min.   :-2.63031  \n 1st Qu.:-0.3435   1st Qu.:-0.60461   1st Qu.:-0.4920   1st Qu.:-0.82633  \n Median : 0.4115   Median :-0.06373   Median : 0.1614   Median :-0.02456  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.: 0.7890   3rd Qu.: 0.47716   3rd Qu.: 0.8147   3rd Qu.: 0.77721  \n Max.   : 1.1665   Max.   : 2.64070   Max.   : 1.3047   Max.   : 1.97986  \n                                                                          \n    zBPVSRS         mean_z_vocab       mean_z_read       zConsistency_H   \n Min.   :-1.9946   Min.   :-2.06910   Min.   :-2.39045   Min.   :-1.4153  \n 1st Qu.:-0.8495   1st Qu.:-0.85941   1st Qu.:-0.43321   1st Qu.:-0.8181  \n Median : 0.1525   Median :-0.01483   Median : 0.08829   Median :-0.4096  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.7967   3rd Qu.: 0.72964   3rd Qu.: 0.68438   3rd Qu.: 0.7157  \n Max.   : 1.9418   Max.   : 1.96083   Max.   : 1.52690   Max.   : 1.6368  \n                                                                          \n\n\nYou should notice one key bit of information in the summary. Focus on the summary for what is in the Participant column. You can see that we have a number of participants in this dataset, listed by Participant identity code in the summary() view e.g. EOF001. For each participant, we have 16 rows of data.\nWhen we ask R for a summary of a nominal variable or factor it will show us the levels of each factor (i.e., each category or class of objects encoded by the categorical variable), and a count for the number of observations for each level.\nTake a look at the rows of data for EOF001.\n\n\n\n\n\nParticipant\nTime\nStudy\nInstructions\nVersion\nWord\nConsistency_H\nOrthography\nMeasure\nScore\nWASImRS\nTOWREsweRS\nTOWREpdeRS\nCC2regRS\nCC2irregRS\nCC2nwRS\nWASIvRS\nBPVSRS\nSpelling.transcription\nLevenshtein.Score\nzTOWREsweRS\nzTOWREpdeRS\nzCC2regRS\nzCC2irregRS\nzCC2nwRS\nzWASIvRS\nzBPVSRS\nmean_z_vocab\nmean_z_read\nzConsistency_H\n\n\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nAccolade\n1.9142393\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nacalade\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.4095955\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nCataclysm\n3.5060075\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nCataclysm\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n1.1763372\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nContrition\n1.7486898\nabsent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nContrition\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.5745381\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nDebacle\n2.9008386\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\ndibarcle\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.5733869\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nDormancy\n1.6263089\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\ndoormensy\n3\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.6964704\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nEpigram\n1.3822337\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nEpigram\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.9396508\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nFoible\n2.7051987\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nFoible\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.3784641\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nFracas\n3.1443345\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nfracar\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.8159901\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nLassitude\n0.9048202\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nlacitude\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-1.4153141\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nLuminary\n1.0985931\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nloomenery\n4\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-1.2222516\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nNonentity\n3.9681391\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nnonenterty\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n1.6367746\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nPlatitude\n0.9048202\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nPlatitude\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-1.4153141\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nPropensity\n1.6861898\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\npropencity\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.6368090\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nRaconteur\n3.8245334\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nraconter\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n1.4936954\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nSyncopation\n3.0436450\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nsincipation\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.7156697\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nVeracity\n2.8693837\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nvaracity\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.5420473\n\n\n\n\n\n\n\nYou can see that for EOF001, as for every participant, we have information on the conditions under which we observed their responses (Instructions, Orthography), as well as information about the stimuli that we asked participants to respond to (e.g., Word, Consistency_H), information about the responses or outcomes we recorded (Measure, Score, Spelling.transcription,  Levenshtein.Score), and information about the participants themselves (e.g., TOWREsweRS, TOWREpdeRS).\n\n\n\nWe almost always need to process data in order to render the information ready for discovery or communication data visualization.\n\n\nYou will have seen that data processing began when we first read the data in for use. Let’s go back and take a look at the code steps.\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\",\n\n                      col_types = cols(\n\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n\n                        )\n                      )\n\nThe chunk of code is doing two things: first, we tell R what .csv file we want to read into the environment, and what we want to call the dataset; and then we tell R how we want to classify the data variable columns.\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\" first reads the named .csv file, creating an object I will call conc.orth: a dataset or tibble we can now work with in R.\n\n\nYou have been using the read.csv() function to read in data files.\nThe read_csv() function is the more modern tidyverse form of the function you were introduced to.\nBoth versions work in similar ways but read_csv() is a bit more efficient, and it allows us to do what we do next.\n\n\ncol_types = cols( ... ) tells R how to interpret some of the columns in the .csv.\n\n\nThe read_csv() function is excellent at working out what types of data are held in each column but sometimes we have to tell it what to do.\nHere, I am specifying with e.g. Participant = col_factor() that the Participant column should be treated as a categorical or nominal variable, a factor.\n\nUsing the col_types = cols( ... ) argument saves me from having to first read the data in then using code like the following to require, technically, coerce R into recognizing the nominal nature of variables like Participant with code like\n\nconc.orth$Participant &lt;- as.factor(conc.orth$Participant)\n\n\n\nI do not have to do step 2 of the read-in process, here. What happens if we use just read_csv()? Try it.\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\")\n\n\n\n\nYou can read more about read_csv() here\nYou can read more about col_types = cols() here\n\n\n\n\nThe @ricketts2021 dataset orth.conc is a moderately sized and rich dataset with several observations, on multiple variables, for each of many participants. Sometimes, we want to extract information from a more complex dataset because we want to understand or present a part of it, or a relatively simple account of it. We look at an example of how you might do that now.\nAs you saw when you looked at the summary of the orth.conc dataset, we have multiple rows of data for each participant. Recall the design of the study. For each participant, we recorded their response to a stimulus word, in a test of word learning, for 16 words.\nFor each participant, we have a separate row for each response the participant made to each word. But you will have noticed that information about the participant is repeated. So, for participant EOF001, we have data about their performance e.g. on the BPVSRS vocabulary test (they scored 126). Notice that that score is repeated: the same value is copied for each row, for this participant, in the BPVSRS column. The reason the data are structured like this are not relevant here 1 but it does require us to do some data processing, as I explain next.\nIt is a very common task to want to present a summary of the attributes of your participants or stimuli when you are reporting data in a report of a psychological research project. We could get a summary of the participant attributes using the psych library describe function as follows.\n\nconc.orth %&gt;%\n  select(WASImRS:BPVSRS) %&gt;%\n  describe(ranges = FALSE, skew = FALSE)\n\n           vars    n   mean    sd   se\nWASImRS       1 1167  16.00  4.30 0.13\nTOWREsweRS    2 1167  74.23  8.67 0.25\nTOWREpdeRS    3 1167  41.59  9.66 0.28\nCC2regRS      4 1167  36.91  2.65 0.08\nCC2irregRS    5 1167  25.24  3.70 0.11\nCC2nwRS       6 1167  32.01  6.12 0.18\nWASIvRS       7 1167  29.12  4.99 0.15\nBPVSRS        8 1167 130.87 13.97 0.41\n\n\nBut you can see that part of the information in the summary does not appear to make sense at first glance. We do not have 1167 participants in this dataset, as @ricketts2021 report.\nHow do we extract the participant attribute variable data for each unique participant code for the participants in our dataset?\n\nconc.orth.subjs &lt;- conc.orth %&gt;%\n  group_by(Participant) %&gt;%\n  mutate(mean.score = mean(Levenshtein.Score)) %&gt;%\n  ungroup() %&gt;%\n  distinct(Participant, .keep_all = TRUE) %&gt;%\n  select(WASImRS:BPVSRS, mean.score, Participant)\n\nWe create a new dataset conc.orth.subjs by taking conc.orth and piping it through a series of processing steps. As part of the process, we want to extract the data for each unique unique Participant identity code using distinct(). Along the way, we want to calculate the mean accuracy of response on the outcome measure (Score), that is, the average number of edits separating a child’s spelling of a target word from the correct spelling.\nThis is how we do it.\n\nconc.orth.subjs &lt;- ... tells R to create a new dataset conc.orth.subjs.\nconc.orth %&gt;% ... we do this by telling R to take conc.orth and pipe it through the following steps.\ngroup_by(Participant) %&gt;% first we group the data by Participant identity code.\nmutate(mean.score = mean(Score)) %&gt;% then we use mutate() to create the new variable mean.score by calculating the mean() of the Score variable values (i.e. the average score) for each participant. We then pipe to the next step.\nungroup() %&gt;% we tell R to ungroup the data because we want to work with all rows for what comes next, and we then pipe to the next step.\ndistinct(Participant, .keep_all = TRUE) %&gt;% requires R to extract from the full orth.conc dataset the set of (here, 16) data rows we have for each distinct (uniquely identified) Participant. We use the argument .keep_all = TRUE to tell R that we want to keep all columns. This requires the next step, so we tell R to pipe %&gt;% the data.\nselect(WASImRS:BPVSRS, mean.score, Participant) then tells R to select just the columns with information about participant attributes. (WASImRS:BPVSRS tells R to select every column between WASImRS and BPVSRS inclusive. mean.score, Participant tells R we also want those columns, specified by name, including the mean.score column of average response scores we calculated just earlier.\n\nWe can now get a sensible summary of the descriptive statistics for the participants in Study 2 of the @ricketts2021 investigation.\n\nconc.orth.subjs %&gt;%\n  select(-Participant) %&gt;%\n  describe(ranges = FALSE, skew = FALSE)\n\n           vars  n   mean    sd   se\nWASImRS       1 73  16.00  4.33 0.51\nTOWREsweRS    2 73  74.22  8.73 1.02\nTOWREpdeRS    3 73  41.58  9.73 1.14\nCC2regRS      4 73  36.90  2.67 0.31\nCC2irregRS    5 73  25.23  3.72 0.44\nCC2nwRS       6 73  32.00  6.17 0.72\nWASIvRS       7 73  29.12  5.02 0.59\nBPVSRS        8 73 130.88 14.06 1.65\nmean.score    9 73   1.38  0.62 0.07\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is exactly the kind of tabled summary of descriptive statistics we would expect to produce in a report, in a presentation of the participant characteristics for a study sample (in e.g., the Methods section).\nNotice:\n\nThe table has not yet been formatted according to APA rules.\nWe would prefer to use real words for row name labels instead of dataset variable column labels, e.g, replace TOWREsweRS with: “TOWRE word reading score”.\n\n\n\n\n\nIn these bits of demonstration code, we extract information relating just to participants. However, in this study, we recorded the responses participants made to 16 stimulus words, and we include in the dataset information about the word properties Consistency_H.\n\nCan you adapt the code you see here in order to calculate a mean score for each word, and then extract the word-level information for each distinct stimulus word identity?\n\n\n\n\nYou can read more about the psych library, which is often useful, here. You can read more about the distinct() function here.\n\n\n\n\n\nIt has taken us a while but now we are ready to examine the data using visualizations. Remember, we are engaging in visualization to (1.) do discovery, to get a sense of our data, and maybe reveal unexpected aspects, and (2.) potentially to communicate to ourselves and others what we have observed or perhaps what insights we can gain.\nWe have been learning to use histograms, in other classes, so let’s start there.\n\n\n\nWe can use histograms to visualize the distribution of observed values for a numeric variable. Let’s start simple, and then explore how to elaborate the plotting code, in a series of edits, to polish the plot presentation.\n\nggplot(data = conc.orth.subjs, aes(x = WASImRS)) +\n  geom_histogram()\n\n\n\n\nFigure 5: Distribution of WASImRS intelligence scores\n\n\n\n\nThis is how the code works.\n\nggplot(data = conc.orth.subjs, ... tells R what function to use ggplot() and what data to work with data = conc.orth.subjs.\naes(x = WASImRS) tells R what aesthetic mapping to use: we want to map values on the WASImRS variable (small to large) to locations on the x-axis (left to right).\ngeom_histogram() tells R to construct a histogram, presenting a statistical summary of the distribution of intelligence scores.\n\nWith histograms, we are visualizing the distribution of a single continuous variable by dividing the variable values into bins (i.e. subsets) and counting the number of observations in each bin. Histograms display the counts with bars.\nYou can see more information about geom_histogram here.\nFigure 5 shows how intelligence (WASImRS) scores vary in the Ricketts Study 2 dataset. Scores peak around 17, with a long tail of lower scores towards 5, and a maximum around 25.\n\nWhere I use the word “peak” I am talking about the tallest bar in the plot (or, later the highest point in a density curve). At this point, we have the most observations of the value under the bar. Here, we observed the score WASImRS \\(= 17\\) for the most children in this sample.\n\nA primary function of discovery visualization is to assess whether the distribution of scores on a variable is consistent with expectations, granted assumptions about a sample (e.g., that the children are typically developing). We would normally use research area knowledge to assess whether this distribution fits expectations for a sample of typically developing school-aged children in the UK. However, I shall leave that concern aside, here, so that we can focus on enriching the plot presentation, next.\nThere are two main problems with the plot:\n\nThe bars are “gappy” in the histogram, suggesting we have not grouped observed values in sufficiently wide subsets (bins). This is a problem because it weakens our ability to gain or communicate a visual sense of the distribution of scores.\nThe axis labeling uses the dataset variable name WASImRS but if we were to present the plot to others we could not expect them to know what that means.\n\nWe can fix both these problems, and polish the plot for presentation, through the following code steps.\n\nggplot(data = conc.orth.subjs, aes(x = WASImRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"Scores on the Wechsler Abbreviated Scale of Intelligence\") +\n  theme_bw()\n\n\n\n\nFigure 6: Distribution of WASImRS intelligence scores\n\n\n\n\nFigure 6 shows the same data, and furnishes us with the same picture of the distribution of intelligence scores but it is a bit easier to read. We achieve this by making three edits.\n\ngeom_histogram(binwidth = 2) + we change the binwidth.\n\n\nThis is so that more different observed values of the data variable are included in bins (subsets corresponding to bars) so that the bars correspond to information about a wider range of values.\nThis makes the bars bigger, wider, and closes the gaps.\nAnd this means we can focus the eyes of the audience for our plot on the visual impression we wish to communicate: the skewed distribution of intelligence scores.\n\n\nlabs(x = \"Scores on the Wechsler Abbreviated Scale of Intelligence\") + changes the label to something that should be understandable by people, in our audience, who do not have access to variable information (as we do) about the dataset.\ntheme_bw() we change the overall appearance of the plot by changing the theme.\n\n\n\nWe could, if we wanted, add a line and annotation to indicate the mean value, as you saw in Figure 2.\n\nCan you add the necessary code to indicate the mean value of WASI scores, for this plot?\n\nWe can, of course, plot histograms to indicate the distributions of other variables.\n\nCan you apply the histogram code to plot histograms of other variables?\n\n\n\n\n\nWe may wish to discover or communicate how values vary on dataset variables in two different ways. Sometimes, we need to examine how values vary on different variables. And sometimes, we need to examine how values vary on the same variable but in different groups of participants (or stimuli) or under different conditions. We look at this next. We begin by looking at how you might compare how values vary on different variables.\n\n\nIt can be useful to compare the distributions of different variables. Why?\nConsider the @ricketts2021 investigation dataset. Like many developmental investigations (see also clinical investigations), we tested children and recorded their scores on a series of standardized measures, here, measures of ability on a range of dimensions. We did this, in part, to establish that the children in our sample are operating at about the level one might expect for typically developing children in cognitive ability dimensions of interest: dimensions like intelligence, reading ability or spelling ability. So, one of the aspects of the data we are considering is whether scores on these dimensions are higher or lower than typical threshold levels. But we also want to examine the distributions of scores because we want to find out:\n\nif participants are varied in ability (wide distribution) or if maybe they are all similar (narrow distribution) as would be the case if the ability measures are too easy (so all scores are at ceiling) or too hard (so all scores are at floor);\nif there are subgroups within the sample, maybe reflected by two or more peaks;\nif there are unusual scores, maybe reflected by small peaks at very low or very high scores.\n\nWe could look at each variable, one plot at a time. Instead, next, I will show you how to produce a set of histogram plots, and present them all as a single grid of plots.\n\n\n\n\n\n\nWarning\n\n\n\nI have to warn you that the way I write the code is not good practice. The code is written with repeats of the ggplot() block of code to produce each plot. This repetition is inefficient and leaves the coding vulnerable to errors because it is hard to spot a mistake in more code. What I should do is encapsulate the code as a function (see here). The reason I do not, here, is because I want to focus our attention on just the plotting.\n\n\nFigure 7 presents a grid of plots showing how scores vary for each ability test measure, for the children in the @ricketts2021 investigation dataset. We need to go through the code steps, next, and discuss what the plots show us (discovery and communication).\n\np.WASImRS &lt;- ggplot(data = conc.orth.subjs, aes(x = WASImRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"WASI matrix\") +\n  theme_bw()\n\np.TOWREsweRS &lt;- ggplot(data = conc.orth.subjs, aes(x = TOWREsweRS)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"TOWRE words\") +\n  theme_bw()\n\np.TOWREpdeRS &lt;- ggplot(data = conc.orth.subjs, aes(x = TOWREpdeRS)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"TOWRE phonemic\") +\n  theme_bw()\n\np.CC2regRS &lt;- ggplot(data = conc.orth.subjs, aes(x = CC2regRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"CC regular words\") +\n  theme_bw()\n\np.CC2irregRS &lt;- ggplot(data = conc.orth.subjs, aes(x = CC2irregRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"CC irregular words\") +\n  theme_bw()\n\np.CC2nwRS &lt;- ggplot(data = conc.orth.subjs, aes(x = CC2nwRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"CC nonwords\") +\n  theme_bw()\n\np.WASIvRS &lt;- ggplot(data = conc.orth.subjs, aes(x = WASIvRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"WASI vocabulary\") +\n  theme_bw()\n\np.BPVSRS &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS)) +\n  geom_histogram(binwidth = 3) +\n  labs(x = \"BPVS vocabulary\") +\n  theme_bw()\n\np.mean.score &lt;- ggplot(data = conc.orth.subjs, aes(x = mean.score)) +\n  geom_histogram(binwidth = .25) +\n  labs(x = \"Mean orthographic test score\") +\n  theme_bw()\n\np.mean.score + p.BPVSRS + p.WASIvRS + p.WASImRS +\n  p.CC2nwRS + p.CC2irregRS + p.CC2regRS + \n  p.TOWREpdeRS + p.TOWREsweRS + plot_layout(ncol = 3)\n\n\n\n\nFigure 7: Distribution of childrens’ scores on ability measures\n\n\n\n\nThis is how the code works, step by step:\n\np.WASImRS &lt;- ggplot(...) first creates a plot object, which we call p.WASImRS.\nggplot(data = conc.orth.subjs, aes(x = WASImRS)) + tells R what data to use, and what aesthetic mapping to work with mapping the variable WASImRS here to the x-axis location.\ngeom_histogram(binwidth = 2) + tells R to sort the values of WASImRS scores into bins and create a histogram to show how many children in the sample present scores of different sizes.\nlabs(x = \"WASI matrix\") + changes the x-axis label to make it more informative.\ntheme_bw() changes the theme to make it a bit cleaner looking.\n\nWe do this bit of code separately for each variable. We change the plot object name, the x = variable specification, and the axis label text for each variable. We adjust the binwidth where it appears to be necessary.\nWe then use the following plot code to put all the plots together in a single grid.\n\np.mean.score + p.BPVSRS + p.WASIvRS + p.WASImRS +\n  p.CC2nwRS + p.CC2irregRS + p.CC2regRS + \n  p.TOWREpdeRS + p.TOWREsweRS + plot_layout(ncol = 3)\n\n\nIn the code, we add a series of plots together e.g. p.mean.score + p.BPVSRS + p.WASIvRS ...\nand then specify we want a grid of plots with a layout of three columns plot_layout(ncol = 3).\n\nThis syntax requires the library(patchwork) and more information about this very useful library can be found here.\nWhat do the plots show us?\nFigure 7 shows a grid of 9 histogram plots. Each plot presents the distribution of scores for the @ricketts2021 Study 2 participant sample on a separate ability measure, including scores on the BPVS vocabulary, WASI vocabulary, TOWRE words and TOWRE nonwords reading tests, as well as scores on the Castles and Coltheart regular words, irregular words and nonwords reading tests, and the mean Levenshtein distance (spelling score) outcome measure of performance for the experimental word learning post-test.\nTake a look, you may notice the following features.\n\nThe mean orthographic test score suggests that many children produced spellings to the words they learned in the @ricketts2021 study that, on average, were correct (0 edits) or were one or two edits (e.g., a letter deletion or replacement) away from the target word spelling. The children were learning the words, and most of the time, they learned the spellings of the words effectively. However, one or two children tended to produce spellings that were 2-3 edits distant from the target spelling.\n\n\nWe can see these features because we can see that the histogram peaks around 1 (at Levenshtein distance score \\(= 1\\)) but that there is a small bar of scores at around 3.\n\n\nWe can see that there are two peaks on the BPVS and WASI measures of vocabulary. What is going on there?\n\n\nIs it the case that we have two sub-groups of children within the overall sample? For example, on the BPVS test, maybe one sub-group of children has a distribution of vocabulary scores with a peak around 120 (the peak shows where most children have scores) while another sub-group of children has a distribution of vocabulary scores with a peak around 140.\n\n\nIf we look at the CC nonwords and CC regular words tests of reading ability, we may notice that while most children present relatively high scores on these tests (CC nonwords peak around 35, CC regular words peak around 37) there is a skewed distribution. Many of the children’s scores are piled up towards the maximum value in the data on the measures. But we can also see that, on both measures, there are long tails in the distributions because relatively small numbers of children have substantially lower scores.\n\n\nDevelopmental samples are often highly varied (just like clinical samples). Are all the children in the sample at the same developmental stage, or are they all typically developing?\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that in presenting a grid of plots like this, we offer a compact visual way to present the same summary information we might otherwise present using a table of descriptive statistics. In some ways, this grid of plots is more informative than the descriptive statistics because the mean and SD values do not tell you what you can see:\n\nthe characteristics of the variation in values, like the presence of two peaks;\nor the presence of unusually high or low scores (for this sample).\n\n\n\nGrids of plots like this can be helpful to inspect the distributions of variables in a concise approach. They are not really too useful for comparing the distributions because they require your eyes to move between plots, repeatedly, to do the comparison.\nHere is a more compact way to code the grid of histograms using the library(ggridges) function geom_density_ridges(). I do not discuss it in detail because I want to focus your attention on core tidyverse functions (I show you more information in the Notes tab).\nNotice that if you produce all the plots so that the are in line in the same column with a shared x-axis it becomes much easier to compare the distributions of scores. You lose some of the fine detail, discussed in relation to Figure 7, but this style allows you to gain an impression, quickly, of how for distributions of scores compare between measures. For example, we can see that within the Castles and Coltheart (CC) measures of reading ability, children do better on regular words than on nonwords, and on nonwords better than on irregular words.\n\nPlotNotes\n\n\n\nlibrary(ggridges)\nconc.orth.subjs %&gt;%\n  pivot_longer(names_to = \"task\", values_to = \"score\", cols = WASImRS:mean.score) %&gt;% \n  ggplot(aes(y = task, x = score)) +\n  geom_density_ridges(stat = \"binline\", bins = 20, scale = 0.95, draw_baseline = FALSE) +\n  theme_ridges()\n\n\n\n\nFigure 8: Distribution of childrens’ scores on ability measures\n\n\n\n\n\n\n\nlibrary(ggridges) get the library we need.\nconc.orth.subjs %&gt;% pipe the dataset for processing.\npivot_longer(names_to = \"task\", values_to = \"score\", cols = WASImRS:mean.score) %&gt;% pivot the data so all test scores are in the same column, “scores” wwith coding for “task” name, and pipe to the next step for plotting.\nggplot(aes(y = task, x = score)) + create a plot for the scores on each task.\ngeom_density_ridges(stat = \"binline\", bins = 20, scale = 0.95, draw_baseline = FALSE) + show the plots as histograms.\ntheme_ridges() change the theme to the specific theme suitable for showing a grid of ridges.\n\nYou can find more information on ggridges here.\n\n\n\n\n\n\nWe will often want to compare the distributions of variable values between groups or between conditions. This need may appear when, for example, we are conducting a between-groups manipulation of some condition and we want to check that the groups are approximately matched on dimensions that are potentially linked to outcomes (i.e., on potential confounds). The need may appear when, alternatively, we have recruited or selected participant (or stimulus) samples and we want to check that the sample sub-groups are approximately matched or detectably different on one or more dimensions of interest or of concern.\nAs a demonstration of the visualization work we can do in such contexts, let’s pick up on an observation we made earlier, that there are two peaks on the BPVS and WASI measures of vocabulary. I asked: Is it the case that we have two sub-groups of children within the overall sample? Actually, we know the answer to that question because @ricketts2021 state that they recruited one set of children for their Study 1 and then, for Study 2:\n\nThirty-three children from an additional three socially mixed schools in the South-East of England were added to the Study 1 sample (total N = 74). These additional children were older (\\(M_{age}\\) = 12.57, SD = 0.29, 17 female)\n\nDo the younger (Study 1) children differ in any way from the older (additional) children?\nWe can check this through data visualization. Our aim is to present the distributions of variables side-by-side or superimposed to ensure easy comparison. We can do this in different ways, so I will demonstrate one approach with an outline explanation of the actions, and offer suggestions for further approaches.\nI am going to process the data before I do the plotting. I will re-use the code I used before (see Section 1.7.4.2) with one additional change. I will add a line to create a group coding variable. This addition shows you how to do an action that is very often useful in the data processing part of your workflow.\n\n\nYou have seen that the @ricketts2021 report states that an additional group of children was recruited for the investigation’s second study. How do we know who they are? If you recall the summary view of the complete dataset, there is one variable we can use to code group identity.\n\nsummary(conc.orth$Study)\n\nStudy1&2   Study2 \n     655      512 \n\n\nThis summary tells us that we have 512 observations concerning the additional group of children recruited for Study 2, and 655 observations for the (younger) children whose data were analyzed for both Study 1 and Study 2 (i.e., coded as Study1&2 in the Study variable column). We can use this information to create a coding variable. (If we had age data, we could use that instead but we do not.) This is how we do that.\n\nconc.orth.subjs &lt;- conc.orth %&gt;%\n  group_by(Participant) %&gt;%\n  mutate(mean.score = mean(Levenshtein.Score)) %&gt;%\n  ungroup() %&gt;%\n  distinct(Participant, .keep_all = TRUE) %&gt;%\n  mutate(age.group = fct_recode(Study,\n    \n    \"young\" = \"Study1&2\",\n    \"old\" = \"Study2\"\n    \n  )) %&gt;%\n  select(WASImRS:BPVSRS, mean.score, Participant, age.group)\n\nThe code block is mostly the same as the code I used in Section Section 1.7.4.2 to extract the data for each participant, with two changes:\n\nFirst, mutate(age.group = fct_recode(...) tells R that I want to create a new variable age.group through the process of recoding, with fct_recode(...) the variable I specify next, in the way that I specify.\nfct_recode(Study, ...) tells R I want to recode the variable Study.\n\"young\" = \"Study1&2\", \"old\" = \"Study2\" specifies what I want recoded.\n\n\nI am telling R to look in the Study column and (a.) whenever it finds the value Study1&2 replace it with young whereas (b.) whenever it finds the value Study2 replace it with old.\nNotice that the syntax in recoding is fct_recode: “new name” = “old name”.\nHaving done that, I tell R to pipe the data, including the recoded variable, to the next step.\n\n\nselect(WASImRS:BPVSRS, mean.score, Participant, age.group) where I add the new recoded variable to the selection of variables I want to include in the new dataset conc.orth.subjs.\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that R handles categorical or nominal variables like Study (or, in other data, variables e.g. gender, education or ethnicity) as factors.\n\nWithin a classification scheme like education, we may have different classes or categories or groups e.g. “further, higher, school”. We can code these different classes with numbers (e.g. \\(school = 1\\)) or with words “further, higher, school”. Whatever we use, the different classes or groups are referred to as levels and each level has a name.\nIn factor recoding, we are changing level names while keeping the underlying data the same.\n\n\n\nThe tidyverse collection includes the forcats library of functions for working with categorical variables (forcats = factors). These functions are often very useful and you can read more about them here.\nChanging factors level coding by hand is, for many, a common task, and the fct_recode() function makes it easy. You can find the technical information on the function, with further examples, here.\n\n\n\nThere are different ways to examine the distributions of variables so that we can compare the distributions of the same variable between groups.\nFigure 9 presents some alternatives as a grid of 4 different kinds of plots designed to enable the same comparison. Each plot presents the distribution of scores for the @ricketts2021 Study 2 participant sample on the BPVS vocabulary measure so that we can compare the distribution of vocabulary scores between age groups.\nThe plots differ in method using:\n\nfacetted histograms showing the distribution of vocabulary scores, separately for each group, in side-by-side histograms for comparison;\nboxplots, showing the distribution of scores for each group, indicated by the y-axis locations of the edges of the boxes (25% and 75% quartiles) and the middle lines (medians);\nsuperimposed histograms, where the histograms for the separate groups are laid on top of each other but given different colours to allow comparison; and\nsuperimposed density plots where the densities for the separate groups are laid on top of each other but given different colours to allow comparison.\n\n\n\n\n\n\n\nTip\n\n\n\nThere is one thing you should notice about all these plots.\n\nIt looks like the BPVS vocabulary scores have their peak – most children show this value – at around 120 for the young group and at around 140 for the old group.\n\nWe return to this shortly.\n\n\n\nI am going to hide the coding and the explanation of the coding behind the Notes tab. Click on the tab to get a step-by-step explanation. Of these alternatives, I focus on one which I explain in more depth, following: d. Superimposed density plots.\n\nPlotNotes\n\n\n\n\n\n\n\nFigure 9: Distribution of childrens’ scores on the BPVS vocabulary measure: distributions are compared between the younger and older age groups\n\n\n\n\n\n\n\np.facet.hist &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"BPVS vocabulary score\", title = \"a. Faceted histograms\") +\n  facet_wrap(~ age.group) +\n  theme_bw()\n\np.colour.boxplot &lt;- ggplot(data = conc.orth.subjs, aes(y = BPVSRS, colour = age.group)) +\n  geom_boxplot() +\n  labs(x = \"BPVS vocabulary score\", title = \"b. Boxplots\") +\n  theme_bw()\n\np.colour.hist &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS, colour = age.group, fill = age.group)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"BPVS vocabulary score\", title = \"c. Superimposed histograms\") +\n  theme_bw()\n\np.colour.density &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS, colour = age.group, fill = age.group)) +\n  geom_density(alpha = .5, size = 1.5) +\n  labs(x = \"BPVS vocabulary score\", title = \"d. Superimposed density plots\") +\n  theme_bw()\n\np.facet.hist + p.colour.boxplot + p.colour.hist + p.colour.density\n\n\nIn plot “a. Faceted histograms”, we use the code to construct a histogram but the difference is we use:\n\n\nfacet_wrap(~ age.group) to tell R to split the data by age.group then present the histograms indicating vocabulary score distributions separately for each group.\n\n\nIn plot “b. Boxplots”, we use the geom_boxplot() code to construct a boxplot to summarize the distributions of vocabulary scores – as you have seen previously – but the difference is we use:\n\n\naes(y = BPVSRS, colour = age.group) to tell R to assign different colours to different levels of age.group to help distinguish the data from each group.\n\n\nIn plot “c. Superimposed histograms”, we use the code to construct a histogram but the difference is we use:\n\n\naes(x = BPVSRS, colour = age.group, fill = age.group) to tell R to assign different colours to different levels of age.group to help distinguish the data from each group.\nNotice that the fill gives the colour inside the bars and colour gives the colour of the outline edges of the bars.\n\n\nIn plot “d. Superimposed density plots”, we use the code geom_density(...) to construct what is called a density plot.\n\n\nA density plot presents a smoothed histogram to show the distribution of variable values.\nWe add arguments in geom_density(alpha = .5, size = 1.5) to adjust the thickness of the line (size = 1.5) drawn to show the shape of the distribution and adjust the transparency of the colour fill inside the line alpha = .5).\nWe useaes(x = BPVSRS, colour = age.group, fill = age.group) to tell R to assign different colours to different levels of age.group to help distinguish the data from each group.\nNotice that the fill gives the colour inside the density plots and colour gives the colour of the outline edges of the densities.\n\n\n\n\nDensity plots can be helpful when we wish to compare distributions. This is because we can superimpose distribution plots on top of each other, enabling us or our audience to directly compare the distributions: directly because the distributions are shown on the same scale, in the same image.\nWe can (roughly) understand a density plot as working like a smoothed version of the histogram. Imagine how the heights of the bars in the histogram represent how many observations we have of the values in a particular bin. If we draw a smooth curving line through the tops of the bars then we are representing the chances that an observation in our sample has a value (the value under the curve) at any specific location on the x-axis. You can see that in Figure 10.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nFigure 10: Distribution of childrens’ scores on the BPVS vocabulary measure. The figure shows the histogram versus density plot representation of the same data distribution\n\n\n\n\nYou can find the ggplot2 reference information on the geom_density() function, with further examples, here. You can find technical information on density functions here and here.\nWe can develop the density plot to enrich the information we can discover or communicate through the plot. Figure 11 shows the distribution of scores on both the BPVS and WASI vocabulary knowledge measures.\n\np.BPVSRS.density &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS, colour = age.group, fill = age.group)) +\n  geom_density(alpha = .5, size = 1.5) +\n  geom_rug(alpha = .5) +\n  geom_vline(xintercept = 120, linetype = \"dashed\") +\n  geom_vline(xintercept = 140, linetype = \"dotted\") +\n  labs(x = \"BPVSRS vocabulary score\") +\n  theme_bw()\n\np.WASIvRS.density &lt;- ggplot(data = conc.orth.subjs, aes(x = WASIvRS, colour = age.group, fill = age.group)) +\n  geom_density(alpha = .5, size = 1.5) +\n  geom_rug(alpha = .5) +\n  labs(x = \"WASI vocabulary score\") +\n  theme_bw()\n\np.BPVSRS.density + p.WASIvRS.density + plot_layout(guides = 'collect')\n\n\n\n\nFigure 11: Distribution of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\nHere is what the code does:\n\np.BPVRS.density &lt;- ggplot(...) creates a plot object called p.BPVRS.density.\ndata = conc.orth.subjs, ... says we use the conc.orth.subjs dataset to do this.\naes(x = BPVRS, colour = age.group, fill = age.group)) + says we want to map BPVRS scores to x-axis location, and age.group level coding (young, old) to both colour and fill.\ngeom_density(alpha = .5, size = 1.5) + draws a density plot; note that we said earlier what we want for colour and fill but here we also say that:\n\n\nalpha = .5 we want the fill to be transparent;\nsize = 1.5 we want the density curve line to be thicker than usual.\n\n\ngeom_rug(alpha = .5) + adds a one-dimensional plot, a series of tick marks, to show where we have observations of BPVRS scores for specific children. We ask R to make the tick marks semi-transparent.\ngeom_vline(xintercept = 120, linetype = \"dashed\") + draws a vertical dashed line where BPVRS = 120.\ngeom_vline(xintercept = 140, linetype = \"dotted\") + draws a vertical dotted line where BPVRS = 140.\nlabs(x = \"BPVS vocabulary score\") + makes the x-axis label something understandable to someone who does not know about the study.\ntheme_bw() changes the theme.\n\n\n\n\nAs we work with visualization, we should aim to develop skills in reading plots, so:\n\nWhat do we see?\n\nWhen we look at Figure 11, we can see that the younger and older children in the @ricketts2021 sample have broadly overlapping distributions of vocabulary scores. However, as we have noticed previously, the peak of the distribution is a bit lower for the younger children compared to the older children. This appears to be the case whether we are looking at the BPVS or at the WASI measures of vocabulary, suggesting that the observation does not depend on the particular vocabulary test. Is this observation unexpected? Probably not, as we should hope to see vocabulary knowledge increase as children get older. Is this observation a problem for our analysis? You need to read the paper to find out what we decided.\n\n\n\nIn the demonstration examples, I focused on comparing age groups on vocabulary, what about the other measures?\nI used superimposed density plots: are other plotting styles more effective, for you? Try using boxplots or superimposed or faceted histograms instead.\n\n\n\n\n\nSo far, we have looked at how and why we may examine the distributions of numeric variables. We have used histograms to visualize the distribution of variable values. We have explored the construction of grids of plots to enable the quick examination or concise communication of information about the distributions of multiple variables at the same time. And we have used histograms, boxplots and density plots to examine how the distributions of variables may differ between groups.\nThe comparison of the distributions of variable values in different groups (or, similarly, between different conditions) may be the kind of work we would need to do, in data visualization, as part of an analysis ending in, for example, a t-test comparison of mean values.\nWhile boxplots, density plots and histograms are typically used to examine how the values of a numeric variable vary, scatterplots are typically used when we wish to examine, to make sense of or communicate potential associations or relations between two (or more) numeric variables. We turn to scatterplots, next.\n\n\n\nMany of us start learning about scatterplots in high school math classes. Using the modern tools made available to us through the ggplot2 library (as part of tidyverse), we can produce effective, nice-looking, scatterplots for a range of discovery or communication scenarios.\nWe continue working with the @ricketts2021 dataset. In the context of the @ricketts2021 investigation, there is interest in how children vary in the reading, spelling and vocabulary abilities that may influence the capacity of children to learn new words. So, in this context, we can begin to progress our development in visualization skills by usefully considering the potential association between participant attributes in the Study 2 sample.\nLater on, we will look at more advanced plots that help us to communicate the impact of the experimental manipulations implemented by @ricketts2021, and also to discover the ways that these impacts may vary between children.\n\n\nWe can begin by asking a simple research question we can guess the answer to:\n\nDo vocabulary knowledge scores on two alternative measures, the BPVS and the WASI, relate to each other?\n\nIf two measurement instruments or tests are intended to measure individual differences in the same psychological attribute, here, vocabulary knowledge, then we would reasonably expect that scores on one test should covary with scores on the second test.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point() +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  theme_bw()\n\n\n\n\nFigure 12: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\nWhat does the plot show us?\nAs a reminder of how scatterplots work, we can recall that they present integrated information. Each point, for the @ricketts2021 data, represents information about both the BPVS and the WASI score for each child.\n\nThe vertical height of a point tells us the BPVS score recorded for a child: higher points represent higher scores.\nThe left-to-right horizontal position of the same point tells us the WASI score for the same child: points located more on the right represent higher scores.\n\nFigure 12 is a scatterplot comparing variation in childrens’ scores on the BPVS and WASI vocabulary measures: variation in BPVS scores are shown on the y-axis and variation in WASI scores are shown on the x-axis. Critically, the scientific insight the plot gives us is this: higher WASI scores are associated with higher BPVS scores.\nHow does the code work? We have seen scatterplots before but, to ensure we are comfortable with the coding, we can go through them step by step.\n\nggplot(data = conc.orth.subjs...) + tells R we want to produce a plot using ggplot() with the conc.orth.subjs dataset.\naes(x = WASIvRS, y = BPVSRS) tells R that, in the plot, WASIvRS values are mapped to x-axis (horizontal) position and BPVSRS values are mapped to y-axis (vertical) position.\ngeom_point() + constructs a scatterplot, using these data and these position mappings.\nlabs(x = \"WASI vocabulary score\", ... fixes the x-axis label.\ny = \"BPVSRS vocabulary score\",... fixes the y-axis label.\ntitle = \"Are WASI and BPVS vocabulary scores associated?\") + fixes the title.\ntheme_bw() changes the theme.\n\n\n\n\nFor this pair of variables in this dataset, the potential association in the variation of scores is quite obvious. However, sometimes it is helpful to guide the audience by imposing a smoother. There are different ways to do this, for different objectives and in different contexts. Here, we look at two different approaches. In addition, as we go, we examine how to adjust the appearance of the plot to address different potential discovery or communication needs.\nWe begin by adding what is called a LOESS smoother.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  theme_bw()\n\n\n\n\nFigure 13: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\nThe only coding difference between this plot Figure 13 and the previous plot Figure 12 appears at line 3:\n\ngeom_smooth()\n\nThe addition of this bit of code results in the addition of the curving line you see in Figure 13. The blue line is curving, and visually suggests that the relation between BPVS and WASI scores is different – sometimes more sometimes less steep – for different values of WASI vocabulary score.\nThis line is generated by the geom_smooth() code, by default, in an approach in which the dataset is effectively split into sub-sets, dividing the data up into sub-sets from the lowest to the highest WASI scores, and the predicted association between the y-axis variable (here, BPVS score) and the x-axis variable (here, WASI score) is calculated bit by bit, in a series of regression analyses, working in order through sub-sets of the data. This calculation of what is called the LOESS (locally estimated scatterplot smoothing) trend is done by ggplot for us. And this approach to visualizing the trend in a potential association between variables is often a helpful way to discover curved or non-linear relations.\nYou can find technical information on geom_smooth() here and an explanation of LOESS here.\nFor us, this default visualization is not helpful for two reasons:\n\nWe have not yet learned about linear models, so learning about LOESS comes a bit early in our development.\nIt is hard to look at Figure 13 and identify a convincing curvilinear relation between the two variables. A lot of the curve for low WASI scores appears to be linked to the presence of a small number of data points.\n\nAt this stage, it is more helpful to adjust the addition of the smoother. We can do that by adding an argument to the geom_smooth() function code.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point() +\n  geom_smooth(method = 'lm') +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  theme_bw()\n\n\n\n\nFigure 14: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\nNotice the difference between Figure 13 and Figure 14:\n\ngeom_smooth(method = 'lm') tells R to draw a trend line, a smoother, using the lm method.\n\nThe lm method requires R to estimate the association between the two variables, here, BPVS and WASI, assuming a linear model. Of course, we are going to learn about linear models but, in short, right now, what we need to know is that we assume a “straight line” relationship between the variables. This assumption requires that for any interval of WASI scores – e.g., whether we are talking about WASI scores between 20-25 or about WASI scores between 30-35 – the relation between BPVS and WASI scores has the same shape: the direction and steepness of the slope of the line is the same.\n\n\n\n\nDeveloping skill in working with data visualizations is not just about developing coding skills, it is also about developing skills in reading, and critically evaluating, the information the plots we produce show us.\n\nStop and take a good look at the scatterplot in Figure 14. Use the visual representation of data to critically evaluate the potential association between the BPVS and WASI variables. What can you see?\nYou can train your critical evaluation by asking yourself questions like the following:\n\nHow does variation in the x-axis variable relate to variation in values of the y-axis variable?\n\n\nWe can see, here, that higher WASI scores are associated with higher BPVS scores.\n\n\nHow strong is the relation?\n\n\nThe strength of the relation can be indicated by the steepness of the trend indicated by the smoother, here, the blue line.\nIf you track the position of the line, you can see, for example, that going from a WASI score of 20 to a WASI score of 40 is associated with going from a BPVS score of a little over 110 to a BPVS score of about a 150.\nThat seems like a big difference.\n\n\nHow well does the trend we are looking at capture the data in our sample?\n\n\nHere, we are concerned with how close the points are to the trend line.\nIf the trend line represents a set of predictions about how the BPVS scores vary (in height) given variation in WASI scores, we can see that in places the prediction is not very good.\nTake a look at the points located at WASI 25. We can see that there there are points indicating that different children have the same WASI score of 25 but BPVS scores ranging from about 115 to 140.\n\n\n\n\nFigure 14 presents a satisfactory looking plot but it is worth checking what edits we can make to the appearance of the plot, to indicate some of the ways that you can exercise choice in determining what a plot looks like. This will be helpful to you when you are constructing plots for presentation and report and you want to ensure the plots are as effective as possible.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point(alpha = .5, size = 2) +\n  geom_smooth(method = 'lm', colour = \"red\", size = 1.5) +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  xlim(0, 40) + ylim(0, 160) +\n  theme_bw()\n\n\n\n\nFigure 15: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\nIf you inspect the code, you can see that I have made three changes:\n\ngeom_point(alpha = .5, size = 2 changes the size of the points and their transparency (using alpha).\ngeom_smooth(method = 'lm', colour = \"red\", size = 1.5) change the colour of the smoother line, and the thickness (size) of the line.\nxlim(0, 40) + ylim(0, 160) changes the axis limits.\n\nThe last step — changing the axis limits — reveals how the sample data can be understood in the context of possible scores on these ability measures. Children could get BPVS scores of 0 or WASI scores of 0. By showing the start of the axes we get a more realistic sense of how our sample compares to the possible ranges of scores we could see in the wider population of children. This perhaps offers a more honest or realistic visualization of the potential association between BPVS and WASI vocabulary scores.\n\n\n\nAs we have seen previously, we can construct a series of plots and present them all at once in a grid or lattice. Figure 16 presents just such a grid: of scatterplots, indicating a series of potential associations.\nLet’s suppose that we are primarily interested in what factors influence the extent to which children in the @ricketts2021 word learning experiment are able to correctly spell the target words they were given to learn. As explained earlier, in Section 1.7.2, @ricketts2021 examined the spellings produced by participant children in response to target words, counting how many string edits (i.e., letter deletions etc.) separated the spelling each child produced from the target spelling they should have produced.\nWe can calculate the mean spelling accuracy score for each child, over all the target words we observed their response to. We can identify mean spelling score as the outcome variable. We can then examine whether the outcome spelling scores are or are not influenced by participant attributes like vocabulary knowledge.\nFigure 16 presents a grid of scatterplots indicating the potential association between mean spelling score and each of the variables we have in the conc.orth dataset, including the Castles and Coltheart (CC) and TOWRE measures of word or nonword reading skill, WASI and BPVS measures of vocabulary knowledge, and the WASI matrix measure of intelligence, as well as (our newly coded) age group factor.\nI hide an explanation of the coding behind the Notes tab, because we have seen how to produce grids of plots, but you can take a look if you want to learn how the plot is produced.\n\nPlotNotes\n\n\n\n\n\n\n\nFigure 16: Grid of scatterplots showing the potential association between mean spelling score, for each child, and variation in the Castles and Coltheart (CC) and TOWRE measures of word or nonword reading skill, WASI and BPVS measures of vocabulary knowledge, the WASI matrix measure of intelligence, and age group factor\n\n\n\n\n\n\nThe code to produce the figure is set out as follows.\n\np.wordsvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = TOWREsweRS, \n                              y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"Word reading\", \n       y = \"Spelling score\",\n       title = \"(a.)\") +\n  theme_bw()\n\np.nonwordsvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = TOWREsweRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"Nonword reading\", \n       y = \"Spelling score\",\n       title = \"(b.)\") +\n  theme_bw()\n\np.WASIvRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = WASIvRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"WASI vocabulary\", \n       y = \"Spelling score\",\n       title = \"(c.)\") +\n  theme_bw()\n\np.BPVSRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = BPVSRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"BPVS vocabulary score\", \n       y = \"Spelling score\",\n       title = \"(d.)\") +\n  theme_bw()\n\np.WASImRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = WASImRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"WASI matrix\", \n       y = \"Spelling score\",\n       title = \"(e.)\") +\n  theme_bw()\n\np.CC2regRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = CC2regRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"CC regular words\", \n       y = \"Spelling score\",\n       title = \"(f.)\") +\n  theme_bw()\n\np.CC2irregRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = CC2irregRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"CC irregular words\", \n       y = \"Spelling score\",\n       title = \"(g.)\") +\n  theme_bw()\n\np.CC2nwRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = CC2nwRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"CC nonwords\", \n       y = \"Spelling score\",\n       title = \"(h.)\") +\n  theme_bw()\n\np.age.groupvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = age.group, \n                                  y = mean.score)) +\n  geom_boxplot() +\n  labs(x = \"Age group\", \n       y = \"Spelling score\",\n       title = \"(i.)\") +\n  theme_bw()\n\np.wordsvsmean.score + p.nonwordsvsmean.score + p.WASIvRSvsmean.score +\n  p.BPVSRSvsmean.score + p.WASImRSvsmean.score + p.CC2regRSvsmean.score +\n  p.CC2irregRSvsmean.score + p.CC2nwRSvsmean.score + p.age.groupvsmean.score\n\n\nTo produce the grid of plots, we first create a series of plot objects using code like that shown in the chunk.\n\n\np.wordsvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = TOWREsweRS, \n                              y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"Word reading\", \n       y = \"Spelling score\",\n       title = \"(a.)\") +\n  theme_bw()\n\n\np.wordsvsmean.score &lt;- ggplot(...) creates the plot.\ndata = conc.orth.subjs tells R what data to work with.\naes(x = TOWREsweRS, y = mean.score) specifies the aesthetic data mappings.\ngeom_point(alpha = .5, size = 3) tells R to produce a scatterplot, specifying the size and transparency of the points.\ngeom_smooth(method = 'lm', size = 1.5) tells R to add a smoother, specifying the method and the thickness of the line.\nlabs(x = \"Word reading\", y = \"Spelling score\", title = \"(a.)\") fixes the labels.\ntheme_bw() adjusts the theme.\n\n\nWe then put the plots together, using the patchwork syntax where we list the plot objects by name, separating each name by a +.\n\n\np.BPVSRSvsmean.score + p.WASImRSvsmean.score + p.CC2regRSvsmean.score +\n  p.CC2irregRSvsmean.score + p.CC2nwRSvsmean.score + p.age.groupvsmean.score\n\n\n\n\nFigure 16 allows us to visually represent the potential association between an outcome measure, the average spelling score, and a series of other variables that may or may not have an influence on that outcome. Using a grid in this fashion allows us to compare the extent to which different variables appear to have an influence on the outcome. We can see, for example, that measures of variation in word reading skill appear to have stronger association (the trend lines are more steeply slowed) than measures of vocabulary knowledge or intelligence, or age group.\nUsing grids of plots like this allow us to compactly communicate these potential associations in a single figure.\n\n\n\n\n\n\nWarning\n\n\n\nLevenshtein distance scores are higher if a child makes more errors in producing the letters in a spelling response.\n\nThis means that if we want to see what factors help a child to learn a word, including its spelling, then we want to see that helpful factors are associated with lower Levenshtein scores.\n\n\n\n\n\n\n\nAs explained in Section 1.7.2, in the @ricketts2021 study, we taught children taught 16 novel words in a study with a 2 x 2 factorial design. The presence of orthography (orthography absent vs. orthography present) was manipulated within participants: for all children, eight of the words were taught with orthography (the word spelling) present and eight with orthography absent. Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not. The @ricketts2021 investigation was primarily concerned with the effects on word learning of presenting words for learning with or without showing the words with their spellings, with or without instructing students explicitly that they would be helped by the presence of the spellings.\nWe can analyze the effects of orthography and instruction using a linear model.\n\nmodel &lt;- lm(Levenshtein.Score ~ Instructions*Orthography, data = conc.orth)\n\nThe model code estimates variation in spelling score (values of the Levenshtein.Score) variable, given variation in the levels of the Instructions and Orthography factors, and their interaction.\nThis model is a limited approximation of the analysis we would need to do with these data to estimate the effects of orthography and instruction; see @ricketts2021 for more information on what analysis is required (in our view). However, it is good enough as a basis for exploring the kind of data visualization work — in terms of both discovery and communication — that you can do when you are working with data from an experimental study.\nWe can get a summary of the model results which presents the estimated effect of each experimental factor. These estimates represent the predicted change in spelling score, given variation in Orthography (present, absent) or Instruction (explicit, incidental), and given the possibility that the effect of the presence of orthography is different for different levels of instruction.\nNotice that some of the p-values are incorrectly shown as 0.000. This is a result of using functions to automatically take a model summary and generate a table. I am going to leave this error with a warning because our focus is on visualization, next.\n\n\n\nModel summary\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.584\n0.072\n21.857\n0.000\n\n\nInstructionsincidental\n-0.041\n0.103\n-0.396\n0.692\n\n\nOrthographypresent\n-0.409\n0.103\n-3.987\n0.000\n\n\nInstructionsincidental:Orthographypresent\n0.060\n0.146\n0.409\n0.683\n\n\n\n\n\n\n\nVery often, when we complete a statistical analysis of outcome data, in which we estimate or test the effects on outcomes of variation in some variables or of variation in experimental conditions, then we present a table summary of the analysis results. However, these estimates are typically difficult to interpret (it gets easier with practice) and talk about. Take a look at the summary table. We are often to focus on whether effects are significant or not significant. But, really, what we should consider is how much the outcome changes given the different experimental conditions.\nHow do we get that information from the analysis results? We can communicate results — to ourselves or to an audience — by constructing plots from the model information. The ggeffects library extends ggplot2 to enable us to do this quite efficiently.\nWhen we write code to fit a linear model like:\n\nmodel &lt;- lm(Levenshtein.Score ~ Instructions*Orthography, data = conc.orth)\n\nWe record the results as an object called model because we specify model &lt;- lm(...). We can take these results and ask R to create a plot showing predicted change in outcome (spelling) given our model. We can then present the effects of the variables, as shown in Figure 17.\n\ndat &lt;- ggpredict(model, terms = c(\"Instructions\", \"Orthography\"))\nplot(dat, facet = TRUE) + ylim(0, 3)\n\n\n\n\nFigure 17: Dot and whisker plots showing the predicted effect on outcome spelling (Levenshtein) score, given different experimental conditions: Orthography (present, absent) x Instruction (explicit, incidental).\n\n\n\n\nThe code works as follows:\n\ndat &lt;- ggpredict(model, terms = c(\"Instructions\", \"Orthography\")) tells R to calculate predicted outcomes, given our model information, for the factors \"Instructions\", \"Orthography\".\nplot(dat, facet = TRUE) plot the effects, given the predictions, showing the effect of different instruction conditions in different plot facets (the left and right panels).\nylim(0, 3) fix the y-axis to show a more honest indication of the effect on outcomes, given the potential range of spelling scores can start at 0.\n\nIn Figure 17, the dots represent the linear model estimates of outcome spelling, predicted under different conditions. The plots indicate that spelling scores are predicted to be lower when orthography is present. There appears to be little or no effect associated with different kinds of instruction.\nThe vertical lines (often termed “whiskers”) indicate the 95% confidence interval about these estimates. Confidence intervals (CIs) are often mis-interpreted so I will give the quick definition outlined by @Hoekstra2014 here:\n\nA CI is a numerical interval constructed around the estimate of a parameter [i.e. the model estimate of the effect]. Such an interval does not, however, directly indicate a property of the parameter; instead, it indicates a property of the procedure, as is typical for a frequentist technique. Specifically, we may find that a particular procedure, when used repeatedly across a series of hypothetical data sets (i.e., the sample space), yields intervals that contain the true parameter value in 95 % of the cases.\n\nIn short, the interval shows us the range of values within which we can expect to capture the effects of interest, in the long run, if we were to run our experiment over and over again.\nGiven our data and our model, these intervals indicate where the outcome might be expected to vary, given different conditions, and that is quite useful information. If you look at Figure 17, you can see that the presence of orthography (present versus absent) appears to shift outcome spelling, on average, by about a quarter of a letter edit: from over 1.5 to about 1.25. This is about one quarter of the difference, on average, between getting a target spelling correct and getting it wrong by one letter (e.g., the response ‘epegram’ for the target ‘epigram’). This is a relatively small effect but we may consider how such small effects add up, over a child’s development, cumulatively, in making the difference between wrong or nearly right spellings to correct spellings.\nIn the @ricketts2021 paper, we conducted Bayesian analyses which allow us to plot the estimated effects of experimental conditions along with what are called credible intervals indicating our uncertainty about the estimates. In a Bayesian analysis, we can indicate the probable or plausible effect of conditions, or range of plausible effects, given our data and our model. (This intuitive sense of the probable location of effects is, sometimes, what researchers and students mis-interpret confidence intervals as showing; @Hoekstra2014.) Accounting for our uncertainty is a productive approach to considering how much we learn from the evidence we collect in experiments.\nBut this gets ahead of where we are now in our development of skills and understanding. There is another way to discover how uncertain we may be about the results of our analysis. This is an approach we have already experienced: plotting trends or estimates together with the observed data points. We present an example in Figure 18.\n\nplot(dat, add.data = TRUE)\n\n\n\n\nFigure 18: Dot and whisker plots showing the predicted effect on outcome spelling (Levenshtein) score, given different experimental conditions: Orthography (present, absent) x Instruction (explicit, incidental). The estimates are shown as dot-whisker points. In addition, the plot shows as points the spelling score observed for each child for each response recorded in the conc.orth dataset.\n\n\n\n\nFigure 18 reveals the usefulness of plotting model estimates of effects alongside the raw observed outcomes. We can make two critical observations.\n\nWe can see that the observed scores clearly cluster around outcome spelling values of 0, 1, 2, 3, 4, and 5.\n\n\nThis is not a surprise because @ricketts2021 scored each response in their test of spelling knowledge by counting the number of letter edits (letter deletions, additions etc.) separating a spelling response from a target response.\nBut the plot does suggest that the linear model is missing something about the outcome data because there is no recognition in the model or the results of this bunching or clustering around whole number values of the outcome variable. (This is why @ricketts2021 use a different analysis approach.)\n\n\nWe can also see that it is actually quite difficult to distinguish the effects of the experimental condition differences on the observed spelling responses. There is a lot of variation in the responses.\n\nHow can we make sense of this variation?\nAnother approach we can take to experimental data is to examine visually how the effects of experimental conditions vary between individual participants. Usually, in teaching, learning and doing foundation or introductory statistical analyses we think about the average impact on outcomes of the experimental conditions or some set of predictor variables. It often makes sense, also, or instead, to consider the ways that the impact on outcomes vary between individuals.\nHere, it might be worthwhile to look at the effect of the conditions for each child. We can do that in different ways. In the following, we will look at a couple of approaches that are often useful. We will focus on the effect of variation in the Orthography condition (present, absent)\nTo begin our work, we first calculate the average outcome (Levenshtein.Score) spelling score for each child in each of the experimental conditions (Orthography, present versus absent):\nWe do this in a series of steps.\n\nscore.by.subj &lt;- conc.orth %&gt;%\n  group_by(Participant, Orthography) %&gt;%\n  summarise(mean.score = mean(Levenshtein.Score))\n\n\nscore.by.subj &lt;- conc.orth %&gt;% create a new dataset score.by.subj by taking the original data conc.orth and piping it through a series of processing steps, to follow.\ngroup_by(Participant, Orthography) %&gt;% first group the rows of the original dataset and piped the grouped data to the next bit. We group the data by participant identity code and by Orthography condition\nsummarise(mean.score = mean(Levenshtein.Score)) then calculate the mean Levenshtein.Score for each participant, for their responses in the Orthography present and in the Orthography absent conditions.\n\nThis first step produces a summary version of the original dataset, with two mean outcome spelling scores for each child, for their responses in the Orthography present and in the Orthography absent conditions. This arranges the summary mean scores in rows, with two rows per child: one for the absent, one for the present condition. You can see what we get in the extract from the dataset, shown next.\n\n\n\n\n\nParticipant\nOrthography\nmean.score\n\n\n\n\nEOF001\nabsent\n1.750\n\n\nEOF001\npresent\n0.875\n\n\nEOF002\nabsent\n1.375\n\n\nEOF002\npresent\n2.125\n\n\nEOF004\nabsent\n1.625\n\n\nEOF004\npresent\n1.000\n\n\nEOF006\nabsent\n0.750\n\n\nEOF006\npresent\n0.500\n\n\nEOF007\nabsent\n1.500\n\n\nEOF007\npresent\n0.625\n\n\n\n\n\n\n\nIn the second step, we also calculate the difference between spelling scores in the different Orthography conditions. We do this because @ricketts2021 were interested in whether spelling responses were different in the different conditions.\n\nscore.by.subj.diff &lt;- score.by.subj %&gt;%\n  pivot_wider(names_from = Orthography, values_from = mean.score) %&gt;%\n  mutate(difference.score = absent - present) %&gt;%\n  pivot_longer(cols = c(absent, present), \n               names_to = 'Orthography',\n               values_to = 'mean.score') \n\n\nscore.by.subj.diff &lt;- score.by.subj %&gt;% creates a new version of the summary dataset from the dataset we just produced.\npivot_wider(names_from = Orthography, values_from = mean.score) %&gt;% re-arranges the dataset so that the absent, present mean scores are side-by-side, in different columns, for each child.\nmutate(difference.score = absent - present) %&gt;% calculates the difference between the absent, present mean scores, creating a new variable, difference.score.\npivot_longer(cols = c(absent, present) ...) re-arranges the data back again so that the dataset is in tidy format, with one column of mean spelling scores, with two rows for each participant for the absent, present mean scores.\n\nThis code arranges the summary mean scores in rows, with two rows per child: one for the absent, one for the present condition — plus a difference score.\n\n\n\n\n\nParticipant\ndifference.score\nOrthography\nmean.score\n\n\n\n\nEOF001\n0.875\nabsent\n1.750\n\n\nEOF001\n0.875\npresent\n0.875\n\n\nEOF002\n-0.750\nabsent\n1.375\n\n\nEOF002\n-0.750\npresent\n2.125\n\n\nEOF004\n0.625\nabsent\n1.625\n\n\nEOF004\n0.625\npresent\n1.000\n\n\nEOF006\n0.250\nabsent\n0.750\n\n\nEOF006\n0.250\npresent\n0.500\n\n\nEOF007\n0.875\nabsent\n1.500\n\n\nEOF007\n0.875\npresent\n0.625\n\n\n\n\n\n\n\nNow we can use these data to consider how the impact of the experimental condition (Orthography: present versus absent) varies between individual participants. We do this by showing the mean outcome spelling score, separately for each participant, in each condition.\nFigure 19 shows dot plots indicating the different outcome spelling (Levenshtein) scores, for each participant, in the different experimental conditions: Orthography (present, absent). Plots are ordered, from top left to bottom right, by the difference between mean spelling scores in the absent versus present conditions. The plots indicate that some children show higher spelling scores in the present than in the absent condition (top left plots), some children show little difference between conditions (middle rows), while some children show higher spelling scores in the absent than in the present condition (bottom rows).\n\nggplot(data = score.by.subj.diff, \n       aes(x = Orthography, y = mean.score,\n           colour = Orthography)) +\n  geom_point() +\n  facet_wrap(~ reorder(Participant, difference.score)) +\n  theme(axis.text.x = element_blank())\n\n\n\n\nFigure 19: Dot plots showing the different outcome spelling (Levenshtein) scores, for each participant, in the different experimental conditions: Orthography (present, absent). Plots are ordered, from top left to bottom right, by the difference between mean spelling scores in the absent versus present conditions.\n\n\n\n\nOnce we have done the data processing in preparation, the code to produce the plot is fairly compact.\n\nggplot(data = score.by.subj.diff ... tells R to produce a plot, using ggplot() and the newly created score.by.subj.diff dataset.\naes(x = Orthography, y = mean.score,... specifies the aesthetic mappings: we tell R to locate mean.score on the y-axis and Orthography condition on the x-axis/\naes(...colour = Orthography)) + specifies a further aesthetic mapping: we tell R to map different Orthography conditions to different colours.\ngeom_point() + tells R to take the data and produce a scatterplot, given our mapping specifications.\nfacet_wrap(...) + tells to split the dataset into sub-sets (facets).\nfacet_wrap(~ reorder(Participant, difference.score)) tells R that we want the sub-sets to be organized by Participant, and we want the facets to be ordered by the difference.score calculated for each participant.\ntheme(axis.text.x = element_blank()) removes the x-axis labels because it is too crowded with the axis labels left in, and the information is already present in the colour guide legend shown on the right of the plot.\n\n\n\n\nVisualizing associations between variables encompasses a wide range of the things we have to do, in terms of both discovery and communication, when we work with data from psychological experiments.\nThe conventional method to visualize how the distribution of values in one variable covaries with the distribution of values in another variable is through using a scatterplot. However, the construction of a scatterplot can be elaborated in various ways to enrich the information we present or communicate to our audiences, or to ourselves.\n\nWe can add elements like smoothers to indicate trends.\nWe can add annotation, as with the histograms, to highlight specific thresholds.\nWe can facet the plots to indicate how trends may vary between sub-sets of the data.\n\nIn the final phases of our practical work, we started by presenting model-based predictions of the effects of experimental manipulations. However, you will have noticed that presenting plots of effects is not where we stop when we engage with a dataset. Further plotting indicates quite marked variation between participants in the effects of the conditions. This kind of insight is something we can and should seek to reveal through our visualization work."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html#next-steps-for-development",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html#next-steps-for-development",
    "title": "Data visualization",
    "section": "",
    "text": "To take your development further, take a look at the resources listed in Section 1.9.\nIn my experience, the most productive way to learn about visualization and about coding the production of plots, is by doing. And this work is most interesting if you have a dataset you care about: for your research report, or for your dissertation study.\nAs you have the alternate datasets described in Section 1.7.1.2.1, you can start with the data from the other task or the other study in @ricketts2021. @ricketts2021 recorded children’s responses in two different outcome tasks, the orthographic spelling task we have looked at, and a semantic or meaning-based task. It would be a fairly short step to adapt the code you see in the example code chunks to work with the semantic datasets.\nAlternatively, you can look at the data reported by @rodríguez-ferreiro2020. @rodríguez-ferreiro2020 present both measures of individual differences (on schizotypyal traits) and experimental manipulations (of semantic priming) so you can do similar things with those data as we have explored here."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html#sec-resources",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html#sec-resources",
    "title": "Data visualization",
    "section": "",
    "text": "We typically use the ggplot library (part of the tidyverse) to produce plots. Clear technical information, with useful examples you can copy and run, can be found in the reference webpages:\n\nhttps://ggplot2.tidyverse.org/reference/index.html\n\nA source of inspiration can be found here:\n\nhttps://r-graph-gallery.com\nIf you are trying to work out how to do things by searching for information online, you often find yourself at tutorial webpages. You will develop a sense of quality and usefulness with experience. Most often, what you are looking for is a tutorial that provides some explanation, and example code you can adapt for your own purposes. Here are some examples.\n\nCedric Scherer on producing raincloud plots:\n\nhttps://www.cedricscherer.com/2021/06/06/visualizing-distributions-with-raincloud-plots-and-how-to-create-them-with-ggplot2/\n\nWinston Chang on colours and colour blind palettes:\n\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\n\nThomas Lin Pedersen (and others) on putting together plots into a single presentation using the patchwork library functions:\n\nhttps://patchwork.data-imaginist.com/articles/patchwork.html\n\n\n\n\nThe book “R for Data Science” [@wickham2016] will guide you through the data analysis workflow, including data visualization, and the latest version can be accessed in an online free version here:\n\nhttps://r4ds.hadley.nz\n\nThe “ggplot2: Elegant Graphics for Data Analysis” book [@R-ggplot2] corresponding to the ggplot library was written by Hadley Wickham in its first edition, it is now in its third edition (as a work in progress, co-authored by Wickham, Danielle Navarro and Thomas Lin Pedersen) and this latest version can be accessed in an online free version here:\n\nhttps://ggplot2-book.org/index.html\n\nThe “R graphics cookbook” [@Chang2013a], and the latest version can be accessed in an online free version here:\n\nhttps://r-graphics.org\n\nThe book “Fundamentals of Data Visualization” [@wilke] is about different aspects of visualization, and can be accessed in an online free version here:\n\nhttps://clauswilke.com/dataviz/"
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/visualization.html#footnotes",
    "href": "PSYC402/extra_ToBeOrganised/visualization.html#footnotes",
    "title": "Data visualization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs you can see if you read the @ricketts2021 paper, and the associated guide to the data and analysis on the OSF repository, we analysed the word learning data using Generalized Linear Mixed-effects Models (GLMM). GLMMs are used when we are analyzing data with a multilevel structure. These structures are very common and can be identified whenever we have groups or clusters observations: here, we have multiple observations of the test response, for each participant and for each stimulus word. When we fit GLMMs, the functions we use to do the analysis require the data to be structured in this tidy fashion, with different rows for each response or outcome observation, and repeated information for each participant or stimulus (if present).↩︎"
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/what.html",
    "href": "PSYC402/extra_ToBeOrganised/what.html",
    "title": "What",
    "section": "",
    "text": "We present the following guidelines to help you to complete the coursework assessment. If you have any questions, email Rob Davies at: r.davies1@lancaster.ac.uk\n\nYou can read more about the motivations for the exercise, and the benefits to learning, in ?@sec-report-intro-why.\nYou can read about ways – methods – to approach the exercise in ?@sec-how.\n\n\n\n\n\n\n\nImportant\n\n\n\nHere we outline what we expect you to do.\n\n\n\n\nStudents have taken a variety of approaches to the assignment (see ?@sec-how for a discussion of these options).\n\nSome students choose to complete an analysis of a publicly available dataset, analyzed previously, data for which the report has been published in a journal article.\nSome students choose to complete an analysis of a publicly available dataset that has been made available (for a report published as a data journal) but has not been analysed previously.\nSome students choose to complete an analysis of one of the data-sets used for practical exercises in class: the example or demonstration data we collect together as the curated data.\n\n\n\n\nThe marks for the report will be depend primarily on the quality of the Results and Discussion sections of the report. This is because, in most cases, you will be using data for your analysis that were collected, previously, by other authors for an already published report.\nWe cannot give you much credit for writing about the background research literature in the Introduction or about the Method of data collection because the authors of the original report did that work if you are using published data, or because Rob did that work if you are using demonstration data. We can give you credit for concisely and clearly summarizing the background research literature and method of data collection.\nWhile we cannot give you much credit for the Introduction and Methods, we advise you that these sections will need to be effective in preparing the reader to:\n\nunderstand the nature of the research question(-s) or prediction(-s), their background and their motivation;\nunderstand the nature of the dataset analyzed, the measures taken, the participants or stimuli sampled;\nunderstand the motivation for, and the decision making involved in, the analysis approach adopted.\n\nIn short: what you write in the Introduction and Method sections will have function in making the results and Discussion effective.\nWe expect students to use one of the analysis methods taught in the module.\n\n\n\n\n\n\nTip\n\n\n\nMarks will be awarded depending:\n\non how appropriate the method is to the context, to the study design, to answering the research question, and to the features of the data;\non how effectively the analysis is explained – students must explain the motivations for their decisions, explain their methods, and explain their findings effectively to gain points.\n\n\n\nOf course, some interests, datasets, or questions warrant the use of methods that are not taught but that students can learn independently.\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to use a method that has not been taught in the module then you must discuss this first with Rob Davies.\n\n\n\n\n\nYou will prepare your report in the style of a short research article in the journal Psychological Science. You can view examples of articles here.\nFor the journal Psychological Science, a research article has to be written in a specific format with specific properties: we want something a bit different for this coursework, and we specify what we want here.\nWe require information about the background for the study, the research question, the hypotheses, the method of data collection, the method of data analysis, the results of the analysis, and the psychological interpretation of those results.\n\n\n\n\n\n\nImportant\n\n\n\n\nWord count limit: no more than 2000 words are allowed for all materials.\nThe reports should include abstract, introduction, methods, results, discussion and references sections, like most psychological research articles.\nWrite in complete sentences in full paragraphs.\n\n\n\n\n\n\nThe reports should include Abstract, Introduction, Methods, Results, Discussion and References sections, like a short research article in the journal Psychological Science.\nThe focus of marking will be on the quality of the Results and Discussion sections. Nevertheless, as explained, the Introduction and Method sections need to be effective so that the Results and Discussion sections can be effective.\nOne way to check that what you say in earlier sections is sufficient to prepare the reader for what they encounter in later sections is to write forwards and then review backwards through the text of your report.\n\n\n\n\n\n\nTip\n\n\n\n\nWrite forwards: enter information, as appropriate, progressing from Introduction to Methods to Results to Discussion.\nThen, in review, work backwards:\n\n\nCan the reader understand the Discussion given what was presented in the Results?\nCan they understand the analysis, what you did, why and how, given the information in the Method?\nCan they understand the Method, what the measures are, what was done, and why, given the motivations and aims explained in the Introduction?\n\n\n\n\n\nSome general advice on what you need to communicate in the Introduction and Methods sections is that what you write in these sections should provide the reader with the information they need to answer the following questions:\n\nIntroduction\n\n\nWhat did the researchers do and why did the researchers do it?\nWhat was the question addressed in the study and why is it interesting?\nWhat were the hypotheses or predictions?\nWhat results were expected and how would they relate to the hypotheses?\n\n\nMethod\n\n\nWhat was done to collect the data?\nWho were tested (Participants)?\nWhat materials were used in testing (Materials)?\nWhat was the design of the study?\nWhat procedure was used?\n\nYou will need to rely on your experience reading primary research articles to help you to judge how much information you need to present and when you need to present it.\n\n\n\n\n\n\nTip\n\n\n\nIf you are unsure, pick an article you admire in a journal and analyze how they present information, what they explain, and to what level of detail.\n\n\n\n\n\nYou must write text that explains to the reader what analysis you did, why you did it, what results you found, and what you think the implications of those results may be.\n\n\n\n\n\n\nTip\n\n\n\nBe clear on:\n\nwhat the outcome measure or dependent variable for analysis was,\nand what factors or predictor variables were brought into the analysis of that outcome\n\n\n\nYou need to be clear about these things because the reader needs to understand these things well to be able to understand your results reporting.\nYou need to ensure that the text you write for the Results section provides the reader with the information they need to answer the following questions:-\n\nResults\n\n\nWhat hypotheses were tested?\nWhat methods were used to test the hypotheses?\nWhy are they appropriate?\nWhat were the results? What were the direction and relative size of effects?\n\nStudents often ask what analysis methods they can use. The answer to that question depends on the question or hypothesis, the nature of the dataset, and the appropriateness of the different analysis options.\nYou can read a discuss of the nature of the choices open to any data analysis job in ?@sec-multiverse. You can read about ways to identify how you can approach the analysis task in ?@sec-whatanalysis or ?@sec-try-multiverse.\n\n\n\n\n\n\nTip\n\n\n\n\nDo what seems reasonable using one or more of the analysis methods practiced in class\nAnd explain your reasoning\n\n\n\nIf you use a dataset that is already published in a journal such as Psychological Science, then your presentation of the results may differ from the presentation in the article in ways that highlight new features of the data. You may wish to identify the specific difference between the analysis you do and the analysis that was reported in the article that is the source for your data, if you derive your data from a published article, and if that article includes a statistical analysis.\nIt will help the reader and, sometimes, you if you indicate the source of the data and share your analysis code directly.\n\n\n\n\n\n\nTip\n\n\n\n\nIf you get your data from a published article then you will cite that article but it may be helpful if, in the Method section, you add a link to the data repository (e.g. on OSF) so that your reader can access the data if necessary.\nPut the code for the analysis you do and report, with comments, in an Appendix. The appendix will not be counted towards word count limits.\n\n\n\nBy the time the reader gets to the Discussion, they should be prepared for your outline of the questions or predictions, your identification of the main findings, and your critical evaluation of those findings, in context, and in relation to theoretical or practical implications.\nYou need to ensure that the text you write for the Discussion section enables the reader to understand the answer to the following questions:\n\nDiscussion\n\n\nWhat are the theoretical implications of the study findings?\nWhat are the practical implications?\n\nYou will understand that not every dataset can be analyzed in ways that have either or both theoretical or practical implications.\nThe main point is that you should keep in mind what the reader should get out of (what benefit) reading your report.\n\n\n\n\n\n\nTip\n\n\n\nOne way to think about the task for the Discussion is that you need to make sense of the results for the reader.\n\n\nYou will want to:\n\nlocate the ways in which your results fit into a wider (research or practice or theoretical) context;\nexplain how the results do or do not add to or change understanding;\nexplain if there are limitations in the data (maybe inherited from the source) or analysis;\nexplain what future research may usefully do;\nidentify the conclusions that can be drawn from the report.\n\n\n\n\n\n\n\nTip\n\n\n\nAny or every analysis will have limitations so you would be well advised to be selective and constructive:\n\nselective: are the limitations you identify likely to affect or bias results?\nconstructive: in what ways would practicable future research address these limitations?\n\n\n\nExplaining implications or limitations is not a box to tick in this exercise so that you can say you have met a criterion for assessment. This is an opportunity for you to work in a scholarly way and for you to present your work in a scholarly way. This will require reflection, critical evaluation, and a grasp of elements of the relevant literature.\n\n\n\n\nReports should present enough information that the reader can understand:\n\nthe background and motivation for a study;\nthe features of the data analyzed and the methods of data collection;\nthe approach taken in analysis, the analysis steps, and the results;\nthe relationship between the observed results and the expected results, and the interpretation of findings in relation to previous work.\n\n\n\n\n\n\n\nTip\n\n\n\nTo be clear about clarity:\n\nexplain, spell things out (decisions, reasoning, interpretations) as if you were explaining them to a reasonably intelligent reader, a Psychologist who is not a specialist in the area of study occupied by the study reported, i.e. me.\n\n\n\n\n\n\n\n\nFor general APA formatting of reports:\nhttps://owl.purdue.edu/owl/research_and_citation/apa_style/apa_style_introduction.html\nAnd for APA formatting of statistics and numbers:\nhttps://owl.purdue.edu/owl/research_and_citation/apa_style/apa_formatting_and_style_guide/apa_numbers_statistics.html\nThough the APA guidelines are the authoritative guide."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/what.html#what-data-can-i-analyse",
    "href": "PSYC402/extra_ToBeOrganised/what.html#what-data-can-i-analyse",
    "title": "What",
    "section": "",
    "text": "Students have taken a variety of approaches to the assignment (see ?@sec-how for a discussion of these options).\n\nSome students choose to complete an analysis of a publicly available dataset, analyzed previously, data for which the report has been published in a journal article.\nSome students choose to complete an analysis of a publicly available dataset that has been made available (for a report published as a data journal) but has not been analysed previously.\nSome students choose to complete an analysis of one of the data-sets used for practical exercises in class: the example or demonstration data we collect together as the curated data."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/what.html#how-will-we-evaluate-reports",
    "href": "PSYC402/extra_ToBeOrganised/what.html#how-will-we-evaluate-reports",
    "title": "What",
    "section": "",
    "text": "The marks for the report will be depend primarily on the quality of the Results and Discussion sections of the report. This is because, in most cases, you will be using data for your analysis that were collected, previously, by other authors for an already published report.\nWe cannot give you much credit for writing about the background research literature in the Introduction or about the Method of data collection because the authors of the original report did that work if you are using published data, or because Rob did that work if you are using demonstration data. We can give you credit for concisely and clearly summarizing the background research literature and method of data collection.\nWhile we cannot give you much credit for the Introduction and Methods, we advise you that these sections will need to be effective in preparing the reader to:\n\nunderstand the nature of the research question(-s) or prediction(-s), their background and their motivation;\nunderstand the nature of the dataset analyzed, the measures taken, the participants or stimuli sampled;\nunderstand the motivation for, and the decision making involved in, the analysis approach adopted.\n\nIn short: what you write in the Introduction and Method sections will have function in making the results and Discussion effective.\nWe expect students to use one of the analysis methods taught in the module.\n\n\n\n\n\n\nTip\n\n\n\nMarks will be awarded depending:\n\non how appropriate the method is to the context, to the study design, to answering the research question, and to the features of the data;\non how effectively the analysis is explained – students must explain the motivations for their decisions, explain their methods, and explain their findings effectively to gain points.\n\n\n\nOf course, some interests, datasets, or questions warrant the use of methods that are not taught but that students can learn independently.\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to use a method that has not been taught in the module then you must discuss this first with Rob Davies."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/what.html#how-should-reports-be-written",
    "href": "PSYC402/extra_ToBeOrganised/what.html#how-should-reports-be-written",
    "title": "What",
    "section": "",
    "text": "You will prepare your report in the style of a short research article in the journal Psychological Science. You can view examples of articles here.\nFor the journal Psychological Science, a research article has to be written in a specific format with specific properties: we want something a bit different for this coursework, and we specify what we want here.\nWe require information about the background for the study, the research question, the hypotheses, the method of data collection, the method of data analysis, the results of the analysis, and the psychological interpretation of those results.\n\n\n\n\n\n\nImportant\n\n\n\n\nWord count limit: no more than 2000 words are allowed for all materials.\nThe reports should include abstract, introduction, methods, results, discussion and references sections, like most psychological research articles.\nWrite in complete sentences in full paragraphs."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/what.html#what-structure-should-reports-take",
    "href": "PSYC402/extra_ToBeOrganised/what.html#what-structure-should-reports-take",
    "title": "What",
    "section": "",
    "text": "The reports should include Abstract, Introduction, Methods, Results, Discussion and References sections, like a short research article in the journal Psychological Science.\nThe focus of marking will be on the quality of the Results and Discussion sections. Nevertheless, as explained, the Introduction and Method sections need to be effective so that the Results and Discussion sections can be effective.\nOne way to check that what you say in earlier sections is sufficient to prepare the reader for what they encounter in later sections is to write forwards and then review backwards through the text of your report.\n\n\n\n\n\n\nTip\n\n\n\n\nWrite forwards: enter information, as appropriate, progressing from Introduction to Methods to Results to Discussion.\nThen, in review, work backwards:\n\n\nCan the reader understand the Discussion given what was presented in the Results?\nCan they understand the analysis, what you did, why and how, given the information in the Method?\nCan they understand the Method, what the measures are, what was done, and why, given the motivations and aims explained in the Introduction?\n\n\n\n\n\nSome general advice on what you need to communicate in the Introduction and Methods sections is that what you write in these sections should provide the reader with the information they need to answer the following questions:\n\nIntroduction\n\n\nWhat did the researchers do and why did the researchers do it?\nWhat was the question addressed in the study and why is it interesting?\nWhat were the hypotheses or predictions?\nWhat results were expected and how would they relate to the hypotheses?\n\n\nMethod\n\n\nWhat was done to collect the data?\nWho were tested (Participants)?\nWhat materials were used in testing (Materials)?\nWhat was the design of the study?\nWhat procedure was used?\n\nYou will need to rely on your experience reading primary research articles to help you to judge how much information you need to present and when you need to present it.\n\n\n\n\n\n\nTip\n\n\n\nIf you are unsure, pick an article you admire in a journal and analyze how they present information, what they explain, and to what level of detail.\n\n\n\n\n\nYou must write text that explains to the reader what analysis you did, why you did it, what results you found, and what you think the implications of those results may be.\n\n\n\n\n\n\nTip\n\n\n\nBe clear on:\n\nwhat the outcome measure or dependent variable for analysis was,\nand what factors or predictor variables were brought into the analysis of that outcome\n\n\n\nYou need to be clear about these things because the reader needs to understand these things well to be able to understand your results reporting.\nYou need to ensure that the text you write for the Results section provides the reader with the information they need to answer the following questions:-\n\nResults\n\n\nWhat hypotheses were tested?\nWhat methods were used to test the hypotheses?\nWhy are they appropriate?\nWhat were the results? What were the direction and relative size of effects?\n\nStudents often ask what analysis methods they can use. The answer to that question depends on the question or hypothesis, the nature of the dataset, and the appropriateness of the different analysis options.\nYou can read a discuss of the nature of the choices open to any data analysis job in ?@sec-multiverse. You can read about ways to identify how you can approach the analysis task in ?@sec-whatanalysis or ?@sec-try-multiverse.\n\n\n\n\n\n\nTip\n\n\n\n\nDo what seems reasonable using one or more of the analysis methods practiced in class\nAnd explain your reasoning\n\n\n\nIf you use a dataset that is already published in a journal such as Psychological Science, then your presentation of the results may differ from the presentation in the article in ways that highlight new features of the data. You may wish to identify the specific difference between the analysis you do and the analysis that was reported in the article that is the source for your data, if you derive your data from a published article, and if that article includes a statistical analysis.\nIt will help the reader and, sometimes, you if you indicate the source of the data and share your analysis code directly.\n\n\n\n\n\n\nTip\n\n\n\n\nIf you get your data from a published article then you will cite that article but it may be helpful if, in the Method section, you add a link to the data repository (e.g. on OSF) so that your reader can access the data if necessary.\nPut the code for the analysis you do and report, with comments, in an Appendix. The appendix will not be counted towards word count limits.\n\n\n\nBy the time the reader gets to the Discussion, they should be prepared for your outline of the questions or predictions, your identification of the main findings, and your critical evaluation of those findings, in context, and in relation to theoretical or practical implications.\nYou need to ensure that the text you write for the Discussion section enables the reader to understand the answer to the following questions:\n\nDiscussion\n\n\nWhat are the theoretical implications of the study findings?\nWhat are the practical implications?\n\nYou will understand that not every dataset can be analyzed in ways that have either or both theoretical or practical implications.\nThe main point is that you should keep in mind what the reader should get out of (what benefit) reading your report.\n\n\n\n\n\n\nTip\n\n\n\nOne way to think about the task for the Discussion is that you need to make sense of the results for the reader.\n\n\nYou will want to:\n\nlocate the ways in which your results fit into a wider (research or practice or theoretical) context;\nexplain how the results do or do not add to or change understanding;\nexplain if there are limitations in the data (maybe inherited from the source) or analysis;\nexplain what future research may usefully do;\nidentify the conclusions that can be drawn from the report.\n\n\n\n\n\n\n\nTip\n\n\n\nAny or every analysis will have limitations so you would be well advised to be selective and constructive:\n\nselective: are the limitations you identify likely to affect or bias results?\nconstructive: in what ways would practicable future research address these limitations?\n\n\n\nExplaining implications or limitations is not a box to tick in this exercise so that you can say you have met a criterion for assessment. This is an opportunity for you to work in a scholarly way and for you to present your work in a scholarly way. This will require reflection, critical evaluation, and a grasp of elements of the relevant literature."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/what.html#summary",
    "href": "PSYC402/extra_ToBeOrganised/what.html#summary",
    "title": "What",
    "section": "",
    "text": "Reports should present enough information that the reader can understand:\n\nthe background and motivation for a study;\nthe features of the data analyzed and the methods of data collection;\nthe approach taken in analysis, the analysis steps, and the results;\nthe relationship between the observed results and the expected results, and the interpretation of findings in relation to previous work.\n\n\n\n\n\n\n\nTip\n\n\n\nTo be clear about clarity:\n\nexplain, spell things out (decisions, reasoning, interpretations) as if you were explaining them to a reasonably intelligent reader, a Psychologist who is not a specialist in the area of study occupied by the study reported, i.e. me."
  },
  {
    "objectID": "PSYC402/extra_ToBeOrganised/what.html#what-format-is-required",
    "href": "PSYC402/extra_ToBeOrganised/what.html#what-format-is-required",
    "title": "What",
    "section": "",
    "text": "For general APA formatting of reports:\nhttps://owl.purdue.edu/owl/research_and_citation/apa_style/apa_style_introduction.html\nAnd for APA formatting of statistics and numbers:\nhttps://owl.purdue.edu/owl/research_and_citation/apa_style/apa_formatting_and_style_guide/apa_numbers_statistics.html\nThough the APA guidelines are the authoritative guide."
  },
  {
    "objectID": "PSYC402/Week18.html",
    "href": "PSYC402/Week18.html",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "Linear mixed-effects models are important, interesting, and sometimes challenging. We have worked through two chapters (see ?@sec-intro-multilevel, ?@sec-intro-mixed) in which we have aimed to learn:\n\nTo recognize the situations where we shall see multilevel structured data and therefore where we will need to apply multilevel or mixed-effects models.\nTo understand the nature and the advantages of these models: what they are, and why they work better than other kinds of models, given multilevel data.\nTo practice how we code for mixed-effects models, and how we read or write about the results.\n\nWe now need to develop our understanding and skills further. And we now need to examine some of the complexities that we may face when we work with mixed-effects models.\nOur approach will continue to depend on verbal explanation, visualization and a practical code-based approach to the modeling.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nShrinkage or regularization means that models of data should be excited by the data but not too excited.\n\n\nThis means our models work better if they are informed by all the data, and take into account random differences but also if they are not too strongly influenced by individual (participant or item) data.\n\n\n\nWe are probably now at a stage, in the development of our skills and understanding, where we can be more specific about our targets for learning: what capacities or abilities we want to have by the time we complete the course. I have held back specifying the targets in this way because, first, we had to learn the basic vocabulary. Now that we have done that, we can lay out the targets against which we can assess the progression of our learning.\n\n\n\n\n\n\nImportant\n\n\n\nWe have three capacities we seek to develop. These include the capacity:\n\nto understand mixed-effects models;\nto work with these models practically or efficiently in R;\nand to communicate their results effectively (to ourselves and others).\n\n\n\nWe should be aware that the development of skills and understanding in relation to each of these capacities will travel at different speeds for different people, and within any person at different speeds for different capacities.\nWe should also be aware that our internal evaluation of our understanding will not exactly match the evaluation that comes from external assessment. In other words, we might not be satisfied with our understanding but, still, our understanding might be satisfactory. It might be that we can learn to say in words what mixed-effects models are or involve, or what their results mean, very effectively even if we remain unsure about our understanding.\nFor these reasons, I specify what we are aiming to develop in terms of what we can do. You can test your development against this checklist of targets for learning.\n\nWe want to develop the capacity to understand mixed-effects models, the capacity to:\n\n\nrecognize where data have a multilevel structure;\nrecognize where multilevel or mixed-effects models are required;\ndistinguish the elements of a mixed-effects model, including fixed effects and random effects;\nexplain how random effects can be understood in terms of random differences (or deviations) between groups or classes or individuals, in intercepts or slopes;\nexplain how random effects can be understood in terms of variances, as a means to account for random differences between groups or classes or individuals in intercepts or slopes;\nexplain how mixed-effects models work better than linear models, for multilevel structured data;\nexplain how mixed-effects models work better because they allow partial-pooling of estimates.\n\n\nWe want to develop the capacity to work practically with mixed-effects models in R, the capacity to:\n\n\nspecify a mixed-effects model in lmer() code;\nidentify how the mixed-effects model code varies, depending on the kinds of random effects that are assumed;\nidentify the elements of the output or results that come from an lmer() mixed-effects analysis;\ninterpret the fixed-effects estimates;\ninterpret the random effects estimates, including both the variance and covariance estimates.\n\n\nWe want to develop the capacity to communicate the results of mixed-effects models effectively, to ourselves and to others, the capacity to:\n\n\ndescribe in words and summary tables the results of a mixed-effects model;\nvisualize the effects estimates or predictions from a mixed-effects model.\n\n\n\n\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 13 minutes\nPart 2 – about 13 minutes\nPart 3 – about 24 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter. The lectures provide a summary of the main points.\n2. Chapter: 03-mixed\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data with crossed random effects.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example ML word recognition study dataset.\nEdit example code to create alternate visualizations of variable distributions and of the relationships between critical variables.\nExperiment with the .R code used to work with the example data.\nRun linear mixed-effects models of demonstration data.\nRun linear mixed-effects models of alternate data sets.\nReview the recommended readings (Section 1.16).\n\n3. Practical workbook materials\n3.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning.\n\n\n\nIn this chapter, we will be working with the ML word recognition study dataset. ML examined visual word recognition in younger and older adults using the lexical decision task.\nIn lexical decision, participants are presented with a stimulus: a string of letters that is either a real word (e.g., ‘car’) or a made-up or non-word (e.g., ‘cas’). Participants are required to respond to the stimulus by pressing a button to indicate either that they think the stimulus is a word or that they think it is a non-word. Each complete sequence of events, in which a stimulus is presented and a response is recorded, is known as a trial. In the lexical decision task implemented by ML, all study participants were presented with a mix of 160 word stimuli and 160 non-word stimuli, in random order, in a total of 320 trials.\nEach stimulus was presented one at a time on the computer screen. The critical outcome measure was the reaction time (RT) or latency for each response. Observed RT represents the interval of time from the moment the stimulus was first presented (the stimulus onset) to the moment the response was made (the response onset).\nLexical decision is a very popular technique for examining word recognition, especially in adults. While not every graduate student will be interested in word recognition, or reading, everyone should understand that tasks like lexical decision are similar to a range of other tasks used in experimental psychological science.\nThe critical feature of the study, here, is that we have an outcome – a decision response – observed multiple times (for each stimulus) for each participant. We shall be analyzing the speed of response, reaction time (RT), measured in milliseconds (ms).\nIn our analyses, the focus of our interest will be on the ways in which participant attributes (like age) or word properties (like frequency) influence the speed of response in a task designed measure the ability to recognize visually presented English words. In analyzing the effects of participant attributes on recognition response RTs, we will use data – about those attributes – that were recorded using a mix of survey questions (about age, etc.) and standardized ability tests that were administered to study participants alongside the lexical decision task.\nThe total number of participants for this study was 39, including a group of younger adults and a group of older adults. Information was collected about the participants’ age, education and gender. In addition, participants were asked to complete ability measures (TOWRE sight word and phonemic tests, @torgesen1999towre) and a measure of reading experience (Author Recognition Test, ART, @Masterson2007c).\n\n\nInstead of posing a simple and general research question, we shall orient our work around a set of quite specific predictions. ML hypothesized:\n\nEffects of stimulus attributes\n\n\nPredicting that words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\n\n\nEffects of participant attributes\n\n\nPredicting that older readers would be faster and more accurate than younger readers in word recognition.\n\n\nEffects of interactions between the effects of word attributes and person attributes.\n\n\nPredicting that better (older) readers will show smaller effects of word attributes.\n\nIn this chapter, we can focus on one specific prediction as we work through the practical steps of conducting an analysis using linear mixed-effects models.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\n\n\n\n\n\n\nIn summary, ML collected data on lexical decision task response reaction times (RTs) and accuracy and information on participants, including age, reading ability and reading experience. In addition, she collected information on the properties of the lexical decision stimulus items, including variables like the length or frequency of words (values taken from the English Lexicon Project, @Balota2007).\nThe ML study data includes the following variables that we will work with (as well as some you can ignore):\n\nIdentifying variables\n\n\nsubjectID – identifying code for participants\nitem_name – words presented as stimuli\nitem_number – identifying code for words presented\n\n\nResponse variables\n\n\nRT – response reaction time (ms), for responses to words\n\n\nParticipant attribute variables\n\n\nAge – in years\nGender – coded M (male), F (female)\nTOWRE_wordacc – word reading skill, words read correctly (out of 104)\nTOWRE_nonwordacc – nonword reading skill, nonwords (made up words) read correctly (out of 63)\nART_HRminusFR – reading experience score\n\n\nStimulus property variables\n\n\nLength – word length, in letters\nOrtho_N – orthographic neighbourhood size, how many other words in English a stimulus word looks like\nOLD – orthographic Levenshtein distance, how many letter edits (addition, deletion or substitution) it would take to make a stimulus word look like another English word (a measure of orthographic neighbourhood) [@Yarkoni2008b]\nBG_Sum, BG_Mean, BG_Freq_By_Pos – measures of how common are pairs of letters that compose stimulus words\nSUBTLWF, LgSUBTLWF, SUBTLCD, LgSUBTLCD – measures of how common stimulus words are, taken from the SUBTLEX corpus analysis of word frequency [@Brysbaert2009a]\n\n\n\n\nYou can download the data-03-mixed.zip files folder to get the data you need for the practical work we will be doing for this chapter.\nThe data are held in one file:\n\nsubjects.behaviour.words-310114.csv which holds information about the (word) stimuli, participants, and the responses recorded in the ML study.\n\nThe .csv file is a comma separated values file and can be opened in Excel.\nThe data file is collected together with the .R scripts:\n\n03-mixed-workbook.R the workbook you will need to do the practical exercises.\n03-mixed-workbook-answers.R with answers to questions and code for exercises.\n\n\n\n\n\nIn ?@sec-intro-mixed-tidy-data, we saw how we may need to tidy the data we collect in experimental studies: combining data about responses with data about participant attributes or stimulus properties, and restructuring the data so that they are in a tidy format. For this class, many steps in the process of data tidying were completed previously. Thus, we only need to perform steps 1, 3 and 4 of the usual data tidying process:\n\nImport the data or read the data into R, see Section 1.6.1;\nRestructure the data;\nSelect or transform variables, see Section 1.6.4;\nFilter observations, see Section 1.6.3.\n\nWe are going to first filter the observations, then transform the outcome variable. We will explain why we have to do this as we proceed.\nWe will use tidyverse library functions to do this work, as usual.\n\nlibrary(tidyverse)\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read one file into R.\n\nML.all &lt;- read_csv(\"subjects.behaviour.words-310114.csv\", na = \"-999\")\n\nThe data file subjects.behaviour.words-310114.csv holds all the data about everything (behaviour, participants, stimuli) we need for our analysis work.\n\n\n\n\n\n\nTip\n\n\n\nIt is always a good idea to first inspect what you have got when you read a data file into R before you do anything more demanding.\n\nYou cannot assume that the data are what you think they are\nor that the data are structured or coded in the ways that you think (or have been told) they should be structured or coded.\n\n\n\nYou can inspect the first few rows of the dataset using head().\n\n\nWarning in subjectID == c(\"GB9\", \"NH1\", \"A15\", \"B18\"): longer object length is\nnot a multiple of shorter object length\n\n\n\n\n\nitem_number\nsubjectID\nTest\nAge\nYears_in_education\nGender\nTOWRE_wordacc\nTOWRE_nonwordacc\nART_HRminusFR\nRT\nCOT\nSubject\nTrial.order\nitem_name\nLength\nOrtho_N\nBG_Sum\nBG_Mean\nBG_Freq_By_Pos\nitem_type\nSUBTLWF\nLgSUBTLWF\nSUBTLCD\nLgSUBTLCD\nOLD\n\n\n\n\n1\nGB9\nALT\n21\n11\nF\n78\n41\n18\n368.66\n134057.8\nGB9\n54\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nNH1\nTAL\n52\n18\nM\n78\n56\n33\n724.83\n742737.4\nNH1\n148\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nA15\nLTA\n21\n16\nF\n95\n57\n9\n483.71\n861801.0\nA15\n278\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nB18\nTLA\n69\n11\nM\n85\n54\n10\n517.62\n1024583.4\nb18\n318\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n\n\n\n\n\nYou can examine all the variables using summary().\n\nsummary(ML.all)\n\n  item_number      subjectID             Test                Age       \n Min.   :  1.00   Length:5440        Length:5440        Min.   :16.00  \n 1st Qu.: 40.75   Class :character   Class :character   1st Qu.:21.00  \n Median : 80.50   Mode  :character   Mode  :character   Median :21.00  \n Mean   : 80.50                                         Mean   :36.94  \n 3rd Qu.:120.25                                         3rd Qu.:53.00  \n Max.   :160.00                                         Max.   :73.00  \n Years_in_education    Gender          TOWRE_wordacc    TOWRE_nonwordacc\n Min.   :11.00      Length:5440        Min.   : 68.00   Min.   :16.00   \n 1st Qu.:13.00      Class :character   1st Qu.: 84.00   1st Qu.:50.00   \n Median :16.00      Mode  :character   Median : 93.00   Median :55.50   \n Mean   :14.94                         Mean   : 91.24   Mean   :52.41   \n 3rd Qu.:16.00                         3rd Qu.: 98.00   3rd Qu.:57.00   \n Max.   :19.00                         Max.   :104.00   Max.   :63.00   \n ART_HRminusFR         RT               COT            Subject         \n Min.   : 1.00   Min.   :-2000.0   Min.   :  50094   Length:5440       \n 1st Qu.: 7.00   1st Qu.:  498.1   1st Qu.: 297205   Class :character  \n Median :11.00   Median :  577.6   Median : 552854   Mode  :character  \n Mean   :15.15   Mean   :  565.3   Mean   : 575780                     \n 3rd Qu.:21.00   3rd Qu.:  677.4   3rd Qu.: 810108                     \n Max.   :43.00   Max.   : 1978.4   Max.   :1583651                     \n  Trial.order     item_name             Length       Ortho_N      \n Min.   : 21.0   Length:5440        Min.   :3.0   Min.   : 0.000  \n 1st Qu.:100.8   Class :character   1st Qu.:4.0   1st Qu.: 3.000  \n Median :180.5   Mode  :character   Median :4.0   Median : 6.000  \n Mean   :180.5                      Mean   :4.3   Mean   : 7.069  \n 3rd Qu.:260.2                      3rd Qu.:5.0   3rd Qu.:11.000  \n Max.   :340.0                      Max.   :6.0   Max.   :24.000  \n     BG_Sum          BG_Mean       BG_Freq_By_Pos   item_type        \n Min.   :  3.00   Min.   :  1.00   Min.   :  1.0   Length:5440       \n 1st Qu.: 81.75   1st Qu.: 67.75   1st Qu.: 74.5   Class :character  \n Median :151.50   Median :153.50   Median :158.0   Mode  :character  \n Mean   :155.89   Mean   :153.82   Mean   :149.6                     \n 3rd Qu.:234.75   3rd Qu.:239.25   3rd Qu.:227.0                     \n Max.   :314.00   Max.   :316.00   Max.   :295.0                     \n    SUBTLWF          LgSUBTLWF        SUBTLCD        LgSUBTLCD    \n Min.   :   0.57   Min.   :1.477   Min.   : 0.32   Min.   :1.447  \n 1st Qu.:  17.36   1st Qu.:2.947   1st Qu.: 6.67   1st Qu.:2.748  \n Median :  69.30   Median :3.549   Median :23.64   Median :3.298  \n Mean   : 442.01   Mean   :3.521   Mean   :36.52   Mean   :3.137  \n 3rd Qu.: 290.70   3rd Qu.:4.171   3rd Qu.:65.24   3rd Qu.:3.739  \n Max.   :6161.41   Max.   :5.497   Max.   :99.70   Max.   :3.922  \n      OLD       \n Min.   :1.000  \n 1st Qu.:1.288  \n Median :1.550  \n Mean   :1.512  \n 3rd Qu.:1.750  \n Max.   :2.050  \n\n\nThe summary shows some features of the dataset, or of how R interprets the dataset, that are of immediate interest to us, though we do not necessarily have to do anything about them.\n\nWe can see statistical summaries – showing the mean, median, minimum and maximum, etc. – of numeric variables like the outcome variable RT.\nWe can see statistical summaries, also, of variables that comprise number values but which we do not want to be treated as numbers, e.g., the word stimulus coding variable item_number.\nWe can see that some variables are simply listed as Class: character. That tells us that one or more values in the columns in the datasheet that correspond to these variables are words or strings of letters or alphanumeric characters.\nThere is no sign of the presence of missing values in this dataset, no counts of NAs.\n\nWe do not really want R to treat a coding variable like item_number as numeric: it functions as a categorical or nominal variable, a factor. And we want R to treat coding variables like subjectID as factors. In ?@sec-multi-data-coerce, we saw how we can require R to handle variables exactly as we require it to using coercion. In ?@sec-intro-mixed-data-load, we saw how we can determine how R treats variables at the read-in stage, using col_types() specification. We are going to do neither here because we do not have to do this work; not doing it will have no impact on our analyses at this point.\nWhat we do need to do is deal with a problem that is already apparent in the summary statistics – did you spot it? If we look at the summary, we can see that RT includes values as low as -2000. That cannot be right.\n\n\n\nWe should examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms). Observations about variable value distributions are a part of Exploratory Data Analysis and serve to catch errors in the dataset (e.g. incorrectly recorded scores) but also to inform the researcher’s understanding of their own data.\nWe shall examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms), using density plots. An alternative method would be to use histograms. I choose to use density plots because they allow the easy comparison of the distributions of values of a continuous numeric variable like reaction time. A density plot shows a curve. You can say that the density corresponds to the height of the curve for a given value of the variable being depicted, and that it is related to the probability of observing values of the variable within some range of values [@howell2016fundamental].\nGetting a density plot of RTs of responses is easy using ggplot() code.\n\nML.all %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) +\n  geom_rug(alpha = .2) +\n  ggtitle(\"Raw RT\") +\n  theme_bw()  \n\n\n\n\nFigure 1: Density plot showing word recognition reaction time, correct and incorrect responses\n\n\n\n\nThe code delivers a plot (Figure 1) showing three peaks in the distribution of RT values. You can see that there is a peak of RT observations around 500-1000ms, another smaller peak around -500ms, and a third smaller peak around -2000ms.\nThe density plot shows the reaction times recorded for participants’ button press ‘yes’ responses to word stimuli in the lexical decision task. The peaks of negative RTs represent observations that are impossible.\nRemember that reaction time, in a task like lexical decision, represents the interval in time between the onset of a task stimulus (in lexical decision, a word or a nonword) and the onset of the response (the button press to indicate the lexical decision). We cannot have negative time intervals. The explanation is that ML collected her data using the DMDX experimental software application [@Forster2003a]. DMDX records the reaction times for incorrect responses as negative RTs.\nThe code to produce Figure 1 works in a series of steps.\n\nML.all %&gt;% takes the dataset, from the ML study, that we have read in to the R workspace and pipes it to the visualization code, next.\nggplot(aes(x = RT)) + creates a plot object in which the x-axis variable is specified as RT. The values of this variable will be mapped to geometric objects, i.e. plot features, that you can see, next.\ngeom_density(size=1.5) + first displays the distribution of values in the variable RT as a density curve. The argument size=1.5 tells R to make the line \\(1.5 \\times\\) the thickness of the line used by default to show variation in density.\n\nSome further information is added to the plot, next.\n\ngeom_rug(alpha = .2) + with a command that tells R to add a rug plot below the density curve.\nggtitle(\"Raw RT\") makes a plot title.\n\nNotice that beneath the curve of the density plot, you can see a series of vertical lines. Each line represents the x-axis location of an RT observation in the ML study data set. This rug plot represents the distribution of RT observations in one dimension.\n\ngeom_rug() draws a vertical line at each location on the x-axis that we observe a value of the variable, RT, named in aes(x = RT).\ngeom_rug(alpha = .2) reduces the opacity of each line, using alpha, to ensure the reader can see how the RT observations are denser in some places than others.\n\nYou can see that we have many more observations of RTs from around 250ms to 1250ms, where the rug of lines is thickest, under the peak of the density plot. This indicates what the two kinds of plots are doing.\n\n\nYou should try out alternative visualisation methods to reveal the patterns in the distribution of variables in the ML dataset (or in your own data).\n\nTake a look at the geoms documented in the {ggplot2} library reference section here.\nExperiment with code to answer the following questions:\n\n\nWould a histogram or a frequency polygon provide a more informative view? Take a look here for advice.\nWhat about a dotplot? Take a look here for advice\n\n\n\n\n\nThe density plot shows us that the raw ML lexical decision RT variable includes negative RT values corresponding to incorrect response. These have to be removed. We can do this quite efficiently by creating a subset of the original “raw” data, defined according to the RT variable using the {dpyr} library filter() function.\n\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200)\n\nAfter we have removed negative (error) RTs, we check that the size of the dataset – here, the number of rows – matches our expectations. We do this to make sure that we did the filter operation correctly.\n\nlength(ML.all$RT)\n\n[1] 5440\n\nlength(ML.all.correct$RT)\n\n[1] 5257\n\n\nIf you run the length() function calls then you should see that the length or number of observations or rows in the ML.all.correct dataset should be smaller than the number of observations in the ML.all dataset.\n\n\n\n\n\n\nTip\n\n\n\nIt is wise to check that the operations you perform to tidy, process or wrangle data actually do do what you mean them to do. Checks can be performed, for each processing stage, by:\n\nForming expectations or predictions about what the operation is supposed to do e.g. filter out some rows by some number;\nCheck what you get against these predictions e.g. count the number of rows before versus after filtering.\n\n\n\nHaving obtained a new data frame with data on just those trials where responses were correct, we can plot the distribution of RTs for just the correct responses (Figure 2).\n\nML.all.correct %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct RTs\") +\n  theme_bw()\n\n\n\n\nFigure 2: Density plot showing word recognition reaction time, correct responses only\n\n\n\n\nThe filter code is written to subset the data by rows using a condition on the values of the RT variable.\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200) works as follows.\n\nML.all.correct &lt;- filter(ML.all ...) creates a new dataset with a new name ML.all.correct from the old dataset ML.all using the filter() function.\nfilter(... RT &gt;= 200) specifies an argument for the filter() function.\n\nIn effect, we are asking R to check every value in the RT column.\n\nR will do a check through the ML.all dataset, row by row.\nIf a row includes an RT that is greater than or equal to 200 then that row will be included in the new dataset ML.all.correct. This is what I mean by using a condition.\nBut if a row includes an RT that is less than 200, then that row will not be included. We express this condition as RT &gt;= 200.\n\nThe length() function will count the elements in whatever object is specified as an argument in the function call.\n\nThis means that if you put a variable name into the function as in length(dataset$variable) it will count how long that variable is – how many rows there are in the column.\nIf that variable happens to be, as here, part of a dataset, the same calculation will tell you how many rows there are in the dataset as a whole.\nIf you just enter length(dataset), naming some dataset, then the function will return a count of the number of columns in the dataset.\n\n\n\nVary the filter conditions in different ways.\n\nChange the threshold for including RTs from RT &gt;= 200 to something else: you can change the number, or you can change the operator from &gt;= to a different comparison (try =, &lt;, &lt;=, &gt;.\nCan you assess what impact the change has?\n\nNote that you can count the number of observations (rows) in a dataset using e.g. length().\n\n\n\nI choose to filter out or exclude not only error responses (where \\(RT &lt; 0ms\\)) but also short reaction times (where \\(RT &lt; 200ms\\)). I think that any response in the lexical decision task that is recorded as less than 200ms cannot possibly represent a real word recognition response. Participants who complete experimental psychological tasks can and do press the button before they have time to engage the psychological processes (like word recognition) that the tasks we administer are designed to probe (like lexical decision).\nThere is some relevant literature that concerns the speed at which neural word recognition processes operate. However, I think you should note that the threshold I am setting for exclusion, here, is essentially arbitrary. If you think about it, I could have set the threshold at any number from \\(100-300ms\\) or some other range.\n\n\n\n\n\n\nWarning\n\n\n\nWhat is guiding me in setting the filter threshold is experience. But other researchers will have different experiences and set different thresholds.\n\nThis is why using exclusion criteria to remove data is problematic.\n\n\n\nFiltering or re-coding observations is an important element of the research workflow in psychological science. How we do or do not remove observations from original data may have an impact on our results (as explored by @steegen2016). It is important, therefore, that we learn how to do this reproducibly using, for example, R scripts that we can share with our research reports.\nI would argue that, at minimum, a researcher should report their research including:\n\nWhat exclusion criteria they use to remove data, explaining why.\nReport analyses with and without exclusions, to indicate if their results are sensitive to their decisions.\n\nYou can read further information about the practicalities of using R to do filtering here.\nYou can read a brief discussion of the impacts of researcher choices in dataset construction in ?@sec-multiversedata and in @steegen2016.\n\n\n\n\nFigure 2 shows that we have successfully removed all errors (negative RTs) but now we see just how skewed the RT distribution is. Note the long tail of longer RTs.\nMost researchers assume that participants – healthy young adults – take about 500-1000ms to perform the task and that values outside that range correspond to either fast guesses (RTs that are too short) or to distracted or tired or bored responses (RTs that are too long). In theory, the lexical decision task should be probing automatic cognitive processes, measuring the steps from perception to visual word recognition in the time interval between the moment the stimulus is first shown and the moment the button is pressed by the participant to indicate a response. Thus, it might seem natural to exclude extreme RT values which might correspond not to automatic cognitive processes but to unknowable distraction events or boredom and inattention. However, we shall complete no further data exclusions.\nFor now, we can look at a commonly used method to deal with the skew that we typically see when we examine reaction time distributions. RT distributions are usually skewed with a long tail of longer RTs. You can always take longer to press the button but there is a limit to how much faster you can make your response.\nGenerally, we assume that departures from a model’s predictions about our observations (the linear model residuals) are normally distributed, and we often assume that the relationship between outcome and predictor variables is linear [@Cohen2003c]. We can ensure that our data are compliant with both assumptions by transforming the RT distribution.\nIt is not cheating to transform variables. Transformations of data variables can be helpful for a variety of reasons in the analysis of psychological data [@Cohen2003c; @Gelman2007ga]. I do recommend, however, that you are careful to report what transformations you use, and why you do them.\nPsychology researchers often take the log (often the log base 10) of RT values before performing an analysis. Transforming RTs to the log base 10 of RT values has the effect of correcting the skew – bringing the larger RTs ‘closer’ (e.g., \\(1000 = 3\\) in log10) to those near the middle which do not change as much (e.g. \\(500 = 2.7\\) in log10).\n\nML.all.correct$logrt &lt;- log10(ML.all.correct$RT)            \n\nWe can see the effect of the transformation if we plot the log10 transformed RTs (see Figure 3). We arrive at a distribution that more closely approximates the normal distribution.\n\nML.all.correct %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct log10 RTs\") +\n  theme_bw()\n\n\n\n\nFigure 3: Density plot showing log10 transformed reaction time, correct responses only\n\n\n\n\nThe log10() function works as follows:-\n\nML.all.correct$logrt &lt;- log10(...) creates a a new variable logrt, adding it to the ML.all.correct dataset. The variable is created using the transformation function log10().\nlog10(ML.all.correct$RT) creates a the new variable by transforming (to log10) the values of the old variable, RT.\n\n\n\n\n\n\n\nTip\n\n\n\nThere are other log transformation functions and we often see researchers using the natural log instead of the log base 10 as discussed here\n\n\n\n\n\nEven when data have been structured appropriately, we will still, often, need to do some tidying before we can do an analysis. Most research work involving quantitative evidence requires a big chunk of data tidying or other processing before you get to the statistics.\nOur data are now ready for analysis.\n\n\n\n\nAs we saw in ?@sec-intro-mixed, many Psychologists conduct studies where it is not sensible to think of observations as being nested [@baayen2008]. In this chapter, we turn to the ML word recognition study dataset, which has a structure similar to the CP study data that we worked with previously. Again, the core concern is that the data come from a study with a repeated-measures design where the experimenter presented multiple stimuli for response to each participant, for several participants, so that we have multiple observations for each participant and multiple observations for each stimulus. Getting practice with this kind of data will help you to easily recognize what you have got when you see it in your own work.\nML asked all participants in a sample of people to read a selection of words, a sample of words from the language.\nFor each participant, we will have multiple observations and these observations will not be independent. One participant will tend to be slower or less accurate compared to another. Her responses may be more or less susceptible to the effects of the experimental variables. The lowest trial-level observations can be grouped with respect to participants. However, the data can also be grouped by stimuli.\nFor each stimulus word, there are multiple observations and these observations will not be independent. One stimulus may prove to be more challenging to all participants compared to another, eliciting slower or less accurate responses on average. In addition, if there are within-items effects, we may ask if the impact of those within-items effects is more prominent, stronger, among responses to some items compared to others.\nGiven this common repeated-measures design, we can analyse the outcome variable in relation to:\n\nfixed effects: the impact of independent variables like participant reading skill or word frequency;\nrandom effects: the impact of random or unexplained differences between participants and also between stimuli.\n\n\n\n\nWe are going to respond to the multilevel (or crossed random effects) structure in the data by using linear mixed-effects models to analyze the data. This week, we are going to look at what mixed-effects models do from a new perspective.\nOur concern will be with different ways of thinking about why mixed-effects models are superior to linear models where data have a multilevel structure. Mixed-effects models tend to be more accurate in this (very common) situation because of what is called partial pooling and shrinkage or regularization. We use our practical example to explore these ideas.\n\n\nTo get started, we can examine – for each individual separately – the distribution of log RT observations, in Figure 4.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\n\n\n\nFigure 4: Density plot showing log10 transformed reaction time, correct responses, separately for each participant\n\n\n\n\nFigure 4 shows that RT distributions vary considerably between people. The plot imposes a dashed red line to indicate where the mean log10 RT is, calculated over all observations in the dataset. The plot shows the distribution of log RT for each participant, as a density drawn separately for each person. The individual plots are ordered by the mean log RT calculated per person, so plots appear in order from the fastest to the slowest.\nThe grid of plots illustrates some interesting features about the data in the ML study sample. You can see how the distribution of log RT varies between individuals: some people show widely spread reaction times; some people show quite tight or narrow distributions. You can see how the shapes of the distributions varies: some people show skew; others do not. I do not see that the variation in the shapes of the distributions is related to the average speed of the person’s responses.\nI think the key message of the plot is that some distributions are wider (RTs are more spread out) than others. We might be concerned that people who present more variable reaction times (wider distributions) may be associated with less reliable estimates of their average response speed, or of the impact of word attributes (like word frequency) on their response speed.\n\n\nThe plotting code I used to produce Figure 4 progresses through a series of steps. This example demonstrates how you can combine data tidying and plotting steps in a single sequence, using tidyverse functions and the %&gt;% pipe, so I will take the time to explain what is going on.\nMy aim is to create a grid of individual plots, showing the distribution of log RTs for each participant, so that the plots are presented in order, from the fastest participant to the slowest. Take a look at the plotting code. We can explain how it works, step by step.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\nYou will see that we present the distribution of RTs using geom_density() and that we present a separate plot for each person’s data using facet_wrap(). To these elements, we add some pre-processing steps to calculate the average response speed of each individual, and to reorder the dataset by those averages.\nIt will make it easier to understand what is going on if we consider the code in chunks.\nFirst, we pre-process the data before we feed it into the plotting code.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ...\n\n\nML.all.correct %&gt;% takes the selected filtered dataset ML.all.correct and pipes it %&gt;% to the next step.\ngroup_by(subjectID) %&gt;% tells R to group the data by subject ID. We have a set of multiple log RT observations for each subjectID because each participant was asked to respond to multiple word stimuli.\nmutate(mean_logrt = mean(logrt, na.rm = TRUE)) next calculates and stores the mean log RT for each person. We create a new variable mean_logrt. We calculate the average of the set of log RTs recorded for each subjectID and construct the new variable mean_logrt from these averages.\n\nWe do not need to treat the data in groups so we remove the grouping, next.\n\nUsing ungroup() %&gt;% means that, having grouped the data to calculate the mean log RTs, we ungroup the dataset so that R can look at all observations in the next step.\nmutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;% asks R to look at all log RT observations in the dataset, and change the top-to-bottom order of the rows.\n\nWe ask R to order observations – using subjectID in fct_reorder() – so that each person’s data are listed by their average speed, mean_logrt from the fastest to the slowest. We then pipe these ordered data to the plotting code, next.\nIf you delete or comment out these first lines, you will see that R uses just a default ordering, drawing the plot for each person in the alphabetical order of their subjectID codes.\nTry it. Don’t forget to start with ML.all.correct %&gt;%.\nSecond, we draw the plots, using the data we have pre-processed.\n\nML.all.correct %&gt;%\n...\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n...\n\nThe key functions that create a grid of density plots are the following.\n\nggplot(aes(x = logrt)) tells R to work with logrt as the x-axis variable. We shall be plotting the distribution of logrt.\ngeom_density(...) draws a density plot to show the distribution of log RT, using a thicker line size = 1.25\nfacet_wrap(~ subjectID) creates a different plot for each level of the subjectID factor: we want to see a separate plot for each participant.\n\n\nfacet_wrap(~ subjectID) works to split the dataset up by participant, with observations corresponding to each participant identified by their subjectID, and to then split the plotting to show the distribution of log RT separately for each participant.\n\nI wanted to present the plots in order of the average speed of response of participants. If you look at Figure 4, you can see that the position of the peak of the log RT distribution for each participant moves, from the fastest plots where the peak is around \\(log RT = 2.5\\) (shown from the top left of the grid), to the slowest plots where the peak is around \\(log RT = 2.75\\) (shown towards the bottom right of the grid)\nWe can then use further ggplot functions to edit the appearance of the plot, to make it more useful.\n\n...\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\n\ngeom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) draws a vertical red dashed line at the location of the mean log RT, the average of all log RTs over all participants in the dataset.\nscale_x_continuous(breaks = c(2.5,3)) adjusts the x-axis labeling. The ggplot default might draw too many x-axis labels i.e. showing possible log RT values as tick marks on the bottom line of the plot. I want to avoid this as sometimes all the labels can be crowded together, making them harder to read.\n\n\nDrawing a vertical line at the mean calculated overall is designed to help the reader (you) calibrate their comparison of the data from different people.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is often useful to experiment with example code to figure out how it works.\n\n\nOne way you can do this is by commenting out one line of code, at a time by putting the # at the start of the line.\n\nIf you do this, you can see what the line of code does by, effectively, asking R to ignore it.\n\nAnother way you can experiment with code is by seeing what you can change and what effect the changes have.\n\nCan you work out how to adapt the plotting code to show a grid of histograms instead of density plots?\nCan you work out how to adapt the code to show a grid of plots indicating the distribution of log RT by different words instead of participants?\n\n\n\n\n\nAs we have discussed in previous chapters, a good way to approach a mixed-effects analysis is by first estimating the effects of the experimental variables (here, frequency) using linear models, ignoring the hierarchical structure in the data.\n\n\n\n\n\n\nNote\n\n\n\nA linear model of multilevel structured data can be regarded as an approximation to the better analysis.\n\n\nWe model the effects of interest, using all the data (hence, complete pooling) but ignoring the differences between participants. This means we can see something of the ‘true’ picture of our data through the linear model results but the linear model misses important information, which the mixed-effects model will include, that would improve its performance.\nAs we saw, in a similar analysis in ?@sec-intro-mixed, we can estimate the relationship between reading reaction times (here, lexical decision RTs) and word frequency using a linear model:\n\\[\nY_{ij} = \\beta_0 + \\beta_1X_j + e_{ij}\n\\]\nWhere:\n\n\\(Y_{ij}\\) is the value of the observed outcome variable, the log RT of the response made by the \\(i\\) participant to the \\(j\\) item;\n\\(\\beta_1X_j\\) refers to the fixed effect of the explanatory variable (here, word frequency), where the frequency value \\(X_j\\) is different for different words \\(j\\), and \\(\\beta_1\\) is the estimated coefficient of the effect due to the relationship between response speed and word frequency;\n\\(e_{ij}\\) is the residual error term, representing the differences between observed \\(Y_{ij}\\) and predicted values (given the model) for each response made by the \\(i\\) participant to the \\(j\\) item.\n\nThe linear model is fit in R using the lm() function.\n\nML.all.correct.lm  &lt;- lm(logrt ~\n                             \n                             LgSUBTLCD,     \n                           \n                           data = ML.all.correct)\n\nsummary(ML.all.correct.lm)\n\n\nCall:\nlm(formula = logrt ~ LgSUBTLCD, data = ML.all.correct)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41677 -0.07083 -0.01163  0.05489  0.53411 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.885383   0.007117  405.41   &lt;2e-16 ***\nLgSUBTLCD   -0.033850   0.002209  -15.32   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1095 on 5255 degrees of freedom\nMultiple R-squared:  0.04277,   Adjusted R-squared:  0.04259 \nF-statistic: 234.8 on 1 and 5255 DF,  p-value: &lt; 2.2e-16\n\n\nIn the estimates from this linear model, we see an approximate first answer to our prediction.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\nResult: We can see that, in this first analysis, the estimated effect of word frequency is \\(\\beta = -0.033850\\).\n\n\n\nI know this looks like a very small number but you should realize that the estimates for the coefficients of fixed effects like the frequency effect are scaled according to the outcome. Here, the outcome is log10 RT, where a log10 RT of 3 equals 1000ms, and, as we can calculate in R\n\nlog10(0.925)\n\n[1] -0.03385827\n\n\nAlso, remember that frequency is scaled in logs too, so the estimate of the coefficient tells us how log10 RT changes for unit change in log frequency. The coefficient represents the estimated change in log10 RT for unit change in log frequency LgSUBTLCD.\nThe estimate indicates that as word log frequency increases, responses logRT decreases by \\(-0.033850\\).\nIn this model, all the information from all participants is analyzed. In discussions of mixed-effects analyses, we say that this is a complete pooling model. This is because all the data have been pooled together, that is, we use all observations in the sample to estimate the effect of frequency.\nIn this model, the observations are assumed to be independent. However, we suppose that the assumption of independence is questionable given the expectation that participants will differ in their overall speed, and in the extent to which their response speed is affected by factors like word frequency.\n\n\n\nVary the linear model using different outcomes or predictors.\n\n\nThe ML study data, like the CP study data, are rich with possibility. It would be useful to experiment with it.\n\n\nChange the predictor from frequency to something else: what do you see when you visualize the relationship between outcome and predictor variables using scatterplots?\nSpecify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?\n\nI would recommend that you both estimate the effects of variables and visualize the relationships between variables using scatterplots. If you combine reflection on the model estimates with evaluation of what the plots show you then you will be able to see how reading model results and reading plots can reveal the correspondences between the two ways of looking at your data.\n\n\n\n\nWe can examine variation between participants by analyzing the data for each participant’s responses separately, fitting a different linear model of the effect of word frequency on lexical decision RTs for each participant separately. Figure 5 presents a grid or trellis of plots, one plot per person. In each plot, you can see points corresponding to the log RT of the responses made by each participant to the stimulus words.\n\n\n\n\n\n\nTip\n\n\n\nIn working with R, we often benefit from the vast R knowledge ecosystem.\n\nI was able to produce the sequence of plots Figure 5, Figure 6, Figure 7 and Figure 8 thanks to this very helpful blog post by TJ Mahr\n\n\n\nIn all plots, the pink or red line represents the complete pooling model estimate of the effect of frequency on response RTs. The line is the same for each participant because there is only one estimated effect, based on all data for all participants.\nIn addition, in each plot, you can see a green line. You can see that the line varies between participants. This represents the effect of frequency estimated using just the data for each participant, analyzed separately. These are the no pooling estimates. We call them the no pooling estimates because each is based just on the data from one participant.\n\n\n\n\n\nFigure 5: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant; red-pink line shows the complete pooling estimate, blue-green line shows the no-pooling estimate\n\n\n\n\nFigure 5 reveals substantial differences between participants in both average response speed and the frequency effect.\nWe may further predict variation in standard errors between participants given, also, the differences between participants in the spread of log RT, illustrated by Figure 4. Basically, where the distribution of log RT is more widely spread out, for any one participant, there it will be harder for us to estimate with certainty the mean or the sources of variance for the participant’s response speed.\nYou will notice that the no pooling and complete pooling estimates tend to be quite similar. But for some participants – more than for others – there is variation between the estimates.\nYou can reflect that the complete pooling is unsatisfactory because it ignores the variation between the participants: some people are slower than others; some people do show a larger frequency effect than others. You can also reflect that the no pooling is unsatisfactory because it ignores the similarities between the participants.\nWhile there is variation between participants there is also similarity across the group so that the effect of frequency is similar between participants.\n\n\n\n\n\n\nImportant\n\n\n\nWhat we need is an analytic method that is capable of both estimating the overall average population-level effect (here, of word frequency) and taking into account the differences between sampling units (here, participants).\nThat method is linear mixed-effects modeling.\n\n\n\n\n\n\n\n\nAs you have seen before, we can account for the variation – the differences between participants in intercepts and slopes.\nFirst, we model the intercept as two terms:\n\\[\n\\beta_{0i} = \\gamma_0 + U_{0i}\n\\]\nWhere:\n\n\\(\\gamma_{0}\\) is the average intercept, and\n\\(u_{0i}\\) is the difference for each participant between their intercept and the average intercept.\n\nSecond, we can model the frequency effect as two terms:\n\\[\n\\beta_{1i} = \\gamma_1 + U_{1i}\n\\]\nWhere:\n\n\\(\\gamma_{10}\\) is the average slope, and:\n\\(U_{1i}\\) represents the difference for each participant between the slope of their frequency effect and the average slope.\n\nWe can then incorporate in a single model the fixed effects due to the average intercept and the average frequency effect, as well as the random effects – the error variance due to unexplained differences between participants in intercepts and in frequency effects:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n\\]\nWhere the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and differences between \\(i\\) participants in the intercept \\(U_{0i}\\);\nthe average effect of the explanatory variable frequency \\(\\gamma_1X_j\\) and differences between \\(i\\) participants in the slope \\(U_{1i}X_j\\);\nin addition to residual error variance \\(e_{ij}\\).\n\n\n\n\nAs we first saw in ?@sec-intro-mixed, in conducting mixed-effects analyses, we do not aim to examine the specific deviation (here, for each participant) from the average intercept or the average effect or slope. We estimate just the spread of deviations by-participants.\nA mixed-effects model like our final model actually includes fixed effects corresponding to the intercept and the slope of the word frequency effect plus the variances:\n\n\\(var(U_{0i})\\) variance of deviations by-participants from the average intercept;\n\\(var(U_{1i}X_j)\\) variance of deviations by-participants from the average slope of the frequency effect;\n\\(var(e_{ij})\\) residuals, at the response level, after taking into account all other terms.\n\nWe may expect the random effects of participants or items to covary: for example, participants who are slow to respond may also be more susceptible to the frequency effect. Thus our specification of the random effects of the model can incorporate terms corresponding to the covariance of random effects:\n\n\\(covar(U_{0i}, U_{1i}X_j)\\)\n\n\n\n\nAs we know, some words elicit slower and some elicit faster responses on average. As we discussed in the last chapter (?@sec-intro-mixed-fixed-effect-fallacy), if we did not take such variation into account, we might spuriously identify an experimental effect actually due just to unexplained between-items differences in intercepts [@clark1973; @raaijmakers1999] committing an error: the language as fixed effect fallacy.\nWe can model the random effect of items on intercepts by modeling the intercept as two terms:\n\\[\n\\beta_{0j} = \\gamma_0 + W_{0j}\n\\]\nWhere:\n\n\\(\\gamma_{0}\\) is the average intercept, and\n\\(W_{0j}\\) represents the deviation, for each word, between the average intercept and the per-word intercept.\n\nNote that I ignore the possibility, for now, of differences between items in the slopes of fixed effects but I do come back to this.\nThe term the language as fixed effect fallacy [@clark1973; @raaijmakers1999] implies that thinking about the random effects of stimulus differences applies only when we are looking at experiments about language. But you should remember that we need to think about the impact of random differences between stimuli whenever we present samples of stimuli to participants, and we collect observations about multiple responses for each stimulus. This is true whatever the nature of the stimuli [see e.g. @judd2012].\n\n\n\nOur model can now incorporate the random effects of participants as well as items:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + W_{0j} + e_{ij}\n\\]\nIn this model, the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and the word frequency effect \\(\\gamma_1X_j\\);\nplus random effects due to unexplained differences between participants in intercepts \\(U_{0i}\\) and in the slope of the frequency effect \\(U_{1i}X_j\\);\nas well as random differences between items in intercepts \\(W_{0j}\\);\nin addition to the residual term \\(e_{ij}\\).\n\n\n\n\nWe fit a mixed-effects model of the \\(logrt \\sim \\text{frequency}\\) relationship using the lmer() function, taking into account:\n\nthe fact that the study data have a hierarchical structure – with observations sensibly grouped by participant;\nthe fact that both the frequency effect, and average speed, may vary between participants;\nand the fact that the average speed of response can vary between responses to different stimuli.\n\nThe model syntax corresponds to the statistical formula and the code is written as:\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n\n                           LgSUBTLCD +\n\n                           (LgSUBTLCD + 1|subjectID) +\n\n                           (1|item_name),\n\n                         data = ML.all.correct)\n\nsummary(ML.all.correct.lmer)\n\nAs will now be getting familiar, the code works as follows:\n\nML.all.correct.lmer  &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nlogrt ~ LgSUBTLCD the fixed effect in the model is expressed as a formula in which the outcome or dependent variable logrt is predicted ~ by the independent or predictor variable LgSUBTLCD word frequency.\n\n\n\n\n\n\n\nTip\n\n\n\nIf there were more terms in the model, the terms would be added in series separated by +\n\n\nThe random effects part of the model is then specified as follows.\n\nWe first have the random effects associated with random differences between participants:\n\n\n(...|subjectID) adds random effects corresponding to random differences between sample groups (participants subjects) coded by the subjectID variable.\n(...1 |subjectID) including random differences between sample groups (subjectID) in intercepts coded 1.\n(LgSUBTLCD... |subjectID) and random differences between sample groups (subjectID) in the slopes of the frequency effect coded by using theLgSUBTLCD variable name.\n\n\nThen, we have the random effects associated with random differences between stimuli:\n\n\n(1|item_name) adds a random effect to account for random differences between sample groups (item_name) in intercepts coded 1.\n\n\n...(..., data = ML.all.correct) specifies the dataset in which you can find the variables named in the model fitting code.\nLastly, we can then specify summary(ML.all.correct.lmer) to get a summary of the fitted model.\n\n\n\n\nIf you run the model code as written then you would see the following results.\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n\n                           LgSUBTLCD +\n\n                           (LgSUBTLCD + 1|subjectID) +\n\n                           (1|item_name),\n\n                         data = ML.all.correct)\n\nsummary(ML.all.correct.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887997   0.015479 186.577\nLgSUBTLCD   -0.034471   0.003693  -9.333\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\nIn these results, we see:\n\nFirst, information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula logrt ~ LgSUBTLCD + (LgSUBTLCD + 1|subjectID) + (1|item_name).\nThen, we see REML criterion at convergence about the model fitting process, which we can usually ignore.\nThen, we see information about the distribution of the model residuals.\nThen, we see theRandom Effects.\n\nNotice that the statistics are Variance Std.Dev. Corr., that is, the variance, the corresponding standard deviation, and the correlation estimates associated with the random effects.\n\nWe see Residual error variance, just like in a linear model, corresponding to a distribution or spread of deviations between the model prediction and the observed RT for each response made by a participant to a stimulus.\nWe see Variance terms corresponding to what can be understood as group-level residuals. Here, the variance is estimated for the spread in random differences between the average intercept (over all data) and the intercept for each participant, and the variance due to random differences between the average slope of the frequency effect and the slope for each participant.\nWe also see the variance estimated for the spread in random differences between the average intercept (over all data) and the intercept for responses to each word stimulus.\nAnd we see the Corr estimate, telling us about the covariance between random deviations (between participants) in the intercepts and in the slopes of the frequency effect.\n\n\nLast, just as for linear models, we see estimates of the coefficients (of the slopes) of the fixed effects, the intercept and the slope of the logrts ~ LgSUBTLCD relationship.\n\nWe can compare this estimate with our previous lm() estimate for the effect of frequency.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\nResult: We can see that, in this mixed-effects analysis, the estimated effect of word frequency is now \\(\\beta = -0.034471\\).\n\n\n\nThe estimate is different, a bit smaller. While the change in the estimate is also small, we may remember that we are looking at slope estimates for predicted change in log RT, in an experimental research area in which effects are often of the order of 10s of milliseconds. The estimates, and changes in the estimates, will tend to be quite small.\n\n\n\n\n\n\nWarning\n\n\n\nNote that we see coefficient estimates, as in a linear model summary but no p-values.\n\nWe will come back to this, see Section 1.13.4.\nHowever, note that if \\(t &gt;= 2\\) we can suppose that (for a large dataset) an effect is significant at the \\(.05\\) significance level.\n\n\n\n\n\n\n\nWhat is the impact of the incorporation of random effects – the variance and covariance terms – in mixed-effects models? Mixed-effects models can be understood, in general, as a method to compromise between ignoring the differences between groups (here, participants constitute groups of data) as in complete pooling or focusing entirely on each group (participant) as in no pooling [@Gelman2007ga]. In this discussion, I am going to refer to the differences between participants but you can assume that the lesson applies generally to any situation in which you have different units in a multilevel structured dataset in which the units correspond to groups or clusters of data.\n\n\nThe problem with ignoring the differences between groups (participants), as in the complete pooling model (here, the linear model), has been obvious when we examined the differences between participants (or between classes) in slopes and intercepts in previous weeks. The problem with focusing entirely on each participant, as in the no pooling model, has not been made apparent in our discussion yet.\nIf we analyze each participant separately then we will get, for each participant, for our model of the frequency effect, the per-participant estimate of the intercept and the per-participant estimate of the slope of the frequency effect. These no-pooling estimates will tend to exaggerate or overstate the differences between participants [@Gelman2007ga]. By basing the estimates on just the data for a person, in each per-participant analysis, the no-pooling approach overfits the data.\nYou could say that the no-pooling approach gives us estimates that depend too much on the sample of data we have got, and are unlikely to be similar to the estimates we would see in other samples in future studies.\n\n\n\n\n\n\nTip\n\n\n\nThe no-pooling estimates are too strongly influenced by the data we are currently analyzing.\n\n\n\n\n\nIf we look closely at Figure 5, we can see that there are similarities as well as differences between participants. Our analysis must take both into account.\nWhat happens in mixed-effects models is that we pool information, calculating the estimates for each participant, in part based on the information we have for the whole sample (all participants, in complete pooling), in part based on the information we have about the specific participant (one participant, in no pooling). Thus, for example, the estimated intercept for a participant in a mixed-effects model is given by the weighted average [@Snijders2004a] of:\n\nthe intercept estimate given by an analysis of just that participant’s data (no pooling estimate;\nand the intercept estimate given by analysis of all participants’ data (complete pooling estimate).\n\nThe weighted average will reflect our relative level of information about the participant’s responses compared to how much information we have about all participants’ responses.\nFor some participants, we will have less information – maybe they made many errors, so we have fewer correct responses for an analysis. For these people, because we have less information, the intercept estimate will get pulled (shrunk) towards the overall (complete pooling, all data) estimate.\nFor other participants, we have more information – maybe they made all correct responses. For these people, because we have more information, the intercept estimate will be based more on the data for each participant.\nTo make sense of what this means, think about the differences between participants in how much reliable information we can have, given our sample, about their average level of response speed or about how they are affected by experimental variables. Think back to my comments about Figure 4, about the differences between participants in how spread out the distributions of their log RT values are. Recall that I said that where participants’ responses are more spread out – just as where we have less observations for some participants than for others – we shall inevitably have less certainty about our estimates for the effects that influence their performance if we base our account on just their data. Mixed-effects models perform better – as prediction models – than no pooling approaches because they are not relying, for any participant, on just their sometimes unreliable data.\nWe can look again at a plot showing the data for each participant. Figure 6 presents a grid or trellis of plots, one plot per person. In each plot, you can see points corresponding to the RT of each response made by a participant to a stimulus word. In all plots, the pink line represents the complete pooling data model estimate of the effect of frequency on response RTs. In each plot, the green line represents the effect of frequency estimated using just the data for each participant, the no pooling estimates. Now, we also see blue lines that represent the mixed-effects model partial pooling estimates.\n\n\n\n\n\nFigure 6: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant; pink line shows the complete pooling estimate green line shows the no-pooling estimate; and blue line shows the linear mixed-effects model partial pooling estimate\n\n\n\n\nIt is quite difficult to identify, in this sample, where the partial pooling and no pooling estimates differ. We can focus on a few clear examples. Figure 7) presents a grid of plots for just four participants. I have picked some extreme examples but the plot illustrates how: (1.) for some participants e.g. AA1 all estimates are practically identical; (2.) for some participants EB5 JL3 JP3 the no-pooling and complete-pooling estimates are really quite different and (3.) for some participants JL3 JP3 the no-pooling and partial-pooling estimates are quite different.\n\n\n\n\n\nFigure 7: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant – for participants AA1, EB5, JL3 and JP3; pink line shows the complete pooling estimate green line shows the no-pooling estimate; and blue line shows the linear mixed-effects model partial pooling estimate\n\n\n\n\nIn general, partial pooling will apply both to estimates of intercepts and to estimates of the slopes of fixed effects like the influence of word frequency in reaction time. Likewise, if we consider this idea in general, we can see how it should work whether we are talking about groups or clusters of data grouped by participant or by stimulus or by school, class or clinic, etc.\nFormally, whether an estimate for a participant (in our example) is pulled more or less towards the overall estimate will depend not just on the number of data-points we have for that person. The optimal combined estimate for a participant is termed the Empirical Bayes estimate and the weighting – the extent to which the per-participant ‘estimate’ depends on the participant’s data or the overall data – depends on the reliability of the estimate (of the intercept or the frequency effect) given by analyzing that participant’s data [@Snijders2004a]. If you think about it, smaller samples – e.g. where a participant completed less correct responses – will give you less reliable estimates (and so will samples that show more variation).\nWhat we are looking at, here, is a form of regularization in which we use all the sources of information we can to ensure we take into account the variability in the data while not getting over-excited by extreme differences [@mcelreath2020]. We want to see estimates pulled towards an overall average where we have little data or unreliable estimates. We can see how strongly estimates can be shrunk in a plot like Figure 8.\nFigure 8 illustrates the shrinkage effect. I plotted a scatterplot of intercept and slope parameters from each model (models with different kinds of pooling), and connect estimates for the same participant. The plot uses arrows to connect the different estimates for each participant, different estimates from no-pooling (per-participant) compared to partial-pooling (mixed-effects) models. The plot shows how more extreme estimates are shrunk towards the global average estimate.\n\n\n\n\n\nFigure 8: Plot illustrating shrinkage: big green and pink points show the complete pooling and partial pooling (average) estimates for the slope and intercept; orange and purple points show the no pooling (orange) and partial pooling (purple) estimates for each person; estimates for a person are connected by arrows to show the direction towards which no pooling estimates are pulled or shrunk\n\n\n\n\nWe can see how estimates are pulled towards the average intercept and frequency effect estimates. The shrinkage effect is stronger for more extreme estimates like JL3 JP3. It is weaker for estimates more (realistically) like the overall group estimates like AA1.\n\n\n\n\nBefore we move on, we can think briefly about how the mixed-effects models are estimated [@Snijders2004a]. Where do the numbers come from? I am happy to stick to a fairly non-technical intuitive explanation of the computation of LMEs but others, wishing to understand things more deeply, can find computational details in @Pinheiro2000aa, among other places. Mixed-effects models are estimated iteratively:\n\n\nIf we knew the random effects, we could find the fixed effects estimates by minimizing differences – like linear modeling.\nIf we knew the fixed effects – the regression coefficients – we could work out the residuals and the random effects.\n\nAt the start, we know neither, but we can move between partial estimation of fixed and random effect in an iterative approach.\n\nUsing provisional values for the fixed effects to estimate the random effects.\nUsing provisional values for the random effects to estimate the fixed effects again.\nTo converge on the maximum likelihood estimates of effects – when the estimates stop changing.\n\nIn mixed-effects models, the things that are estimated are the fixed effects (the intercept, the slope of the frequency effect, in our example), along with the variance and correlation terms associated with the random effects. Previously, I referred to the partial-pooling mixed-effects ‘estimates’ of the intercept or the frequency effect for each person, using the quotation marks because, strictly, these estimates are actually predictions, Best Unbiased Linear Predictions (BLUPs), based on the estimates of the fixed and random effects.\n\n\nMostly, our main concern, in working with mixed-effects models, is over what effects we should include, what model we should specify. But we should prepare for the fact sometimes happens that models fail to converge, which is to say, the model fitting algorithm fails to settle on some set of parameter estimates but has reached the limit in the number of iterations over which it has attempted to find a satisfactory set of estimates.\nIn my experience, convergence problems do arise, typically, if one is analyzing categorical outcome data (e.g accuracy) where there may be not enough observations to distinguish satisfactory estimates given a quite complex hypothesized model. In other words, you might run into convergence problems but it will not happen often and only where you are already dealing with quite a complex situation. We take a look at this concern in more depth, in the next chapter ?@sec-glmm-intro.\n\n\n\n\nUp to this point, we have discussed the empirical or conceptual reasons we should expect to take into account, in our model, the effects on the outcome due to systematic differences in the experimental variables, e.g., in stimulus word frequency frequency, or to random differences between participants or between stimuli. We can now think about how we should statistically evaluate the relative usefulness of these different fixed effects or random effects, where usefulness is judged in relation to our capacity to explain outcome variance, or to improve model fit to sample data. We shall take an approach that follows the approach set out by Baayen, Bates and others [@Baayen2008; @bates2015parsimonious; @matuschek2017].\nIn this approach, we shall look at the choices that psychology researchers have to make. Researchers using statistical models are always faced with choices. As we have seen, these choices begin even before we start to do analyses, as when we make decisions about dataset construction [@steegen2016]. The need to make choices is always present for all the kinds of models we work with (as I discuss in ?@sec-multiverse). This may not always be obvious because, for example, in using some data analysis software, researchers may rely on defaults with limited indication that that is what they are doing.\n\n\n\n\n\n\nImportant\n\n\n\nJust because we are making choices does not mean we are operating subjectively in a non-scientific fashion. Rather, provided we work in an appropriate mode of transparency or reflexiveness, we can work with an awareness of our options and the context for the data analysis [see the very helpful discussion in @gelman2017].\n\n\n\n\nIt is very common to see researchers using a process of model comparison to try to identify an account for their data in terms of estimates of fixed and random effects. A few key concepts are relevant to taking this approach effectively.\nWe will focus on building a series of models up to the most complex model supported by the data. What does model complexity mean here? I am talking about something like the difference between a model including just main effects (simpler) and a model including both main effects and the interaction between the effects (more complex), or, I am talking about a model included just fixed effects (simpler) versus a model including fixed effects as well as random effects (more complex).\nResearchers may engage in comparing models to examine if one or more random effects should be included in their linear mixed-effects model. They may not be sure if they should include all random effects, that is, all random effects that could be included, given a range of grouping variables, like participant, class or stimulus, and given a range of possible effects, such as whether slopes or intercepts might vary.\nResearchers may do model comparison to check if adding the effect of an experimental variable is justified. Maybe they are conducting an exploratory study in which they want to investigate if using some measurement variable helps to explain variation in the outcome. Perhaps they are conducting an experimental study in which they want to test if the experimental manipulation, or the difference between conditions, has an impact on the outcome.\n\n\n\n\n\n\nTip\n\n\n\nAcross these scenarios, we can test if an effect should be included or if its inclusion in a model is justified by comparing models with versus without the term that corresponds to the effect.\n\n\nIn some studies, researchers conduct model comparisons like this in order to obtain null hypothesis significance tests for the effects of the experimental variables.\nTypically, the model comparisons are focused on whether some measurement of model fit is or is not different when we do versus do not include the effect in question in the model.\n\n\nAs our discussion progresses, I think it would be helpful to reflect on some of the questions that you may be asking yourself.\n1. What about multiple comparisons?\nYou might well ask yourself:\n\nIf we engage in a bunch of comparisons to check if we should or should not include a variable, isn’t this just exploiting researcher degrees of freedom?\n\nOr, you might ask:\n\nIf we are conducting multiple tests on the same data, aren’t we running the risk of raising the Type I error (false positive) rate because we are doing multiple comparisons?\n\nI think these are good questions but, here, my task is to explain what people do, why they do it, and how it helps in your data analysis.\n2. Is any model the best?\n\nSo you are looking at models with varying fixed effects (fitted using ML) or models with varying random effects (fitted using REML). How do you decide which model is better?\n\nSome researchers argue that trying to decide which model is better or best is inappropriate [see e.g. @Gelman2015a]. As the famous saying by George Box has it [@box1976]: “All models are wrong.” 1 We may say, nevertheless, that some models are useful. Some models are more useful than others, perhaps, because they explain or predict outcomes better, depending on your criteria, and the cost-benefit analysis.\nHere, I will explain the model comparison process while acknowledging this point. This is because researchers often model comparison techniques to evaluate the relative usefulness of different alternate models.\n\n\n\n\nYou will often encounter, in the psychological research literature, Information Criteria statistics like BIC: they are understood within an approach: Information-theoretic methods. They are grounded in the insight that you have reality and then you have approximating models. The distance between a model and reality corresponds to the information lost when we use a model to approximate reality. Information criteria – AIC or BIC – are estimates of information loss. The process of model selection aims to minimize information loss.\nI will not discuss information criteria methods of model evaluation in detail, here, because psychologists frequently use the Likelihood Ratio Test method [@meteyard2020a], see following. (Take a look at, e.g., @Burnham2004 for a readable discussion, if you are interested.) However, you should have some idea of what information criteria statistics (like AIC and BIC) mean because you will see these statistics in the outputs from model comparisons using the anova() function, which we shall review a bit later (Section 1.12.3).\nIn summary, Akaike showed you could estimate information loss in terms of the likelihood of the model given the data – Akaike Information Criteria, AIC:\n\n\n\\[\nAIC = -2ln(l) + 2k\n\\]\nWhere:\n\n\\(-2ln(l)\\) is -2 times the log of the likelihood of the model given the data,\nwhere \\((l)\\) the likelihood\nis proportional to the probability of observed data conditional on some hypothesis being true.\n\nYou want a more likely model – less information loss, closer to reality – you want more negative or lower AIC. You can identify models that are more likely – closer to reality – with models with less wide errors, i.e. smaller residuals.\nYou could better approximate reality by including lots of predictors, specifying a more complex model. Models with more parameters may fit the data better but some of those effects may be spurious. Adding \\(+ 2k\\) penalizes complexity, speaking crudely, and so helps us to focus on the more parsimonious less complex model that best fits the data.\nSchwartz proposed an alternative estimate – Bayesian Information Criteria: BIC:\n\n\n\\[\nBIC = -2ln(l) + kln(N)\n\\]\nWhere:\n\n\\(-2ln(l)\\) is -2 times the log of the likelihood of the model given the data.\n\\(+ kln(N)\\) is the number of parameters in the model times the log of the sample size.\n\nThus the penalty for greater complexity is heavier in BIC.\nWe see that AIC and BIC differ in the second term. A deeper difference is that AIC estimates information loss when the true model may not be among the models being considered while BIC assumes that the true model is within the set of models being considered.\nAt this point we just need to think about Model selection and judgment using AIC and BIC.\n\nCompare a simpler model: model 1, just main effects; model 2, main effects plus interactions.\n\nIf the more complex model better approximates reality then it will be more likely given the data.\nBIC or AIC will be closer to negative infinity: \\(-2ln(l)\\) will be larger e.g. 10 is better than 1000, -1000 better than -10.\n\nAIC and BIC should move in the same direction. They usually will.\nAIC will tend to allow more complex models and that may be necessary when the researcher is engaged in a more exploratory study or wants more accurate predictions (that would be better supported by maximising the information going into the model). Using the BIC will tend to favour simpler models and that may be necessary when the researcher seeks models that replicate over the long run. Maybe a simpler model will less likely include predictors estimated because they are needed to fit noise or random outcome variation.\n\n\n\n@Pinheiro2000aa [see also @Barr2013a; @matuschek2017] recommend that models of varying predictor sets can be compared using Likelihood Ratio Test comparison (LRTs) where the simple model is nested inside the more complex model.\nThe “nested”, here, means that the predictors in the simpler model are a subset of the predictors in the more complex model. For example, you might have just main effects in the simpler model but both main and interaction effects in the more complex model. Or, in another example, you might have just random effects of subjects or items on intercepts in the simpler model but both random effects on intercepts and random effects on slopes of fixed effects in the more complex model.\n\n\n\n\n\n\nWarning\n\n\n\nWhen you compare models using the Likelihood ratio test, LRT, you are comparing alternate models of the same data.\n\n\n@Barr2013a note that we can compare models varying in the fixed effects (but constant in the random effects) or models varying in the random effects (but constant in the fixed effects) using LRTs. I have frequently reported model comparisons using the Likelihood ratio test, LRT. In part, this is for analytic reasons: I can compare simple and complex models getting multiple information criteria statistics for the models being compared in one function call, anova([model1], [model2]. In part, it is for social pragmatic reasons: the LRT comparison yields a significance p-value so that I can say, using the comparison, something like “The more complex model provided a significantly better fit to observation (LRT comparison, … p \\(=\\) …”\nIn a Likelihood Ratio Test, the test statistic is the comparison of the likelihood of the simpler model with the more complex model. Fortunately for us, we can R to calculate the model likelihood and do the model comparison (Section 1.13.2).\nThe comparison of models works by division: we divide the likelihood of the more complex model by the likelihood of the simpler model, calculating a likelihood ratio.\n\\[\n\\chi^2 = 2log\\frac{likelihood-complex}{likelihood-simple}\n\\]\nThe likelihood ratio value is then compared to the \\(\\chi^2\\) distribution for a significance test. In this significance test, we assume the null hypothesis that the simpler model is adequate as an account of the outcome variance. We calculate the p-value for the significance test using a number for the degrees of freedom equal to the difference in the number of parameters of the models being compared.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHow should you proceed when you decide to use mixed-effects models?\n\nI think the answer to that question depends on whether you are doing a study that is confirmatory or exploratory.\n\n\n\nIn short, if you have pre-registered the design of your study and, as part of that registration, you recorded the hypotheses you plan to test, as well as the analysis method you plan to use to test your hypotheses, then the answer is simple: fit the model you said you were going to use.\nThese days, if you have not pre-registered your analysis plans, you are practically-speaking engaged in exploratory work. If you are doing an exploratory study, then you will need to make some choices, in part, depending on the nature of the sample you are working with, and other aspects of the research context, but it will help to keep things simple.\nIn an exploratory study, I would keep things simple by comparing a series of models, fitted with different sets of predictor variables (fixed effects).\n\n\n\n\n\n\nWarning\n\n\n\nNote: if you are running mixed-effects models in R you cannot run lmer() models with just fixed effects.\n\n\nWhat I do is this: for a dataset like the ML study data, where the data were collected using a repeated-measures design:\n\nso that all participants saw all stimuli,\nand both participants and stimuli were sampled (from the wider populations of readers or words),\nthen I would run a series of models\nso that the different models have varying sets of fixed effects\nbut all models in the series have the same random effects: the random effects of subjects and items on intercepts.\n\nIn my experience, the estimates and associated significance levels associated with fixed effects can vary quite a bit depending on what other variables are included in the model. This has led me to take an approach where I am not varying too much how predictors are included in the model.\nAs noted, this will not really apply if you are doing an confirmatory study in which you are obliged to include the manipulated variables. However, if you are doing something a bit more exploratory then you might have to think about the kinds of predictors you include in your model, and how or when you include them.\n\n\n\n\n\n\nTip\n\n\n\nIn what order should you examine the usefulness of different sets of fixed effects?\n\nThis is a difficult question to answer and the difficulty is one reason why I think we need to be cautious when we engage in model comparison to try to get to a model of our data.\n\n\n\nMy advice would be to plan out in advance a sequence of model comparisons.\n\nYou should begin with simpler models with fewer effects.\nYou should begin with those effects whose impacts are well established and well understood by you.\nIf there is a whole set of well established effects typically included in an analysis in the field in which you are working, it might be sensible to include all the effects in a single step.\nThen, I would use subsequent incremental steps to increase model complexity by adding effects that are theoretically justified, i.e., hypothesized, but which may be new, or may depend on the experimental manipulation you are testing out.\n\nHaving established a model with some set of sensible fixed effects (guided by information criteria or LRT statistics), I would then turn my attention to the random effects component of the model. As noted, we may expect to see random differences between subjects (and possibly between items) in both the level of average performance – random effects of subjects or items on intercepts – and in the slopes of fixed effects – random effects of subjects or items on slopes.\nWhat I do is this:\n\nFor a dataset like ML’s, I examine firstly if both random effects of subjects and items on intercepts are required.\nI then check if random effects of subjects or items on slopes are additionally required in the model.\n\nThe distinction between exploratory and confirmatory studies breaks down, in my experience, when we start thinking about what random effects should be included in a model. It will be useful to review, here, @Barr2013a and @matuschek2017 for an interesting discussion, and contrasting approaches.\n\n\nBefore we go any further, we need to briefly discuss one key choice that we face in working with mixed-effects models. This concerns the difference between Restricted Maximum Likelihood (REML) and Maximum Likelihood (ML) estimation methods. Both methods are iterative.\nThe lmer() function has defaults, like any analysis function, so we often do not need to make the choice explicit. We do need to when we compare models that vary in fixed effects, or in random effects.\n\nRestricted maximum likelihood\n\nIn R: REML=TRUE is stated in the lmer() function call.\n\nREML estimates the variance components while taking into account the loss of degrees of freedom resulting from the estimation of the fixed effects: REML estimates vary if the fixed effects vary.\nTherefore it is not recommended to compare the likelihood of models varying in fixed effects and fitted using REML [@Pinheiro2000aa].\nThe REML method is recommended for comparing the likelihood of models with the same fixed effects but different random effects.\nREML is more accurate for random effects estimation.\n\n\nMaximum likelihood\n\nIn R: REML=FALSE is stated in thelmer() function call.\n\nML estimation methods can be used to fit models with varying fixed effects but the same random effects.\nML estimation: a good place to start when building-up model complexity – adding parameters to an empty model.\n@Pinheiro2000aa advise that the approach is anti-conservative (it will sometimes indicate effects where there are none there) but @Barr2013a argue that their analyses suggest that that is not so.\n\n\n\n\n\n\n\nAs noted, it is recommended [@Pinheiro2000aa] that we compare models of varying random effects using Restricted Maximum Likelihood (REML) fitting. We might be comparing different models with different sets of random effects if we are in the process of working out whether our model should include both random intercepts and random slopes. I think it is sensible to build up model complexity in the random component so that we are working through a series of model comparisons, comparing more simple with more complex models where the more complex model includes the same terms as the simpler model but adds some more.\nIn analyzing the effect of frequency on log RT for the ML study data, we can examine whether the random effects of subjects or of items on intercepts are necessary. Then we can examine if we should take into account random effects of subjects on the slope of the fixed effect of frequency, in addition to the random effects on intercepts.\nTo begin with, we look at a simpler model. We can fit a model with just the fixed effects of intercept and frequency, and the random effects of participants or items on intercepts only. We exclude the (LgSUBTLCD + ...|subjectID) specification for the random effect of participants on the slope of the frequency LgSUBTLCD effect.\nWe use REML fitting, as follows:\n\nML.all.correct.lmer.REML.si  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                    \n                                          (1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.si)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9845.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5339 -0.6375 -0.1567  0.4364  5.5851 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0003204 0.01790 \n subjectID (Intercept) 0.0032650 0.05714 \n Residual              0.0085285 0.09235 \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887697   0.013253   217.9\nLgSUBTLCD   -0.034390   0.002774   -12.4\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.658\n\n\nIf you look at the code chunk, you can see that:\n\nREML = TRUE is the only change to the code: it specifies the change in model fitting method;\nalso, I changed the model name to ML.all.correct.lmer.REML.si to be able to distinguish the maximum likelihood from the restricted maximum likelihood model.\n\nFollowing @baayen2008, we can then run a series of models with just one random effect. Firstly, just the random effect of items on intercepts:\n\nML.all.correct.lmer.REML.i  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.i)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -8337\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7324 -0.6455 -0.1053  0.4944  4.8970 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0002364 0.01537 \n Residual              0.0117640 0.10846 \nNumber of obs: 5257, groups:  item_name, 160\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.886765   0.009047  319.07\nLgSUBTLCD   -0.034206   0.002811  -12.17\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.977\n\n\nSecondly, just the random effect of subjects on intercepts:\n\nML.all.correct.lmer.REML.s  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|subjectID),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.s)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9786.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5843 -0.6443 -0.1589  0.4434  5.5266 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n subjectID (Intercept) 0.003275 0.05723 \n Residual              0.008837 0.09401 \nNumber of obs: 5257, groups:  subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.885751   0.011561  249.60\nLgSUBTLCD   -0.033888   0.001897  -17.87\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.517\n\n\nIf we now run Likelihood Ratio Test comparisons of these models, we are effectively examining if one of the random effects can be dispensed with: if its inclusion makes no difference to the likelihood of the model then it is not needed. Is the random effect of subjects on intercepts justified?\n\nCompare models, first, with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.i the random effect of subjects on intercepts.\nThen compare models with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.s the random effect of items on intercepts.\n\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)\n\n\n\nWe compare models using the anova() function.\n\nanova() does the model comparison, for the models named in the list in brackets.\n\nYou can do this for the foregoing series of models but notice that in the code we specify:\n\nrefit = FALSE`\n\nWhat happens if we do not add that bit? What you will see if you run the anova() function call, without therefit = FALSE argument – try it – is that you will then get the warning refitting model(s) with ML (instead of REML).\nWhy? The immediate reason for this warning is that we have to specify refit = FALSE because otherwise R will compare ML fitted models. The refitting occurs by default.\nWhat is the reason for the imposition of this default?\nYou will recall that @Pinheiro2000aa advise that if one is fitting models with random effects the estimates are more accurate if the models are fitted using Restricted Maximum Likelihood (REML). That is achieved in the lmer() function call by adding the argument REML=TRUE. @Pinheiro2000aa further recommend (see, e.g., pp.82-) that if you compare models:\n\nwith the same fixed effects\nbut with varying random effects\nthen the models should be fitted using Restricted Maximum Likelihood.\n\nR refits models, for the anova() comparison using ML even if we originally specified REML fitting. It does this to stop users from comparing REML-fitted models when those models are specified with different sets of fixed effects, as discussed here. The reason for this is explained by Ben Bolker (in this discussion): analyses of simulated data analyses suggest that it does not make much difference whether we use REML or ML when we are comparing models with the same fixed effects but varying random effects. It does matter very much, however, that we should fit models using ML when we are comparing models with the same random effects but differing fixed effects. You will remember that REML estimates vary if the fixed effects vary.\n\n\n\nWhen we run the anova() function call, it can be seen that the random effects of subjects on intercepts is required.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.i: logrt ~ LgSUBTLCD + (1 | item_name)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik deviance  Chisq Df\nML.all.correct.lmer.REML.i     4 -8329.0 -8302.7 4168.5  -8337.0          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6  -9845.1 1508.1  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.i                \nML.all.correct.lmer.REML.si  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you look at the results of the model comparison then you should notice:\n\nThe ML.all.correct.lmer.REML.si model is more complex than the ML.all.correct.lmer.REML.i model.\n\n\nML.all.correct.lmer.REML.si includes LgSUBTLCD + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.i includes LgSUBTLCD + (1 | item_name).\n\n\nThe more complex model ML.all.correct.lmer.REML.si has AIC (-9835.1) and BIC (-9802.3) numbers that are larger or more negative, and has a likelihood (4922.6) that is larger than the simpler model ML.all.correct.lmer.REML.i which has AIC (-8329.0), BIC (-8302.7) and likelihood (4168.5).\n\n\nThe \\(\\chi^2 = 1508.1\\) statistic, on 1 Df has a p-value of Pr(&gt;Chisq) &lt;2.2e-16.\n\nYou can say that the comparison of the model ML.all.correct.lmer.REML.si (with the random effect of participants on intercepts) versus the model ML.all.correct.lmer.REML.i (without the random effect of participants on intercepts) shows that the inclusion of the random effect of participants on intercepts is warranted by a significant difference in model fit. (I highlight here the language you can use in your reporting.)\nThe second model comparison shows that the random effects of items on intercepts is also justified.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.s: logrt ~ LgSUBTLCD + (1 | subjectID)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik deviance  Chisq Df\nML.all.correct.lmer.REML.s     4 -9778.3 -9752.0 4893.2  -9786.3          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6  -9845.1 58.825  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.s                \nML.all.correct.lmer.REML.si  1.723e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you look at the results of the model comparison then you should notice:\n\nThe ML.all.correct.lmer.REML.si model is more complex than the ML.all.correct.lmer.REML.s model.\n\n\nML.all.correct.lmer.REML.si includes LgSUBTLCD + (1 | subjectID) + (1 | item_name).\nML.all.correct.lmer.REML.s includes LgSUBTLCD + (1 | subjectID)\n\n\nThe more complex model ML.all.correct.lmer.REML.si has AIC (-9835.1) and BIC (-9802.3) numbers that are larger or more negative, and has a likelihood (4922.6) that is larger than the simpler model ML.all.correct.lmer.REML.s which has AIC (-9778.3) and BIC (-9752.0) and likelihood (4893.2).\n\n\nThe \\(\\chi^2 = 58.825\\) statistic, on 1 Df has a p-value of Pr(&gt;Chisq) 1.723e-14.\n\nYou can say that the comparison of a model ML.all.correct.lmer.REML.si with versus a model ML.all.correct.lmer.REML.s without the random effect of items on intercepts shows that the inclusion of the random effect of items on intercepts is warranted by a significant difference in model fit.\nI would conclude that both random effects of subjects and items on intercepts are required.\nWe can draw this conclusion because the difference between the model including just the random effect of items on intercepts anova-ML-all-correct-lmer-REML-i, or the model including just the random effect of subjects on intercepts anova-ML-all-correct-lmer-REML-s, compared to the model including both the random effect of items on intercepts and of subjects on intercepts anova-ML-all-correct-lmer-REML is significant. This tells us that the absence of the term accounting for the random effect of subjects on intercepts is associated with a significant decrease in model fit to data, in model likelihood.\n\n\n\n\nWe should next consider whether it is justified or warranted to include in our model a term capturing the random effect of participants in the slope of the frequency effect. We may hold or we may make theoretical assumptions that justify including this random effect. Some researchers might ask: does the inclusion of the random effect seem warranted by improved model fit to data?\nI should acknowledge, here, that there is an on-going discussion over what random effects should be included in mixed-effects models (see @meteyard2020a for an overview). The discussion can be seen from a number of different perspectives. Key articles include those published by @Baayen2008, @bates2015parsimonious, @Barr2013a and @matuschek2017.\nYou could be advised that a mixed-effects model should include all random effects that make sense a priori, so, here, we are talking about the random effects of participants on intercepts and on the slopes of all fixed effects that are in your model (variances and covariances) as well as all the random effects of items on intercepts and on slopes. This is characterized as the keep it maximal approach, associated with @Barr2013a, though the discussion in that article is more nuanced than this sounds.\nOr, you could be advised that a mixed-effects model should only include those random effects that appear to be justified or warranted by their usefulness in accounting for the data. In practice, this may mean, you should include only those random effects that appear justified by improved model fit to data, as indicated by a model comparison [see e.g. @bates2015parsimonious; @matuschek2017].\n\n\n\n\n\n\nTip\n\n\n\nI think, in practice, that maximal models can run into convergence problems. This means that many researchers adopt an approach which you could call: Maximum justifiable.\n\nThis involves fitting a model, including all the random effects that make sense,\nthat are justified by improved model fit to data (given a significance test, model comparison)\nfor a model that actually converges.\n\n\n\nAt present, viewpoints in discussions around the specification of random effects are associated with arguments that, as @Barr2013a discuss, more comprehensive models appear to control the Type I (false positive) error rate better, or that, as @matuschek2017 argue, control over the risk of false positives may come at the cost of increasing the Type II (false negative) error rate).\nI think that it would seem to be axiomatic that a researcher should seek to account for all the potential sources of variance – fixed effects or random effects – that may influence observed outcomes. In practice, however, you may have insufficient data or inadequate measures to enable you to fit a model that converges with all the random effects, or to enable you to fit a model that converges that can estimate what may, in fact, be very small random effects variances or covariances. This is why some researchers are moving to adopt Bayesian mixed-effects modeling methods, as discussed by the developmental Psychologist, Michael Frank, for example, here. And as exemplified by my work here.\n\n\n\n\n\n\nWarning\n\n\n\nThis discussion raises a question.\n\nRandom slopes of what?\n\n\n\nIn general, and simplifying things a bit, if an effect is manipulated within grouping units then we should specify random effects terms that allow us to take into account random differences between groups (e.g, between participants, stimulus words, or classes) in intercepts or in the slopes of the effects of theoretical interest, the fixed effects. The language of within-subjects or between-subjects effects is common in statistical education in psychological science and, I guess, it is a legacy of the focus of that education on ANOVA. A nice explanation of the difference between within-subjects or between-subjects effects can be found in @Barr2013a.\nIn short, if a participant provides response data under multiple levels of an experimental condition, or in response to multiple levels of a predictor variable (e.g. a person responds to multiple words, with differing frequency levels) then we are going to estimate or test the effect of that condition or that variable as if the condition is manipulated within-subjects or as if the responses to the variable vary within-subjects. If and only if we are in this situation, we can draw a plot of the kind you see in Figure 6: where we may be able to see the way that the slope of the effect of the variable differs between participants. (In contrast, for example, outside of longitudinal studies, we would identify age as a between-subjects rather than a within-subjects variable and, if you think about it, we could not draw a grid of plots like Figure 6 to examine how the slope of the age effect might differ between participants.) In this situation, we can and should (as @Barr2013a argue) specify random effects terms to account for between-participant differences in the slopes of the fixed effect.\nWe can examine the utility of random effects by comparing models with the same fixed effects but with varying random effects. We can specify a fixed effect term inside the random effects part of the mixed-effects model code, as we saw in Section 1.9.5.\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                                (LgSUBTLCD + 1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nLooking at the code:\n\nWith (LgSUBTLCD + 1 |subjectID) we specify a random effect of subjects on intercepts and on the slope of the frequency effects.\nWe do not specify – it happens by default – the estimation of the covariance of random differences among subjects in intercepts and random differences among subjects in the slope of the frequency effect.\n\nAnd as before, we can use anova() to check whether the increase in model complexity associated with the addition of random slopes terms is justified by an increase in model fit to data.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.slopes, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.slopes: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n                                npar     AIC     BIC logLik deviance  Chisq Df\nML.all.correct.lmer.REML.si        5 -9835.1 -9802.3 4922.6  -9845.1          \nML.all.correct.lmer.REML.slopes    7 -9854.1 -9808.1 4934.0  -9868.1 22.934  2\n                                Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.si                   \nML.all.correct.lmer.REML.slopes  1.047e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInspection of the results shows us that, here, adjusting the model to include random effects of subjects in the slopes of the fixed effect of word frequency does improve model fit to data. In this situationn, we can report that:\n\nThe inclusion of the random effect is warranted by improved model fit to data (\\(\\chi^2 (1 df) = 22.9, p &lt; .001\\))}\n\n\n\n\nIf you look at the fixed effects summary, you can see that we do not get p-values by default. To calculate p-values, we need to count residual degrees of freedom. The authors of the {lme4} library that furnishes the lmer() function do not [as e.g. @Baayen2008 discuss] think that it is sensible to estimate the residual degrees of freedom for a model in terms of the number of observations. This is because the number of observations concerns one level of a multilevel dataset that might be structured with respect to some number of subjects, some number of items. This means that one cannot then accurately calculate p-values to go with the t-tests on the coefficients estimates; therefore they do not.\nWhile this makes sense to me (see comments earlier on Bayesian methods), Psychologists will often need p-values. This is now relatively easy.\nWe can run mixed-effects models with p-values from significance tests on the estimates of the fixed effects coefficients using the library(lmerTest).\n\nlibrary(lmerTest)\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                                (LgSUBTLCD + 1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.slopes)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  2.887997   0.015479 47.782839 186.577  &lt; 2e-16 ***\nLgSUBTLCD   -0.034471   0.003693 60.338787  -9.333 2.59e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\nBasically, the call to access the lmerTest library ensures that when we run the lmer() function we get a calculation of an approximation to the denominator degrees of freedom that enables the calculation of the p-value for the t-test for the fixed effects coefficient. An alternative, as I have noted (Section 1.12.3) is to compare models with versus without the effect of interest.\n\n\nIt will be useful for you to examine model comparisons with a different set of models for the same data.\nYou could try to run a series of models in which the fixed effects variable is something different, for example, the effect of word Length: or the effect of orthographic neighbourhood sizeOrtho_N:.\nI would consider the model comparisons in the sequence shown in the foregoing, one pair of models at a time, to keep it simple. When you look at the model comparison, ask: is the difference between the models a piece of complexity (an effect) whose inclusion in the more complex model is justified or warranted by improved model fit to data?\n\n\n\n\n\n\n\nHow do we report the analyses and their results?\nI think it may help if I analyze the structure and content of a results report that I did, in @Davies2013b. The report is published together with data and analysis code.\nIf you look at the report, you can identify the kinds of information that I think you should communicate.\n\nBecause it was an exploratory study, I started by reporting the comparison of models varying in fixed effects.\nI explain what predictors are included in each model.\nI explain how I make decisions about which model to select.\nI then go on to discuss the comparison of models varying in random effects.\n\nI think it is important to be as clear as possible about what model comparison process (if any) you may undertake.\n\nWe stepped through a series of models. Firstly, assuming the same random effects of subjects and items on intercepts, we compared models differing in fixed effects: a model (model 1) with just initialstress factors; a model (model 2) with initialstress factors plus linear effects due to the orthographic.form, frequency, semantic, and bigram.frequency factors; and lastly a model (model 3) with the same factors as model 2 but adding restricted cubic splines for the frequency and orthographic.form factors to examine the evidence for the presence of curvilinear effects of frequency and length (the orthographic.form factor loads heavily on length).\n\nNotice also that I try to standardize the language and structure of the paragraphs – that kind of repetition or rhythm helps the reader, I think, by making what is not repeated – the model specifications – more apparent. Your style may differ, however, and that’s alright.\n\nWe evaluated whether the inclusion of random effects was necessary in the final model (model 3) using LRT comparisons between models with the same fixed effects structure but differing random effects. Here, following Pinheiro & Bates (2000; see, also, Baayen, 2008), models were fitted using the REML=TRUE setting in lmer. We compared models that included: (i.) both random effects of subjects and items, as specified for model 3; (ii.) just the random effect of subjects; (iii.) just the random effect of items.\n\n\n\n\n\n\n\nTip\n\n\n\nI want you to notice something more, concerning the predictors included in each different model:\n\nI do not include predictors one at a time, I include predictors in sets.\n\n\n\nFor the @Davies2013b dataset, I include first the set of phonetic coding variables then the set of psycholinguistic variables (see the paper for details). I include linear effects then additional terms allowing the effects to be curvilinear.\nFinally, you can see that I report model comparisons in terms of Likelihood Ratio Tests. I do this, firstly, in order to report comparisons conducted to examine the basis for selecting one model out of a set of possible models varying in fixed effects:\n\nComparing models 1 and 2, models with initialstress factors but differing in whether they did or did not include key psycholinguistic factors like orthographic.form, the LRT statistic was significant (\\(\\chi^2 = 1,007, 4 df, p = 2 * 10^-16\\)). Comparing models 2 and 3, i.e. models with initialstress and key psycholinguistic components but differing in whether they did or did not use restricted cubic splines to fit the orthographic.form and frequency effects, the LRT statistic was significant (\\(\\chi^2 = 23, 2 df, p = 1 * 10^-5\\)).\n\nThen I report the selection of models varying in random effects:\n\nWe compared models that included: (i.) both random effects of subjects and items, as specified for model 3; (ii.) just the random effect of subjects; (iii.) just the random effect of items. The difference between models (i.) and (ii.) was significant (\\(\\chi^2 = 185, 1 df, p = 2 * 10^-16\\)) indicating inclusion of an item effect was justified. The difference between models (i.) and (iii.) was significant (\\(\\chi^2 = 17,388, 1 df, p = 2 * 10^-16\\)) indicating inclusion of a subject effect was justified.\n\nIf you look at the report, you will see, also, that present a summary table showing the estimates for the fixed effects, and a series of plots indicating the predicted change in outcome (reading RT) given variation in the values of the predictor variables.\nIn summary, I think we can and should report both an outline of the process of development of the model or models we use to estimate the effects of interest, and the estimates we derive through the modeling.\n\n\n\n\n\n\nTip\n\n\n\nI would advise you to report:\n\nA summary of fixed effects – just like in linear models, with coefficient estimates, standard errors, t and p (if you use it);\nRandom effects variance and covariance (as applicable);\nModel building processes or model comparisons (if used).\n\nI recommend presenting the final model summary in a table that is structured like a multiple regression model summary table showing both random and the fixed effects.\n\n\nI also think it helps the reader to know what model is the basis for any estimates presented [see @meteyard2020a for further advice].\n\n\n\n\nWe examined another example of data from a repeated measures design study, this time, from a study involving adults responding to the lexical decision task, the ML study dataset.\nWe explored in more depth why linear mixed-effects models are more effective than other kinds of models when we are analyzing data with multilevel or crossed random effects structure. We discussed the critical ideas: pooling, and shrinkage. And we looked at how mixed-effects models employ partial-pooling so as to be more effective than alternative approaches dependent on complete pooling or no pooling estimates.\nMixed-effects models work better because they use both information from the whole dataset and information about each group (item or participant). This ensures that model estimates take into account random differences but are regularized so that they are not dominated by less reliable group-level information.\nWe considered, briefly, how mixed-effects models are estimated.\nThen we examined, in depth, how mixed-effects models are fitted, compared and evaluated. The model comparison approach was set out, and we looked at both practical steps and at some of the tricky questions that, in practice, psychologists are learning to deal with.\nWe discussed how to compare models with varying random or fixed effects. We focused, especially, on the comparison of models with varying random effects. Methods for model comparison, including the use of information criteria and the Likelihood Ratio Test, were considered.\nWe discussed p-values, questions about calculating them, and a simple method for getting them when we need to report significance tests.\nWe discussed how mixed-effects models should be reported.\n\n\nWe used two functions to fit and evaluate mixed-effects models.\n\nlmer() to fit mixed-effects models\nanova() to compare two or more models using AIC, BIC and the Likelihood Ratio Test.\nWe used the lmerTest library to furnish significance tests for coefficient estimates of fixed effects.\n\n\n\n\n\nThe most influential papers, at present, for the practice of mixed-effects modeling in psychological science are those by @Baayen2008, @bates2015parsimonious, @Barr2013a and @matuschek2017. Each of these papers makes critical points and, in my view, each is clearly written with a good use of examples grounded in the scenarios psychologists often encounter.\nBroader concerns about how are approach modeling, and what we look for as scientists, are discussed in @Burnham2004, @Gelman2007ga and @gelman2017.\nA very useful FAQ on the practicalities of working with mixed-effects models can be found here."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-motivations",
    "href": "PSYC402/Week18.html#sec-dev-mixed-motivations",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "Linear mixed-effects models are important, interesting, and sometimes challenging. We have worked through two chapters (see ?@sec-intro-multilevel, ?@sec-intro-mixed) in which we have aimed to learn:\n\nTo recognize the situations where we shall see multilevel structured data and therefore where we will need to apply multilevel or mixed-effects models.\nTo understand the nature and the advantages of these models: what they are, and why they work better than other kinds of models, given multilevel data.\nTo practice how we code for mixed-effects models, and how we read or write about the results.\n\nWe now need to develop our understanding and skills further. And we now need to examine some of the complexities that we may face when we work with mixed-effects models.\nOur approach will continue to depend on verbal explanation, visualization and a practical code-based approach to the modeling."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-ideas",
    "href": "PSYC402/Week18.html#sec-dev-mixed-ideas",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "Important\n\n\n\nShrinkage or regularization means that models of data should be excited by the data but not too excited.\n\n\nThis means our models work better if they are informed by all the data, and take into account random differences but also if they are not too strongly influenced by individual (participant or item) data."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-targets",
    "href": "PSYC402/Week18.html#sec-dev-mixed-targets",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "We are probably now at a stage, in the development of our skills and understanding, where we can be more specific about our targets for learning: what capacities or abilities we want to have by the time we complete the course. I have held back specifying the targets in this way because, first, we had to learn the basic vocabulary. Now that we have done that, we can lay out the targets against which we can assess the progression of our learning.\n\n\n\n\n\n\nImportant\n\n\n\nWe have three capacities we seek to develop. These include the capacity:\n\nto understand mixed-effects models;\nto work with these models practically or efficiently in R;\nand to communicate their results effectively (to ourselves and others).\n\n\n\nWe should be aware that the development of skills and understanding in relation to each of these capacities will travel at different speeds for different people, and within any person at different speeds for different capacities.\nWe should also be aware that our internal evaluation of our understanding will not exactly match the evaluation that comes from external assessment. In other words, we might not be satisfied with our understanding but, still, our understanding might be satisfactory. It might be that we can learn to say in words what mixed-effects models are or involve, or what their results mean, very effectively even if we remain unsure about our understanding.\nFor these reasons, I specify what we are aiming to develop in terms of what we can do. You can test your development against this checklist of targets for learning.\n\nWe want to develop the capacity to understand mixed-effects models, the capacity to:\n\n\nrecognize where data have a multilevel structure;\nrecognize where multilevel or mixed-effects models are required;\ndistinguish the elements of a mixed-effects model, including fixed effects and random effects;\nexplain how random effects can be understood in terms of random differences (or deviations) between groups or classes or individuals, in intercepts or slopes;\nexplain how random effects can be understood in terms of variances, as a means to account for random differences between groups or classes or individuals in intercepts or slopes;\nexplain how mixed-effects models work better than linear models, for multilevel structured data;\nexplain how mixed-effects models work better because they allow partial-pooling of estimates.\n\n\nWe want to develop the capacity to work practically with mixed-effects models in R, the capacity to:\n\n\nspecify a mixed-effects model in lmer() code;\nidentify how the mixed-effects model code varies, depending on the kinds of random effects that are assumed;\nidentify the elements of the output or results that come from an lmer() mixed-effects analysis;\ninterpret the fixed-effects estimates;\ninterpret the random effects estimates, including both the variance and covariance estimates.\n\n\nWe want to develop the capacity to communicate the results of mixed-effects models effectively, to ourselves and to others, the capacity to:\n\n\ndescribe in words and summary tables the results of a mixed-effects model;\nvisualize the effects estimates or predictions from a mixed-effects model."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-guide",
    "href": "PSYC402/Week18.html#sec-dev-mixed-guide",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "I have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 13 minutes\nPart 2 – about 13 minutes\nPart 3 – about 24 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter. The lectures provide a summary of the main points.\n2. Chapter: 03-mixed\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data with crossed random effects.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example ML word recognition study dataset.\nEdit example code to create alternate visualizations of variable distributions and of the relationships between critical variables.\nExperiment with the .R code used to work with the example data.\nRun linear mixed-effects models of demonstration data.\nRun linear mixed-effects models of alternate data sets.\nReview the recommended readings (Section 1.16).\n\n3. Practical workbook materials\n3.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-data",
    "href": "PSYC402/Week18.html#sec-dev-mixed-data",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "In this chapter, we will be working with the ML word recognition study dataset. ML examined visual word recognition in younger and older adults using the lexical decision task.\nIn lexical decision, participants are presented with a stimulus: a string of letters that is either a real word (e.g., ‘car’) or a made-up or non-word (e.g., ‘cas’). Participants are required to respond to the stimulus by pressing a button to indicate either that they think the stimulus is a word or that they think it is a non-word. Each complete sequence of events, in which a stimulus is presented and a response is recorded, is known as a trial. In the lexical decision task implemented by ML, all study participants were presented with a mix of 160 word stimuli and 160 non-word stimuli, in random order, in a total of 320 trials.\nEach stimulus was presented one at a time on the computer screen. The critical outcome measure was the reaction time (RT) or latency for each response. Observed RT represents the interval of time from the moment the stimulus was first presented (the stimulus onset) to the moment the response was made (the response onset).\nLexical decision is a very popular technique for examining word recognition, especially in adults. While not every graduate student will be interested in word recognition, or reading, everyone should understand that tasks like lexical decision are similar to a range of other tasks used in experimental psychological science.\nThe critical feature of the study, here, is that we have an outcome – a decision response – observed multiple times (for each stimulus) for each participant. We shall be analyzing the speed of response, reaction time (RT), measured in milliseconds (ms).\nIn our analyses, the focus of our interest will be on the ways in which participant attributes (like age) or word properties (like frequency) influence the speed of response in a task designed measure the ability to recognize visually presented English words. In analyzing the effects of participant attributes on recognition response RTs, we will use data – about those attributes – that were recorded using a mix of survey questions (about age, etc.) and standardized ability tests that were administered to study participants alongside the lexical decision task.\nThe total number of participants for this study was 39, including a group of younger adults and a group of older adults. Information was collected about the participants’ age, education and gender. In addition, participants were asked to complete ability measures (TOWRE sight word and phonemic tests, @torgesen1999towre) and a measure of reading experience (Author Recognition Test, ART, @Masterson2007c).\n\n\nInstead of posing a simple and general research question, we shall orient our work around a set of quite specific predictions. ML hypothesized:\n\nEffects of stimulus attributes\n\n\nPredicting that words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\n\n\nEffects of participant attributes\n\n\nPredicting that older readers would be faster and more accurate than younger readers in word recognition.\n\n\nEffects of interactions between the effects of word attributes and person attributes.\n\n\nPredicting that better (older) readers will show smaller effects of word attributes.\n\nIn this chapter, we can focus on one specific prediction as we work through the practical steps of conducting an analysis using linear mixed-effects models.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\n\n\n\n\n\n\nIn summary, ML collected data on lexical decision task response reaction times (RTs) and accuracy and information on participants, including age, reading ability and reading experience. In addition, she collected information on the properties of the lexical decision stimulus items, including variables like the length or frequency of words (values taken from the English Lexicon Project, @Balota2007).\nThe ML study data includes the following variables that we will work with (as well as some you can ignore):\n\nIdentifying variables\n\n\nsubjectID – identifying code for participants\nitem_name – words presented as stimuli\nitem_number – identifying code for words presented\n\n\nResponse variables\n\n\nRT – response reaction time (ms), for responses to words\n\n\nParticipant attribute variables\n\n\nAge – in years\nGender – coded M (male), F (female)\nTOWRE_wordacc – word reading skill, words read correctly (out of 104)\nTOWRE_nonwordacc – nonword reading skill, nonwords (made up words) read correctly (out of 63)\nART_HRminusFR – reading experience score\n\n\nStimulus property variables\n\n\nLength – word length, in letters\nOrtho_N – orthographic neighbourhood size, how many other words in English a stimulus word looks like\nOLD – orthographic Levenshtein distance, how many letter edits (addition, deletion or substitution) it would take to make a stimulus word look like another English word (a measure of orthographic neighbourhood) [@Yarkoni2008b]\nBG_Sum, BG_Mean, BG_Freq_By_Pos – measures of how common are pairs of letters that compose stimulus words\nSUBTLWF, LgSUBTLWF, SUBTLCD, LgSUBTLCD – measures of how common stimulus words are, taken from the SUBTLEX corpus analysis of word frequency [@Brysbaert2009a]\n\n\n\n\nYou can download the data-03-mixed.zip files folder to get the data you need for the practical work we will be doing for this chapter.\nThe data are held in one file:\n\nsubjects.behaviour.words-310114.csv which holds information about the (word) stimuli, participants, and the responses recorded in the ML study.\n\nThe .csv file is a comma separated values file and can be opened in Excel.\nThe data file is collected together with the .R scripts:\n\n03-mixed-workbook.R the workbook you will need to do the practical exercises.\n03-mixed-workbook-answers.R with answers to questions and code for exercises."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-tidy",
    "href": "PSYC402/Week18.html#sec-dev-mixed-tidy",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "In ?@sec-intro-mixed-tidy-data, we saw how we may need to tidy the data we collect in experimental studies: combining data about responses with data about participant attributes or stimulus properties, and restructuring the data so that they are in a tidy format. For this class, many steps in the process of data tidying were completed previously. Thus, we only need to perform steps 1, 3 and 4 of the usual data tidying process:\n\nImport the data or read the data into R, see Section 1.6.1;\nRestructure the data;\nSelect or transform variables, see Section 1.6.4;\nFilter observations, see Section 1.6.3.\n\nWe are going to first filter the observations, then transform the outcome variable. We will explain why we have to do this as we proceed.\nWe will use tidyverse library functions to do this work, as usual.\n\nlibrary(tidyverse)\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read one file into R.\n\nML.all &lt;- read_csv(\"subjects.behaviour.words-310114.csv\", na = \"-999\")\n\nThe data file subjects.behaviour.words-310114.csv holds all the data about everything (behaviour, participants, stimuli) we need for our analysis work.\n\n\n\n\n\n\nTip\n\n\n\nIt is always a good idea to first inspect what you have got when you read a data file into R before you do anything more demanding.\n\nYou cannot assume that the data are what you think they are\nor that the data are structured or coded in the ways that you think (or have been told) they should be structured or coded.\n\n\n\nYou can inspect the first few rows of the dataset using head().\n\n\nWarning in subjectID == c(\"GB9\", \"NH1\", \"A15\", \"B18\"): longer object length is\nnot a multiple of shorter object length\n\n\n\n\n\nitem_number\nsubjectID\nTest\nAge\nYears_in_education\nGender\nTOWRE_wordacc\nTOWRE_nonwordacc\nART_HRminusFR\nRT\nCOT\nSubject\nTrial.order\nitem_name\nLength\nOrtho_N\nBG_Sum\nBG_Mean\nBG_Freq_By_Pos\nitem_type\nSUBTLWF\nLgSUBTLWF\nSUBTLCD\nLgSUBTLCD\nOLD\n\n\n\n\n1\nGB9\nALT\n21\n11\nF\n78\n41\n18\n368.66\n134057.8\nGB9\n54\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nNH1\nTAL\n52\n18\nM\n78\n56\n33\n724.83\n742737.4\nNH1\n148\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nA15\nLTA\n21\n16\nF\n95\n57\n9\n483.71\n861801.0\nA15\n278\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nB18\nTLA\n69\n11\nM\n85\n54\n10\n517.62\n1024583.4\nb18\n318\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n\n\n\n\n\nYou can examine all the variables using summary().\n\nsummary(ML.all)\n\n  item_number      subjectID             Test                Age       \n Min.   :  1.00   Length:5440        Length:5440        Min.   :16.00  \n 1st Qu.: 40.75   Class :character   Class :character   1st Qu.:21.00  \n Median : 80.50   Mode  :character   Mode  :character   Median :21.00  \n Mean   : 80.50                                         Mean   :36.94  \n 3rd Qu.:120.25                                         3rd Qu.:53.00  \n Max.   :160.00                                         Max.   :73.00  \n Years_in_education    Gender          TOWRE_wordacc    TOWRE_nonwordacc\n Min.   :11.00      Length:5440        Min.   : 68.00   Min.   :16.00   \n 1st Qu.:13.00      Class :character   1st Qu.: 84.00   1st Qu.:50.00   \n Median :16.00      Mode  :character   Median : 93.00   Median :55.50   \n Mean   :14.94                         Mean   : 91.24   Mean   :52.41   \n 3rd Qu.:16.00                         3rd Qu.: 98.00   3rd Qu.:57.00   \n Max.   :19.00                         Max.   :104.00   Max.   :63.00   \n ART_HRminusFR         RT               COT            Subject         \n Min.   : 1.00   Min.   :-2000.0   Min.   :  50094   Length:5440       \n 1st Qu.: 7.00   1st Qu.:  498.1   1st Qu.: 297205   Class :character  \n Median :11.00   Median :  577.6   Median : 552854   Mode  :character  \n Mean   :15.15   Mean   :  565.3   Mean   : 575780                     \n 3rd Qu.:21.00   3rd Qu.:  677.4   3rd Qu.: 810108                     \n Max.   :43.00   Max.   : 1978.4   Max.   :1583651                     \n  Trial.order     item_name             Length       Ortho_N      \n Min.   : 21.0   Length:5440        Min.   :3.0   Min.   : 0.000  \n 1st Qu.:100.8   Class :character   1st Qu.:4.0   1st Qu.: 3.000  \n Median :180.5   Mode  :character   Median :4.0   Median : 6.000  \n Mean   :180.5                      Mean   :4.3   Mean   : 7.069  \n 3rd Qu.:260.2                      3rd Qu.:5.0   3rd Qu.:11.000  \n Max.   :340.0                      Max.   :6.0   Max.   :24.000  \n     BG_Sum          BG_Mean       BG_Freq_By_Pos   item_type        \n Min.   :  3.00   Min.   :  1.00   Min.   :  1.0   Length:5440       \n 1st Qu.: 81.75   1st Qu.: 67.75   1st Qu.: 74.5   Class :character  \n Median :151.50   Median :153.50   Median :158.0   Mode  :character  \n Mean   :155.89   Mean   :153.82   Mean   :149.6                     \n 3rd Qu.:234.75   3rd Qu.:239.25   3rd Qu.:227.0                     \n Max.   :314.00   Max.   :316.00   Max.   :295.0                     \n    SUBTLWF          LgSUBTLWF        SUBTLCD        LgSUBTLCD    \n Min.   :   0.57   Min.   :1.477   Min.   : 0.32   Min.   :1.447  \n 1st Qu.:  17.36   1st Qu.:2.947   1st Qu.: 6.67   1st Qu.:2.748  \n Median :  69.30   Median :3.549   Median :23.64   Median :3.298  \n Mean   : 442.01   Mean   :3.521   Mean   :36.52   Mean   :3.137  \n 3rd Qu.: 290.70   3rd Qu.:4.171   3rd Qu.:65.24   3rd Qu.:3.739  \n Max.   :6161.41   Max.   :5.497   Max.   :99.70   Max.   :3.922  \n      OLD       \n Min.   :1.000  \n 1st Qu.:1.288  \n Median :1.550  \n Mean   :1.512  \n 3rd Qu.:1.750  \n Max.   :2.050  \n\n\nThe summary shows some features of the dataset, or of how R interprets the dataset, that are of immediate interest to us, though we do not necessarily have to do anything about them.\n\nWe can see statistical summaries – showing the mean, median, minimum and maximum, etc. – of numeric variables like the outcome variable RT.\nWe can see statistical summaries, also, of variables that comprise number values but which we do not want to be treated as numbers, e.g., the word stimulus coding variable item_number.\nWe can see that some variables are simply listed as Class: character. That tells us that one or more values in the columns in the datasheet that correspond to these variables are words or strings of letters or alphanumeric characters.\nThere is no sign of the presence of missing values in this dataset, no counts of NAs.\n\nWe do not really want R to treat a coding variable like item_number as numeric: it functions as a categorical or nominal variable, a factor. And we want R to treat coding variables like subjectID as factors. In ?@sec-multi-data-coerce, we saw how we can require R to handle variables exactly as we require it to using coercion. In ?@sec-intro-mixed-data-load, we saw how we can determine how R treats variables at the read-in stage, using col_types() specification. We are going to do neither here because we do not have to do this work; not doing it will have no impact on our analyses at this point.\nWhat we do need to do is deal with a problem that is already apparent in the summary statistics – did you spot it? If we look at the summary, we can see that RT includes values as low as -2000. That cannot be right.\n\n\n\nWe should examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms). Observations about variable value distributions are a part of Exploratory Data Analysis and serve to catch errors in the dataset (e.g. incorrectly recorded scores) but also to inform the researcher’s understanding of their own data.\nWe shall examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms), using density plots. An alternative method would be to use histograms. I choose to use density plots because they allow the easy comparison of the distributions of values of a continuous numeric variable like reaction time. A density plot shows a curve. You can say that the density corresponds to the height of the curve for a given value of the variable being depicted, and that it is related to the probability of observing values of the variable within some range of values [@howell2016fundamental].\nGetting a density plot of RTs of responses is easy using ggplot() code.\n\nML.all %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) +\n  geom_rug(alpha = .2) +\n  ggtitle(\"Raw RT\") +\n  theme_bw()  \n\n\n\n\nFigure 1: Density plot showing word recognition reaction time, correct and incorrect responses\n\n\n\n\nThe code delivers a plot (Figure 1) showing three peaks in the distribution of RT values. You can see that there is a peak of RT observations around 500-1000ms, another smaller peak around -500ms, and a third smaller peak around -2000ms.\nThe density plot shows the reaction times recorded for participants’ button press ‘yes’ responses to word stimuli in the lexical decision task. The peaks of negative RTs represent observations that are impossible.\nRemember that reaction time, in a task like lexical decision, represents the interval in time between the onset of a task stimulus (in lexical decision, a word or a nonword) and the onset of the response (the button press to indicate the lexical decision). We cannot have negative time intervals. The explanation is that ML collected her data using the DMDX experimental software application [@Forster2003a]. DMDX records the reaction times for incorrect responses as negative RTs.\nThe code to produce Figure 1 works in a series of steps.\n\nML.all %&gt;% takes the dataset, from the ML study, that we have read in to the R workspace and pipes it to the visualization code, next.\nggplot(aes(x = RT)) + creates a plot object in which the x-axis variable is specified as RT. The values of this variable will be mapped to geometric objects, i.e. plot features, that you can see, next.\ngeom_density(size=1.5) + first displays the distribution of values in the variable RT as a density curve. The argument size=1.5 tells R to make the line \\(1.5 \\times\\) the thickness of the line used by default to show variation in density.\n\nSome further information is added to the plot, next.\n\ngeom_rug(alpha = .2) + with a command that tells R to add a rug plot below the density curve.\nggtitle(\"Raw RT\") makes a plot title.\n\nNotice that beneath the curve of the density plot, you can see a series of vertical lines. Each line represents the x-axis location of an RT observation in the ML study data set. This rug plot represents the distribution of RT observations in one dimension.\n\ngeom_rug() draws a vertical line at each location on the x-axis that we observe a value of the variable, RT, named in aes(x = RT).\ngeom_rug(alpha = .2) reduces the opacity of each line, using alpha, to ensure the reader can see how the RT observations are denser in some places than others.\n\nYou can see that we have many more observations of RTs from around 250ms to 1250ms, where the rug of lines is thickest, under the peak of the density plot. This indicates what the two kinds of plots are doing.\n\n\nYou should try out alternative visualisation methods to reveal the patterns in the distribution of variables in the ML dataset (or in your own data).\n\nTake a look at the geoms documented in the {ggplot2} library reference section here.\nExperiment with code to answer the following questions:\n\n\nWould a histogram or a frequency polygon provide a more informative view? Take a look here for advice.\nWhat about a dotplot? Take a look here for advice\n\n\n\n\n\nThe density plot shows us that the raw ML lexical decision RT variable includes negative RT values corresponding to incorrect response. These have to be removed. We can do this quite efficiently by creating a subset of the original “raw” data, defined according to the RT variable using the {dpyr} library filter() function.\n\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200)\n\nAfter we have removed negative (error) RTs, we check that the size of the dataset – here, the number of rows – matches our expectations. We do this to make sure that we did the filter operation correctly.\n\nlength(ML.all$RT)\n\n[1] 5440\n\nlength(ML.all.correct$RT)\n\n[1] 5257\n\n\nIf you run the length() function calls then you should see that the length or number of observations or rows in the ML.all.correct dataset should be smaller than the number of observations in the ML.all dataset.\n\n\n\n\n\n\nTip\n\n\n\nIt is wise to check that the operations you perform to tidy, process or wrangle data actually do do what you mean them to do. Checks can be performed, for each processing stage, by:\n\nForming expectations or predictions about what the operation is supposed to do e.g. filter out some rows by some number;\nCheck what you get against these predictions e.g. count the number of rows before versus after filtering.\n\n\n\nHaving obtained a new data frame with data on just those trials where responses were correct, we can plot the distribution of RTs for just the correct responses (Figure 2).\n\nML.all.correct %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct RTs\") +\n  theme_bw()\n\n\n\n\nFigure 2: Density plot showing word recognition reaction time, correct responses only\n\n\n\n\nThe filter code is written to subset the data by rows using a condition on the values of the RT variable.\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200) works as follows.\n\nML.all.correct &lt;- filter(ML.all ...) creates a new dataset with a new name ML.all.correct from the old dataset ML.all using the filter() function.\nfilter(... RT &gt;= 200) specifies an argument for the filter() function.\n\nIn effect, we are asking R to check every value in the RT column.\n\nR will do a check through the ML.all dataset, row by row.\nIf a row includes an RT that is greater than or equal to 200 then that row will be included in the new dataset ML.all.correct. This is what I mean by using a condition.\nBut if a row includes an RT that is less than 200, then that row will not be included. We express this condition as RT &gt;= 200.\n\nThe length() function will count the elements in whatever object is specified as an argument in the function call.\n\nThis means that if you put a variable name into the function as in length(dataset$variable) it will count how long that variable is – how many rows there are in the column.\nIf that variable happens to be, as here, part of a dataset, the same calculation will tell you how many rows there are in the dataset as a whole.\nIf you just enter length(dataset), naming some dataset, then the function will return a count of the number of columns in the dataset.\n\n\n\nVary the filter conditions in different ways.\n\nChange the threshold for including RTs from RT &gt;= 200 to something else: you can change the number, or you can change the operator from &gt;= to a different comparison (try =, &lt;, &lt;=, &gt;.\nCan you assess what impact the change has?\n\nNote that you can count the number of observations (rows) in a dataset using e.g. length().\n\n\n\nI choose to filter out or exclude not only error responses (where \\(RT &lt; 0ms\\)) but also short reaction times (where \\(RT &lt; 200ms\\)). I think that any response in the lexical decision task that is recorded as less than 200ms cannot possibly represent a real word recognition response. Participants who complete experimental psychological tasks can and do press the button before they have time to engage the psychological processes (like word recognition) that the tasks we administer are designed to probe (like lexical decision).\nThere is some relevant literature that concerns the speed at which neural word recognition processes operate. However, I think you should note that the threshold I am setting for exclusion, here, is essentially arbitrary. If you think about it, I could have set the threshold at any number from \\(100-300ms\\) or some other range.\n\n\n\n\n\n\nWarning\n\n\n\nWhat is guiding me in setting the filter threshold is experience. But other researchers will have different experiences and set different thresholds.\n\nThis is why using exclusion criteria to remove data is problematic.\n\n\n\nFiltering or re-coding observations is an important element of the research workflow in psychological science. How we do or do not remove observations from original data may have an impact on our results (as explored by @steegen2016). It is important, therefore, that we learn how to do this reproducibly using, for example, R scripts that we can share with our research reports.\nI would argue that, at minimum, a researcher should report their research including:\n\nWhat exclusion criteria they use to remove data, explaining why.\nReport analyses with and without exclusions, to indicate if their results are sensitive to their decisions.\n\nYou can read further information about the practicalities of using R to do filtering here.\nYou can read a brief discussion of the impacts of researcher choices in dataset construction in ?@sec-multiversedata and in @steegen2016.\n\n\n\n\nFigure 2 shows that we have successfully removed all errors (negative RTs) but now we see just how skewed the RT distribution is. Note the long tail of longer RTs.\nMost researchers assume that participants – healthy young adults – take about 500-1000ms to perform the task and that values outside that range correspond to either fast guesses (RTs that are too short) or to distracted or tired or bored responses (RTs that are too long). In theory, the lexical decision task should be probing automatic cognitive processes, measuring the steps from perception to visual word recognition in the time interval between the moment the stimulus is first shown and the moment the button is pressed by the participant to indicate a response. Thus, it might seem natural to exclude extreme RT values which might correspond not to automatic cognitive processes but to unknowable distraction events or boredom and inattention. However, we shall complete no further data exclusions.\nFor now, we can look at a commonly used method to deal with the skew that we typically see when we examine reaction time distributions. RT distributions are usually skewed with a long tail of longer RTs. You can always take longer to press the button but there is a limit to how much faster you can make your response.\nGenerally, we assume that departures from a model’s predictions about our observations (the linear model residuals) are normally distributed, and we often assume that the relationship between outcome and predictor variables is linear [@Cohen2003c]. We can ensure that our data are compliant with both assumptions by transforming the RT distribution.\nIt is not cheating to transform variables. Transformations of data variables can be helpful for a variety of reasons in the analysis of psychological data [@Cohen2003c; @Gelman2007ga]. I do recommend, however, that you are careful to report what transformations you use, and why you do them.\nPsychology researchers often take the log (often the log base 10) of RT values before performing an analysis. Transforming RTs to the log base 10 of RT values has the effect of correcting the skew – bringing the larger RTs ‘closer’ (e.g., \\(1000 = 3\\) in log10) to those near the middle which do not change as much (e.g. \\(500 = 2.7\\) in log10).\n\nML.all.correct$logrt &lt;- log10(ML.all.correct$RT)            \n\nWe can see the effect of the transformation if we plot the log10 transformed RTs (see Figure 3). We arrive at a distribution that more closely approximates the normal distribution.\n\nML.all.correct %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct log10 RTs\") +\n  theme_bw()\n\n\n\n\nFigure 3: Density plot showing log10 transformed reaction time, correct responses only\n\n\n\n\nThe log10() function works as follows:-\n\nML.all.correct$logrt &lt;- log10(...) creates a a new variable logrt, adding it to the ML.all.correct dataset. The variable is created using the transformation function log10().\nlog10(ML.all.correct$RT) creates a the new variable by transforming (to log10) the values of the old variable, RT.\n\n\n\n\n\n\n\nTip\n\n\n\nThere are other log transformation functions and we often see researchers using the natural log instead of the log base 10 as discussed here\n\n\n\n\n\nEven when data have been structured appropriately, we will still, often, need to do some tidying before we can do an analysis. Most research work involving quantitative evidence requires a big chunk of data tidying or other processing before you get to the statistics.\nOur data are now ready for analysis."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-crossed-random",
    "href": "PSYC402/Week18.html#sec-dev-mixed-crossed-random",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "As we saw in ?@sec-intro-mixed, many Psychologists conduct studies where it is not sensible to think of observations as being nested [@baayen2008]. In this chapter, we turn to the ML word recognition study dataset, which has a structure similar to the CP study data that we worked with previously. Again, the core concern is that the data come from a study with a repeated-measures design where the experimenter presented multiple stimuli for response to each participant, for several participants, so that we have multiple observations for each participant and multiple observations for each stimulus. Getting practice with this kind of data will help you to easily recognize what you have got when you see it in your own work.\nML asked all participants in a sample of people to read a selection of words, a sample of words from the language.\nFor each participant, we will have multiple observations and these observations will not be independent. One participant will tend to be slower or less accurate compared to another. Her responses may be more or less susceptible to the effects of the experimental variables. The lowest trial-level observations can be grouped with respect to participants. However, the data can also be grouped by stimuli.\nFor each stimulus word, there are multiple observations and these observations will not be independent. One stimulus may prove to be more challenging to all participants compared to another, eliciting slower or less accurate responses on average. In addition, if there are within-items effects, we may ask if the impact of those within-items effects is more prominent, stronger, among responses to some items compared to others.\nGiven this common repeated-measures design, we can analyse the outcome variable in relation to:\n\nfixed effects: the impact of independent variables like participant reading skill or word frequency;\nrandom effects: the impact of random or unexplained differences between participants and also between stimuli."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-working-lme",
    "href": "PSYC402/Week18.html#sec-dev-mixed-working-lme",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "We are going to respond to the multilevel (or crossed random effects) structure in the data by using linear mixed-effects models to analyze the data. This week, we are going to look at what mixed-effects models do from a new perspective.\nOur concern will be with different ways of thinking about why mixed-effects models are superior to linear models where data have a multilevel structure. Mixed-effects models tend to be more accurate in this (very common) situation because of what is called partial pooling and shrinkage or regularization. We use our practical example to explore these ideas.\n\n\nTo get started, we can examine – for each individual separately – the distribution of log RT observations, in Figure 4.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\n\n\n\nFigure 4: Density plot showing log10 transformed reaction time, correct responses, separately for each participant\n\n\n\n\nFigure 4 shows that RT distributions vary considerably between people. The plot imposes a dashed red line to indicate where the mean log10 RT is, calculated over all observations in the dataset. The plot shows the distribution of log RT for each participant, as a density drawn separately for each person. The individual plots are ordered by the mean log RT calculated per person, so plots appear in order from the fastest to the slowest.\nThe grid of plots illustrates some interesting features about the data in the ML study sample. You can see how the distribution of log RT varies between individuals: some people show widely spread reaction times; some people show quite tight or narrow distributions. You can see how the shapes of the distributions varies: some people show skew; others do not. I do not see that the variation in the shapes of the distributions is related to the average speed of the person’s responses.\nI think the key message of the plot is that some distributions are wider (RTs are more spread out) than others. We might be concerned that people who present more variable reaction times (wider distributions) may be associated with less reliable estimates of their average response speed, or of the impact of word attributes (like word frequency) on their response speed.\n\n\nThe plotting code I used to produce Figure 4 progresses through a series of steps. This example demonstrates how you can combine data tidying and plotting steps in a single sequence, using tidyverse functions and the %&gt;% pipe, so I will take the time to explain what is going on.\nMy aim is to create a grid of individual plots, showing the distribution of log RTs for each participant, so that the plots are presented in order, from the fastest participant to the slowest. Take a look at the plotting code. We can explain how it works, step by step.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\nYou will see that we present the distribution of RTs using geom_density() and that we present a separate plot for each person’s data using facet_wrap(). To these elements, we add some pre-processing steps to calculate the average response speed of each individual, and to reorder the dataset by those averages.\nIt will make it easier to understand what is going on if we consider the code in chunks.\nFirst, we pre-process the data before we feed it into the plotting code.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ...\n\n\nML.all.correct %&gt;% takes the selected filtered dataset ML.all.correct and pipes it %&gt;% to the next step.\ngroup_by(subjectID) %&gt;% tells R to group the data by subject ID. We have a set of multiple log RT observations for each subjectID because each participant was asked to respond to multiple word stimuli.\nmutate(mean_logrt = mean(logrt, na.rm = TRUE)) next calculates and stores the mean log RT for each person. We create a new variable mean_logrt. We calculate the average of the set of log RTs recorded for each subjectID and construct the new variable mean_logrt from these averages.\n\nWe do not need to treat the data in groups so we remove the grouping, next.\n\nUsing ungroup() %&gt;% means that, having grouped the data to calculate the mean log RTs, we ungroup the dataset so that R can look at all observations in the next step.\nmutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;% asks R to look at all log RT observations in the dataset, and change the top-to-bottom order of the rows.\n\nWe ask R to order observations – using subjectID in fct_reorder() – so that each person’s data are listed by their average speed, mean_logrt from the fastest to the slowest. We then pipe these ordered data to the plotting code, next.\nIf you delete or comment out these first lines, you will see that R uses just a default ordering, drawing the plot for each person in the alphabetical order of their subjectID codes.\nTry it. Don’t forget to start with ML.all.correct %&gt;%.\nSecond, we draw the plots, using the data we have pre-processed.\n\nML.all.correct %&gt;%\n...\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n...\n\nThe key functions that create a grid of density plots are the following.\n\nggplot(aes(x = logrt)) tells R to work with logrt as the x-axis variable. We shall be plotting the distribution of logrt.\ngeom_density(...) draws a density plot to show the distribution of log RT, using a thicker line size = 1.25\nfacet_wrap(~ subjectID) creates a different plot for each level of the subjectID factor: we want to see a separate plot for each participant.\n\n\nfacet_wrap(~ subjectID) works to split the dataset up by participant, with observations corresponding to each participant identified by their subjectID, and to then split the plotting to show the distribution of log RT separately for each participant.\n\nI wanted to present the plots in order of the average speed of response of participants. If you look at Figure 4, you can see that the position of the peak of the log RT distribution for each participant moves, from the fastest plots where the peak is around \\(log RT = 2.5\\) (shown from the top left of the grid), to the slowest plots where the peak is around \\(log RT = 2.75\\) (shown towards the bottom right of the grid)\nWe can then use further ggplot functions to edit the appearance of the plot, to make it more useful.\n\n...\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\n\ngeom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) draws a vertical red dashed line at the location of the mean log RT, the average of all log RTs over all participants in the dataset.\nscale_x_continuous(breaks = c(2.5,3)) adjusts the x-axis labeling. The ggplot default might draw too many x-axis labels i.e. showing possible log RT values as tick marks on the bottom line of the plot. I want to avoid this as sometimes all the labels can be crowded together, making them harder to read.\n\n\nDrawing a vertical line at the mean calculated overall is designed to help the reader (you) calibrate their comparison of the data from different people.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is often useful to experiment with example code to figure out how it works.\n\n\nOne way you can do this is by commenting out one line of code, at a time by putting the # at the start of the line.\n\nIf you do this, you can see what the line of code does by, effectively, asking R to ignore it.\n\nAnother way you can experiment with code is by seeing what you can change and what effect the changes have.\n\nCan you work out how to adapt the plotting code to show a grid of histograms instead of density plots?\nCan you work out how to adapt the code to show a grid of plots indicating the distribution of log RT by different words instead of participants?\n\n\n\n\n\nAs we have discussed in previous chapters, a good way to approach a mixed-effects analysis is by first estimating the effects of the experimental variables (here, frequency) using linear models, ignoring the hierarchical structure in the data.\n\n\n\n\n\n\nNote\n\n\n\nA linear model of multilevel structured data can be regarded as an approximation to the better analysis.\n\n\nWe model the effects of interest, using all the data (hence, complete pooling) but ignoring the differences between participants. This means we can see something of the ‘true’ picture of our data through the linear model results but the linear model misses important information, which the mixed-effects model will include, that would improve its performance.\nAs we saw, in a similar analysis in ?@sec-intro-mixed, we can estimate the relationship between reading reaction times (here, lexical decision RTs) and word frequency using a linear model:\n\\[\nY_{ij} = \\beta_0 + \\beta_1X_j + e_{ij}\n\\]\nWhere:\n\n\\(Y_{ij}\\) is the value of the observed outcome variable, the log RT of the response made by the \\(i\\) participant to the \\(j\\) item;\n\\(\\beta_1X_j\\) refers to the fixed effect of the explanatory variable (here, word frequency), where the frequency value \\(X_j\\) is different for different words \\(j\\), and \\(\\beta_1\\) is the estimated coefficient of the effect due to the relationship between response speed and word frequency;\n\\(e_{ij}\\) is the residual error term, representing the differences between observed \\(Y_{ij}\\) and predicted values (given the model) for each response made by the \\(i\\) participant to the \\(j\\) item.\n\nThe linear model is fit in R using the lm() function.\n\nML.all.correct.lm  &lt;- lm(logrt ~\n                             \n                             LgSUBTLCD,     \n                           \n                           data = ML.all.correct)\n\nsummary(ML.all.correct.lm)\n\n\nCall:\nlm(formula = logrt ~ LgSUBTLCD, data = ML.all.correct)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41677 -0.07083 -0.01163  0.05489  0.53411 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.885383   0.007117  405.41   &lt;2e-16 ***\nLgSUBTLCD   -0.033850   0.002209  -15.32   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1095 on 5255 degrees of freedom\nMultiple R-squared:  0.04277,   Adjusted R-squared:  0.04259 \nF-statistic: 234.8 on 1 and 5255 DF,  p-value: &lt; 2.2e-16\n\n\nIn the estimates from this linear model, we see an approximate first answer to our prediction.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\nResult: We can see that, in this first analysis, the estimated effect of word frequency is \\(\\beta = -0.033850\\).\n\n\n\nI know this looks like a very small number but you should realize that the estimates for the coefficients of fixed effects like the frequency effect are scaled according to the outcome. Here, the outcome is log10 RT, where a log10 RT of 3 equals 1000ms, and, as we can calculate in R\n\nlog10(0.925)\n\n[1] -0.03385827\n\n\nAlso, remember that frequency is scaled in logs too, so the estimate of the coefficient tells us how log10 RT changes for unit change in log frequency. The coefficient represents the estimated change in log10 RT for unit change in log frequency LgSUBTLCD.\nThe estimate indicates that as word log frequency increases, responses logRT decreases by \\(-0.033850\\).\nIn this model, all the information from all participants is analyzed. In discussions of mixed-effects analyses, we say that this is a complete pooling model. This is because all the data have been pooled together, that is, we use all observations in the sample to estimate the effect of frequency.\nIn this model, the observations are assumed to be independent. However, we suppose that the assumption of independence is questionable given the expectation that participants will differ in their overall speed, and in the extent to which their response speed is affected by factors like word frequency.\n\n\n\nVary the linear model using different outcomes or predictors.\n\n\nThe ML study data, like the CP study data, are rich with possibility. It would be useful to experiment with it.\n\n\nChange the predictor from frequency to something else: what do you see when you visualize the relationship between outcome and predictor variables using scatterplots?\nSpecify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?\n\nI would recommend that you both estimate the effects of variables and visualize the relationships between variables using scatterplots. If you combine reflection on the model estimates with evaluation of what the plots show you then you will be able to see how reading model results and reading plots can reveal the correspondences between the two ways of looking at your data.\n\n\n\n\nWe can examine variation between participants by analyzing the data for each participant’s responses separately, fitting a different linear model of the effect of word frequency on lexical decision RTs for each participant separately. Figure 5 presents a grid or trellis of plots, one plot per person. In each plot, you can see points corresponding to the log RT of the responses made by each participant to the stimulus words.\n\n\n\n\n\n\nTip\n\n\n\nIn working with R, we often benefit from the vast R knowledge ecosystem.\n\nI was able to produce the sequence of plots Figure 5, Figure 6, Figure 7 and Figure 8 thanks to this very helpful blog post by TJ Mahr\n\n\n\nIn all plots, the pink or red line represents the complete pooling model estimate of the effect of frequency on response RTs. The line is the same for each participant because there is only one estimated effect, based on all data for all participants.\nIn addition, in each plot, you can see a green line. You can see that the line varies between participants. This represents the effect of frequency estimated using just the data for each participant, analyzed separately. These are the no pooling estimates. We call them the no pooling estimates because each is based just on the data from one participant.\n\n\n\n\n\nFigure 5: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant; red-pink line shows the complete pooling estimate, blue-green line shows the no-pooling estimate\n\n\n\n\nFigure 5 reveals substantial differences between participants in both average response speed and the frequency effect.\nWe may further predict variation in standard errors between participants given, also, the differences between participants in the spread of log RT, illustrated by Figure 4. Basically, where the distribution of log RT is more widely spread out, for any one participant, there it will be harder for us to estimate with certainty the mean or the sources of variance for the participant’s response speed.\nYou will notice that the no pooling and complete pooling estimates tend to be quite similar. But for some participants – more than for others – there is variation between the estimates.\nYou can reflect that the complete pooling is unsatisfactory because it ignores the variation between the participants: some people are slower than others; some people do show a larger frequency effect than others. You can also reflect that the no pooling is unsatisfactory because it ignores the similarities between the participants.\nWhile there is variation between participants there is also similarity across the group so that the effect of frequency is similar between participants.\n\n\n\n\n\n\nImportant\n\n\n\nWhat we need is an analytic method that is capable of both estimating the overall average population-level effect (here, of word frequency) and taking into account the differences between sampling units (here, participants).\nThat method is linear mixed-effects modeling."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-lme",
    "href": "PSYC402/Week18.html#sec-dev-mixed-lme",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "As you have seen before, we can account for the variation – the differences between participants in intercepts and slopes.\nFirst, we model the intercept as two terms:\n\\[\n\\beta_{0i} = \\gamma_0 + U_{0i}\n\\]\nWhere:\n\n\\(\\gamma_{0}\\) is the average intercept, and\n\\(u_{0i}\\) is the difference for each participant between their intercept and the average intercept.\n\nSecond, we can model the frequency effect as two terms:\n\\[\n\\beta_{1i} = \\gamma_1 + U_{1i}\n\\]\nWhere:\n\n\\(\\gamma_{10}\\) is the average slope, and:\n\\(U_{1i}\\) represents the difference for each participant between the slope of their frequency effect and the average slope.\n\nWe can then incorporate in a single model the fixed effects due to the average intercept and the average frequency effect, as well as the random effects – the error variance due to unexplained differences between participants in intercepts and in frequency effects:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n\\]\nWhere the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and differences between \\(i\\) participants in the intercept \\(U_{0i}\\);\nthe average effect of the explanatory variable frequency \\(\\gamma_1X_j\\) and differences between \\(i\\) participants in the slope \\(U_{1i}X_j\\);\nin addition to residual error variance \\(e_{ij}\\).\n\n\n\n\nAs we first saw in ?@sec-intro-mixed, in conducting mixed-effects analyses, we do not aim to examine the specific deviation (here, for each participant) from the average intercept or the average effect or slope. We estimate just the spread of deviations by-participants.\nA mixed-effects model like our final model actually includes fixed effects corresponding to the intercept and the slope of the word frequency effect plus the variances:\n\n\\(var(U_{0i})\\) variance of deviations by-participants from the average intercept;\n\\(var(U_{1i}X_j)\\) variance of deviations by-participants from the average slope of the frequency effect;\n\\(var(e_{ij})\\) residuals, at the response level, after taking into account all other terms.\n\nWe may expect the random effects of participants or items to covary: for example, participants who are slow to respond may also be more susceptible to the frequency effect. Thus our specification of the random effects of the model can incorporate terms corresponding to the covariance of random effects:\n\n\\(covar(U_{0i}, U_{1i}X_j)\\)\n\n\n\n\nAs we know, some words elicit slower and some elicit faster responses on average. As we discussed in the last chapter (?@sec-intro-mixed-fixed-effect-fallacy), if we did not take such variation into account, we might spuriously identify an experimental effect actually due just to unexplained between-items differences in intercepts [@clark1973; @raaijmakers1999] committing an error: the language as fixed effect fallacy.\nWe can model the random effect of items on intercepts by modeling the intercept as two terms:\n\\[\n\\beta_{0j} = \\gamma_0 + W_{0j}\n\\]\nWhere:\n\n\\(\\gamma_{0}\\) is the average intercept, and\n\\(W_{0j}\\) represents the deviation, for each word, between the average intercept and the per-word intercept.\n\nNote that I ignore the possibility, for now, of differences between items in the slopes of fixed effects but I do come back to this.\nThe term the language as fixed effect fallacy [@clark1973; @raaijmakers1999] implies that thinking about the random effects of stimulus differences applies only when we are looking at experiments about language. But you should remember that we need to think about the impact of random differences between stimuli whenever we present samples of stimuli to participants, and we collect observations about multiple responses for each stimulus. This is true whatever the nature of the stimuli [see e.g. @judd2012].\n\n\n\nOur model can now incorporate the random effects of participants as well as items:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + W_{0j} + e_{ij}\n\\]\nIn this model, the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and the word frequency effect \\(\\gamma_1X_j\\);\nplus random effects due to unexplained differences between participants in intercepts \\(U_{0i}\\) and in the slope of the frequency effect \\(U_{1i}X_j\\);\nas well as random differences between items in intercepts \\(W_{0j}\\);\nin addition to the residual term \\(e_{ij}\\).\n\n\n\n\nWe fit a mixed-effects model of the \\(logrt \\sim \\text{frequency}\\) relationship using the lmer() function, taking into account:\n\nthe fact that the study data have a hierarchical structure – with observations sensibly grouped by participant;\nthe fact that both the frequency effect, and average speed, may vary between participants;\nand the fact that the average speed of response can vary between responses to different stimuli.\n\nThe model syntax corresponds to the statistical formula and the code is written as:\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n\n                           LgSUBTLCD +\n\n                           (LgSUBTLCD + 1|subjectID) +\n\n                           (1|item_name),\n\n                         data = ML.all.correct)\n\nsummary(ML.all.correct.lmer)\n\nAs will now be getting familiar, the code works as follows:\n\nML.all.correct.lmer  &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nlogrt ~ LgSUBTLCD the fixed effect in the model is expressed as a formula in which the outcome or dependent variable logrt is predicted ~ by the independent or predictor variable LgSUBTLCD word frequency.\n\n\n\n\n\n\n\nTip\n\n\n\nIf there were more terms in the model, the terms would be added in series separated by +\n\n\nThe random effects part of the model is then specified as follows.\n\nWe first have the random effects associated with random differences between participants:\n\n\n(...|subjectID) adds random effects corresponding to random differences between sample groups (participants subjects) coded by the subjectID variable.\n(...1 |subjectID) including random differences between sample groups (subjectID) in intercepts coded 1.\n(LgSUBTLCD... |subjectID) and random differences between sample groups (subjectID) in the slopes of the frequency effect coded by using theLgSUBTLCD variable name.\n\n\nThen, we have the random effects associated with random differences between stimuli:\n\n\n(1|item_name) adds a random effect to account for random differences between sample groups (item_name) in intercepts coded 1.\n\n\n...(..., data = ML.all.correct) specifies the dataset in which you can find the variables named in the model fitting code.\nLastly, we can then specify summary(ML.all.correct.lmer) to get a summary of the fitted model.\n\n\n\n\nIf you run the model code as written then you would see the following results.\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n\n                           LgSUBTLCD +\n\n                           (LgSUBTLCD + 1|subjectID) +\n\n                           (1|item_name),\n\n                         data = ML.all.correct)\n\nsummary(ML.all.correct.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887997   0.015479 186.577\nLgSUBTLCD   -0.034471   0.003693  -9.333\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\nIn these results, we see:\n\nFirst, information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula logrt ~ LgSUBTLCD + (LgSUBTLCD + 1|subjectID) + (1|item_name).\nThen, we see REML criterion at convergence about the model fitting process, which we can usually ignore.\nThen, we see information about the distribution of the model residuals.\nThen, we see theRandom Effects.\n\nNotice that the statistics are Variance Std.Dev. Corr., that is, the variance, the corresponding standard deviation, and the correlation estimates associated with the random effects.\n\nWe see Residual error variance, just like in a linear model, corresponding to a distribution or spread of deviations between the model prediction and the observed RT for each response made by a participant to a stimulus.\nWe see Variance terms corresponding to what can be understood as group-level residuals. Here, the variance is estimated for the spread in random differences between the average intercept (over all data) and the intercept for each participant, and the variance due to random differences between the average slope of the frequency effect and the slope for each participant.\nWe also see the variance estimated for the spread in random differences between the average intercept (over all data) and the intercept for responses to each word stimulus.\nAnd we see the Corr estimate, telling us about the covariance between random deviations (between participants) in the intercepts and in the slopes of the frequency effect.\n\n\nLast, just as for linear models, we see estimates of the coefficients (of the slopes) of the fixed effects, the intercept and the slope of the logrts ~ LgSUBTLCD relationship.\n\nWe can compare this estimate with our previous lm() estimate for the effect of frequency.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\nResult: We can see that, in this mixed-effects analysis, the estimated effect of word frequency is now \\(\\beta = -0.034471\\).\n\n\n\nThe estimate is different, a bit smaller. While the change in the estimate is also small, we may remember that we are looking at slope estimates for predicted change in log RT, in an experimental research area in which effects are often of the order of 10s of milliseconds. The estimates, and changes in the estimates, will tend to be quite small.\n\n\n\n\n\n\nWarning\n\n\n\nNote that we see coefficient estimates, as in a linear model summary but no p-values.\n\nWe will come back to this, see Section 1.13.4.\nHowever, note that if \\(t &gt;= 2\\) we can suppose that (for a large dataset) an effect is significant at the \\(.05\\) significance level."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-regularisation",
    "href": "PSYC402/Week18.html#sec-dev-mixed-regularisation",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "What is the impact of the incorporation of random effects – the variance and covariance terms – in mixed-effects models? Mixed-effects models can be understood, in general, as a method to compromise between ignoring the differences between groups (here, participants constitute groups of data) as in complete pooling or focusing entirely on each group (participant) as in no pooling [@Gelman2007ga]. In this discussion, I am going to refer to the differences between participants but you can assume that the lesson applies generally to any situation in which you have different units in a multilevel structured dataset in which the units correspond to groups or clusters of data.\n\n\nThe problem with ignoring the differences between groups (participants), as in the complete pooling model (here, the linear model), has been obvious when we examined the differences between participants (or between classes) in slopes and intercepts in previous weeks. The problem with focusing entirely on each participant, as in the no pooling model, has not been made apparent in our discussion yet.\nIf we analyze each participant separately then we will get, for each participant, for our model of the frequency effect, the per-participant estimate of the intercept and the per-participant estimate of the slope of the frequency effect. These no-pooling estimates will tend to exaggerate or overstate the differences between participants [@Gelman2007ga]. By basing the estimates on just the data for a person, in each per-participant analysis, the no-pooling approach overfits the data.\nYou could say that the no-pooling approach gives us estimates that depend too much on the sample of data we have got, and are unlikely to be similar to the estimates we would see in other samples in future studies.\n\n\n\n\n\n\nTip\n\n\n\nThe no-pooling estimates are too strongly influenced by the data we are currently analyzing.\n\n\n\n\n\nIf we look closely at Figure 5, we can see that there are similarities as well as differences between participants. Our analysis must take both into account.\nWhat happens in mixed-effects models is that we pool information, calculating the estimates for each participant, in part based on the information we have for the whole sample (all participants, in complete pooling), in part based on the information we have about the specific participant (one participant, in no pooling). Thus, for example, the estimated intercept for a participant in a mixed-effects model is given by the weighted average [@Snijders2004a] of:\n\nthe intercept estimate given by an analysis of just that participant’s data (no pooling estimate;\nand the intercept estimate given by analysis of all participants’ data (complete pooling estimate).\n\nThe weighted average will reflect our relative level of information about the participant’s responses compared to how much information we have about all participants’ responses.\nFor some participants, we will have less information – maybe they made many errors, so we have fewer correct responses for an analysis. For these people, because we have less information, the intercept estimate will get pulled (shrunk) towards the overall (complete pooling, all data) estimate.\nFor other participants, we have more information – maybe they made all correct responses. For these people, because we have more information, the intercept estimate will be based more on the data for each participant.\nTo make sense of what this means, think about the differences between participants in how much reliable information we can have, given our sample, about their average level of response speed or about how they are affected by experimental variables. Think back to my comments about Figure 4, about the differences between participants in how spread out the distributions of their log RT values are. Recall that I said that where participants’ responses are more spread out – just as where we have less observations for some participants than for others – we shall inevitably have less certainty about our estimates for the effects that influence their performance if we base our account on just their data. Mixed-effects models perform better – as prediction models – than no pooling approaches because they are not relying, for any participant, on just their sometimes unreliable data.\nWe can look again at a plot showing the data for each participant. Figure 6 presents a grid or trellis of plots, one plot per person. In each plot, you can see points corresponding to the RT of each response made by a participant to a stimulus word. In all plots, the pink line represents the complete pooling data model estimate of the effect of frequency on response RTs. In each plot, the green line represents the effect of frequency estimated using just the data for each participant, the no pooling estimates. Now, we also see blue lines that represent the mixed-effects model partial pooling estimates.\n\n\n\n\n\nFigure 6: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant; pink line shows the complete pooling estimate green line shows the no-pooling estimate; and blue line shows the linear mixed-effects model partial pooling estimate\n\n\n\n\nIt is quite difficult to identify, in this sample, where the partial pooling and no pooling estimates differ. We can focus on a few clear examples. Figure 7) presents a grid of plots for just four participants. I have picked some extreme examples but the plot illustrates how: (1.) for some participants e.g. AA1 all estimates are practically identical; (2.) for some participants EB5 JL3 JP3 the no-pooling and complete-pooling estimates are really quite different and (3.) for some participants JL3 JP3 the no-pooling and partial-pooling estimates are quite different.\n\n\n\n\n\nFigure 7: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant – for participants AA1, EB5, JL3 and JP3; pink line shows the complete pooling estimate green line shows the no-pooling estimate; and blue line shows the linear mixed-effects model partial pooling estimate\n\n\n\n\nIn general, partial pooling will apply both to estimates of intercepts and to estimates of the slopes of fixed effects like the influence of word frequency in reaction time. Likewise, if we consider this idea in general, we can see how it should work whether we are talking about groups or clusters of data grouped by participant or by stimulus or by school, class or clinic, etc.\nFormally, whether an estimate for a participant (in our example) is pulled more or less towards the overall estimate will depend not just on the number of data-points we have for that person. The optimal combined estimate for a participant is termed the Empirical Bayes estimate and the weighting – the extent to which the per-participant ‘estimate’ depends on the participant’s data or the overall data – depends on the reliability of the estimate (of the intercept or the frequency effect) given by analyzing that participant’s data [@Snijders2004a]. If you think about it, smaller samples – e.g. where a participant completed less correct responses – will give you less reliable estimates (and so will samples that show more variation).\nWhat we are looking at, here, is a form of regularization in which we use all the sources of information we can to ensure we take into account the variability in the data while not getting over-excited by extreme differences [@mcelreath2020]. We want to see estimates pulled towards an overall average where we have little data or unreliable estimates. We can see how strongly estimates can be shrunk in a plot like Figure 8.\nFigure 8 illustrates the shrinkage effect. I plotted a scatterplot of intercept and slope parameters from each model (models with different kinds of pooling), and connect estimates for the same participant. The plot uses arrows to connect the different estimates for each participant, different estimates from no-pooling (per-participant) compared to partial-pooling (mixed-effects) models. The plot shows how more extreme estimates are shrunk towards the global average estimate.\n\n\n\n\n\nFigure 8: Plot illustrating shrinkage: big green and pink points show the complete pooling and partial pooling (average) estimates for the slope and intercept; orange and purple points show the no pooling (orange) and partial pooling (purple) estimates for each person; estimates for a person are connected by arrows to show the direction towards which no pooling estimates are pulled or shrunk\n\n\n\n\nWe can see how estimates are pulled towards the average intercept and frequency effect estimates. The shrinkage effect is stronger for more extreme estimates like JL3 JP3. It is weaker for estimates more (realistically) like the overall group estimates like AA1."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-estimation",
    "href": "PSYC402/Week18.html#sec-dev-mixed-estimation",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "Before we move on, we can think briefly about how the mixed-effects models are estimated [@Snijders2004a]. Where do the numbers come from? I am happy to stick to a fairly non-technical intuitive explanation of the computation of LMEs but others, wishing to understand things more deeply, can find computational details in @Pinheiro2000aa, among other places. Mixed-effects models are estimated iteratively:\n\n\nIf we knew the random effects, we could find the fixed effects estimates by minimizing differences – like linear modeling.\nIf we knew the fixed effects – the regression coefficients – we could work out the residuals and the random effects.\n\nAt the start, we know neither, but we can move between partial estimation of fixed and random effect in an iterative approach.\n\nUsing provisional values for the fixed effects to estimate the random effects.\nUsing provisional values for the random effects to estimate the fixed effects again.\nTo converge on the maximum likelihood estimates of effects – when the estimates stop changing.\n\nIn mixed-effects models, the things that are estimated are the fixed effects (the intercept, the slope of the frequency effect, in our example), along with the variance and correlation terms associated with the random effects. Previously, I referred to the partial-pooling mixed-effects ‘estimates’ of the intercept or the frequency effect for each person, using the quotation marks because, strictly, these estimates are actually predictions, Best Unbiased Linear Predictions (BLUPs), based on the estimates of the fixed and random effects.\n\n\nMostly, our main concern, in working with mixed-effects models, is over what effects we should include, what model we should specify. But we should prepare for the fact sometimes happens that models fail to converge, which is to say, the model fitting algorithm fails to settle on some set of parameter estimates but has reached the limit in the number of iterations over which it has attempted to find a satisfactory set of estimates.\nIn my experience, convergence problems do arise, typically, if one is analyzing categorical outcome data (e.g accuracy) where there may be not enough observations to distinguish satisfactory estimates given a quite complex hypothesized model. In other words, you might run into convergence problems but it will not happen often and only where you are already dealing with quite a complex situation. We take a look at this concern in more depth, in the next chapter ?@sec-glmm-intro."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-evaluating",
    "href": "PSYC402/Week18.html#sec-dev-mixed-evaluating",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "Up to this point, we have discussed the empirical or conceptual reasons we should expect to take into account, in our model, the effects on the outcome due to systematic differences in the experimental variables, e.g., in stimulus word frequency frequency, or to random differences between participants or between stimuli. We can now think about how we should statistically evaluate the relative usefulness of these different fixed effects or random effects, where usefulness is judged in relation to our capacity to explain outcome variance, or to improve model fit to sample data. We shall take an approach that follows the approach set out by Baayen, Bates and others [@Baayen2008; @bates2015parsimonious; @matuschek2017].\nIn this approach, we shall look at the choices that psychology researchers have to make. Researchers using statistical models are always faced with choices. As we have seen, these choices begin even before we start to do analyses, as when we make decisions about dataset construction [@steegen2016]. The need to make choices is always present for all the kinds of models we work with (as I discuss in ?@sec-multiverse). This may not always be obvious because, for example, in using some data analysis software, researchers may rely on defaults with limited indication that that is what they are doing.\n\n\n\n\n\n\nImportant\n\n\n\nJust because we are making choices does not mean we are operating subjectively in a non-scientific fashion. Rather, provided we work in an appropriate mode of transparency or reflexiveness, we can work with an awareness of our options and the context for the data analysis [see the very helpful discussion in @gelman2017].\n\n\n\n\nIt is very common to see researchers using a process of model comparison to try to identify an account for their data in terms of estimates of fixed and random effects. A few key concepts are relevant to taking this approach effectively.\nWe will focus on building a series of models up to the most complex model supported by the data. What does model complexity mean here? I am talking about something like the difference between a model including just main effects (simpler) and a model including both main effects and the interaction between the effects (more complex), or, I am talking about a model included just fixed effects (simpler) versus a model including fixed effects as well as random effects (more complex).\nResearchers may engage in comparing models to examine if one or more random effects should be included in their linear mixed-effects model. They may not be sure if they should include all random effects, that is, all random effects that could be included, given a range of grouping variables, like participant, class or stimulus, and given a range of possible effects, such as whether slopes or intercepts might vary.\nResearchers may do model comparison to check if adding the effect of an experimental variable is justified. Maybe they are conducting an exploratory study in which they want to investigate if using some measurement variable helps to explain variation in the outcome. Perhaps they are conducting an experimental study in which they want to test if the experimental manipulation, or the difference between conditions, has an impact on the outcome.\n\n\n\n\n\n\nTip\n\n\n\nAcross these scenarios, we can test if an effect should be included or if its inclusion in a model is justified by comparing models with versus without the term that corresponds to the effect.\n\n\nIn some studies, researchers conduct model comparisons like this in order to obtain null hypothesis significance tests for the effects of the experimental variables.\nTypically, the model comparisons are focused on whether some measurement of model fit is or is not different when we do versus do not include the effect in question in the model.\n\n\nAs our discussion progresses, I think it would be helpful to reflect on some of the questions that you may be asking yourself.\n1. What about multiple comparisons?\nYou might well ask yourself:\n\nIf we engage in a bunch of comparisons to check if we should or should not include a variable, isn’t this just exploiting researcher degrees of freedom?\n\nOr, you might ask:\n\nIf we are conducting multiple tests on the same data, aren’t we running the risk of raising the Type I error (false positive) rate because we are doing multiple comparisons?\n\nI think these are good questions but, here, my task is to explain what people do, why they do it, and how it helps in your data analysis.\n2. Is any model the best?\n\nSo you are looking at models with varying fixed effects (fitted using ML) or models with varying random effects (fitted using REML). How do you decide which model is better?\n\nSome researchers argue that trying to decide which model is better or best is inappropriate [see e.g. @Gelman2015a]. As the famous saying by George Box has it [@box1976]: “All models are wrong.” 1 We may say, nevertheless, that some models are useful. Some models are more useful than others, perhaps, because they explain or predict outcomes better, depending on your criteria, and the cost-benefit analysis.\nHere, I will explain the model comparison process while acknowledging this point. This is because researchers often model comparison techniques to evaluate the relative usefulness of different alternate models.\n\n\n\n\nYou will often encounter, in the psychological research literature, Information Criteria statistics like BIC: they are understood within an approach: Information-theoretic methods. They are grounded in the insight that you have reality and then you have approximating models. The distance between a model and reality corresponds to the information lost when we use a model to approximate reality. Information criteria – AIC or BIC – are estimates of information loss. The process of model selection aims to minimize information loss.\nI will not discuss information criteria methods of model evaluation in detail, here, because psychologists frequently use the Likelihood Ratio Test method [@meteyard2020a], see following. (Take a look at, e.g., @Burnham2004 for a readable discussion, if you are interested.) However, you should have some idea of what information criteria statistics (like AIC and BIC) mean because you will see these statistics in the outputs from model comparisons using the anova() function, which we shall review a bit later (Section 1.12.3).\nIn summary, Akaike showed you could estimate information loss in terms of the likelihood of the model given the data – Akaike Information Criteria, AIC:\n\n\n\\[\nAIC = -2ln(l) + 2k\n\\]\nWhere:\n\n\\(-2ln(l)\\) is -2 times the log of the likelihood of the model given the data,\nwhere \\((l)\\) the likelihood\nis proportional to the probability of observed data conditional on some hypothesis being true.\n\nYou want a more likely model – less information loss, closer to reality – you want more negative or lower AIC. You can identify models that are more likely – closer to reality – with models with less wide errors, i.e. smaller residuals.\nYou could better approximate reality by including lots of predictors, specifying a more complex model. Models with more parameters may fit the data better but some of those effects may be spurious. Adding \\(+ 2k\\) penalizes complexity, speaking crudely, and so helps us to focus on the more parsimonious less complex model that best fits the data.\nSchwartz proposed an alternative estimate – Bayesian Information Criteria: BIC:\n\n\n\\[\nBIC = -2ln(l) + kln(N)\n\\]\nWhere:\n\n\\(-2ln(l)\\) is -2 times the log of the likelihood of the model given the data.\n\\(+ kln(N)\\) is the number of parameters in the model times the log of the sample size.\n\nThus the penalty for greater complexity is heavier in BIC.\nWe see that AIC and BIC differ in the second term. A deeper difference is that AIC estimates information loss when the true model may not be among the models being considered while BIC assumes that the true model is within the set of models being considered.\nAt this point we just need to think about Model selection and judgment using AIC and BIC.\n\nCompare a simpler model: model 1, just main effects; model 2, main effects plus interactions.\n\nIf the more complex model better approximates reality then it will be more likely given the data.\nBIC or AIC will be closer to negative infinity: \\(-2ln(l)\\) will be larger e.g. 10 is better than 1000, -1000 better than -10.\n\nAIC and BIC should move in the same direction. They usually will.\nAIC will tend to allow more complex models and that may be necessary when the researcher is engaged in a more exploratory study or wants more accurate predictions (that would be better supported by maximising the information going into the model). Using the BIC will tend to favour simpler models and that may be necessary when the researcher seeks models that replicate over the long run. Maybe a simpler model will less likely include predictors estimated because they are needed to fit noise or random outcome variation.\n\n\n\n@Pinheiro2000aa [see also @Barr2013a; @matuschek2017] recommend that models of varying predictor sets can be compared using Likelihood Ratio Test comparison (LRTs) where the simple model is nested inside the more complex model.\nThe “nested”, here, means that the predictors in the simpler model are a subset of the predictors in the more complex model. For example, you might have just main effects in the simpler model but both main and interaction effects in the more complex model. Or, in another example, you might have just random effects of subjects or items on intercepts in the simpler model but both random effects on intercepts and random effects on slopes of fixed effects in the more complex model.\n\n\n\n\n\n\nWarning\n\n\n\nWhen you compare models using the Likelihood ratio test, LRT, you are comparing alternate models of the same data.\n\n\n@Barr2013a note that we can compare models varying in the fixed effects (but constant in the random effects) or models varying in the random effects (but constant in the fixed effects) using LRTs. I have frequently reported model comparisons using the Likelihood ratio test, LRT. In part, this is for analytic reasons: I can compare simple and complex models getting multiple information criteria statistics for the models being compared in one function call, anova([model1], [model2]. In part, it is for social pragmatic reasons: the LRT comparison yields a significance p-value so that I can say, using the comparison, something like “The more complex model provided a significantly better fit to observation (LRT comparison, … p \\(=\\) …”\nIn a Likelihood Ratio Test, the test statistic is the comparison of the likelihood of the simpler model with the more complex model. Fortunately for us, we can R to calculate the model likelihood and do the model comparison (Section 1.13.2).\nThe comparison of models works by division: we divide the likelihood of the more complex model by the likelihood of the simpler model, calculating a likelihood ratio.\n\\[\n\\chi^2 = 2log\\frac{likelihood-complex}{likelihood-simple}\n\\]\nThe likelihood ratio value is then compared to the \\(\\chi^2\\) distribution for a significance test. In this significance test, we assume the null hypothesis that the simpler model is adequate as an account of the outcome variance. We calculate the p-value for the significance test using a number for the degrees of freedom equal to the difference in the number of parameters of the models being compared."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-model-steps",
    "href": "PSYC402/Week18.html#sec-dev-mixed-model-steps",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "Important\n\n\n\nHow should you proceed when you decide to use mixed-effects models?\n\nI think the answer to that question depends on whether you are doing a study that is confirmatory or exploratory.\n\n\n\nIn short, if you have pre-registered the design of your study and, as part of that registration, you recorded the hypotheses you plan to test, as well as the analysis method you plan to use to test your hypotheses, then the answer is simple: fit the model you said you were going to use.\nThese days, if you have not pre-registered your analysis plans, you are practically-speaking engaged in exploratory work. If you are doing an exploratory study, then you will need to make some choices, in part, depending on the nature of the sample you are working with, and other aspects of the research context, but it will help to keep things simple.\nIn an exploratory study, I would keep things simple by comparing a series of models, fitted with different sets of predictor variables (fixed effects).\n\n\n\n\n\n\nWarning\n\n\n\nNote: if you are running mixed-effects models in R you cannot run lmer() models with just fixed effects.\n\n\nWhat I do is this: for a dataset like the ML study data, where the data were collected using a repeated-measures design:\n\nso that all participants saw all stimuli,\nand both participants and stimuli were sampled (from the wider populations of readers or words),\nthen I would run a series of models\nso that the different models have varying sets of fixed effects\nbut all models in the series have the same random effects: the random effects of subjects and items on intercepts.\n\nIn my experience, the estimates and associated significance levels associated with fixed effects can vary quite a bit depending on what other variables are included in the model. This has led me to take an approach where I am not varying too much how predictors are included in the model.\nAs noted, this will not really apply if you are doing an confirmatory study in which you are obliged to include the manipulated variables. However, if you are doing something a bit more exploratory then you might have to think about the kinds of predictors you include in your model, and how or when you include them.\n\n\n\n\n\n\nTip\n\n\n\nIn what order should you examine the usefulness of different sets of fixed effects?\n\nThis is a difficult question to answer and the difficulty is one reason why I think we need to be cautious when we engage in model comparison to try to get to a model of our data.\n\n\n\nMy advice would be to plan out in advance a sequence of model comparisons.\n\nYou should begin with simpler models with fewer effects.\nYou should begin with those effects whose impacts are well established and well understood by you.\nIf there is a whole set of well established effects typically included in an analysis in the field in which you are working, it might be sensible to include all the effects in a single step.\nThen, I would use subsequent incremental steps to increase model complexity by adding effects that are theoretically justified, i.e., hypothesized, but which may be new, or may depend on the experimental manipulation you are testing out.\n\nHaving established a model with some set of sensible fixed effects (guided by information criteria or LRT statistics), I would then turn my attention to the random effects component of the model. As noted, we may expect to see random differences between subjects (and possibly between items) in both the level of average performance – random effects of subjects or items on intercepts – and in the slopes of fixed effects – random effects of subjects or items on slopes.\nWhat I do is this:\n\nFor a dataset like ML’s, I examine firstly if both random effects of subjects and items on intercepts are required.\nI then check if random effects of subjects or items on slopes are additionally required in the model.\n\nThe distinction between exploratory and confirmatory studies breaks down, in my experience, when we start thinking about what random effects should be included in a model. It will be useful to review, here, @Barr2013a and @matuschek2017 for an interesting discussion, and contrasting approaches.\n\n\nBefore we go any further, we need to briefly discuss one key choice that we face in working with mixed-effects models. This concerns the difference between Restricted Maximum Likelihood (REML) and Maximum Likelihood (ML) estimation methods. Both methods are iterative.\nThe lmer() function has defaults, like any analysis function, so we often do not need to make the choice explicit. We do need to when we compare models that vary in fixed effects, or in random effects.\n\nRestricted maximum likelihood\n\nIn R: REML=TRUE is stated in the lmer() function call.\n\nREML estimates the variance components while taking into account the loss of degrees of freedom resulting from the estimation of the fixed effects: REML estimates vary if the fixed effects vary.\nTherefore it is not recommended to compare the likelihood of models varying in fixed effects and fitted using REML [@Pinheiro2000aa].\nThe REML method is recommended for comparing the likelihood of models with the same fixed effects but different random effects.\nREML is more accurate for random effects estimation.\n\n\nMaximum likelihood\n\nIn R: REML=FALSE is stated in thelmer() function call.\n\nML estimation methods can be used to fit models with varying fixed effects but the same random effects.\nML estimation: a good place to start when building-up model complexity – adding parameters to an empty model.\n@Pinheiro2000aa advise that the approach is anti-conservative (it will sometimes indicate effects where there are none there) but @Barr2013a argue that their analyses suggest that that is not so.\n\n\n\n\n\n\n\nAs noted, it is recommended [@Pinheiro2000aa] that we compare models of varying random effects using Restricted Maximum Likelihood (REML) fitting. We might be comparing different models with different sets of random effects if we are in the process of working out whether our model should include both random intercepts and random slopes. I think it is sensible to build up model complexity in the random component so that we are working through a series of model comparisons, comparing more simple with more complex models where the more complex model includes the same terms as the simpler model but adds some more.\nIn analyzing the effect of frequency on log RT for the ML study data, we can examine whether the random effects of subjects or of items on intercepts are necessary. Then we can examine if we should take into account random effects of subjects on the slope of the fixed effect of frequency, in addition to the random effects on intercepts.\nTo begin with, we look at a simpler model. We can fit a model with just the fixed effects of intercept and frequency, and the random effects of participants or items on intercepts only. We exclude the (LgSUBTLCD + ...|subjectID) specification for the random effect of participants on the slope of the frequency LgSUBTLCD effect.\nWe use REML fitting, as follows:\n\nML.all.correct.lmer.REML.si  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                    \n                                          (1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.si)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9845.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5339 -0.6375 -0.1567  0.4364  5.5851 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0003204 0.01790 \n subjectID (Intercept) 0.0032650 0.05714 \n Residual              0.0085285 0.09235 \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887697   0.013253   217.9\nLgSUBTLCD   -0.034390   0.002774   -12.4\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.658\n\n\nIf you look at the code chunk, you can see that:\n\nREML = TRUE is the only change to the code: it specifies the change in model fitting method;\nalso, I changed the model name to ML.all.correct.lmer.REML.si to be able to distinguish the maximum likelihood from the restricted maximum likelihood model.\n\nFollowing @baayen2008, we can then run a series of models with just one random effect. Firstly, just the random effect of items on intercepts:\n\nML.all.correct.lmer.REML.i  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.i)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -8337\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7324 -0.6455 -0.1053  0.4944  4.8970 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0002364 0.01537 \n Residual              0.0117640 0.10846 \nNumber of obs: 5257, groups:  item_name, 160\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.886765   0.009047  319.07\nLgSUBTLCD   -0.034206   0.002811  -12.17\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.977\n\n\nSecondly, just the random effect of subjects on intercepts:\n\nML.all.correct.lmer.REML.s  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|subjectID),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.s)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9786.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5843 -0.6443 -0.1589  0.4434  5.5266 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n subjectID (Intercept) 0.003275 0.05723 \n Residual              0.008837 0.09401 \nNumber of obs: 5257, groups:  subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.885751   0.011561  249.60\nLgSUBTLCD   -0.033888   0.001897  -17.87\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.517\n\n\nIf we now run Likelihood Ratio Test comparisons of these models, we are effectively examining if one of the random effects can be dispensed with: if its inclusion makes no difference to the likelihood of the model then it is not needed. Is the random effect of subjects on intercepts justified?\n\nCompare models, first, with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.i the random effect of subjects on intercepts.\nThen compare models with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.s the random effect of items on intercepts.\n\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)\n\n\n\nWe compare models using the anova() function.\n\nanova() does the model comparison, for the models named in the list in brackets.\n\nYou can do this for the foregoing series of models but notice that in the code we specify:\n\nrefit = FALSE`\n\nWhat happens if we do not add that bit? What you will see if you run the anova() function call, without therefit = FALSE argument – try it – is that you will then get the warning refitting model(s) with ML (instead of REML).\nWhy? The immediate reason for this warning is that we have to specify refit = FALSE because otherwise R will compare ML fitted models. The refitting occurs by default.\nWhat is the reason for the imposition of this default?\nYou will recall that @Pinheiro2000aa advise that if one is fitting models with random effects the estimates are more accurate if the models are fitted using Restricted Maximum Likelihood (REML). That is achieved in the lmer() function call by adding the argument REML=TRUE. @Pinheiro2000aa further recommend (see, e.g., pp.82-) that if you compare models:\n\nwith the same fixed effects\nbut with varying random effects\nthen the models should be fitted using Restricted Maximum Likelihood.\n\nR refits models, for the anova() comparison using ML even if we originally specified REML fitting. It does this to stop users from comparing REML-fitted models when those models are specified with different sets of fixed effects, as discussed here. The reason for this is explained by Ben Bolker (in this discussion): analyses of simulated data analyses suggest that it does not make much difference whether we use REML or ML when we are comparing models with the same fixed effects but varying random effects. It does matter very much, however, that we should fit models using ML when we are comparing models with the same random effects but differing fixed effects. You will remember that REML estimates vary if the fixed effects vary.\n\n\n\nWhen we run the anova() function call, it can be seen that the random effects of subjects on intercepts is required.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.i: logrt ~ LgSUBTLCD + (1 | item_name)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik deviance  Chisq Df\nML.all.correct.lmer.REML.i     4 -8329.0 -8302.7 4168.5  -8337.0          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6  -9845.1 1508.1  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.i                \nML.all.correct.lmer.REML.si  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you look at the results of the model comparison then you should notice:\n\nThe ML.all.correct.lmer.REML.si model is more complex than the ML.all.correct.lmer.REML.i model.\n\n\nML.all.correct.lmer.REML.si includes LgSUBTLCD + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.i includes LgSUBTLCD + (1 | item_name).\n\n\nThe more complex model ML.all.correct.lmer.REML.si has AIC (-9835.1) and BIC (-9802.3) numbers that are larger or more negative, and has a likelihood (4922.6) that is larger than the simpler model ML.all.correct.lmer.REML.i which has AIC (-8329.0), BIC (-8302.7) and likelihood (4168.5).\n\n\nThe \\(\\chi^2 = 1508.1\\) statistic, on 1 Df has a p-value of Pr(&gt;Chisq) &lt;2.2e-16.\n\nYou can say that the comparison of the model ML.all.correct.lmer.REML.si (with the random effect of participants on intercepts) versus the model ML.all.correct.lmer.REML.i (without the random effect of participants on intercepts) shows that the inclusion of the random effect of participants on intercepts is warranted by a significant difference in model fit. (I highlight here the language you can use in your reporting.)\nThe second model comparison shows that the random effects of items on intercepts is also justified.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.s: logrt ~ LgSUBTLCD + (1 | subjectID)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik deviance  Chisq Df\nML.all.correct.lmer.REML.s     4 -9778.3 -9752.0 4893.2  -9786.3          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6  -9845.1 58.825  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.s                \nML.all.correct.lmer.REML.si  1.723e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you look at the results of the model comparison then you should notice:\n\nThe ML.all.correct.lmer.REML.si model is more complex than the ML.all.correct.lmer.REML.s model.\n\n\nML.all.correct.lmer.REML.si includes LgSUBTLCD + (1 | subjectID) + (1 | item_name).\nML.all.correct.lmer.REML.s includes LgSUBTLCD + (1 | subjectID)\n\n\nThe more complex model ML.all.correct.lmer.REML.si has AIC (-9835.1) and BIC (-9802.3) numbers that are larger or more negative, and has a likelihood (4922.6) that is larger than the simpler model ML.all.correct.lmer.REML.s which has AIC (-9778.3) and BIC (-9752.0) and likelihood (4893.2).\n\n\nThe \\(\\chi^2 = 58.825\\) statistic, on 1 Df has a p-value of Pr(&gt;Chisq) 1.723e-14.\n\nYou can say that the comparison of a model ML.all.correct.lmer.REML.si with versus a model ML.all.correct.lmer.REML.s without the random effect of items on intercepts shows that the inclusion of the random effect of items on intercepts is warranted by a significant difference in model fit.\nI would conclude that both random effects of subjects and items on intercepts are required.\nWe can draw this conclusion because the difference between the model including just the random effect of items on intercepts anova-ML-all-correct-lmer-REML-i, or the model including just the random effect of subjects on intercepts anova-ML-all-correct-lmer-REML-s, compared to the model including both the random effect of items on intercepts and of subjects on intercepts anova-ML-all-correct-lmer-REML is significant. This tells us that the absence of the term accounting for the random effect of subjects on intercepts is associated with a significant decrease in model fit to data, in model likelihood.\n\n\n\n\nWe should next consider whether it is justified or warranted to include in our model a term capturing the random effect of participants in the slope of the frequency effect. We may hold or we may make theoretical assumptions that justify including this random effect. Some researchers might ask: does the inclusion of the random effect seem warranted by improved model fit to data?\nI should acknowledge, here, that there is an on-going discussion over what random effects should be included in mixed-effects models (see @meteyard2020a for an overview). The discussion can be seen from a number of different perspectives. Key articles include those published by @Baayen2008, @bates2015parsimonious, @Barr2013a and @matuschek2017.\nYou could be advised that a mixed-effects model should include all random effects that make sense a priori, so, here, we are talking about the random effects of participants on intercepts and on the slopes of all fixed effects that are in your model (variances and covariances) as well as all the random effects of items on intercepts and on slopes. This is characterized as the keep it maximal approach, associated with @Barr2013a, though the discussion in that article is more nuanced than this sounds.\nOr, you could be advised that a mixed-effects model should only include those random effects that appear to be justified or warranted by their usefulness in accounting for the data. In practice, this may mean, you should include only those random effects that appear justified by improved model fit to data, as indicated by a model comparison [see e.g. @bates2015parsimonious; @matuschek2017].\n\n\n\n\n\n\nTip\n\n\n\nI think, in practice, that maximal models can run into convergence problems. This means that many researchers adopt an approach which you could call: Maximum justifiable.\n\nThis involves fitting a model, including all the random effects that make sense,\nthat are justified by improved model fit to data (given a significance test, model comparison)\nfor a model that actually converges.\n\n\n\nAt present, viewpoints in discussions around the specification of random effects are associated with arguments that, as @Barr2013a discuss, more comprehensive models appear to control the Type I (false positive) error rate better, or that, as @matuschek2017 argue, control over the risk of false positives may come at the cost of increasing the Type II (false negative) error rate).\nI think that it would seem to be axiomatic that a researcher should seek to account for all the potential sources of variance – fixed effects or random effects – that may influence observed outcomes. In practice, however, you may have insufficient data or inadequate measures to enable you to fit a model that converges with all the random effects, or to enable you to fit a model that converges that can estimate what may, in fact, be very small random effects variances or covariances. This is why some researchers are moving to adopt Bayesian mixed-effects modeling methods, as discussed by the developmental Psychologist, Michael Frank, for example, here. And as exemplified by my work here.\n\n\n\n\n\n\nWarning\n\n\n\nThis discussion raises a question.\n\nRandom slopes of what?\n\n\n\nIn general, and simplifying things a bit, if an effect is manipulated within grouping units then we should specify random effects terms that allow us to take into account random differences between groups (e.g, between participants, stimulus words, or classes) in intercepts or in the slopes of the effects of theoretical interest, the fixed effects. The language of within-subjects or between-subjects effects is common in statistical education in psychological science and, I guess, it is a legacy of the focus of that education on ANOVA. A nice explanation of the difference between within-subjects or between-subjects effects can be found in @Barr2013a.\nIn short, if a participant provides response data under multiple levels of an experimental condition, or in response to multiple levels of a predictor variable (e.g. a person responds to multiple words, with differing frequency levels) then we are going to estimate or test the effect of that condition or that variable as if the condition is manipulated within-subjects or as if the responses to the variable vary within-subjects. If and only if we are in this situation, we can draw a plot of the kind you see in Figure 6: where we may be able to see the way that the slope of the effect of the variable differs between participants. (In contrast, for example, outside of longitudinal studies, we would identify age as a between-subjects rather than a within-subjects variable and, if you think about it, we could not draw a grid of plots like Figure 6 to examine how the slope of the age effect might differ between participants.) In this situation, we can and should (as @Barr2013a argue) specify random effects terms to account for between-participant differences in the slopes of the fixed effect.\nWe can examine the utility of random effects by comparing models with the same fixed effects but with varying random effects. We can specify a fixed effect term inside the random effects part of the mixed-effects model code, as we saw in Section 1.9.5.\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                                (LgSUBTLCD + 1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nLooking at the code:\n\nWith (LgSUBTLCD + 1 |subjectID) we specify a random effect of subjects on intercepts and on the slope of the frequency effects.\nWe do not specify – it happens by default – the estimation of the covariance of random differences among subjects in intercepts and random differences among subjects in the slope of the frequency effect.\n\nAnd as before, we can use anova() to check whether the increase in model complexity associated with the addition of random slopes terms is justified by an increase in model fit to data.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.slopes, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.slopes: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n                                npar     AIC     BIC logLik deviance  Chisq Df\nML.all.correct.lmer.REML.si        5 -9835.1 -9802.3 4922.6  -9845.1          \nML.all.correct.lmer.REML.slopes    7 -9854.1 -9808.1 4934.0  -9868.1 22.934  2\n                                Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.si                   \nML.all.correct.lmer.REML.slopes  1.047e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInspection of the results shows us that, here, adjusting the model to include random effects of subjects in the slopes of the fixed effect of word frequency does improve model fit to data. In this situationn, we can report that:\n\nThe inclusion of the random effect is warranted by improved model fit to data (\\(\\chi^2 (1 df) = 22.9, p &lt; .001\\))}\n\n\n\n\nIf you look at the fixed effects summary, you can see that we do not get p-values by default. To calculate p-values, we need to count residual degrees of freedom. The authors of the {lme4} library that furnishes the lmer() function do not [as e.g. @Baayen2008 discuss] think that it is sensible to estimate the residual degrees of freedom for a model in terms of the number of observations. This is because the number of observations concerns one level of a multilevel dataset that might be structured with respect to some number of subjects, some number of items. This means that one cannot then accurately calculate p-values to go with the t-tests on the coefficients estimates; therefore they do not.\nWhile this makes sense to me (see comments earlier on Bayesian methods), Psychologists will often need p-values. This is now relatively easy.\nWe can run mixed-effects models with p-values from significance tests on the estimates of the fixed effects coefficients using the library(lmerTest).\n\nlibrary(lmerTest)\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                                (LgSUBTLCD + 1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.slopes)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  2.887997   0.015479 47.782839 186.577  &lt; 2e-16 ***\nLgSUBTLCD   -0.034471   0.003693 60.338787  -9.333 2.59e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\nBasically, the call to access the lmerTest library ensures that when we run the lmer() function we get a calculation of an approximation to the denominator degrees of freedom that enables the calculation of the p-value for the t-test for the fixed effects coefficient. An alternative, as I have noted (Section 1.12.3) is to compare models with versus without the effect of interest.\n\n\nIt will be useful for you to examine model comparisons with a different set of models for the same data.\nYou could try to run a series of models in which the fixed effects variable is something different, for example, the effect of word Length: or the effect of orthographic neighbourhood sizeOrtho_N:.\nI would consider the model comparisons in the sequence shown in the foregoing, one pair of models at a time, to keep it simple. When you look at the model comparison, ask: is the difference between the models a piece of complexity (an effect) whose inclusion in the more complex model is justified or warranted by improved model fit to data?"
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-reporting-results",
    "href": "PSYC402/Week18.html#sec-dev-mixed-reporting-results",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "How do we report the analyses and their results?\nI think it may help if I analyze the structure and content of a results report that I did, in @Davies2013b. The report is published together with data and analysis code.\nIf you look at the report, you can identify the kinds of information that I think you should communicate.\n\nBecause it was an exploratory study, I started by reporting the comparison of models varying in fixed effects.\nI explain what predictors are included in each model.\nI explain how I make decisions about which model to select.\nI then go on to discuss the comparison of models varying in random effects.\n\nI think it is important to be as clear as possible about what model comparison process (if any) you may undertake.\n\nWe stepped through a series of models. Firstly, assuming the same random effects of subjects and items on intercepts, we compared models differing in fixed effects: a model (model 1) with just initialstress factors; a model (model 2) with initialstress factors plus linear effects due to the orthographic.form, frequency, semantic, and bigram.frequency factors; and lastly a model (model 3) with the same factors as model 2 but adding restricted cubic splines for the frequency and orthographic.form factors to examine the evidence for the presence of curvilinear effects of frequency and length (the orthographic.form factor loads heavily on length).\n\nNotice also that I try to standardize the language and structure of the paragraphs – that kind of repetition or rhythm helps the reader, I think, by making what is not repeated – the model specifications – more apparent. Your style may differ, however, and that’s alright.\n\nWe evaluated whether the inclusion of random effects was necessary in the final model (model 3) using LRT comparisons between models with the same fixed effects structure but differing random effects. Here, following Pinheiro & Bates (2000; see, also, Baayen, 2008), models were fitted using the REML=TRUE setting in lmer. We compared models that included: (i.) both random effects of subjects and items, as specified for model 3; (ii.) just the random effect of subjects; (iii.) just the random effect of items.\n\n\n\n\n\n\n\nTip\n\n\n\nI want you to notice something more, concerning the predictors included in each different model:\n\nI do not include predictors one at a time, I include predictors in sets.\n\n\n\nFor the @Davies2013b dataset, I include first the set of phonetic coding variables then the set of psycholinguistic variables (see the paper for details). I include linear effects then additional terms allowing the effects to be curvilinear.\nFinally, you can see that I report model comparisons in terms of Likelihood Ratio Tests. I do this, firstly, in order to report comparisons conducted to examine the basis for selecting one model out of a set of possible models varying in fixed effects:\n\nComparing models 1 and 2, models with initialstress factors but differing in whether they did or did not include key psycholinguistic factors like orthographic.form, the LRT statistic was significant (\\(\\chi^2 = 1,007, 4 df, p = 2 * 10^-16\\)). Comparing models 2 and 3, i.e. models with initialstress and key psycholinguistic components but differing in whether they did or did not use restricted cubic splines to fit the orthographic.form and frequency effects, the LRT statistic was significant (\\(\\chi^2 = 23, 2 df, p = 1 * 10^-5\\)).\n\nThen I report the selection of models varying in random effects:\n\nWe compared models that included: (i.) both random effects of subjects and items, as specified for model 3; (ii.) just the random effect of subjects; (iii.) just the random effect of items. The difference between models (i.) and (ii.) was significant (\\(\\chi^2 = 185, 1 df, p = 2 * 10^-16\\)) indicating inclusion of an item effect was justified. The difference between models (i.) and (iii.) was significant (\\(\\chi^2 = 17,388, 1 df, p = 2 * 10^-16\\)) indicating inclusion of a subject effect was justified.\n\nIf you look at the report, you will see, also, that present a summary table showing the estimates for the fixed effects, and a series of plots indicating the predicted change in outcome (reading RT) given variation in the values of the predictor variables.\nIn summary, I think we can and should report both an outline of the process of development of the model or models we use to estimate the effects of interest, and the estimates we derive through the modeling.\n\n\n\n\n\n\nTip\n\n\n\nI would advise you to report:\n\nA summary of fixed effects – just like in linear models, with coefficient estimates, standard errors, t and p (if you use it);\nRandom effects variance and covariance (as applicable);\nModel building processes or model comparisons (if used).\n\nI recommend presenting the final model summary in a table that is structured like a multiple regression model summary table showing both random and the fixed effects.\n\n\nI also think it helps the reader to know what model is the basis for any estimates presented [see @meteyard2020a for further advice]."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-summary",
    "href": "PSYC402/Week18.html#sec-dev-mixed-summary",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "We examined another example of data from a repeated measures design study, this time, from a study involving adults responding to the lexical decision task, the ML study dataset.\nWe explored in more depth why linear mixed-effects models are more effective than other kinds of models when we are analyzing data with multilevel or crossed random effects structure. We discussed the critical ideas: pooling, and shrinkage. And we looked at how mixed-effects models employ partial-pooling so as to be more effective than alternative approaches dependent on complete pooling or no pooling estimates.\nMixed-effects models work better because they use both information from the whole dataset and information about each group (item or participant). This ensures that model estimates take into account random differences but are regularized so that they are not dominated by less reliable group-level information.\nWe considered, briefly, how mixed-effects models are estimated.\nThen we examined, in depth, how mixed-effects models are fitted, compared and evaluated. The model comparison approach was set out, and we looked at both practical steps and at some of the tricky questions that, in practice, psychologists are learning to deal with.\nWe discussed how to compare models with varying random or fixed effects. We focused, especially, on the comparison of models with varying random effects. Methods for model comparison, including the use of information criteria and the Likelihood Ratio Test, were considered.\nWe discussed p-values, questions about calculating them, and a simple method for getting them when we need to report significance tests.\nWe discussed how mixed-effects models should be reported.\n\n\nWe used two functions to fit and evaluate mixed-effects models.\n\nlmer() to fit mixed-effects models\nanova() to compare two or more models using AIC, BIC and the Likelihood Ratio Test.\nWe used the lmerTest library to furnish significance tests for coefficient estimates of fixed effects."
  },
  {
    "objectID": "PSYC402/Week18.html#sec-dev-mixed-recommended-reading",
    "href": "PSYC402/Week18.html#sec-dev-mixed-recommended-reading",
    "title": "Developing linear mixed-effects models",
    "section": "",
    "text": "The most influential papers, at present, for the practice of mixed-effects modeling in psychological science are those by @Baayen2008, @bates2015parsimonious, @Barr2013a and @matuschek2017. Each of these papers makes critical points and, in my view, each is clearly written with a good use of examples grounded in the scenarios psychologists often encounter.\nBroader concerns about how are approach modeling, and what we look for as scientists, are discussed in @Burnham2004, @Gelman2007ga and @gelman2017.\nA very useful FAQ on the practicalities of working with mixed-effects models can be found here."
  },
  {
    "objectID": "PSYC402/Week18.html#footnotes",
    "href": "PSYC402/Week18.html#footnotes",
    "title": "Developing linear mixed-effects models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBox also says: “It is inappropriate to be concerned about mice when there are tigers about.” [@box1976] That is too wise a saying to leave out.↩︎"
  },
  {
    "objectID": "PSYC402/Week19.html",
    "href": "PSYC402/Week19.html",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We have been discussing how we can use Linear Mixed-effects models to analyze multilevel structured data, the kind of data that we commonly acquire in experimental psychological studies, for example, when our studies have repeated measures designs. The use of Linear Mixed-effects models is appropriate where the outcome variable is a continuous numeric variable like reaction time. In this chapter, we extend our understanding and skills by moving to examine data where the outcome variable is categorical: this is a context that requires the use of Generalized Linear Mixed-effects Models (GLMMs).\nWe will begin by looking at the motivations for using GLMMs. We will then look at a practical example of a GLMM analysis, in an exploration in which we shall reveal some of the challenges that can arise in such work. The R code to do the modeling is very similar to the code we have used before. The way we can understand the models is also similar but with one critical difference. We start to understand that difference here.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nCategorical outcomes cannot be analyzed using linear models, in whatever form, without having to make some important compromises.\n\n\nYou need to do something about the categorical nature of the outcome.\n\n\n\nIn this chapter, we look at Generalized Linear Mixed-effects Models (GLMMs): we can use these models to analyze outcome variables of different kinds, including outcome variables like response accuracy that are coded using discrete categories (e.g. correct vs. incorrect). Our aims are to:\n\nRecognize the limitations of alternative methods for analyzing such outcomes, Section 1.8.1.\nUnderstand practically the reasons for using GLMMs when we analyze discrete outcome variables, Section 1.8.2.\nPractice running GLMMs with varying random effects structures.\nPractice reporting the results of GLMMs, including through the use of model plots.\n\n\n\n\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 20 minutes\nPart 2 – about 16 minutes\nPart 3 – about 19 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter. The lectures provide a summary of the main points.\n2. Chapter: 04-glmm\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to work with categorical outcomes data using GLMMs.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example dataset.\nExperiment with the .R code used to work with the example data.\nRun GLMMs of demonstration data.\nRun GLMMs of alternate data sets.\nReview the recommended readings (Section 1.13).\n\n3. Practical workbook materials\n3.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning.\n\n\n\nWe will be working with data collected for a study investigating word learning in children, reported by @ricketts2021. You will see that the study design has both a repeated measures aspect because each child is asked to respond to multiple stimuli, and a longitudinal aspect because responses are recorded at two time points. Because responses were observed to multiple stimuli for each child, and because responses were recorded at multiple time points, the data have a multilevel structure. These features require the use of mixed-effects models for analysis.\nWe will see, also, that the study involves the factorial manipulation of learning conditions. This means that, when you see the description of the study design, you will see embedded in it the 2 x 2 factorial design beloved of psychologists. You will be able to generalize from our work this week to many other research contexts where psychologists conduct experiments in which conditions are manipulated according to a factorial design.\nHowever, our focus here is on the fact that the outcome for analysis is the accuracy of the responses made by children to word targets in a spelling task. The categorical nature of accuracy as an outcome is the reason why we now turn to use Generalized Linear Mixed-effects Models.\n\n\nI am going to present the study information in some detail, in part, to enable you to make sense of the analysis aims and results and, in part, so that we can simulate results reporting in a meaningful context.\n\n\nVocabulary knowledge is essential for processing language in everyday life and it is vital that we know how to optimize vocabulary teaching. One strategy with growing empirical support is orthographic facilitation: children and adults are more likely to learn new spoken words that are taught with their orthography [visual word forms; for a systematic review, see @colenbrander2019]. Why might orthographic facilitation occur? Compared to spoken inputs, written inputs are less transient across time and less variable across contexts. In addition, orthography is more clearly marked (e.g., the ends of letters and words) than the continuous speech stream. Therefore, orthographic forms may be more readily learned than phonological forms.\n@ricketts2021 investigated how school-aged children learn words. We conducted two studies in which children learned phonological forms and meanings of 16 polysyllabic words in the same experimental paradigm. To test whether orthographic facilitation would occur, half of the words were taught with access to the orthographic form (orthography present condition) and the other half were taught without orthographic forms (orthography absent condition). In addition, we manipulated the instructions that children received: approximately half of the children were told that some words would appear with their written form (explicit group); the remaining children did not receive these instructions (incidental group). Finally, we investigated the impact of spelling-sound consistency of word targets for learning, by including words that varied continuously on a measure of pronunciation consistency [after @mousikou2017].\nThe quality of lexical representations was measured in two ways. A cuing hierarchical response task (definition, cued definition, recognition) was used to elicit semantic knowledge from the phonological forms, providing a fine-grained measure of semantic learning. A spelling task indexed the extent of orthographic learning for each word. We focus on the analysis of the spelling task responses in this chapter (you may be interested in reviewing our other analyses, see Section 1.5.2).\n@ricketts2021 reported two studies. We focus on Study 1, in which @ricketts2021 measured knowledge of newly learned words at two intervals: first one week and then, again, eight months after training. Longitudinal studies of word learning are rare and this is the first longitudinal investigation of orthographic facilitation.\nWe addressed three research questions.\n\n\n\n\n\n\nNote\n\n\n\n\nDoes the presence of orthography promote greater word learning?\n\n\nWe predicted that children would demonstrate greater orthographic learning for words that they had seen (orthography present condition) versus not seen (orthography absent condition).\n\n\nWill orthographic facilitation be greater when the presence of orthography is emphasized explicitly during teaching?\n\n\nWe expected to observe an interaction between instructions and orthography, with the highest levels of learning when the orthography present condition was combined with explicit instructions.\n\n\nDoes word consistency moderate the orthographic facilitation effect?\n\n\nFor orthographic learning, we expected that the presence of orthography might be particularly beneficial for words with higher spelling-sound consistency, with learning highest when children saw and heard the word, and these codes provided overlapping information.\n\n\n\n\n\n\nChildren were taught 16 novel words in a \\(2 \\times 2\\) factorial design. The presence of orthography (orthography absent vs. orthography present) was manipulated within participants: for all children, eight of the words were taught with orthography present and eight with orthography absent. Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\n\n\n\nIn Study 1, 41 children aged 9-10 years completed the word learning task and completed semantic and orthographic assessments one week after learning (Time 1), and eight months later (Time 2). We tested children from one socially mixed school in the South-East of England (\\(M_{age} = 9.95, SD = .53\\)).\n\n\n\nStimuli comprised 16 polysyllabic words, all of which were nouns. We indexed consistency at the whole word level using the H uncertainty statistic [@mousikou2017]. An H value of 0 would indicate a consistent item (all participants producing the same pronunciation), with values \\(&gt;0\\) indicating greater inconsistency (pronunciation variability) with increasing magnitude.\n\n\n\nA ‘pre-test’ was conducted to establish participants’ knowledge of the stimulus words before i.e. pre- training was administered. Then, each child was seen for three 45-minute sessions to complete training (Sessions 1 and 2) and post-tests (Session 3).\nIn Study 1, longitudinal post-test data were collected because children were post-tested at two time points. (Here, we refer to ‘post-tests’ as the tests done to test learning, after i.e. post training.) Children were given post-tests in Session 3, as noted: this was Time 1. They were then given post-tests again, about eight months later at Time 2.\n\n\n\nThe Orthographic post-test was used to examine orthographic knowledge after training. Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. For the purposes of our learning in this chapter, we focus on the accuracy of responses. Each response made by a child to a target word was coded as correct or incorrect.\nA more sensitive outcome measure of orthographic knowledge was also taken. Responses were also scored using a Levenshtein distance measure, using the {stringdist} library [@loo2022]. This score indexes the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. In the published report (@ricketts2021) we focus our analysis of the orthographic outcome on the Levenshtein distance measure of response spelling accuracy, and further details on the analysis approach (Poisson rather than Binomial Generalized Linear Mixed-effects Models) can be found in the paper.\n\n\n\n\nYou can download the data-04-glmm.zip files folder for this chapter.\nIn this chapter, we will be working with the data about the orthographic post-test outcome for the longitudinal study:\n\nlong.orth_2020-08-11.csv\n\nThe data file is collected together with the .R scripts:\n\n04-glmm-workbook.R the workbook you will need to do the practical exercises.\n04-glmm-workbook-answers.R with answers to questions and code for exercises.\n\nThe data come from the @ricketts2021 study, and you can access the analysis code and data for that study, in full, at the OSF repository here\nIn addition to these data, you will notice that I refer, Section 1.8.1.1, to another dataset analyzed by @monaghan2015. I enclose the data referenced:\n\nnoun-verb-learning-study.csv\n\nAlong with additional .R you can work with to develop your skills:\n\n402-04-GLMM-exercise-gavagai-data-analysis-notes.R\n\nThe @monaghan2015 paper can be accessed here.\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read the data file into R.\n\nlong.orth &lt;- read_csv(\"long.orth_2020-08-11.csv\", \n                      col_types = cols(\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n                      )\n                    )\n\nYou can see, here, that within the read_csv() function call, I specify col_types, instructing R how to treat a number of different variables. Recall that in ?@sec-intro-mixed-data-load, we saw how we can determine how R treats variables at the read-in stage, using col_types() specification. You can read more about this here.\n\n\n\nIt is always a good to inspect what you have got when you read a data file in to R.\n\nsummary(long.orth)\n\nSome of the variables included in the .csv file are listed, following, with information about value coding or calculation.\n\nParticipant – Participant identity codes were used to anonymize participation.\nTime – Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\nInstructions – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.\nWord – Letter string values showing the words presented as stimuli to the children.\nOrthography – Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.\nConsistency-H – Calculated orthography-to-phonology consistency value for each word. -zConsistency-H – Standardized Consistency H scores\nScore – Outcome variable – for the orthographic post-test, responses were scored as 1 (correct, if the target spelling was produced in full) or 0 (incorrect, if the target spelling was not produced).\n\nThe summary will show you that we have a number of other variables available, including measures of individual differences in reading or reading-related abilities or knowledge, but we do not need to pay attention to them, for our exercises. If you are interested in the dataset, you can find more information about the variables in the Appendix for this chapter (Section 1.14) and, of course, in @ricketts2021.\n\n\n\n\nThe data are already tidy: each column in long.orth_2020-08-11.csv corresponds to a variable and each row corresponds to an observation. However, we need to do a bit of work, before we can run any analyses, to fix the coding of the categorical predictor (or independent) variables, the factors Orthography, Instructions, and Time.\n\n\nBy default, R will dummy code observations at different levels of a factor. So, for a factor or a categorical variable like Orthography (present, absent), R will code one level name e.g. absent as 0 and the other e.g. present as 1. The 0-coded level is termed the reference level, which you could call the baseline level, and by default R will code the level with the name appearing earlier in the alphabet as the reference level.\nAll this is usually not important. When you specify a model in R where you are asking to estimate the effect of a categorical variable like Orthography (present, absent) then, by default, what you will get is an estimate of the average difference in outcome, when all other factors are set to zero, estimated as the difference in outcomes comparing the reference level and the other level or levels of the factor. This will be presented, for example, like the output shown following, for a Generalized Linear Model (i.e., a logistic regression) analysis of the effect of Orthography condition, ignoring the random effects:\n\nsummary(glm(Score ~ Orthography, family = \"binomial\", data = long.orth))\n\n\nCall:\nglm(formula = Score ~ Orthography, family = \"binomial\", data = long.orth)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.8510  -0.8510  -0.6763   1.5436   1.7818  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -1.35879    0.09871 -13.765  &lt; 2e-16 ***\nOrthographypresent  0.52951    0.13124   4.035 5.47e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1431.9  on 1262  degrees of freedom\nResidual deviance: 1415.4  on 1261  degrees of freedom\nAIC: 1419.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nYou can see that you have an estimate, in the summary, of the effect of orthographic condition shown as:\nOrthographypresent  0.52951\nThis model (and default coding) gives us an estimate of how the log odds of a child getting a response correct changes if we compare the responses in the absent condition (here, treated as the baseline or reference level) with responses in the present condition.\n\n\n\n\n\n\nTip\n\n\n\nR tells us about the estimate by adding the name of the factor level that is not the reference level, here, present to the name of the variable Orthography whose effect is being estimated.\n\n\nWe can see that the log odds of a correct response increase by \\(0.52951\\) when the orthography (visual word form or spelling) of a word is present during learning trials.\nHowever, as Dale Barr explains it is better not to use R’s default dummy coding scheme if we are analyzing data where the data come from a study involving two or more factors, and we want to estimate not just the main effects of the factors but also the effect of the interaction between the factors.\nIn our analyses, we want the coding that allows us to get estimates of the main effects of factors, and of the interaction effects, somewhat like what we would get from an ANOVA. This requires us to use effect coding.\nWe can code whether a response was recorded in the absent or present condition using numbers. In dummy coding, for any observation, we would use a column of zeroes or ones to code condition: i.e., absent (0) or present (1). In effect coding, for any observation, we would use a column of ones or minus ones to code condition: i.e., absent (-1) or present (1). (With a factor with more than two levels, we would use more than one column to do the coding: the number of columns we would use would equal the number of factor condition levels minus one.) In effect coding, observations coded -1 are in the reference level.\nWith effect coding, the constant (i.e., the intercept for our model) is equal to the grand mean of all the observed responses. And the coefficient of each of the effect variables is equal to the difference between the mean of the group coded 1 and the grand mean.\nYou can read more about effect coding here or here.\n\n\n\nWe follow recommendations to use sum contrast coding for the experimental factors. Further, to make interpretation easier, we want the coding to work so that for both orthography and presentation conditions, doing something is the “high” level in the factor – hence:\n\nOrthography, absent (-1) vs. present (+1)\nInstructions, incidental (-1) vs. explicit (+1)\nTime, test time 1 (-1) vs. time 2 (+1)\n\nWe use a modified version of the contr.sum() function (provided in the {memisc} library) that allows us to define the base or reference level for the factor manually (see documentation).\n\nlibrary(memisc)\n\n\n\n\n\n\n\nTip\n\n\n\nWe sometimes see that we cannot appear to load library(memisc) and library(tidyverse) at the same time without getting weird warnings.\n\nI would load library(memisc) after I have loaded library(tidyverse)\nand maybe then unload it afterwards: just click on the button next to the package or library name in R-Studio to detach the library (i.e., stop it from being available in the R session).\n\n\n\nIn the following sequence, I first check how R codes the levels of each factor by default, I then change the coding, and check that the change gets me what I want.\nWe want effects coding for the orthography condition factor, with orthography condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Orthography)\n\n        present\nabsent        0\npresent       1\n\n\nYou can see that Orthography condition is initially coded, by default, using dummy coding: absent (0); present (1). We want to change the coding, then check that we have got what we want.\n\ncontrasts(long.orth$Orthography) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Orthography)\n\n         2\nabsent  -1\npresent  1\n\n\nWe want effects coding for the presentation condition factor, with presentation condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Instructions)\n\n           incidental\nexplicit            0\nincidental          1\n\n\nChange it.\n\ncontrasts(long.orth$Instructions) &lt;- contr.sum(2, base = 2)\ncontrasts(long.orth$Instructions)\n\n            1\nexplicit    1\nincidental -1\n\n\nWe want effects coding for the Time factor, with Time coded as -1, +1 Check the coding.\n\ncontrasts(long.orth$Time)\n\n  2\n1 0\n2 1\n\n\nChange it.\n\ncontrasts(long.orth$Time) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Time)\n\n   2\n1 -1\n2  1\n\n\nIn these chunks of code, I use contr.sum(a, base = b) to do the coding, where a is the number of levels in a factor (replace a with the right number), and b tells R which level to use as the baseline or reference level (replace b with the right number). I usually need to check the coding before and after I specify it.\n\n\n\n\n\n\n\nWe often need to analyze outcome or dependent variables which comprise observations of responses that are discrete or categorical. We need to learn to recognize research contexts that require GLMMs. Categorical outcome variables can include any of the following:\n\nThe accuracy of each of a series of responses in some task, e.g., whether a recorded response is correct or incorrect.\nThe location of a recorded eye movement, e.g., a fixation to the left or to the right visual field.\nThe membership of one group out of two possible groups, e.g., is a participant impaired or unimpaired?\nThe membership of one group out of multiple possible groups, e.g., is a participant a member of one out of some number of groups, say, a member of a religious or ethnic group?\n\nResponses that can be coded in terms of ordered categories, e.g., a response on a (Likert) ratings scale.\nOutcomes like the frequency of occurrence of an event, e.g., how many arrests are made at a particular city location.\n\nIn this chapter, we will analyze accuracy data: where the outcome variable consists of responses observed in a behavioural task, the accuracy of responses was recorded, and responses could either be correct or incorrect. The accuracy of a response is, here, coded under a binary or dichotomous classification though we can imagine situations when a response is coded in multiple different ways.\nThose interested in analyzing outcome data from ratings scales, that is, ordered categorical outcome variables, often called ordinal data, may wish to read about ordinal regression analyses, which you can do in R using functions from the {ordinal} library.\nThose interested in analyzing outcome data composed of counts may wish to read about poisson regression analyses in @Gelman2007ga.\nIt will be apparent in our discussion that researchers have used, and will continue to use, a number of ‘traditional’ methods to analyze categorical outcome variables when really they should be using GLMMs. We will talk about these alternatives, next, so that you recognize what is being done when you read research articles. Critically, we will discuss the limitations of such methods because these limitations explain why we bother to learn about GLMMs.\n\n\nIf you want to analyze data from a study where responses can be either correct or incorrect but not both (and not anything else), then your outcome variable is categorical, and your analysis approach ought to respect that. However, if you read enough psychological research articles then you will see many reports of data analyses in which the researchers collected data on the accuracy of responses but then present the results of analyses that ignored the binary or dichotomous nature of accuracy. We often see response accuracy analyzed using an approach that looks something like the following:\n\nThe accuracy of responses (correct vs. incorrect) is counted, e.g., as the number of correct responses or the number of errors.\nThe percentage, or the proportion, of responses that are correct or incorrect is calculated, for each participant, for each level of each experimental condition or factor.\nThe percentage or proportion values are then entered as the outcome or dependent variable in ANOVA or t-test or linear model (multiple regression) analyses of response accuracy.\n\nYou will see many reports of ANOVA or t-test or linear model analyses of accuracy.\n\n\n\n\n\n\nTip\n\n\n\nWhy can’t we follow these examples, and save ourselves the effort of learning how to use GLMMs?\n\nThe reason is that these analyses are, at best, approximations to more appropriate methods.\nTheir results can be expected to be questionable, or misleading, for reasons that we discuss next.\n\n\n\n\n\nTo illustrate the problems associated with using traditional analysis methods (like ANOVA or multiple regression), when working with accuracy as an outcome, we start by looking at data from an artificial vocabulary learning study [reported by @monaghan2015]. @monaghan2015 recorded responses made by participants to stimuli in a test where the response was correct (coded 1) or incorrect (coded 0). In our study, we directly compared learning of noun-object pairings, verb-motion pairings, and learning of both noun and verb pairings simultaneously, using a cross-situational learning task. (Those interested in this dataset can read more about it at the online repository associated with this chapter.) The data will have a multilevel structure because you will have multiple responses recorded for each person, and for each stimulus. But what concerns us is that if you attempt to use a linear model to analyze the effects of the experimental variables then you will see some paradoxical results that are easily demonstrated.\nLet’s imagine that we wish to estimate the effects of experimental variables like learning condition: learning trial block (1-12); or vocabulary condition (noun-only, noun-verb, verb-only). We can calculate the proportion of responses correct made by each person for each condition and learning trial block. We can then plot the regression best fit lines indicating how proportion of responses correct varies by person and condition. Figure 1 shows the results.\nLook at where the best fit lines go.\n\n\n\n\n\nFigure 1: Monaghan et al. (2015) artificial word learning study: plot showing the proportion of responses correct for each participant, in each of 12 blocks of 24 learning trials, in each learning condition; each grey line shows the linear model prediction of the proportion correct, for each person, by learning block, in each condition; black lines show the average prediction of the proportion correct, by learning block, in each condition. The position of points has been jittered.\n\n\n\n\nFigure 1 shows how variation in the outcome, here, the proportion of responses that are correct, is bounded between the y-axis limits of 0 and 1 while the best fit lines exceed those limits. Clearly, if you consider the accuracy of a person’s responses in any set of trials, for any condition in an experiment, the proportion of responses that can be correct can vary only between 0 (no responses are correct) and 1 (all responses are correct). There is no inbuilt or intrinsic limits to the proportion of responses that a linear model can predict would be correct. According to linear model predictions, if you follow the best fit lines in Figure 1 then there are conditions, or there are participants, in which the proportion of a person’s responses that could be correct will be greater than 1. That is impossible.\n\n\n\nThe other fundamental problem with using analysis approaches like ANOVA or regression to analyze categorical outcomes like accuracy is that we cannot assume that the variance in accuracy of responses will be homogenous across different experimental conditions.\nThe logic of the problem can be set out as follows:\n\nGiven a binary outcome, e.g., where the response is correct or incorrect, for every trial, there is a probability \\(p\\) that the response is correct.\nThe variance of the proportion of trials (per condition) with correct responses is dependent on \\(p\\), and it is greater when \\(p \\sim .5\\), the probability that a response will be correct.\n\n@jaeger2008 (p. 3) then explains the problem like this. If the probability of a binomially distributed outcome like response accuracy differs between two conditions (call them conditions 1 and 2), the variances will only be identical if \\(p1\\) (the proportion of correct responses in condition 1) and p2 (the proportion of correct responses in condition 1) are equally distant from 0.5 (e.g. \\(p1 = .4\\) and \\(p2 = .6\\)). The bigger the difference in distance from 0.5, comparing the conditions, the less similar the variances will be.\nSample proportions between 0.3 and 0.7 are considered close enough to 0.5 to assume homogeneous variances (Agresti, 2002). Unfortunately, we usually cannot determine a priori the range of sample proportions in our experiment.\nIn general, variances in two binomially distributed conditions will not be homogeneous but, as you will recall, in both ANOVA and regression analysis, we assume homogeneity of variance in the outcome variable when we compare the effect of differences (in the mean outcome) between the different levels of a factor. This means that if we design a study in which the outcome variable is the accuracy of responses in different experimental conditions and we plan to use ANOVA or regression to estimate the effect of variation in experimental conditions on response accuracy then unless we get lucky our estimation of the experimental effect will take place under circumstances in which the application of the analysis method (ANOVA or regression) and thus the analysis results will be invalid.\n\n\n\nThe application of traditional (parametric) analysis methods like ANOVA or regression to categorical outcome variables like accuracy is very common in the psychological literature. The problem is that these approaches can give us misleading results.\n\nLinear models assume outcomes are unbounded so allow predictions that are impossible when outcomes are, in fact, bounded as is the case for accuracy or other categorical variables.\nLinear models assume homogeneity of variance but that is unlikely and anyway cannot be predicted in advance when outcomes are categorical variables.\nIf we are interested in the effect of an interaction between two effects, using ANOVA or linear models on accuracy (proportions of responses correct) can tell you, wrongly, that the interaction is significant.\n\nTraditionally, researchers have recognized the limitations attached to using methods like ANOVA or regression to analyze categorical outcomes like accuracy and have applied remedies, transforming the outcome variables, e.g. the arcsine root transformation, to render them ‘more normal’. However, as @jaeger2008 demonstrates, the remedies like the arcsine transformation that have traditionally been applied are often not likely to succeed. @jaeger2008 completed a comparison of the results of analyses of accuracy data, where outcomes are raw values for the proportions of correct responses, or arcsine transformed values for proportions correct. His comparison demonstrated that the traditional techniques will show either that effects are significant when they are not or that effects are not significant when they are.\n\n\n\n\nWhat we need, then, is a method that allows us to analyze categorical outcomes. We find the appropriate method in Generalized Linear Models, and in Generalized Linear Mixed-effects Models for repeated measures or multilevel structured data. We can understand these methods, as their name suggests, as generalizations of linear models or linear mixed-effects models: generalizations that allow for the categorical nature of some outcome data.\nYou can understand how Generalized Linear Mixed-effects Models work by seeing them as analyses of categorical outcome data like accuracy where the outcome variable is transformed, as I explain next (see Baguley, 2012, for a nice clear explanation, which I summarize here).\nOur problems begin with the need to estimate effects on a bounded outcome like accuracy with a linear model which, as we have seen, will yield unbounded predictions.\nThe logistic transformation takes \\(p\\) the probability of an event with two possible outcomes, and turns it into a logit: the natural logarithm of the odds of the event. The effect of this transformation is to turn a discrete binary bounded outcome into a continuous unbounded outcome.\n\nTransforming a probability to odds \\(o = \\frac{p}{1-p}\\) is a partial solution.\n\n\nOdds are, for example, the ratio of the probability of the occurrence of an event compared to the probability of the non-occurrence of an event, or, in terms of a response accuracy variable, the ratio of the probability of the response being correct compared to the probability of the response being incorrect.\nAnd odds are continuous numeric quantities that are scaled from zero to infinity.\nYou can see how this works if you run the calculations using the equation \\(o = \\frac{p}{1-p}\\) in R as odds &lt;- p/(1-p): replacing p with various numbers (e.g. p = 0.1, 0.01, 0.001).\n\n\nWe can then use the (natural) logarithm of the odds \\(logit = ln\\frac{p}{1-p}\\) because using the logarithm removes the boundary at zero because log odds range from negative to positive infinity.\n\n\nYou can see how this works if you run the calculations using the equation \\(logit = ln\\frac{p}{1-p}\\) in R as logit &lt;- log(p/(1-p)): replacing p with smaller and smaller numbers (e.g. p = 0.1, 0.01, 0.001) gets you increasing negative log odds.\n\nWhen we model the log odds (logit) that a response will be correct, the model is called a logistic regression or logistic model. We can think of logistic models as working like linear models with log-odds outcomes.\n\\[\nln\\frac{p}{1-p} = logitp = \\beta_0 + \\beta_1X_1 \\dots\n\\]\nWe can describe the predicted log odds of a response of one type as the linear sum of the estimated effects of the included predictor variables. In a logistic regression, the predictors have an additive relationship with respect to the log odds outcome, just like in an ordinary linear model. The log odds range from negative to positive infinity; logit of 0 corresponds to proportion of .5.\nBaguley (2012) notes that is it advantageous that odds and probabilities are both directly interpretable. We are used to seeing and thinking in everyday life about the chances that some event will occur.\n\n\n\n\nA small change in R lmer code allows us to extend what we know about linear mixed-effects models to conduct Generalized Linear Mixed-effects Models. We change the function call from lmer() to glmer(). However, we have to make some other changes, as we detail in the following sections.\nWe will be examining the impact of the experimental effects, that is, the fixed effects associated with the impacts on the outcome Score (accuracy of response in the word spelling test) associated with the following comparisons:\n\nTime: time 1 versus time 2\nOrthography: present versus absent conditions\nInstructions: explicit versus incidental conditions\nStandardized spelling-sound consistency\nInteraction between the effects of Orthography and Instructions\nInteraction between the effects of Orthography and consistency\n\nWe will begin by keeping the random effects structure simple.\n\n\nIn our first model, we will specify just random effects of participants and items on intercepts.\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               Time + Orthography + Instructions + zConsistency_H + \n                               \n                               Orthography:Instructions +\n                               \n                               Orthography:zConsistency_H +\n                               \n                               (1 | Participant) + \n                               \n                               (1 | Word),\n                             \n                             family = \"binomial\", \n                             glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                             data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nThe code works as follows.\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\nglmer() the function name changes because now we want a generalized linear mixed-effects model of accuracy.\n\nThe model specification includes information about fixed effects and about random effects.\n\nWith (1 | Participant) we include random effects of participants on on intercepts.\nWith (1 | Word) we include random effects of stimulus on on intercepts.\n\nSecond, we have the bit that is specific to generalized models.\n\nfamily = \"binomial\" is entered because accuracy is a binary outcome variable (correct, incorrect) so we assume a binomial probability distribution.\n\nWe then specify:\n\nglmerControl(optimizer=\"bobyqa\", ...) to change the underlying mathematical engine (the optimizer) to cope with greater model complexity\n\nand we allow the model fitting functions to take longer to find estimates with optCtrl=list(maxfun=2e5).\n\nNotice how we specify the fixed effects. We want glmer() to estimate “main effects and interactions” that we hypothesized.\nWe specify the main effects with:\n\nTime + Orthography + Instructions + zConsistency_H +\n\nWe specify the interaction effects with:\n\nOrthography:Instructions +\n                               \nOrthography:zConsistency_H +\n\nWhere we ask for estimates of the fixed effects associated with:\n\nOrthography:Instructions the interaction between the effects of Orthography and Instructions;\nOrthography:zConsistency-H the interaction between the effects of Orthography and consistency.\n\n\n\nThere are two forms of notation we can use to specify interactions in R. The simplest form is to use something like this:\n\nOrthography*Instructions\n\nThis will get you estimates of:\n\nThe effect of Orthography: present versus absent conditions.\nThe effect of Instructions: explicit versus incidental conditions.\nAnd the effect of Orthography x Instructions: the interaction between the effects of Orthography and Instructions.\n\nSo, in general, if you want estimates of the effects of variables A, B and the interaction A x B, then you write A*B.\nWe can also use the colon symbol to specify only the interaction, i.e., ignoring main effects, so if you specify A:B then you will get an estimate of the interaction A x B but not the effects A, B.\nWith the coding:\n\nScore ~ Orthography + Instructions + Orthography:Instructions\n\nI would be making explicit that I want estimates for the effects of Orthography, Instruction and the interaction between the effects of Orthography and Instructions.\n\n\n\n\nIf you run the model code, you will get the results shown in the output.\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (1 |  \n    Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n     AIC      BIC   logLik deviance df.resid \n  1040.4   1086.7   -511.2   1022.4     1254 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0994 -0.4083 -0.2018  0.2019  7.4940 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 1.840    1.357   \n Word        (Intercept) 2.224    1.491   \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.878462   0.443957  -4.231 2.32e-05 ***\nTime2                        0.050136   0.083325   0.602    0.547    \nOrthography2                 0.455009   0.086813   5.241 1.59e-07 ***\nInstructions1                0.042289   0.230336   0.184    0.854    \nzConsistency_H              -0.618093   0.384005  -1.610    0.107    \nOrthography2:Instructions1   0.005786   0.083187   0.070    0.945    \nOrthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.008                                   \nOrthogrphy2 -0.059  0.008                            \nInstructns1  0.014  0.025  0.001                     \nzCnsstncy_H  0.016 -0.002 -0.029 -0.001              \nOrthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       \nOrthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007\n\n\nThe output from the model summary first gives us information about the model.\n\nFirst, we see information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula including main effects Score ~ Time + Orthography + Instructions + zConsistencyH,\nAs well as the interactions Orthography:Instructions + Orthography:zConsistencyH,\nAnd the random effects (1 | Participant) + (1 |Word).\nThen we see information about the model algorithm\nThen we see model fit statistics, including AIC BIC logLik\nThen we see information about the distribution of residuals\n\nWe then see information listed under Random Effects: this is where you can see information about the error variance terms estimated by the model.\n\nThe information is listed in four columns: 1. Groups; 2. Name; 3. Variance; and 4. Std.Dev.\n\nWe see in the Random Effects section, the variances associated with:\n\nThe random differences between the average intercept (over all data) and the intercept for each participant;\nThe random differences between the average intercept (over all data) and the intercept for responses to each word stimulus.\n\n\nLastly, just as for linear models, we see estimates of the coefficients of the fixed effects, the intercept and the slopes of the experimental variables.\n\nNote that we get p-values (Pr(&gt;|z|)) for what are called Wald null hypothesis significance tests on the coefficients.\nWe can see that one effect is significant. The estimate for the effect of the presence compared to the absence of Orthography is 0.455009. The positive coefficient tells us that the log odds that a response will be correct is higher when Orthography is present compared to when it is absent.\nWe can also see an effect of consistency that can be considered to be marginal or near-significant by some standards. The estimate for the effect of zConsistency_H is -0.618093 indicating that the log odds of a response being correct decrease for unit increase in the standardized H consistency measure.\n\n\nAs we discussed in the last chapter, we can conduct null hypothesis significance tests by comparing models that differ in the presence or absence of a fixed effect or a random effect, using the Likelihood Ratio Test. In the results output for a GLMM by the glmer() function, you can see that alongside the estimates of the coefficients (and standard error) for the fixed effects we also have z and p-values. Wald z tests for GLMMs test the null hypothesis of no effect by comparing the effect estimate with their standard error, and comparing the resulting test statistic to zero [@bolker2009].\n\n\n\n\nWe usually want to do more than just report whether experimental effects are or are not significant. It helps us to present and interpret the estimates from a model if we can visualize the model prediction. There are a variety of tools that help us to do this.\n\n\nWe can use the plot_model function from the {sjPlot} library. The following sequence of code takes information from the model we have just run, then generates model predictions, of change in the probablity of a correct response (Score) for different levels of the Orthography factor and the consistency variable. I chose these variables because they are the significant or near-significant effects.\n\nporth &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"Orthography\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\npzconsH &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"zConsistency_H\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\ngrid.arrange(porth, pzconsH,\n            ncol=2)\n\n\n\n\nFigure 2: Effect of orthography condition (present versus absent) on probability of a response being correct\n\n\n\n\nThe plots in Figure 2 show clearly how the probability of a correct response is greater for the conditions where Orthography had been present (versus absent) during the word learning phase of the study. We can also see a trend such that the probability of a response being correct decreases as the (in-)consistency of a target word tends to increase.\nTo produce this plot, you will need to install the {sjPlot} library first, and then run the command library(sjPlot) before creating your plot.\nNotice:\n\nplot_model() produces the plot\nplot_modellong.orth.min.glmer) specifies that the plot should be produced given information about the previously fitted model long.orth.min.glmer\ntype = \"pred\" tells R that you want a plot showing the model predictions, of the effect of, e.g., `Orthography: condition\n\nThe function outputs an object whose appearance can be edited as a ggplot object.\nMore information can be found here or here about the {sjPlot} library and plotting model estimates of effects.\n\n\n\n\n\nSo far, we have been considering the results of a random intercepts model in which we take into account the random effects of participants and stimulus word differences on intercepts. We have ignored the possibility that the slopes of the experimental variables might vary between participants or between words.\n\n\n\n\n\n\nTip\n\n\n\nWe now need to examine the question: What random effects should we specify?\n\n\nI must warn you that the question and the answer are complex but the coding is quite simple, and the approaches you can take to address the question are, now, quite well recognized by the psychological community. In other words, the community recognizes the nature of the problem, and recognizes the methods you can potentially follow to solve the problem.\nThe complexity, for us, lies not in the mathematics: the coding is simple and the glmer() function does the work.\nI think the complexity lies, firstly, in how we have to think about the study design, what gets manipulated or allowed to vary. I find it very helpful to sketch out, by hand, what the study design means in relation to who does what in an experiment.\nThe complexity lies, secondly, and in how we have to translate our understanding of the study design to a specification of random effects. We can master that aspect of the challenge through practice.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe question is:\n\nSo, what random effects should we include?\n\n\n\nIf you go back to the description of the study design, then you will be able to see that a number of possibilities follow, theoretically, from the design.\nA currently influential set of recommendations [@Barr2013a; but see discussions in @bates2015parsimonious,@eager and @matuschek2017] has been labeled Keep it maximal.\nIn this approach, if you are testing effects manipulated according to a pre-specified design then you should:\n\nTest random intercepts – due to random differences between subjects or between items (or other sample grouping variables);\nTest random slopes for all within-subjects or within-items (or other) fixed effects.\n\nThis means that specification of the random effects structure requires two sets of information – corresponding to the answers to two questions:\n\nWhat are the fixed effects?\nWhat are the grouping variables?\n\nAs we have seen, we can rephrase the second question in terms: did you test multiple participants using multiple stimuli (e.g., words …) or did you test participants under multiple different conditions (e.g., levels of experimental condition factors)?\nOur answers to these questions then dictate potentially how we specify the random effects structure for our model.\nWe can consider that we should certainly include a random effect of each grouping variable (e.g., participants, stimulus word) on intercepts. We can reflect that we should also include a random effect of a grouping variable e.g. participant on the slope of each variable that was manipulated within the units of that variable.\nWhen we specify models, we should remember that by default if you specify (1 + something | participant) then you are specifying that you want to take into account:\n\nvariance due to the random differences between participants in intercepts (1 ...| participant),\nplus variance due to the random differences between participants in slopes (... something | participant),\nplus the covariance (or correlation) between the random intercepts and the random slopes.\n\nYou will have become familiar with the practice of referring to effects as within-subjects or between-subjects previously, in working with ANOVA. Here, whether an effect is within-subjects or within-items or not has relevance to whether we can or should specify a random effect of subjects or of items on the slope of a fixed effect.\nIn deciding what random effects we should specify, we need to think about what response data we have recorded, for each of the experimental variables, given the study design. This is because if we want to specify a random effect of participants (or stimulus words) on the slope of an experimental condition then we need to have data, for each person, on their responses under all levels of the condition.\nIf we want to estimate the effect of the experimental manipulation of learning condition, for example, the impact of the presence of orthography, for a person, we need to have data for both levels of the condition (orthography absent and orthography present) for that person. If you think about it, we cannot estimate the slope of the effect of the presence of orthography without response data recorded under both the orthography absent condition and the orthography present condition. If we can estimate the slope of the effect then we can estimate how the slope of the effect deviates between participants.\nWe can spell out how the experimental conditions were manipulated for the example study, as follows. (Writing out this kind of account, for yourself, will be helpful perhaps when you are planning an analysis and have to work out what the random effects could be.)\n\nThe effect of Orthography was manipulated within participants and within stimulus words. This is because the presence of orthography (orthography absent versus orthography present) was manipulated so that we have data about test responses to each word under both Orthography conditions, and data about responses from each child under both conditions.\nThe effect of Instructions was manipulated between participants. This is because Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\nWe can say that the effects of Orthography and of Instructions are both manipulated within words. Items were counterbalanced across instruction and orthography conditions, with all words appearing in both orthography conditions for approximately the same number of children within the explicit and incidental groups.\nThe effect of spelling-sound consistency varies between words because different words have different consistency values but the effect of consistency varies within} participants because we have response data for each participant for their responses to words of different levels of consistency.\nWe recorded responses for all participants and all words so we can say that the effect of Time (test time 1 versus time 2) can also be understood to vary within} both participants and words. This means that, for each person’s response to each word on which they are tested, we have response data recorded at both test times.\n\nThese considerations suggests that we should specify a model with the random effects:\n\nThe random effects of participants on intercepts, and on the slopes of the effects of Time, Orthography and spelling-sound consistency, as well as all corresponding covariances.\nThe random effects of stimulus words on intercepts, and on the slopes of the effects of Time, Orthography and Instructions, as well as all corresponding covariances.\n\nThis is simple to do using the code shown following.\n\nlong.orth.max.glmer &lt;- glmer(Score ~ \n                           Time + Orthography + Instructions + zConsistency_H + \n                           \n                           Orthography:Instructions +\n                           \n                           Orthography:zConsistency_H +\n                           \n                           (Time + Orthography + zConsistency_H + 1 | Participant) + \n                           \n                           (Time + Orthography + Instructions + 1 |Word),\n                         \n                         family = \"binomial\",\n                         glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                         \n                         data = long.orth)\n\nsummary(long.orth.max.glmer)\n\nWhere we have the critical terms:\n\n(Time + Orthography + zConsistency_H + 1 | Participant) to account for the random effects of participants on intercepts, and on the slopes of the effects of Time, Orthography and spelling-sound consistency, as well as all corresponding covariances.\n(Time + Orthography + Instructions + 1 |Word) to account for the random effects of stimulus words on intercepts, and on the slopes of the effects of Time, Orthography and Instructions, as well as all corresponding covariances.\n\nIf you run this code, however, you will see that you get warnings along with your estimates.\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (Time +  \n    Orthography + zConsistency_H + 1 | Participant) + (Time +  \n    Orthography + Instructions + 1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n     AIC      BIC   logLik deviance df.resid \n  1053.6   1192.4   -499.8    999.6     1236 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1014 -0.4027 -0.1723  0.2037  7.0331 \n\nRandom effects:\n Groups      Name           Variance Std.Dev. Corr             \n Participant (Intercept)    2.043126 1.42938                   \n             Time2          0.005675 0.07533   0.62            \n             Orthography2   0.079980 0.28281   0.78 -0.01      \n             zConsistency_H 0.065576 0.25608   0.49  0.99 -0.16\n Word        (Intercept)    2.793447 1.67136                   \n             Time2          0.046736 0.21618   0.14            \n             Orthography2   0.093740 0.30617  -0.68 -0.81      \n             Instructions1  0.212706 0.46120  -0.74 -0.05  0.38\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -2.10099    0.49588  -4.237 2.27e-05 ***\nTime2                        0.02077    0.12285   0.169 0.865741    \nOrthography2                 0.52480    0.15496   3.387 0.000708 ***\nInstructions1                0.24467    0.27281   0.897 0.369801    \nzConsistency_H              -0.67818    0.36310  -1.868 0.061802 .  \nOrthography2:Instructions1  -0.05133    0.10004  -0.513 0.607908    \nOrthography2:zConsistency_H  0.05850    0.11634   0.503 0.615064    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.097                                   \nOrthogrphy2 -0.280 -0.187                            \nInstructns1 -0.278  0.023  0.109                     \nzCnsstncy_H  0.062  0.005 -0.064  0.120              \nOrthgrp2:I1  0.024  0.005 -0.071  0.212 -0.065       \nOrthgr2:C_H -0.062  0.049  0.246 -0.031 -0.440  0.001\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nNotice, especially, the warning:\n\nboundary (singular) fit: see ?isSingular\n\nIf you put the warning message text into a search engine, then you will get directed to a variety of discussions about what they mean and what you should do about them.\nA highly instructive blog post by (I think) Ben Bolker provides some very useful advice here.\n\nCheck singularity\nIf the fit is singular or near-singular, there might be a higher chance of a false positive (we’re not necessarily screening out gradient and Hessian checking on singular directions properly); a higher chance that the model has actually misconverged (because the optimization problem is difficult on the boundary); and a reasonable argument that the random effects model should be simplified.\nThe definition of singularity is that some of the constrained parameters of the random effects theta parameters are on the boundary (equal to zero, or very very close to zero …)\n\n(Emphasis added.)\nI am going to take his advice and simplify the random effects part of the model. We know that the random intercepts model converges fine and now we know that the maximal model does not. Thus, our task is now to identify a model that includes random effects of participants or items on slopes and still converges without warnings.\n\n\n\nI am just going to assume we need both random effects of subjects and of items on intercepts so I focus on random slopes here. (This assumption may not always be true but is often useful.)\nWe can fit a series of models as follows. Note that I will not show the results for every model, to save space, but you should run the code to see what happens. Look out for convergence or singularity warnings, where they appear.\n\n\nIn the first model, we have just random effects of participants or items on intercepts. This is where we started.\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               Time + Orthography + Instructions + zConsistency_H + \n                               \n                               Orthography:Instructions +\n                               \n                               Orthography:zConsistency_H +\n                               \n                               (1 | Participant) + \n                               \n                               (1 | Word),\n                             \n                             family = \"binomial\", \n                             glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                             data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nWe saw that the model converged, and we looked at the results previously.\nWhat next? A simple approach we can take is to see if we can add each fixed effect to be included in our random effects terms, one effect at a time.\n\n\n\nIn our second model, we change the random effects terms so that we can account for the random effects of participants and of items on intercepts as well as on the slopes of the Orthography effect. (The Orthography effect is both within-subjects and within-items.)\n\nlong.orth.2.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                             (dummy(Orthography) + 1 || Participant) + \n                             \n                             (dummy(Orthography) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nsummary(long.orth.2.glmer)\n\nThis model converges without warnings.\nNotice that we specify that we || do not want random covariances; we are keeping things simple in each step.\nNote the use of dummy() inside the random effects terms. The ‘dummy’ is a mis-leading name; we are not talking about dummy coding (as above). Here, the dummy() stops R from mis-interpreting the requirement to estimate the effect of the differences between category levels, within random effects.\nThe reason is explained here.\nYou can see what impact it has by specifying, instead, the naked random effect:\n(Orthography + 1 || Participant).\n\n\n\nNext we can add Instructions to take into account random differences between words in the slope of this effect. We show the results for this model as they are instructive.\n\nlong.orth.3.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                             (dummy(Orthography) + 1 || Participant) + \n                             \n                             (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nsummary(long.orth.3.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n     AIC      BIC   logLik deviance df.resid \n  1036.5   1098.2   -506.2   1012.5     1251 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9951 -0.4068 -0.1920  0.1838  5.9308 \n\nRandom effects:\n Groups        Name                Variance Std.Dev.\n Participant   (Intercept)         1.64393  1.2822  \n Participant.1 dummy(Orthography)  0.55604  0.7457  \n Word          (Intercept)         1.94313  1.3940  \n Word.1        dummy(Orthography)  0.01608  0.1268  \n Word.2        dummy(Instructions) 0.86694  0.9311  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621258  0.4400570  -4.459 8.24e-06 ***\nTime2                        0.0507347  0.0843236   0.602  0.54740    \nOrthography2                 0.4263703  0.1114243   3.827  0.00013 ***\nInstructions1                0.1907424  0.2632579   0.725  0.46873    \nzConsistency_H              -0.6270892  0.3669873  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265727  0.1048392  -0.253  0.79991    \nOrthography2:zConsistency_H -0.0006303  0.0878824  -0.007  0.99428    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\n\n\nThis model also converges without warnings.\nTake a look at the random effects summary. You can see:\n\nParticipant (Intercept) 1.64393 estimated variance due to random effect of participants on intercepts\nParticipant.1 dummy(Orthography) 0.55604 variance due to random effect of participants on the slope of the Orthography effect\nWord (Intercept) 1.94314 variance due to random effect of words on intercepts\nWord.1 dummy(Orthography) 0.01607 variance due to random effect of words on the slope of the Orthography effect\nWord.2 dummy(Instructions) 0.86694 variance due to random effect of words on the slope of the Instructions effect\n\nWe do not see correlations (random effects covariances) because we use the || notation to stop them being estimated. We want to stop them being estimated because we want to see what we gain from adding just the requirement, first, to estimate the variance associated with random effects of participants or words on the slopes of the experimental variables.\nAlso, we can suspect that adding the requirement to estimate covariances will blow the model up for two reasons. The maximal model, including random slopes variances and covariances clearly did not converge. Secondly, at least two of the correlations listed in the random effects, for the maximal model, were pretty extreme with Corr \\(=-0.01\\) and \\(=0.99\\); such extreme values (\\(r \\sim \\pm 1\\)) are bad signs; see the discussion in Section 1.10.2.\n\n\n\nIn the following models, because we can see that we can get a model to converge with random effects of participants or items on Orthography, and random effect of participants on Instructions, I am going to keep these random effects in the model. I will check if adding further effects is OK too, in terms of successful convergence. I am going to treat all the following models as variations on a theme, the theme being: can we add anything else to:\n\n                             (dummy(Orthography) + 1 || Participant) + \n                             \n                             (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n\n\n\n\nNext we see if we can add zConsistency_H.\n\nlong.orth.4.a.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                             (dummy(Orthography) + zConsistency_H + 1 || Participant) + \n                             \n                             (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.a.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    zConsistency_H + 1 || Participant) + (dummy(Orthography) +  \n    dummy(Instructions) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n     AIC      BIC   logLik deviance df.resid \n  1038.5   1105.3   -506.2   1012.5     1250 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.282e+00\n Participant.1 dummy(Orthography)  5.560e-01 7.456e-01\n Participant.2 zConsistency_H      1.259e-10 1.122e-05\n Word          (Intercept)         1.943e+00 1.394e+00\n Word.1        dummy(Orthography)  1.604e-02 1.266e-01\n Word.2        dummy(Instructions) 8.669e-01 9.311e-01\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621298  0.4400610  -4.459 8.24e-06 ***\nTime2                        0.0507346  0.0843237   0.602  0.54740    \nOrthography2                 0.4263731  0.1114190   3.827  0.00013 ***\nInstructions1                0.1907320  0.2632610   0.724  0.46876    \nzConsistency_H              -0.6270931  0.3669852  -1.709  0.08749 .  \nOrthography2:Instructions1  -0.0265706  0.1048369  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006353  0.0878784  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nWe see two useful pieces of information when we run this model:\n\nWe get the warning boundary (singular) fit: see ?isSingular that tells us the model algorithm could not converge on effects estimates, given the model we specify, given the data.\nNote, also, we see the random effects variance estimate Participant.2 zConsistency_H 1.259e-10.\n\nThese two things are possibly connected: the singularity warning; and the estimated variance of \\(1.259e-10\\) (i.e. a very very small number) associated with the random effect of participants on the slope of the zConsistency_H. We can expect that the model fitting algorithm is going to have difficulty estimating nothing, or something close to nothing: here, the very very small variance associated with the between-participant differences in the slope of the non-significant effect of word spelling-sound consistency on response accuracy.\n\n\n\nWhat about the random effects of participants or of words on the slope of the effect of Time?\n\nlong.orth.4.b.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                             (dummy(Orthography) + dummy(Time) + 1 || Participant) + \n                             \n                             (dummy(Orthography) + dummy(Instructions) + dummy(Time) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.b.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    dummy(Time) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    dummy(Time) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n     AIC      BIC   logLik deviance df.resid \n  1040.5   1112.5   -506.2   1012.5     1249 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.2821915\n Participant.1 dummy(Orthography)  5.560e-01 0.7456312\n Participant.2 dummy(Time)         0.000e+00 0.0000000\n Word          (Intercept)         1.943e+00 1.3939636\n Word.1        dummy(Orthography)  1.604e-02 0.1266478\n Word.2        dummy(Instructions) 8.669e-01 0.9310620\n Word.3        dummy(Time)         2.161e-07 0.0004648\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621302  0.4400578  -4.459 8.24e-06 ***\nTime2                        0.0507347  0.0843253   0.602  0.54740    \nOrthography2                 0.4263731  0.1114192   3.827  0.00013 ***\nInstructions1                0.1907329  0.2632605   0.725  0.46876    \nzConsistency_H              -0.6270910  0.3669905  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006351  0.0878784  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nNope.\nWe again see two useful pieces of information when we run this model:\n\nWe get the warning boundary (singular) fit: see ?isSingular that tells us the model algorithm could not converge on effects estimates, given the model we specify, given the data\nNote, also, we see the random effects variance estimate Participant.2 dummy(Time) 0.000e+00.\nAnd we see the variance estimate Word.3 dummy(Time) 2.161e-07.\n\nAgain, we can surmise that a mixed-effects model will get into trouble – and we will see convergence warnings – where we have a fixed effect with little impact (like, here, Time) and we are asking for the estimation of variance associated with random differences in slopes where there may be, in fact, little random variation. Possibly, these two things are connected too: we are perhaps unlikely to see random differences in the slope of the effect of an experimental variable if the effect is at or near zero. Possibly, we may see the effect of an experimental variable which is very very consistent. Think back to Week 16 and the effect associated with the relation between maths and physics scores where there seemed to be little variation between classes in the slope representing the relation.\n\n\n\n\nWe can see that a model has difficulty if we see things like:\n\nConvergence warnings, obviously\nVery very small random effects variances\nExtreme random effects correlations of \\(\\pm 1.00\\)\n\nIf we see a warning that the model fitting algorithm nearly failed to converge: boundary (singular) fit: see ?isSingular or failed to converge then this tells us that, given the data, the mathematical engine (optimizer) underlying the lmer() function got into trouble because, in short, it was trying to find estimates for effects that were close to not being there at all.\nIf the variances for the random effects of participants or stimulus items on the slopes of an experimental variable are very small this suggests that the level of complexity in the model cannot really be justified or that the model will have difficulty estimating it. Extreme correlations (near 0 or 1) between random effects on intercepts and on slopes of fixed effects suggest the level of complexity in the model cannot really be justified (see also the discussion in Bates et al., 2015; Matuschek et al., 2017).\n\n\n\nThere is no point comparing the models that do not converge, so we focus on those that do.\nDoes the addition of random slopes improve model fit? We can compare the model in pairs, as follows, to test whether each addition in model complexity improves model fit. We run the code for the model comparisons as follows.\nFirst, we compare the model long.orth.min.glmer (just random intercepts) with long.orth.2.glmer to check if increasing model complexity, by accounting for random differences between participants or words in the slope of the Orthography effect improves model fit to data.\n\nanova(long.orth.min.glmer, long.orth.2.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.2.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + 1 || Word)\n                    npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nlong.orth.min.glmer    9 1040.4 1086.7 -511.20   1022.4                     \nlong.orth.2.glmer     11 1041.0 1097.6 -509.51   1019.0 3.3909  2     0.1835\n\n\nSecond, we compare the model long.orth.min.glmer (just random intercepts) with long.orth.3.glmer to check if increasing model complexity, by accounting for random differences between participants or words in the slope of the Instructions effect improves model fit to data.\n\nanova(long.orth.min.glmer, long.orth.3.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.3.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) + 1 || Word)\n                    npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nlong.orth.min.glmer    9 1040.4 1086.7 -511.20   1022.4                       \nlong.orth.3.glmer     12 1036.5 1098.2 -506.24   1012.5 9.9115  3    0.01933 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI think this justifies reporting the model long.orth.3.glmer.\nIf you look at the outputs, the addition of the random effects of participants and words on the slope of the effect of Orthography cannot be justified (\\(\\chi^2 = 3.3909, 2 df, p = .1835\\)) by improved model fit, in comparison to a model with the random effects of participants and words on intercepts.\nThe model comparison summary indicates that the addition of a random effect of words on the slope of the Instructions effect is justified by significantly improved model fit to data (\\(\\chi^2 = 9.9115, 3 df, p = 0.01933\\)).\nNotice I take a bit of a short-cut here, by adding both random effects for Orthography and Instructions. Logically, if adding the random effect for Orthography does not improve model fit but adding the random effects for both random effects for Orthography and Instructions does then it is the addition of Instructions that is doing the work. However, I should prefer to see as complex a random effects structure as possible, provided a model converges.\nWhile adding the random effect of Orthography does not improve model fit significantly, you see researchers allowing a generous p-value threshold for inclusion of terms (i.e. it is ok to add variables up to where \\(.p &lt; .2\\)). Matuschek et al. (2017) argue that when we are engaged in model selection – here, this is what we are doing because we are trying to figure out what model (with what random effects) we should use – then we should resist the reflex to choose the model that seems justified because the LRT \\(\\chi^2, p &lt; .05\\).\nThe \\(\\chi^2\\) alpha level cannot be interpreted as the expected model selection Type I error rate (in Null Hypothesis Significance Test terms) but rather as the relative weight of model complexity and goodness-of-fit (look back at the discussion of model comparison in the previous chapter). In this sense, setting a threshold such that we include an effect only if \\(\\chi^2, p &lt; .05\\) will always tend to penalize model complexity, and tend therefore to lead us to choose simpler (perhaps too simple) models.\n\n\n\nSometimes (g)lmer() has difficulty finding estimates for effects in a model given a dataset. If it encounters problems, the problems are expressed as warnings about convergence failures. Convergence failures typically arise when the model is too complicated for the data [see the discussion in @bates2015parsimonious; @eager; @matuschek2017; and @meteyard2020a]. As we have seen, problems can occur if you are trying to estimate (or predict) random effects terms that are very small – that do not really explain much variance in performance. Problems can also occur if variables have very large or very small ranges.\nWe can detect and address these problems in a number of ways:\n\nSometimes convergence problems can be fixed by switching the optimizer used to fit the model – we can do this by adding the argument: glmerControl(optimize = \"bobyqa\") to the glmer() function call, as I did for the class example. Switching optimizers is a quick solution to a common problem: models can fail to converge for a number of different reasons. In short, there may not be enough data for the model fitting process to settle on appropriate estimates for fixed or random effects.\nSometimes, a warning message advises us to consider rescaling the continuous numeric predictor variable. For this reason, and others, I usually standardize numeric predictor variables, as a default, before the analysis.\nSometimes, the warnings tell us that we need to simplify the random effects part of the model. We can simplify the random effects structure of a mixed-effects model in a number of ways:\n\n\nAs we examine the estimates resulting from a model fit we can consider whether the variance and covariance terms are small or large.\nUltimately, I decided that the effects of subjects and items on intercepts was important, as was, to some extent, the effect of words on the slope of the effect of Instructions.\n\nThe approach we have progressed through is widely used [see discussions in @Baayen2008; @Barr2013a; @matuschek2017; @meteyard2020a].\nMuch useful advice is set out in the troubleshooting guide by Ben Bolker here.\n\n\nTo demonstrate the impact of these adjustments, you could refit the models discussed in the foregoing:\n\nWithout using the standardized consistency variable as a predictor;\nWithout using the modification to the model fitting code, i.e. deleting the line that includes glmerControl(optimizer = \"bobyqa\");\nMaking the last most complex model more complex by adding further random effects of subjects or items on the slopes of both main effects and the interaction.\n\n\n\n\nMy advice, then, is to consider whether random effects should be included in a model based on:\n\nTheoretical reasons, in terms of what your understanding of a study design allows and requires, with respect to random differences between groups (classes, participants, stimuli etc.) or stimuli;\nModel convergence, as when models do or do not converge;\nOver a series of model comparisons, an evaluation of whether model fit is improved by the inclusion of the random effect.\n\nThis sounds like it involves work, judgment, and a process. It also sounds like people may disagree on the judgment or the process so that you shall have to share data and code, to enable others to check if the results vary depending on different decisions or different approaches. And it sounds like you will need to not only figure out what to do but also justify the approach you take when you report the results.\nI think all these things are true.\nThis is why Lotte Meteyard and I advise that researchers need to explain their approach, and share their data and code, when they report their analyses. Our best practice guidance for reporting mixed-effects models includes, among other things, the advice that in reports …\n\nRandom effects are explicitly specified according to sampling units (e.g., participants, items), the data structure (e.g., repeated measures) and anticipated interactions between fixed effects and sampling units (e.g., intercepts only or intercepts and slopes). Fixed effects and covariates are specified from explicitly stated research questions and/or hypotheses.\nReport the size of the sample analysed in terms of total number of data points and of sampling units (e.g., number of participants, number of items, number of other groups specified as random effects, such as classes of children).\nA clear statement of the methods by which models are compared/selected; e.g., simple to complex, covariates first, random effects first, fixed effects first etc.\nReport comparison method (LRT, AIC, BIC) and justify the choice.\nA complete report of all models compared (e.g., in appendices/supplementary data/analysis scripts) with model equations and the result of comparisons.\nIf models fail to converge, the approach taken to manage this should be comprehensively reported. This should include the formula for each model that did or did not converge and a rationale for a) the simplification method used and b) the final model reported. This may be most easily presented in an analysis script.\n\n\n\n\n\n\n\nTip\n\n\n\nThis looks like a lot of work.\n\nWhy bother?\n\n\n\nI think it is always worth asking this question.\nThe first answer is that it is all relative. In my own experience, a lot of the effort spent in the research workflow used to be occupied by face-to-face data collection: weeks or months of testing; now all data get collected online, and it gets finished overnight. Considerable time and effort (as Hadley Wickham’s joke runs, 80% of analysis effort) was spent on tidying the data before analysis; I still do this work but it is now much faster and less effortful, thanks to {tidyverse}. A lot of effort used to be spent by me or colleagues on the literature review, the power analysis, or the stimulus preparation: that still happens. And a lot of effort used to be spent on doing the analysis and figuring out what the results mean: that, too, still happens. It is up to you if you want to spend ten months on data collection and five minutes on data analysis (as another joke has it, a million bucks on the data and a nickel on the statistics).\nI think we do need to work at understanding the most appropriate analysis for our data, based on both our theoretical expectations and a data-driven evaluation. No-one is going to help us unsee multilevel structure in the data, or save us from the obligation to take into account random effects. Matuschek et al. (2017; p.312) argue that “The goal of model selection is not to obtain a significant p-value; the goal is to identify the most parsimonious model that can be assumed to have generated the data.” This makes sense to me. And their analyses show that determining a parsimonious model with a standard model selection criterion is a defensible choice, a way to take into account random effects, while controlling for both the risk of false positives, and the risk of false negatives.\n\n\n\n\n\nWe have discussed how to report the results of mixed-effects models previously. The same conceptual structure, and similar language, can be used to report the results of Generalized Linear Mixed-effects Models (GLMMs).\nI think you need to think about reporting the analysis as a task in which you first prepare the reader, explaining the motivation for using GLMMs, then present the analysis you did (the process, in outline), then present the results you shall later discuss.\n\nStart by explaining the study design: outline the fixed effects that have to be estimated.\nExplain how random effects structure was selected – be prepared to present a short version of the story in the main part of the report – sharing your code, in an appendix, to illustrate the steps in full.\n\nLotte Meteyard and I recommend that results reporting should:\n\nProvide equation(s) that transparently define the reported model(s). An elegant way to do this is providing the model equation with the table that reports the model output.\nAnd that final model(s) should be reported in a table that includes all parameter estimates for fixed effects (coefficients, standard errors and/or confidence intervals, associated test statistics and p-values if used), random effects (standard deviation and/or variance for each random effect, correlations/covariances if modelled) and some measure of model fit (e.g. R-squared, correlation between fitted values and data).\nWhile researchers should be able to share the coding script used to complete the analysis and, wherever possible, share data that generated the reported results.\n\nFor the word learning study we have been working through, the Results section for the report would include the following elements:\n\nExplain approach\n\n\nWe used mixed-effects models to analyse data because this approach permits modelling of both participant- and item-level variability simultaneously, unlike more traditional approaches such as ANOVA. In this study, multiple participants responded to multiple items, meaning that both participants and items were sources of nonindependence in our data (i.e. responses from the same participant are likely to be correlated, as are responses to the same item). Compared to ANOVA, mixed-effects models offer a more flexible approach, and are better able to handle missing data without significant loss of statistical power (Baayen, Davidson, & Bates, 2008).\n\n\nExplain how you get from the study design to the model you use to test or estimate key effects\n\n\nWe took a hypothesis driven approach, estimating the fixed effects of time (Time 1 versus Time 2), Orthography (absent versus present), Instructions (incidental versus explicit) and consistency (standardized H), as well as the interaction between orthography and instructions and the interaction between orthography and consistency. Different levels of the three binary fixed effects were sum coded… Consistency H, as a numeric predictor variable, was standardized to z scores before entry to models as a predictor. scores before entry to models as predictors. –&gt;\n\n\nOutline the model comparison or model selection work\n\n\nThe models were initially fitted specifying just random effects to account for variation by participants and stimuli in accuracy (random intercepts) plus terms to estimate the fixed effects of the experimental conditions ([name them]), and the interactions [name them]. Following the recommendations of Barr, Levy, Scheepers, and Tily (2013; see also Baayen, 2008; Matuschek et al., 2017), we fitted further models adding both random intercepts and random slopes for the random effects. Likelihood ratio test comparison of models showed that a model with both random intercepts and slopes … fit the data better than a model with just random intercepts \\((\\chi^2(df) = ..., p = ...)\\).\n\n\nUse appendices or supplementary materials\n\n\nTo give the reader full information on models fit, model comparisons.\nTo Help the reader with a concise summary of estimates.\n\nAs I have advised for reporting linear models, I included a tabled summary of coefficient estimates, presenting fixed and random effects (see e.g. Davies et al., 2013; Monaghan et al., 2015)\n\nShow and tell\n\nUse figures – model prediction plots, as seen – to help the reader to see what the fixed effects estimates imply.\n\n\n\n\n\n\nTip\n\n\n\nWhich model do we report?\n\nNote that given the model comparison results we have seen, I would probably report the estimates from long.orth.3.glmer. The model appears to include the most comprehensive account of random effects while still being capable of converging.\n\n\n\n\n\n\nWe focused on the need to use Generalized Linear Mixed-effects Models (GLMMs). We identified the kind of outcome data (like response accuracy) that requires analysis using GLMMs. Alternative methods, and their limitations, were discussed.\nWe examined a study that incorporates repeated measures (participants respond to multiple stimuli), a 2 x 2 factorial design, and a longitudinal aspect (participants tested at two time points), the word learning study [@ricketts2021].\nWe discussed the need to use effect coding for categorical predictor variables (factors). We work through example code to set factor level coding as required.\nWe worked through a random intercepts GLMM, and identified the critical elements of the model code, and of the results summary, including hypothesis test p-values. We examined how to present visualizations of fixed effects estimates (model predictions) using different libraries.\nWe then moved on to considering the question of what random effects we should include in the model. We considered the study design in some depth, and explored what random effects we could, in theory, expect to require. We then worked through a model comparison approach. We looked at some warning signs, what they indicate, and how to deal with them.\nWe considered how to report the model selection (or comparison, or building) process, and how to report the model for presentation of results.\n\n\nWe used two functions to fit and evaluate mixed-effects models.\n\nWe used glmer() to fit a mixed-effects model\nWe used anova() to compare two or more models using AIC, BIC and the Likelihood Ratio Test\n\n\n\n\n\nThe example studies referred to in this chapter are published in [@monaghan2015; @ricketts2021].\nBen Bolker provides a very readable introduction to Generalized Linear Mixed-effects Models [@bolker2009; see also @jaeger2008].\n[@Baayen2008; see also @Barr2013a] discuss mixed-effects models with crossed random effects.\nThe issue of model comparison or model selection, and the appropriate choice of random effects structure is discussed helpfully by @Baayen2008, @bates2015parsimonious, @Barr2013a, @eager and @matuschek2017.\nI wrote a tutorial article on mixed-effects models with Lotte Meteyard [@meteyard2020a]. We discuss how important the approach now is for psychological science, what researchers worry about when they use it, and what they should do and report when they use the method.\nAccessible ook length introductions are provided by @Snijders2004a and @Gelman2007g.\n\n\nCan be found here.\nOther helpful online advice by Ben Bolker (besides his numerous helpful interventions on StackOverflow) can be found here and here.\n\n\n\n\nFurther information about the variables in the long.orth_2020-08-11.csv dataset is listed following.\n\nParticipant\n\nCell values comprise character strings coding for participant. Participant identity codes were used to anonymise participation. Children included in studies 1 and 2 – participants in the longitudinal data collection – were coded EOF[number]. Children included in Study 2 only (i.e., the older, additional, sample) were coded ND[number].\n\nTime\n\nTest time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\n\nStudy\n\nObservations taken for children included in studies 1 and 2 – participants in the longitudinal daa collection – were coded Study1&2. Children included in Study 2 only (i.e., the older, additional, sample) were coded Study2.\n\nInstructions – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.\nVersion – Experiment administration coding\nWord – Letter string values show the words presented as stimuli to children.\nConsistency-H – Calculated orthography-to-phonology value for each word.\nOrthography – Variable coding for whether participants had seen a word in training in the orthography absent} or present} conditions.\nMeasure – Variable coding for the post-test measure: Sem_all: if the semantic post-test;Orth_sp: if the orthographic post-test.\nScore – Variable coding for response category. For the semantic (sequential or dynamic) post-test, responses were scored as corresponding to:\n3 – correct response in the definition task\n2 – correct response in the cued definition task\n1 – correct response in the recognition task\n0 – if the item wasn’t correctly defined or recognised\nFor the orthographic post-test, responses were scored as:\n1 – correct, if the target spelling was produced in full\n0 – incorrect\nWASImRS Raw score – Matrix Reasoning subtest of the Wechsler Abbreviated Scale of Intelligence\nTOWREsweRS Raw score – Sight Word Efficiency (SWE) subtest of the Test of Word Reading Efficiency; number of words read correctly in 45 seconds.\nTOWREpdeRS Raw score – Phonemic Decoding Efficiency (PDE) subtest of the Test of Word Reading Efficiency; number of nonwords read correctly in 45 seconds.\nCC2regRS Raw score – Castles and Coltheart Test 2; number of regular words read correctly\nCC2irregRS Raw score – Castles and Coltheart Test 2; number of irregular words read correctly\nCC2nwRS Raw score – Castles and Coltheart Test 2; number of nonwords read correctly\nWASIvRS Raw score – vocabulary knowledge indexed by the Vocabulary subtest of the WASI-II\nBPVSRS Raw score – vocabulary knowledge indexed by the British Picture Vocabulary Scale – Third Edition\nSpelling.transcription Transcription of the spelling response produced by children in the orthographic post-test\nLevenshtein.Score Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure, using the `stringdist: library (van der Loo, 2019). This score indexes the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. For example, the response ‘epegram’ for target ‘epigram’ attracts a Levenshtein score of 1 (one substitution). Thus, this score gives credit for partially correct responses, as well as entirely correct responses. The maximum score is 0, with higher scores indicating less accurate responses.\nzTOWREsweRS We standardized TOWREsweRS values, calculating the z score as \\(z = \\frac{x - \\bar{x}}{sd_x}\\), over all observations in the longitudinal} (Study 1) or concurrent} (Study 2) data-set, using the scale() function in R.\nzTOWREpdeRS Standardized TOWREpdeRS scores\nzCC2regRS] Standardized CC2regRS scores\nzCC2irregRSStandardized CC2irregRS scores\nzCC2nwRS Standardized CC2nwRS scores\nzWASIvRS Standardized WASIvRS scores\nzBPVSRSStandardized BPVSRS scores"
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-motivations",
    "href": "PSYC402/Week19.html#sec-glmm-motivations",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We have been discussing how we can use Linear Mixed-effects models to analyze multilevel structured data, the kind of data that we commonly acquire in experimental psychological studies, for example, when our studies have repeated measures designs. The use of Linear Mixed-effects models is appropriate where the outcome variable is a continuous numeric variable like reaction time. In this chapter, we extend our understanding and skills by moving to examine data where the outcome variable is categorical: this is a context that requires the use of Generalized Linear Mixed-effects Models (GLMMs).\nWe will begin by looking at the motivations for using GLMMs. We will then look at a practical example of a GLMM analysis, in an exploration in which we shall reveal some of the challenges that can arise in such work. The R code to do the modeling is very similar to the code we have used before. The way we can understand the models is also similar but with one critical difference. We start to understand that difference here."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-ideas",
    "href": "PSYC402/Week19.html#sec-glmm-ideas",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "Important\n\n\n\nCategorical outcomes cannot be analyzed using linear models, in whatever form, without having to make some important compromises.\n\n\nYou need to do something about the categorical nature of the outcome."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-targets",
    "href": "PSYC402/Week19.html#sec-glmm-targets",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "In this chapter, we look at Generalized Linear Mixed-effects Models (GLMMs): we can use these models to analyze outcome variables of different kinds, including outcome variables like response accuracy that are coded using discrete categories (e.g. correct vs. incorrect). Our aims are to:\n\nRecognize the limitations of alternative methods for analyzing such outcomes, Section 1.8.1.\nUnderstand practically the reasons for using GLMMs when we analyze discrete outcome variables, Section 1.8.2.\nPractice running GLMMs with varying random effects structures.\nPractice reporting the results of GLMMs, including through the use of model plots."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-study-guide",
    "href": "PSYC402/Week19.html#sec-glmm-study-guide",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "I have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 20 minutes\nPart 2 – about 16 minutes\nPart 3 – about 19 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter. The lectures provide a summary of the main points.\n2. Chapter: 04-glmm\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to work with categorical outcomes data using GLMMs.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example dataset.\nExperiment with the .R code used to work with the example data.\nRun GLMMs of demonstration data.\nRun GLMMs of alternate data sets.\nReview the recommended readings (Section 1.13).\n\n3. Practical workbook materials\n3.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-word-learning-data",
    "href": "PSYC402/Week19.html#sec-glmm-word-learning-data",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We will be working with data collected for a study investigating word learning in children, reported by @ricketts2021. You will see that the study design has both a repeated measures aspect because each child is asked to respond to multiple stimuli, and a longitudinal aspect because responses are recorded at two time points. Because responses were observed to multiple stimuli for each child, and because responses were recorded at multiple time points, the data have a multilevel structure. These features require the use of mixed-effects models for analysis.\nWe will see, also, that the study involves the factorial manipulation of learning conditions. This means that, when you see the description of the study design, you will see embedded in it the 2 x 2 factorial design beloved of psychologists. You will be able to generalize from our work this week to many other research contexts where psychologists conduct experiments in which conditions are manipulated according to a factorial design.\nHowever, our focus here is on the fact that the outcome for analysis is the accuracy of the responses made by children to word targets in a spelling task. The categorical nature of accuracy as an outcome is the reason why we now turn to use Generalized Linear Mixed-effects Models.\n\n\nI am going to present the study information in some detail, in part, to enable you to make sense of the analysis aims and results and, in part, so that we can simulate results reporting in a meaningful context.\n\n\nVocabulary knowledge is essential for processing language in everyday life and it is vital that we know how to optimize vocabulary teaching. One strategy with growing empirical support is orthographic facilitation: children and adults are more likely to learn new spoken words that are taught with their orthography [visual word forms; for a systematic review, see @colenbrander2019]. Why might orthographic facilitation occur? Compared to spoken inputs, written inputs are less transient across time and less variable across contexts. In addition, orthography is more clearly marked (e.g., the ends of letters and words) than the continuous speech stream. Therefore, orthographic forms may be more readily learned than phonological forms.\n@ricketts2021 investigated how school-aged children learn words. We conducted two studies in which children learned phonological forms and meanings of 16 polysyllabic words in the same experimental paradigm. To test whether orthographic facilitation would occur, half of the words were taught with access to the orthographic form (orthography present condition) and the other half were taught without orthographic forms (orthography absent condition). In addition, we manipulated the instructions that children received: approximately half of the children were told that some words would appear with their written form (explicit group); the remaining children did not receive these instructions (incidental group). Finally, we investigated the impact of spelling-sound consistency of word targets for learning, by including words that varied continuously on a measure of pronunciation consistency [after @mousikou2017].\nThe quality of lexical representations was measured in two ways. A cuing hierarchical response task (definition, cued definition, recognition) was used to elicit semantic knowledge from the phonological forms, providing a fine-grained measure of semantic learning. A spelling task indexed the extent of orthographic learning for each word. We focus on the analysis of the spelling task responses in this chapter (you may be interested in reviewing our other analyses, see Section 1.5.2).\n@ricketts2021 reported two studies. We focus on Study 1, in which @ricketts2021 measured knowledge of newly learned words at two intervals: first one week and then, again, eight months after training. Longitudinal studies of word learning are rare and this is the first longitudinal investigation of orthographic facilitation.\nWe addressed three research questions.\n\n\n\n\n\n\nNote\n\n\n\n\nDoes the presence of orthography promote greater word learning?\n\n\nWe predicted that children would demonstrate greater orthographic learning for words that they had seen (orthography present condition) versus not seen (orthography absent condition).\n\n\nWill orthographic facilitation be greater when the presence of orthography is emphasized explicitly during teaching?\n\n\nWe expected to observe an interaction between instructions and orthography, with the highest levels of learning when the orthography present condition was combined with explicit instructions.\n\n\nDoes word consistency moderate the orthographic facilitation effect?\n\n\nFor orthographic learning, we expected that the presence of orthography might be particularly beneficial for words with higher spelling-sound consistency, with learning highest when children saw and heard the word, and these codes provided overlapping information.\n\n\n\n\n\n\nChildren were taught 16 novel words in a \\(2 \\times 2\\) factorial design. The presence of orthography (orthography absent vs. orthography present) was manipulated within participants: for all children, eight of the words were taught with orthography present and eight with orthography absent. Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\n\n\n\nIn Study 1, 41 children aged 9-10 years completed the word learning task and completed semantic and orthographic assessments one week after learning (Time 1), and eight months later (Time 2). We tested children from one socially mixed school in the South-East of England (\\(M_{age} = 9.95, SD = .53\\)).\n\n\n\nStimuli comprised 16 polysyllabic words, all of which were nouns. We indexed consistency at the whole word level using the H uncertainty statistic [@mousikou2017]. An H value of 0 would indicate a consistent item (all participants producing the same pronunciation), with values \\(&gt;0\\) indicating greater inconsistency (pronunciation variability) with increasing magnitude.\n\n\n\nA ‘pre-test’ was conducted to establish participants’ knowledge of the stimulus words before i.e. pre- training was administered. Then, each child was seen for three 45-minute sessions to complete training (Sessions 1 and 2) and post-tests (Session 3).\nIn Study 1, longitudinal post-test data were collected because children were post-tested at two time points. (Here, we refer to ‘post-tests’ as the tests done to test learning, after i.e. post training.) Children were given post-tests in Session 3, as noted: this was Time 1. They were then given post-tests again, about eight months later at Time 2.\n\n\n\nThe Orthographic post-test was used to examine orthographic knowledge after training. Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. For the purposes of our learning in this chapter, we focus on the accuracy of responses. Each response made by a child to a target word was coded as correct or incorrect.\nA more sensitive outcome measure of orthographic knowledge was also taken. Responses were also scored using a Levenshtein distance measure, using the {stringdist} library [@loo2022]. This score indexes the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. In the published report (@ricketts2021) we focus our analysis of the orthographic outcome on the Levenshtein distance measure of response spelling accuracy, and further details on the analysis approach (Poisson rather than Binomial Generalized Linear Mixed-effects Models) can be found in the paper.\n\n\n\n\nYou can download the data-04-glmm.zip files folder for this chapter.\nIn this chapter, we will be working with the data about the orthographic post-test outcome for the longitudinal study:\n\nlong.orth_2020-08-11.csv\n\nThe data file is collected together with the .R scripts:\n\n04-glmm-workbook.R the workbook you will need to do the practical exercises.\n04-glmm-workbook-answers.R with answers to questions and code for exercises.\n\nThe data come from the @ricketts2021 study, and you can access the analysis code and data for that study, in full, at the OSF repository here\nIn addition to these data, you will notice that I refer, Section 1.8.1.1, to another dataset analyzed by @monaghan2015. I enclose the data referenced:\n\nnoun-verb-learning-study.csv\n\nAlong with additional .R you can work with to develop your skills:\n\n402-04-GLMM-exercise-gavagai-data-analysis-notes.R\n\nThe @monaghan2015 paper can be accessed here.\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read the data file into R.\n\nlong.orth &lt;- read_csv(\"long.orth_2020-08-11.csv\", \n                      col_types = cols(\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n                      )\n                    )\n\nYou can see, here, that within the read_csv() function call, I specify col_types, instructing R how to treat a number of different variables. Recall that in ?@sec-intro-mixed-data-load, we saw how we can determine how R treats variables at the read-in stage, using col_types() specification. You can read more about this here.\n\n\n\nIt is always a good to inspect what you have got when you read a data file in to R.\n\nsummary(long.orth)\n\nSome of the variables included in the .csv file are listed, following, with information about value coding or calculation.\n\nParticipant – Participant identity codes were used to anonymize participation.\nTime – Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\nInstructions – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.\nWord – Letter string values showing the words presented as stimuli to the children.\nOrthography – Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.\nConsistency-H – Calculated orthography-to-phonology consistency value for each word. -zConsistency-H – Standardized Consistency H scores\nScore – Outcome variable – for the orthographic post-test, responses were scored as 1 (correct, if the target spelling was produced in full) or 0 (incorrect, if the target spelling was not produced).\n\nThe summary will show you that we have a number of other variables available, including measures of individual differences in reading or reading-related abilities or knowledge, but we do not need to pay attention to them, for our exercises. If you are interested in the dataset, you can find more information about the variables in the Appendix for this chapter (Section 1.14) and, of course, in @ricketts2021."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-data-tidy",
    "href": "PSYC402/Week19.html#sec-glmm-data-tidy",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "The data are already tidy: each column in long.orth_2020-08-11.csv corresponds to a variable and each row corresponds to an observation. However, we need to do a bit of work, before we can run any analyses, to fix the coding of the categorical predictor (or independent) variables, the factors Orthography, Instructions, and Time.\n\n\nBy default, R will dummy code observations at different levels of a factor. So, for a factor or a categorical variable like Orthography (present, absent), R will code one level name e.g. absent as 0 and the other e.g. present as 1. The 0-coded level is termed the reference level, which you could call the baseline level, and by default R will code the level with the name appearing earlier in the alphabet as the reference level.\nAll this is usually not important. When you specify a model in R where you are asking to estimate the effect of a categorical variable like Orthography (present, absent) then, by default, what you will get is an estimate of the average difference in outcome, when all other factors are set to zero, estimated as the difference in outcomes comparing the reference level and the other level or levels of the factor. This will be presented, for example, like the output shown following, for a Generalized Linear Model (i.e., a logistic regression) analysis of the effect of Orthography condition, ignoring the random effects:\n\nsummary(glm(Score ~ Orthography, family = \"binomial\", data = long.orth))\n\n\nCall:\nglm(formula = Score ~ Orthography, family = \"binomial\", data = long.orth)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.8510  -0.8510  -0.6763   1.5436   1.7818  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -1.35879    0.09871 -13.765  &lt; 2e-16 ***\nOrthographypresent  0.52951    0.13124   4.035 5.47e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1431.9  on 1262  degrees of freedom\nResidual deviance: 1415.4  on 1261  degrees of freedom\nAIC: 1419.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nYou can see that you have an estimate, in the summary, of the effect of orthographic condition shown as:\nOrthographypresent  0.52951\nThis model (and default coding) gives us an estimate of how the log odds of a child getting a response correct changes if we compare the responses in the absent condition (here, treated as the baseline or reference level) with responses in the present condition.\n\n\n\n\n\n\nTip\n\n\n\nR tells us about the estimate by adding the name of the factor level that is not the reference level, here, present to the name of the variable Orthography whose effect is being estimated.\n\n\nWe can see that the log odds of a correct response increase by \\(0.52951\\) when the orthography (visual word form or spelling) of a word is present during learning trials.\nHowever, as Dale Barr explains it is better not to use R’s default dummy coding scheme if we are analyzing data where the data come from a study involving two or more factors, and we want to estimate not just the main effects of the factors but also the effect of the interaction between the factors.\nIn our analyses, we want the coding that allows us to get estimates of the main effects of factors, and of the interaction effects, somewhat like what we would get from an ANOVA. This requires us to use effect coding.\nWe can code whether a response was recorded in the absent or present condition using numbers. In dummy coding, for any observation, we would use a column of zeroes or ones to code condition: i.e., absent (0) or present (1). In effect coding, for any observation, we would use a column of ones or minus ones to code condition: i.e., absent (-1) or present (1). (With a factor with more than two levels, we would use more than one column to do the coding: the number of columns we would use would equal the number of factor condition levels minus one.) In effect coding, observations coded -1 are in the reference level.\nWith effect coding, the constant (i.e., the intercept for our model) is equal to the grand mean of all the observed responses. And the coefficient of each of the effect variables is equal to the difference between the mean of the group coded 1 and the grand mean.\nYou can read more about effect coding here or here.\n\n\n\nWe follow recommendations to use sum contrast coding for the experimental factors. Further, to make interpretation easier, we want the coding to work so that for both orthography and presentation conditions, doing something is the “high” level in the factor – hence:\n\nOrthography, absent (-1) vs. present (+1)\nInstructions, incidental (-1) vs. explicit (+1)\nTime, test time 1 (-1) vs. time 2 (+1)\n\nWe use a modified version of the contr.sum() function (provided in the {memisc} library) that allows us to define the base or reference level for the factor manually (see documentation).\n\nlibrary(memisc)\n\n\n\n\n\n\n\nTip\n\n\n\nWe sometimes see that we cannot appear to load library(memisc) and library(tidyverse) at the same time without getting weird warnings.\n\nI would load library(memisc) after I have loaded library(tidyverse)\nand maybe then unload it afterwards: just click on the button next to the package or library name in R-Studio to detach the library (i.e., stop it from being available in the R session).\n\n\n\nIn the following sequence, I first check how R codes the levels of each factor by default, I then change the coding, and check that the change gets me what I want.\nWe want effects coding for the orthography condition factor, with orthography condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Orthography)\n\n        present\nabsent        0\npresent       1\n\n\nYou can see that Orthography condition is initially coded, by default, using dummy coding: absent (0); present (1). We want to change the coding, then check that we have got what we want.\n\ncontrasts(long.orth$Orthography) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Orthography)\n\n         2\nabsent  -1\npresent  1\n\n\nWe want effects coding for the presentation condition factor, with presentation condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Instructions)\n\n           incidental\nexplicit            0\nincidental          1\n\n\nChange it.\n\ncontrasts(long.orth$Instructions) &lt;- contr.sum(2, base = 2)\ncontrasts(long.orth$Instructions)\n\n            1\nexplicit    1\nincidental -1\n\n\nWe want effects coding for the Time factor, with Time coded as -1, +1 Check the coding.\n\ncontrasts(long.orth$Time)\n\n  2\n1 0\n2 1\n\n\nChange it.\n\ncontrasts(long.orth$Time) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Time)\n\n   2\n1 -1\n2  1\n\n\nIn these chunks of code, I use contr.sum(a, base = b) to do the coding, where a is the number of levels in a factor (replace a with the right number), and b tells R which level to use as the baseline or reference level (replace b with the right number). I usually need to check the coding before and after I specify it."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-categorical-outcomes",
    "href": "PSYC402/Week19.html#sec-glmm-categorical-outcomes",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We often need to analyze outcome or dependent variables which comprise observations of responses that are discrete or categorical. We need to learn to recognize research contexts that require GLMMs. Categorical outcome variables can include any of the following:\n\nThe accuracy of each of a series of responses in some task, e.g., whether a recorded response is correct or incorrect.\nThe location of a recorded eye movement, e.g., a fixation to the left or to the right visual field.\nThe membership of one group out of two possible groups, e.g., is a participant impaired or unimpaired?\nThe membership of one group out of multiple possible groups, e.g., is a participant a member of one out of some number of groups, say, a member of a religious or ethnic group?\n\nResponses that can be coded in terms of ordered categories, e.g., a response on a (Likert) ratings scale.\nOutcomes like the frequency of occurrence of an event, e.g., how many arrests are made at a particular city location.\n\nIn this chapter, we will analyze accuracy data: where the outcome variable consists of responses observed in a behavioural task, the accuracy of responses was recorded, and responses could either be correct or incorrect. The accuracy of a response is, here, coded under a binary or dichotomous classification though we can imagine situations when a response is coded in multiple different ways.\nThose interested in analyzing outcome data from ratings scales, that is, ordered categorical outcome variables, often called ordinal data, may wish to read about ordinal regression analyses, which you can do in R using functions from the {ordinal} library.\nThose interested in analyzing outcome data composed of counts may wish to read about poisson regression analyses in @Gelman2007ga.\nIt will be apparent in our discussion that researchers have used, and will continue to use, a number of ‘traditional’ methods to analyze categorical outcome variables when really they should be using GLMMs. We will talk about these alternatives, next, so that you recognize what is being done when you read research articles. Critically, we will discuss the limitations of such methods because these limitations explain why we bother to learn about GLMMs.\n\n\nIf you want to analyze data from a study where responses can be either correct or incorrect but not both (and not anything else), then your outcome variable is categorical, and your analysis approach ought to respect that. However, if you read enough psychological research articles then you will see many reports of data analyses in which the researchers collected data on the accuracy of responses but then present the results of analyses that ignored the binary or dichotomous nature of accuracy. We often see response accuracy analyzed using an approach that looks something like the following:\n\nThe accuracy of responses (correct vs. incorrect) is counted, e.g., as the number of correct responses or the number of errors.\nThe percentage, or the proportion, of responses that are correct or incorrect is calculated, for each participant, for each level of each experimental condition or factor.\nThe percentage or proportion values are then entered as the outcome or dependent variable in ANOVA or t-test or linear model (multiple regression) analyses of response accuracy.\n\nYou will see many reports of ANOVA or t-test or linear model analyses of accuracy.\n\n\n\n\n\n\nTip\n\n\n\nWhy can’t we follow these examples, and save ourselves the effort of learning how to use GLMMs?\n\nThe reason is that these analyses are, at best, approximations to more appropriate methods.\nTheir results can be expected to be questionable, or misleading, for reasons that we discuss next.\n\n\n\n\n\nTo illustrate the problems associated with using traditional analysis methods (like ANOVA or multiple regression), when working with accuracy as an outcome, we start by looking at data from an artificial vocabulary learning study [reported by @monaghan2015]. @monaghan2015 recorded responses made by participants to stimuli in a test where the response was correct (coded 1) or incorrect (coded 0). In our study, we directly compared learning of noun-object pairings, verb-motion pairings, and learning of both noun and verb pairings simultaneously, using a cross-situational learning task. (Those interested in this dataset can read more about it at the online repository associated with this chapter.) The data will have a multilevel structure because you will have multiple responses recorded for each person, and for each stimulus. But what concerns us is that if you attempt to use a linear model to analyze the effects of the experimental variables then you will see some paradoxical results that are easily demonstrated.\nLet’s imagine that we wish to estimate the effects of experimental variables like learning condition: learning trial block (1-12); or vocabulary condition (noun-only, noun-verb, verb-only). We can calculate the proportion of responses correct made by each person for each condition and learning trial block. We can then plot the regression best fit lines indicating how proportion of responses correct varies by person and condition. Figure 1 shows the results.\nLook at where the best fit lines go.\n\n\n\n\n\nFigure 1: Monaghan et al. (2015) artificial word learning study: plot showing the proportion of responses correct for each participant, in each of 12 blocks of 24 learning trials, in each learning condition; each grey line shows the linear model prediction of the proportion correct, for each person, by learning block, in each condition; black lines show the average prediction of the proportion correct, by learning block, in each condition. The position of points has been jittered.\n\n\n\n\nFigure 1 shows how variation in the outcome, here, the proportion of responses that are correct, is bounded between the y-axis limits of 0 and 1 while the best fit lines exceed those limits. Clearly, if you consider the accuracy of a person’s responses in any set of trials, for any condition in an experiment, the proportion of responses that can be correct can vary only between 0 (no responses are correct) and 1 (all responses are correct). There is no inbuilt or intrinsic limits to the proportion of responses that a linear model can predict would be correct. According to linear model predictions, if you follow the best fit lines in Figure 1 then there are conditions, or there are participants, in which the proportion of a person’s responses that could be correct will be greater than 1. That is impossible.\n\n\n\nThe other fundamental problem with using analysis approaches like ANOVA or regression to analyze categorical outcomes like accuracy is that we cannot assume that the variance in accuracy of responses will be homogenous across different experimental conditions.\nThe logic of the problem can be set out as follows:\n\nGiven a binary outcome, e.g., where the response is correct or incorrect, for every trial, there is a probability \\(p\\) that the response is correct.\nThe variance of the proportion of trials (per condition) with correct responses is dependent on \\(p\\), and it is greater when \\(p \\sim .5\\), the probability that a response will be correct.\n\n@jaeger2008 (p. 3) then explains the problem like this. If the probability of a binomially distributed outcome like response accuracy differs between two conditions (call them conditions 1 and 2), the variances will only be identical if \\(p1\\) (the proportion of correct responses in condition 1) and p2 (the proportion of correct responses in condition 1) are equally distant from 0.5 (e.g. \\(p1 = .4\\) and \\(p2 = .6\\)). The bigger the difference in distance from 0.5, comparing the conditions, the less similar the variances will be.\nSample proportions between 0.3 and 0.7 are considered close enough to 0.5 to assume homogeneous variances (Agresti, 2002). Unfortunately, we usually cannot determine a priori the range of sample proportions in our experiment.\nIn general, variances in two binomially distributed conditions will not be homogeneous but, as you will recall, in both ANOVA and regression analysis, we assume homogeneity of variance in the outcome variable when we compare the effect of differences (in the mean outcome) between the different levels of a factor. This means that if we design a study in which the outcome variable is the accuracy of responses in different experimental conditions and we plan to use ANOVA or regression to estimate the effect of variation in experimental conditions on response accuracy then unless we get lucky our estimation of the experimental effect will take place under circumstances in which the application of the analysis method (ANOVA or regression) and thus the analysis results will be invalid.\n\n\n\nThe application of traditional (parametric) analysis methods like ANOVA or regression to categorical outcome variables like accuracy is very common in the psychological literature. The problem is that these approaches can give us misleading results.\n\nLinear models assume outcomes are unbounded so allow predictions that are impossible when outcomes are, in fact, bounded as is the case for accuracy or other categorical variables.\nLinear models assume homogeneity of variance but that is unlikely and anyway cannot be predicted in advance when outcomes are categorical variables.\nIf we are interested in the effect of an interaction between two effects, using ANOVA or linear models on accuracy (proportions of responses correct) can tell you, wrongly, that the interaction is significant.\n\nTraditionally, researchers have recognized the limitations attached to using methods like ANOVA or regression to analyze categorical outcomes like accuracy and have applied remedies, transforming the outcome variables, e.g. the arcsine root transformation, to render them ‘more normal’. However, as @jaeger2008 demonstrates, the remedies like the arcsine transformation that have traditionally been applied are often not likely to succeed. @jaeger2008 completed a comparison of the results of analyses of accuracy data, where outcomes are raw values for the proportions of correct responses, or arcsine transformed values for proportions correct. His comparison demonstrated that the traditional techniques will show either that effects are significant when they are not or that effects are not significant when they are.\n\n\n\n\nWhat we need, then, is a method that allows us to analyze categorical outcomes. We find the appropriate method in Generalized Linear Models, and in Generalized Linear Mixed-effects Models for repeated measures or multilevel structured data. We can understand these methods, as their name suggests, as generalizations of linear models or linear mixed-effects models: generalizations that allow for the categorical nature of some outcome data.\nYou can understand how Generalized Linear Mixed-effects Models work by seeing them as analyses of categorical outcome data like accuracy where the outcome variable is transformed, as I explain next (see Baguley, 2012, for a nice clear explanation, which I summarize here).\nOur problems begin with the need to estimate effects on a bounded outcome like accuracy with a linear model which, as we have seen, will yield unbounded predictions.\nThe logistic transformation takes \\(p\\) the probability of an event with two possible outcomes, and turns it into a logit: the natural logarithm of the odds of the event. The effect of this transformation is to turn a discrete binary bounded outcome into a continuous unbounded outcome.\n\nTransforming a probability to odds \\(o = \\frac{p}{1-p}\\) is a partial solution.\n\n\nOdds are, for example, the ratio of the probability of the occurrence of an event compared to the probability of the non-occurrence of an event, or, in terms of a response accuracy variable, the ratio of the probability of the response being correct compared to the probability of the response being incorrect.\nAnd odds are continuous numeric quantities that are scaled from zero to infinity.\nYou can see how this works if you run the calculations using the equation \\(o = \\frac{p}{1-p}\\) in R as odds &lt;- p/(1-p): replacing p with various numbers (e.g. p = 0.1, 0.01, 0.001).\n\n\nWe can then use the (natural) logarithm of the odds \\(logit = ln\\frac{p}{1-p}\\) because using the logarithm removes the boundary at zero because log odds range from negative to positive infinity.\n\n\nYou can see how this works if you run the calculations using the equation \\(logit = ln\\frac{p}{1-p}\\) in R as logit &lt;- log(p/(1-p)): replacing p with smaller and smaller numbers (e.g. p = 0.1, 0.01, 0.001) gets you increasing negative log odds.\n\nWhen we model the log odds (logit) that a response will be correct, the model is called a logistic regression or logistic model. We can think of logistic models as working like linear models with log-odds outcomes.\n\\[\nln\\frac{p}{1-p} = logitp = \\beta_0 + \\beta_1X_1 \\dots\n\\]\nWe can describe the predicted log odds of a response of one type as the linear sum of the estimated effects of the included predictor variables. In a logistic regression, the predictors have an additive relationship with respect to the log odds outcome, just like in an ordinary linear model. The log odds range from negative to positive infinity; logit of 0 corresponds to proportion of .5.\nBaguley (2012) notes that is it advantageous that odds and probabilities are both directly interpretable. We are used to seeing and thinking in everyday life about the chances that some event will occur."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-working-models",
    "href": "PSYC402/Week19.html#sec-glmm-working-models",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "A small change in R lmer code allows us to extend what we know about linear mixed-effects models to conduct Generalized Linear Mixed-effects Models. We change the function call from lmer() to glmer(). However, we have to make some other changes, as we detail in the following sections.\nWe will be examining the impact of the experimental effects, that is, the fixed effects associated with the impacts on the outcome Score (accuracy of response in the word spelling test) associated with the following comparisons:\n\nTime: time 1 versus time 2\nOrthography: present versus absent conditions\nInstructions: explicit versus incidental conditions\nStandardized spelling-sound consistency\nInteraction between the effects of Orthography and Instructions\nInteraction between the effects of Orthography and consistency\n\nWe will begin by keeping the random effects structure simple.\n\n\nIn our first model, we will specify just random effects of participants and items on intercepts.\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               Time + Orthography + Instructions + zConsistency_H + \n                               \n                               Orthography:Instructions +\n                               \n                               Orthography:zConsistency_H +\n                               \n                               (1 | Participant) + \n                               \n                               (1 | Word),\n                             \n                             family = \"binomial\", \n                             glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                             data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nThe code works as follows.\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\nglmer() the function name changes because now we want a generalized linear mixed-effects model of accuracy.\n\nThe model specification includes information about fixed effects and about random effects.\n\nWith (1 | Participant) we include random effects of participants on on intercepts.\nWith (1 | Word) we include random effects of stimulus on on intercepts.\n\nSecond, we have the bit that is specific to generalized models.\n\nfamily = \"binomial\" is entered because accuracy is a binary outcome variable (correct, incorrect) so we assume a binomial probability distribution.\n\nWe then specify:\n\nglmerControl(optimizer=\"bobyqa\", ...) to change the underlying mathematical engine (the optimizer) to cope with greater model complexity\n\nand we allow the model fitting functions to take longer to find estimates with optCtrl=list(maxfun=2e5).\n\nNotice how we specify the fixed effects. We want glmer() to estimate “main effects and interactions” that we hypothesized.\nWe specify the main effects with:\n\nTime + Orthography + Instructions + zConsistency_H +\n\nWe specify the interaction effects with:\n\nOrthography:Instructions +\n                               \nOrthography:zConsistency_H +\n\nWhere we ask for estimates of the fixed effects associated with:\n\nOrthography:Instructions the interaction between the effects of Orthography and Instructions;\nOrthography:zConsistency-H the interaction between the effects of Orthography and consistency.\n\n\n\nThere are two forms of notation we can use to specify interactions in R. The simplest form is to use something like this:\n\nOrthography*Instructions\n\nThis will get you estimates of:\n\nThe effect of Orthography: present versus absent conditions.\nThe effect of Instructions: explicit versus incidental conditions.\nAnd the effect of Orthography x Instructions: the interaction between the effects of Orthography and Instructions.\n\nSo, in general, if you want estimates of the effects of variables A, B and the interaction A x B, then you write A*B.\nWe can also use the colon symbol to specify only the interaction, i.e., ignoring main effects, so if you specify A:B then you will get an estimate of the interaction A x B but not the effects A, B.\nWith the coding:\n\nScore ~ Orthography + Instructions + Orthography:Instructions\n\nI would be making explicit that I want estimates for the effects of Orthography, Instruction and the interaction between the effects of Orthography and Instructions.\n\n\n\n\nIf you run the model code, you will get the results shown in the output.\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (1 |  \n    Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n     AIC      BIC   logLik deviance df.resid \n  1040.4   1086.7   -511.2   1022.4     1254 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0994 -0.4083 -0.2018  0.2019  7.4940 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 1.840    1.357   \n Word        (Intercept) 2.224    1.491   \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.878462   0.443957  -4.231 2.32e-05 ***\nTime2                        0.050136   0.083325   0.602    0.547    \nOrthography2                 0.455009   0.086813   5.241 1.59e-07 ***\nInstructions1                0.042289   0.230336   0.184    0.854    \nzConsistency_H              -0.618093   0.384005  -1.610    0.107    \nOrthography2:Instructions1   0.005786   0.083187   0.070    0.945    \nOrthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.008                                   \nOrthogrphy2 -0.059  0.008                            \nInstructns1  0.014  0.025  0.001                     \nzCnsstncy_H  0.016 -0.002 -0.029 -0.001              \nOrthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       \nOrthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007\n\n\nThe output from the model summary first gives us information about the model.\n\nFirst, we see information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula including main effects Score ~ Time + Orthography + Instructions + zConsistencyH,\nAs well as the interactions Orthography:Instructions + Orthography:zConsistencyH,\nAnd the random effects (1 | Participant) + (1 |Word).\nThen we see information about the model algorithm\nThen we see model fit statistics, including AIC BIC logLik\nThen we see information about the distribution of residuals\n\nWe then see information listed under Random Effects: this is where you can see information about the error variance terms estimated by the model.\n\nThe information is listed in four columns: 1. Groups; 2. Name; 3. Variance; and 4. Std.Dev.\n\nWe see in the Random Effects section, the variances associated with:\n\nThe random differences between the average intercept (over all data) and the intercept for each participant;\nThe random differences between the average intercept (over all data) and the intercept for responses to each word stimulus.\n\n\nLastly, just as for linear models, we see estimates of the coefficients of the fixed effects, the intercept and the slopes of the experimental variables.\n\nNote that we get p-values (Pr(&gt;|z|)) for what are called Wald null hypothesis significance tests on the coefficients.\nWe can see that one effect is significant. The estimate for the effect of the presence compared to the absence of Orthography is 0.455009. The positive coefficient tells us that the log odds that a response will be correct is higher when Orthography is present compared to when it is absent.\nWe can also see an effect of consistency that can be considered to be marginal or near-significant by some standards. The estimate for the effect of zConsistency_H is -0.618093 indicating that the log odds of a response being correct decrease for unit increase in the standardized H consistency measure.\n\n\nAs we discussed in the last chapter, we can conduct null hypothesis significance tests by comparing models that differ in the presence or absence of a fixed effect or a random effect, using the Likelihood Ratio Test. In the results output for a GLMM by the glmer() function, you can see that alongside the estimates of the coefficients (and standard error) for the fixed effects we also have z and p-values. Wald z tests for GLMMs test the null hypothesis of no effect by comparing the effect estimate with their standard error, and comparing the resulting test statistic to zero [@bolker2009].\n\n\n\n\nWe usually want to do more than just report whether experimental effects are or are not significant. It helps us to present and interpret the estimates from a model if we can visualize the model prediction. There are a variety of tools that help us to do this.\n\n\nWe can use the plot_model function from the {sjPlot} library. The following sequence of code takes information from the model we have just run, then generates model predictions, of change in the probablity of a correct response (Score) for different levels of the Orthography factor and the consistency variable. I chose these variables because they are the significant or near-significant effects.\n\nporth &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"Orthography\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\npzconsH &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"zConsistency_H\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\ngrid.arrange(porth, pzconsH,\n            ncol=2)\n\n\n\n\nFigure 2: Effect of orthography condition (present versus absent) on probability of a response being correct\n\n\n\n\nThe plots in Figure 2 show clearly how the probability of a correct response is greater for the conditions where Orthography had been present (versus absent) during the word learning phase of the study. We can also see a trend such that the probability of a response being correct decreases as the (in-)consistency of a target word tends to increase.\nTo produce this plot, you will need to install the {sjPlot} library first, and then run the command library(sjPlot) before creating your plot.\nNotice:\n\nplot_model() produces the plot\nplot_modellong.orth.min.glmer) specifies that the plot should be produced given information about the previously fitted model long.orth.min.glmer\ntype = \"pred\" tells R that you want a plot showing the model predictions, of the effect of, e.g., `Orthography: condition\n\nThe function outputs an object whose appearance can be edited as a ggplot object.\nMore information can be found here or here about the {sjPlot} library and plotting model estimates of effects."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-random-effects",
    "href": "PSYC402/Week19.html#sec-glmm-random-effects",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "So far, we have been considering the results of a random intercepts model in which we take into account the random effects of participants and stimulus word differences on intercepts. We have ignored the possibility that the slopes of the experimental variables might vary between participants or between words.\n\n\n\n\n\n\nTip\n\n\n\nWe now need to examine the question: What random effects should we specify?\n\n\nI must warn you that the question and the answer are complex but the coding is quite simple, and the approaches you can take to address the question are, now, quite well recognized by the psychological community. In other words, the community recognizes the nature of the problem, and recognizes the methods you can potentially follow to solve the problem.\nThe complexity, for us, lies not in the mathematics: the coding is simple and the glmer() function does the work.\nI think the complexity lies, firstly, in how we have to think about the study design, what gets manipulated or allowed to vary. I find it very helpful to sketch out, by hand, what the study design means in relation to who does what in an experiment.\nThe complexity lies, secondly, and in how we have to translate our understanding of the study design to a specification of random effects. We can master that aspect of the challenge through practice.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe question is:\n\nSo, what random effects should we include?\n\n\n\nIf you go back to the description of the study design, then you will be able to see that a number of possibilities follow, theoretically, from the design.\nA currently influential set of recommendations [@Barr2013a; but see discussions in @bates2015parsimonious,@eager and @matuschek2017] has been labeled Keep it maximal.\nIn this approach, if you are testing effects manipulated according to a pre-specified design then you should:\n\nTest random intercepts – due to random differences between subjects or between items (or other sample grouping variables);\nTest random slopes for all within-subjects or within-items (or other) fixed effects.\n\nThis means that specification of the random effects structure requires two sets of information – corresponding to the answers to two questions:\n\nWhat are the fixed effects?\nWhat are the grouping variables?\n\nAs we have seen, we can rephrase the second question in terms: did you test multiple participants using multiple stimuli (e.g., words …) or did you test participants under multiple different conditions (e.g., levels of experimental condition factors)?\nOur answers to these questions then dictate potentially how we specify the random effects structure for our model.\nWe can consider that we should certainly include a random effect of each grouping variable (e.g., participants, stimulus word) on intercepts. We can reflect that we should also include a random effect of a grouping variable e.g. participant on the slope of each variable that was manipulated within the units of that variable.\nWhen we specify models, we should remember that by default if you specify (1 + something | participant) then you are specifying that you want to take into account:\n\nvariance due to the random differences between participants in intercepts (1 ...| participant),\nplus variance due to the random differences between participants in slopes (... something | participant),\nplus the covariance (or correlation) between the random intercepts and the random slopes.\n\nYou will have become familiar with the practice of referring to effects as within-subjects or between-subjects previously, in working with ANOVA. Here, whether an effect is within-subjects or within-items or not has relevance to whether we can or should specify a random effect of subjects or of items on the slope of a fixed effect.\nIn deciding what random effects we should specify, we need to think about what response data we have recorded, for each of the experimental variables, given the study design. This is because if we want to specify a random effect of participants (or stimulus words) on the slope of an experimental condition then we need to have data, for each person, on their responses under all levels of the condition.\nIf we want to estimate the effect of the experimental manipulation of learning condition, for example, the impact of the presence of orthography, for a person, we need to have data for both levels of the condition (orthography absent and orthography present) for that person. If you think about it, we cannot estimate the slope of the effect of the presence of orthography without response data recorded under both the orthography absent condition and the orthography present condition. If we can estimate the slope of the effect then we can estimate how the slope of the effect deviates between participants.\nWe can spell out how the experimental conditions were manipulated for the example study, as follows. (Writing out this kind of account, for yourself, will be helpful perhaps when you are planning an analysis and have to work out what the random effects could be.)\n\nThe effect of Orthography was manipulated within participants and within stimulus words. This is because the presence of orthography (orthography absent versus orthography present) was manipulated so that we have data about test responses to each word under both Orthography conditions, and data about responses from each child under both conditions.\nThe effect of Instructions was manipulated between participants. This is because Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\nWe can say that the effects of Orthography and of Instructions are both manipulated within words. Items were counterbalanced across instruction and orthography conditions, with all words appearing in both orthography conditions for approximately the same number of children within the explicit and incidental groups.\nThe effect of spelling-sound consistency varies between words because different words have different consistency values but the effect of consistency varies within} participants because we have response data for each participant for their responses to words of different levels of consistency.\nWe recorded responses for all participants and all words so we can say that the effect of Time (test time 1 versus time 2) can also be understood to vary within} both participants and words. This means that, for each person’s response to each word on which they are tested, we have response data recorded at both test times.\n\nThese considerations suggests that we should specify a model with the random effects:\n\nThe random effects of participants on intercepts, and on the slopes of the effects of Time, Orthography and spelling-sound consistency, as well as all corresponding covariances.\nThe random effects of stimulus words on intercepts, and on the slopes of the effects of Time, Orthography and Instructions, as well as all corresponding covariances.\n\nThis is simple to do using the code shown following.\n\nlong.orth.max.glmer &lt;- glmer(Score ~ \n                           Time + Orthography + Instructions + zConsistency_H + \n                           \n                           Orthography:Instructions +\n                           \n                           Orthography:zConsistency_H +\n                           \n                           (Time + Orthography + zConsistency_H + 1 | Participant) + \n                           \n                           (Time + Orthography + Instructions + 1 |Word),\n                         \n                         family = \"binomial\",\n                         glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                         \n                         data = long.orth)\n\nsummary(long.orth.max.glmer)\n\nWhere we have the critical terms:\n\n(Time + Orthography + zConsistency_H + 1 | Participant) to account for the random effects of participants on intercepts, and on the slopes of the effects of Time, Orthography and spelling-sound consistency, as well as all corresponding covariances.\n(Time + Orthography + Instructions + 1 |Word) to account for the random effects of stimulus words on intercepts, and on the slopes of the effects of Time, Orthography and Instructions, as well as all corresponding covariances.\n\nIf you run this code, however, you will see that you get warnings along with your estimates.\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (Time +  \n    Orthography + zConsistency_H + 1 | Participant) + (Time +  \n    Orthography + Instructions + 1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n     AIC      BIC   logLik deviance df.resid \n  1053.6   1192.4   -499.8    999.6     1236 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1014 -0.4027 -0.1723  0.2037  7.0331 \n\nRandom effects:\n Groups      Name           Variance Std.Dev. Corr             \n Participant (Intercept)    2.043126 1.42938                   \n             Time2          0.005675 0.07533   0.62            \n             Orthography2   0.079980 0.28281   0.78 -0.01      \n             zConsistency_H 0.065576 0.25608   0.49  0.99 -0.16\n Word        (Intercept)    2.793447 1.67136                   \n             Time2          0.046736 0.21618   0.14            \n             Orthography2   0.093740 0.30617  -0.68 -0.81      \n             Instructions1  0.212706 0.46120  -0.74 -0.05  0.38\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -2.10099    0.49588  -4.237 2.27e-05 ***\nTime2                        0.02077    0.12285   0.169 0.865741    \nOrthography2                 0.52480    0.15496   3.387 0.000708 ***\nInstructions1                0.24467    0.27281   0.897 0.369801    \nzConsistency_H              -0.67818    0.36310  -1.868 0.061802 .  \nOrthography2:Instructions1  -0.05133    0.10004  -0.513 0.607908    \nOrthography2:zConsistency_H  0.05850    0.11634   0.503 0.615064    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.097                                   \nOrthogrphy2 -0.280 -0.187                            \nInstructns1 -0.278  0.023  0.109                     \nzCnsstncy_H  0.062  0.005 -0.064  0.120              \nOrthgrp2:I1  0.024  0.005 -0.071  0.212 -0.065       \nOrthgr2:C_H -0.062  0.049  0.246 -0.031 -0.440  0.001\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nNotice, especially, the warning:\n\nboundary (singular) fit: see ?isSingular\n\nIf you put the warning message text into a search engine, then you will get directed to a variety of discussions about what they mean and what you should do about them.\nA highly instructive blog post by (I think) Ben Bolker provides some very useful advice here.\n\nCheck singularity\nIf the fit is singular or near-singular, there might be a higher chance of a false positive (we’re not necessarily screening out gradient and Hessian checking on singular directions properly); a higher chance that the model has actually misconverged (because the optimization problem is difficult on the boundary); and a reasonable argument that the random effects model should be simplified.\nThe definition of singularity is that some of the constrained parameters of the random effects theta parameters are on the boundary (equal to zero, or very very close to zero …)\n\n(Emphasis added.)\nI am going to take his advice and simplify the random effects part of the model. We know that the random intercepts model converges fine and now we know that the maximal model does not. Thus, our task is now to identify a model that includes random effects of participants or items on slopes and still converges without warnings.\n\n\n\nI am just going to assume we need both random effects of subjects and of items on intercepts so I focus on random slopes here. (This assumption may not always be true but is often useful.)\nWe can fit a series of models as follows. Note that I will not show the results for every model, to save space, but you should run the code to see what happens. Look out for convergence or singularity warnings, where they appear.\n\n\nIn the first model, we have just random effects of participants or items on intercepts. This is where we started.\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               Time + Orthography + Instructions + zConsistency_H + \n                               \n                               Orthography:Instructions +\n                               \n                               Orthography:zConsistency_H +\n                               \n                               (1 | Participant) + \n                               \n                               (1 | Word),\n                             \n                             family = \"binomial\", \n                             glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                             data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nWe saw that the model converged, and we looked at the results previously.\nWhat next? A simple approach we can take is to see if we can add each fixed effect to be included in our random effects terms, one effect at a time.\n\n\n\nIn our second model, we change the random effects terms so that we can account for the random effects of participants and of items on intercepts as well as on the slopes of the Orthography effect. (The Orthography effect is both within-subjects and within-items.)\n\nlong.orth.2.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                             (dummy(Orthography) + 1 || Participant) + \n                             \n                             (dummy(Orthography) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nsummary(long.orth.2.glmer)\n\nThis model converges without warnings.\nNotice that we specify that we || do not want random covariances; we are keeping things simple in each step.\nNote the use of dummy() inside the random effects terms. The ‘dummy’ is a mis-leading name; we are not talking about dummy coding (as above). Here, the dummy() stops R from mis-interpreting the requirement to estimate the effect of the differences between category levels, within random effects.\nThe reason is explained here.\nYou can see what impact it has by specifying, instead, the naked random effect:\n(Orthography + 1 || Participant).\n\n\n\nNext we can add Instructions to take into account random differences between words in the slope of this effect. We show the results for this model as they are instructive.\n\nlong.orth.3.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                             (dummy(Orthography) + 1 || Participant) + \n                             \n                             (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nsummary(long.orth.3.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n     AIC      BIC   logLik deviance df.resid \n  1036.5   1098.2   -506.2   1012.5     1251 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9951 -0.4068 -0.1920  0.1838  5.9308 \n\nRandom effects:\n Groups        Name                Variance Std.Dev.\n Participant   (Intercept)         1.64393  1.2822  \n Participant.1 dummy(Orthography)  0.55604  0.7457  \n Word          (Intercept)         1.94313  1.3940  \n Word.1        dummy(Orthography)  0.01608  0.1268  \n Word.2        dummy(Instructions) 0.86694  0.9311  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621258  0.4400570  -4.459 8.24e-06 ***\nTime2                        0.0507347  0.0843236   0.602  0.54740    \nOrthography2                 0.4263703  0.1114243   3.827  0.00013 ***\nInstructions1                0.1907424  0.2632579   0.725  0.46873    \nzConsistency_H              -0.6270892  0.3669873  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265727  0.1048392  -0.253  0.79991    \nOrthography2:zConsistency_H -0.0006303  0.0878824  -0.007  0.99428    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\n\n\nThis model also converges without warnings.\nTake a look at the random effects summary. You can see:\n\nParticipant (Intercept) 1.64393 estimated variance due to random effect of participants on intercepts\nParticipant.1 dummy(Orthography) 0.55604 variance due to random effect of participants on the slope of the Orthography effect\nWord (Intercept) 1.94314 variance due to random effect of words on intercepts\nWord.1 dummy(Orthography) 0.01607 variance due to random effect of words on the slope of the Orthography effect\nWord.2 dummy(Instructions) 0.86694 variance due to random effect of words on the slope of the Instructions effect\n\nWe do not see correlations (random effects covariances) because we use the || notation to stop them being estimated. We want to stop them being estimated because we want to see what we gain from adding just the requirement, first, to estimate the variance associated with random effects of participants or words on the slopes of the experimental variables.\nAlso, we can suspect that adding the requirement to estimate covariances will blow the model up for two reasons. The maximal model, including random slopes variances and covariances clearly did not converge. Secondly, at least two of the correlations listed in the random effects, for the maximal model, were pretty extreme with Corr \\(=-0.01\\) and \\(=0.99\\); such extreme values (\\(r \\sim \\pm 1\\)) are bad signs; see the discussion in Section 1.10.2.\n\n\n\nIn the following models, because we can see that we can get a model to converge with random effects of participants or items on Orthography, and random effect of participants on Instructions, I am going to keep these random effects in the model. I will check if adding further effects is OK too, in terms of successful convergence. I am going to treat all the following models as variations on a theme, the theme being: can we add anything else to:\n\n                             (dummy(Orthography) + 1 || Participant) + \n                             \n                             (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n\n\n\n\nNext we see if we can add zConsistency_H.\n\nlong.orth.4.a.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                             (dummy(Orthography) + zConsistency_H + 1 || Participant) + \n                             \n                             (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.a.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    zConsistency_H + 1 || Participant) + (dummy(Orthography) +  \n    dummy(Instructions) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n     AIC      BIC   logLik deviance df.resid \n  1038.5   1105.3   -506.2   1012.5     1250 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.282e+00\n Participant.1 dummy(Orthography)  5.560e-01 7.456e-01\n Participant.2 zConsistency_H      1.259e-10 1.122e-05\n Word          (Intercept)         1.943e+00 1.394e+00\n Word.1        dummy(Orthography)  1.604e-02 1.266e-01\n Word.2        dummy(Instructions) 8.669e-01 9.311e-01\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621298  0.4400610  -4.459 8.24e-06 ***\nTime2                        0.0507346  0.0843237   0.602  0.54740    \nOrthography2                 0.4263731  0.1114190   3.827  0.00013 ***\nInstructions1                0.1907320  0.2632610   0.724  0.46876    \nzConsistency_H              -0.6270931  0.3669852  -1.709  0.08749 .  \nOrthography2:Instructions1  -0.0265706  0.1048369  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006353  0.0878784  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nWe see two useful pieces of information when we run this model:\n\nWe get the warning boundary (singular) fit: see ?isSingular that tells us the model algorithm could not converge on effects estimates, given the model we specify, given the data.\nNote, also, we see the random effects variance estimate Participant.2 zConsistency_H 1.259e-10.\n\nThese two things are possibly connected: the singularity warning; and the estimated variance of \\(1.259e-10\\) (i.e. a very very small number) associated with the random effect of participants on the slope of the zConsistency_H. We can expect that the model fitting algorithm is going to have difficulty estimating nothing, or something close to nothing: here, the very very small variance associated with the between-participant differences in the slope of the non-significant effect of word spelling-sound consistency on response accuracy.\n\n\n\nWhat about the random effects of participants or of words on the slope of the effect of Time?\n\nlong.orth.4.b.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                             (dummy(Orthography) + dummy(Time) + 1 || Participant) + \n                             \n                             (dummy(Orthography) + dummy(Instructions) + dummy(Time) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.b.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    dummy(Time) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    dummy(Time) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n     AIC      BIC   logLik deviance df.resid \n  1040.5   1112.5   -506.2   1012.5     1249 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.2821915\n Participant.1 dummy(Orthography)  5.560e-01 0.7456312\n Participant.2 dummy(Time)         0.000e+00 0.0000000\n Word          (Intercept)         1.943e+00 1.3939636\n Word.1        dummy(Orthography)  1.604e-02 0.1266478\n Word.2        dummy(Instructions) 8.669e-01 0.9310620\n Word.3        dummy(Time)         2.161e-07 0.0004648\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621302  0.4400578  -4.459 8.24e-06 ***\nTime2                        0.0507347  0.0843253   0.602  0.54740    \nOrthography2                 0.4263731  0.1114192   3.827  0.00013 ***\nInstructions1                0.1907329  0.2632605   0.725  0.46876    \nzConsistency_H              -0.6270910  0.3669905  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006351  0.0878784  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nNope.\nWe again see two useful pieces of information when we run this model:\n\nWe get the warning boundary (singular) fit: see ?isSingular that tells us the model algorithm could not converge on effects estimates, given the model we specify, given the data\nNote, also, we see the random effects variance estimate Participant.2 dummy(Time) 0.000e+00.\nAnd we see the variance estimate Word.3 dummy(Time) 2.161e-07.\n\nAgain, we can surmise that a mixed-effects model will get into trouble – and we will see convergence warnings – where we have a fixed effect with little impact (like, here, Time) and we are asking for the estimation of variance associated with random differences in slopes where there may be, in fact, little random variation. Possibly, these two things are connected too: we are perhaps unlikely to see random differences in the slope of the effect of an experimental variable if the effect is at or near zero. Possibly, we may see the effect of an experimental variable which is very very consistent. Think back to Week 16 and the effect associated with the relation between maths and physics scores where there seemed to be little variation between classes in the slope representing the relation.\n\n\n\n\nWe can see that a model has difficulty if we see things like:\n\nConvergence warnings, obviously\nVery very small random effects variances\nExtreme random effects correlations of \\(\\pm 1.00\\)\n\nIf we see a warning that the model fitting algorithm nearly failed to converge: boundary (singular) fit: see ?isSingular or failed to converge then this tells us that, given the data, the mathematical engine (optimizer) underlying the lmer() function got into trouble because, in short, it was trying to find estimates for effects that were close to not being there at all.\nIf the variances for the random effects of participants or stimulus items on the slopes of an experimental variable are very small this suggests that the level of complexity in the model cannot really be justified or that the model will have difficulty estimating it. Extreme correlations (near 0 or 1) between random effects on intercepts and on slopes of fixed effects suggest the level of complexity in the model cannot really be justified (see also the discussion in Bates et al., 2015; Matuschek et al., 2017).\n\n\n\nThere is no point comparing the models that do not converge, so we focus on those that do.\nDoes the addition of random slopes improve model fit? We can compare the model in pairs, as follows, to test whether each addition in model complexity improves model fit. We run the code for the model comparisons as follows.\nFirst, we compare the model long.orth.min.glmer (just random intercepts) with long.orth.2.glmer to check if increasing model complexity, by accounting for random differences between participants or words in the slope of the Orthography effect improves model fit to data.\n\nanova(long.orth.min.glmer, long.orth.2.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.2.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + 1 || Word)\n                    npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nlong.orth.min.glmer    9 1040.4 1086.7 -511.20   1022.4                     \nlong.orth.2.glmer     11 1041.0 1097.6 -509.51   1019.0 3.3909  2     0.1835\n\n\nSecond, we compare the model long.orth.min.glmer (just random intercepts) with long.orth.3.glmer to check if increasing model complexity, by accounting for random differences between participants or words in the slope of the Instructions effect improves model fit to data.\n\nanova(long.orth.min.glmer, long.orth.3.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.3.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) + 1 || Word)\n                    npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nlong.orth.min.glmer    9 1040.4 1086.7 -511.20   1022.4                       \nlong.orth.3.glmer     12 1036.5 1098.2 -506.24   1012.5 9.9115  3    0.01933 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI think this justifies reporting the model long.orth.3.glmer.\nIf you look at the outputs, the addition of the random effects of participants and words on the slope of the effect of Orthography cannot be justified (\\(\\chi^2 = 3.3909, 2 df, p = .1835\\)) by improved model fit, in comparison to a model with the random effects of participants and words on intercepts.\nThe model comparison summary indicates that the addition of a random effect of words on the slope of the Instructions effect is justified by significantly improved model fit to data (\\(\\chi^2 = 9.9115, 3 df, p = 0.01933\\)).\nNotice I take a bit of a short-cut here, by adding both random effects for Orthography and Instructions. Logically, if adding the random effect for Orthography does not improve model fit but adding the random effects for both random effects for Orthography and Instructions does then it is the addition of Instructions that is doing the work. However, I should prefer to see as complex a random effects structure as possible, provided a model converges.\nWhile adding the random effect of Orthography does not improve model fit significantly, you see researchers allowing a generous p-value threshold for inclusion of terms (i.e. it is ok to add variables up to where \\(.p &lt; .2\\)). Matuschek et al. (2017) argue that when we are engaged in model selection – here, this is what we are doing because we are trying to figure out what model (with what random effects) we should use – then we should resist the reflex to choose the model that seems justified because the LRT \\(\\chi^2, p &lt; .05\\).\nThe \\(\\chi^2\\) alpha level cannot be interpreted as the expected model selection Type I error rate (in Null Hypothesis Significance Test terms) but rather as the relative weight of model complexity and goodness-of-fit (look back at the discussion of model comparison in the previous chapter). In this sense, setting a threshold such that we include an effect only if \\(\\chi^2, p &lt; .05\\) will always tend to penalize model complexity, and tend therefore to lead us to choose simpler (perhaps too simple) models.\n\n\n\nSometimes (g)lmer() has difficulty finding estimates for effects in a model given a dataset. If it encounters problems, the problems are expressed as warnings about convergence failures. Convergence failures typically arise when the model is too complicated for the data [see the discussion in @bates2015parsimonious; @eager; @matuschek2017; and @meteyard2020a]. As we have seen, problems can occur if you are trying to estimate (or predict) random effects terms that are very small – that do not really explain much variance in performance. Problems can also occur if variables have very large or very small ranges.\nWe can detect and address these problems in a number of ways:\n\nSometimes convergence problems can be fixed by switching the optimizer used to fit the model – we can do this by adding the argument: glmerControl(optimize = \"bobyqa\") to the glmer() function call, as I did for the class example. Switching optimizers is a quick solution to a common problem: models can fail to converge for a number of different reasons. In short, there may not be enough data for the model fitting process to settle on appropriate estimates for fixed or random effects.\nSometimes, a warning message advises us to consider rescaling the continuous numeric predictor variable. For this reason, and others, I usually standardize numeric predictor variables, as a default, before the analysis.\nSometimes, the warnings tell us that we need to simplify the random effects part of the model. We can simplify the random effects structure of a mixed-effects model in a number of ways:\n\n\nAs we examine the estimates resulting from a model fit we can consider whether the variance and covariance terms are small or large.\nUltimately, I decided that the effects of subjects and items on intercepts was important, as was, to some extent, the effect of words on the slope of the effect of Instructions.\n\nThe approach we have progressed through is widely used [see discussions in @Baayen2008; @Barr2013a; @matuschek2017; @meteyard2020a].\nMuch useful advice is set out in the troubleshooting guide by Ben Bolker here.\n\n\nTo demonstrate the impact of these adjustments, you could refit the models discussed in the foregoing:\n\nWithout using the standardized consistency variable as a predictor;\nWithout using the modification to the model fitting code, i.e. deleting the line that includes glmerControl(optimizer = \"bobyqa\");\nMaking the last most complex model more complex by adding further random effects of subjects or items on the slopes of both main effects and the interaction.\n\n\n\n\nMy advice, then, is to consider whether random effects should be included in a model based on:\n\nTheoretical reasons, in terms of what your understanding of a study design allows and requires, with respect to random differences between groups (classes, participants, stimuli etc.) or stimuli;\nModel convergence, as when models do or do not converge;\nOver a series of model comparisons, an evaluation of whether model fit is improved by the inclusion of the random effect.\n\nThis sounds like it involves work, judgment, and a process. It also sounds like people may disagree on the judgment or the process so that you shall have to share data and code, to enable others to check if the results vary depending on different decisions or different approaches. And it sounds like you will need to not only figure out what to do but also justify the approach you take when you report the results.\nI think all these things are true.\nThis is why Lotte Meteyard and I advise that researchers need to explain their approach, and share their data and code, when they report their analyses. Our best practice guidance for reporting mixed-effects models includes, among other things, the advice that in reports …\n\nRandom effects are explicitly specified according to sampling units (e.g., participants, items), the data structure (e.g., repeated measures) and anticipated interactions between fixed effects and sampling units (e.g., intercepts only or intercepts and slopes). Fixed effects and covariates are specified from explicitly stated research questions and/or hypotheses.\nReport the size of the sample analysed in terms of total number of data points and of sampling units (e.g., number of participants, number of items, number of other groups specified as random effects, such as classes of children).\nA clear statement of the methods by which models are compared/selected; e.g., simple to complex, covariates first, random effects first, fixed effects first etc.\nReport comparison method (LRT, AIC, BIC) and justify the choice.\nA complete report of all models compared (e.g., in appendices/supplementary data/analysis scripts) with model equations and the result of comparisons.\nIf models fail to converge, the approach taken to manage this should be comprehensively reported. This should include the formula for each model that did or did not converge and a rationale for a) the simplification method used and b) the final model reported. This may be most easily presented in an analysis script.\n\n\n\n\n\n\n\nTip\n\n\n\nThis looks like a lot of work.\n\nWhy bother?\n\n\n\nI think it is always worth asking this question.\nThe first answer is that it is all relative. In my own experience, a lot of the effort spent in the research workflow used to be occupied by face-to-face data collection: weeks or months of testing; now all data get collected online, and it gets finished overnight. Considerable time and effort (as Hadley Wickham’s joke runs, 80% of analysis effort) was spent on tidying the data before analysis; I still do this work but it is now much faster and less effortful, thanks to {tidyverse}. A lot of effort used to be spent by me or colleagues on the literature review, the power analysis, or the stimulus preparation: that still happens. And a lot of effort used to be spent on doing the analysis and figuring out what the results mean: that, too, still happens. It is up to you if you want to spend ten months on data collection and five minutes on data analysis (as another joke has it, a million bucks on the data and a nickel on the statistics).\nI think we do need to work at understanding the most appropriate analysis for our data, based on both our theoretical expectations and a data-driven evaluation. No-one is going to help us unsee multilevel structure in the data, or save us from the obligation to take into account random effects. Matuschek et al. (2017; p.312) argue that “The goal of model selection is not to obtain a significant p-value; the goal is to identify the most parsimonious model that can be assumed to have generated the data.” This makes sense to me. And their analyses show that determining a parsimonious model with a standard model selection criterion is a defensible choice, a way to take into account random effects, while controlling for both the risk of false positives, and the risk of false negatives."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-reporting-results",
    "href": "PSYC402/Week19.html#sec-glmm-reporting-results",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We have discussed how to report the results of mixed-effects models previously. The same conceptual structure, and similar language, can be used to report the results of Generalized Linear Mixed-effects Models (GLMMs).\nI think you need to think about reporting the analysis as a task in which you first prepare the reader, explaining the motivation for using GLMMs, then present the analysis you did (the process, in outline), then present the results you shall later discuss.\n\nStart by explaining the study design: outline the fixed effects that have to be estimated.\nExplain how random effects structure was selected – be prepared to present a short version of the story in the main part of the report – sharing your code, in an appendix, to illustrate the steps in full.\n\nLotte Meteyard and I recommend that results reporting should:\n\nProvide equation(s) that transparently define the reported model(s). An elegant way to do this is providing the model equation with the table that reports the model output.\nAnd that final model(s) should be reported in a table that includes all parameter estimates for fixed effects (coefficients, standard errors and/or confidence intervals, associated test statistics and p-values if used), random effects (standard deviation and/or variance for each random effect, correlations/covariances if modelled) and some measure of model fit (e.g. R-squared, correlation between fitted values and data).\nWhile researchers should be able to share the coding script used to complete the analysis and, wherever possible, share data that generated the reported results.\n\nFor the word learning study we have been working through, the Results section for the report would include the following elements:\n\nExplain approach\n\n\nWe used mixed-effects models to analyse data because this approach permits modelling of both participant- and item-level variability simultaneously, unlike more traditional approaches such as ANOVA. In this study, multiple participants responded to multiple items, meaning that both participants and items were sources of nonindependence in our data (i.e. responses from the same participant are likely to be correlated, as are responses to the same item). Compared to ANOVA, mixed-effects models offer a more flexible approach, and are better able to handle missing data without significant loss of statistical power (Baayen, Davidson, & Bates, 2008).\n\n\nExplain how you get from the study design to the model you use to test or estimate key effects\n\n\nWe took a hypothesis driven approach, estimating the fixed effects of time (Time 1 versus Time 2), Orthography (absent versus present), Instructions (incidental versus explicit) and consistency (standardized H), as well as the interaction between orthography and instructions and the interaction between orthography and consistency. Different levels of the three binary fixed effects were sum coded… Consistency H, as a numeric predictor variable, was standardized to z scores before entry to models as a predictor. scores before entry to models as predictors. –&gt;\n\n\nOutline the model comparison or model selection work\n\n\nThe models were initially fitted specifying just random effects to account for variation by participants and stimuli in accuracy (random intercepts) plus terms to estimate the fixed effects of the experimental conditions ([name them]), and the interactions [name them]. Following the recommendations of Barr, Levy, Scheepers, and Tily (2013; see also Baayen, 2008; Matuschek et al., 2017), we fitted further models adding both random intercepts and random slopes for the random effects. Likelihood ratio test comparison of models showed that a model with both random intercepts and slopes … fit the data better than a model with just random intercepts \\((\\chi^2(df) = ..., p = ...)\\).\n\n\nUse appendices or supplementary materials\n\n\nTo give the reader full information on models fit, model comparisons.\nTo Help the reader with a concise summary of estimates.\n\nAs I have advised for reporting linear models, I included a tabled summary of coefficient estimates, presenting fixed and random effects (see e.g. Davies et al., 2013; Monaghan et al., 2015)\n\nShow and tell\n\nUse figures – model prediction plots, as seen – to help the reader to see what the fixed effects estimates imply.\n\n\n\n\n\n\nTip\n\n\n\nWhich model do we report?\n\nNote that given the model comparison results we have seen, I would probably report the estimates from long.orth.3.glmer. The model appears to include the most comprehensive account of random effects while still being capable of converging."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-summary",
    "href": "PSYC402/Week19.html#sec-glmm-summary",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We focused on the need to use Generalized Linear Mixed-effects Models (GLMMs). We identified the kind of outcome data (like response accuracy) that requires analysis using GLMMs. Alternative methods, and their limitations, were discussed.\nWe examined a study that incorporates repeated measures (participants respond to multiple stimuli), a 2 x 2 factorial design, and a longitudinal aspect (participants tested at two time points), the word learning study [@ricketts2021].\nWe discussed the need to use effect coding for categorical predictor variables (factors). We work through example code to set factor level coding as required.\nWe worked through a random intercepts GLMM, and identified the critical elements of the model code, and of the results summary, including hypothesis test p-values. We examined how to present visualizations of fixed effects estimates (model predictions) using different libraries.\nWe then moved on to considering the question of what random effects we should include in the model. We considered the study design in some depth, and explored what random effects we could, in theory, expect to require. We then worked through a model comparison approach. We looked at some warning signs, what they indicate, and how to deal with them.\nWe considered how to report the model selection (or comparison, or building) process, and how to report the model for presentation of results.\n\n\nWe used two functions to fit and evaluate mixed-effects models.\n\nWe used glmer() to fit a mixed-effects model\nWe used anova() to compare two or more models using AIC, BIC and the Likelihood Ratio Test"
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-recommended-reading",
    "href": "PSYC402/Week19.html#sec-glmm-recommended-reading",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "The example studies referred to in this chapter are published in [@monaghan2015; @ricketts2021].\nBen Bolker provides a very readable introduction to Generalized Linear Mixed-effects Models [@bolker2009; see also @jaeger2008].\n[@Baayen2008; see also @Barr2013a] discuss mixed-effects models with crossed random effects.\nThe issue of model comparison or model selection, and the appropriate choice of random effects structure is discussed helpfully by @Baayen2008, @bates2015parsimonious, @Barr2013a, @eager and @matuschek2017.\nI wrote a tutorial article on mixed-effects models with Lotte Meteyard [@meteyard2020a]. We discuss how important the approach now is for psychological science, what researchers worry about when they use it, and what they should do and report when they use the method.\nAccessible ook length introductions are provided by @Snijders2004a and @Gelman2007g.\n\n\nCan be found here.\nOther helpful online advice by Ben Bolker (besides his numerous helpful interventions on StackOverflow) can be found here and here."
  },
  {
    "objectID": "PSYC402/Week19.html#sec-glmm-appendix",
    "href": "PSYC402/Week19.html#sec-glmm-appendix",
    "title": "Introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "Further information about the variables in the long.orth_2020-08-11.csv dataset is listed following.\n\nParticipant\n\nCell values comprise character strings coding for participant. Participant identity codes were used to anonymise participation. Children included in studies 1 and 2 – participants in the longitudinal data collection – were coded EOF[number]. Children included in Study 2 only (i.e., the older, additional, sample) were coded ND[number].\n\nTime\n\nTest time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\n\nStudy\n\nObservations taken for children included in studies 1 and 2 – participants in the longitudinal daa collection – were coded Study1&2. Children included in Study 2 only (i.e., the older, additional, sample) were coded Study2.\n\nInstructions – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.\nVersion – Experiment administration coding\nWord – Letter string values show the words presented as stimuli to children.\nConsistency-H – Calculated orthography-to-phonology value for each word.\nOrthography – Variable coding for whether participants had seen a word in training in the orthography absent} or present} conditions.\nMeasure – Variable coding for the post-test measure: Sem_all: if the semantic post-test;Orth_sp: if the orthographic post-test.\nScore – Variable coding for response category. For the semantic (sequential or dynamic) post-test, responses were scored as corresponding to:\n3 – correct response in the definition task\n2 – correct response in the cued definition task\n1 – correct response in the recognition task\n0 – if the item wasn’t correctly defined or recognised\nFor the orthographic post-test, responses were scored as:\n1 – correct, if the target spelling was produced in full\n0 – incorrect\nWASImRS Raw score – Matrix Reasoning subtest of the Wechsler Abbreviated Scale of Intelligence\nTOWREsweRS Raw score – Sight Word Efficiency (SWE) subtest of the Test of Word Reading Efficiency; number of words read correctly in 45 seconds.\nTOWREpdeRS Raw score – Phonemic Decoding Efficiency (PDE) subtest of the Test of Word Reading Efficiency; number of nonwords read correctly in 45 seconds.\nCC2regRS Raw score – Castles and Coltheart Test 2; number of regular words read correctly\nCC2irregRS Raw score – Castles and Coltheart Test 2; number of irregular words read correctly\nCC2nwRS Raw score – Castles and Coltheart Test 2; number of nonwords read correctly\nWASIvRS Raw score – vocabulary knowledge indexed by the Vocabulary subtest of the WASI-II\nBPVSRS Raw score – vocabulary knowledge indexed by the British Picture Vocabulary Scale – Third Edition\nSpelling.transcription Transcription of the spelling response produced by children in the orthographic post-test\nLevenshtein.Score Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure, using the `stringdist: library (van der Loo, 2019). This score indexes the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. For example, the response ‘epegram’ for target ‘epigram’ attracts a Levenshtein score of 1 (one substitution). Thus, this score gives credit for partially correct responses, as well as entirely correct responses. The maximum score is 0, with higher scores indicating less accurate responses.\nzTOWREsweRS We standardized TOWREsweRS values, calculating the z score as \\(z = \\frac{x - \\bar{x}}{sd_x}\\), over all observations in the longitudinal} (Study 1) or concurrent} (Study 2) data-set, using the scale() function in R.\nzTOWREpdeRS Standardized TOWREpdeRS scores\nzCC2regRS] Standardized CC2regRS scores\nzCC2irregRSStandardized CC2irregRS scores\nzCC2nwRS Standardized CC2nwRS scores\nzWASIvRS Standardized WASIvRS scores\nzBPVSRSStandardized BPVSRS scores"
  },
  {
    "objectID": "PSYC402/Week20.html",
    "href": "PSYC402/Week20.html",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "Ordinal data are very common in psychological science. Often, we will encounter ordinal data recorded as responses to Likert-style items in which the participant is asked to indicate a response on an ordered scale ranging between two end points (Bürkner and Vuorre 2019; Liddell and Kruschke 2018). An example of a Likert question item might be: How well do you think you have understood this text? (Please check one response) where the participant must respond by checking an option, given 5 options ranging from 1 (not well at all) to 5 (very well). The critical characteristics of such responses are that:\n\nThe responses are ordered, as indicated by the number labels;\nResponse types are categorical or qualitative, not numeric.\n\nWe will be working with study data in which the outcome that is the target for our analyses comprise responses to questions designed to elicit ratings. Ordinal data may, however, also derive from situations in which ordered categorical responses do not derive from ratings items (we will look briefly at sequential responses, Bürkner and Vuorre 2019).\nThe challenge we face is that we will aim to develop skills in using ordinal models when, in contrast, most psychological research articles will report analyses of ordinal data using conventional methods like ANOVA or linear regression. We will work to understand why ordinal models are better. We will learn that applying conventional methods to ordinal data will, in principle, involve a poor account of the data and, in practice, will create the risk of producing misleading results. And we will learn how to work with and interpret the results from ordinal models with or without random effects.\nIn our work in this chapter, we will rely extensively on the ideas set out by Liddell and Kruschke (2018), see Section 1.13.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOrdinal responses are labelled with numbers but ordinal data are not numeric.\n\n\nOrdinal responses are coded with numeric labels. These number labels may indicate order but we do not know that the difference between e.g. response options 1 versus 2 is the same as the difference between 2 versus 3 or 3 versus 4. Ordinal data contrast with metric data (Liddell and Kruschke 2018) which are recorded on scales for which we assume both order and equal intervals. When researchers apply metric models to ordinal data, they incorrectly assume that the response options e.g. in ratings are separated by equal intervals. Yet, in a review of the 68 recently articles that mentioned the term “Likert” in a sample of highly ranked Psychology journals, Liddell and Kruschke (2018) found that ordinal data were treated as metric and the articles presented results from metric models.\nOne way to think about ordinal data is that often (but not always) ratings may be understood to come from psychological processes in which the participant, in response to the Likert question, divides some latent (unobserved) psychological continuum or scale into categories in order to select a response option.\nImagine, for example, that you have been asked the question “How well do you understand this text? (on a scale from 1-5)”. Presumably, to answer this question, you will have to choose a response based on where you think you are on your unobserved measure of your understanding. You may be able to evaluate the cohesion, or some other internal measure, of your understanding of the text. Simplifying a bit, we might assume that your internal measure of understanding is associated with a normal probability distribution so that it peaks over some value (e.g., 3) of the strength of understanding though other values are possible. As Figure 1 suggests, a participant in this common situation will have to map the internal measure (the latent scale, e.g., of understanding) to a number from the response options you are given (e.g., rating scale values ranging 1-5). But there is no reason to suppose that your internal measure of your understanding is divided into an ordered metric scale.\n\n\n\n\n\nFigure 1: A latent scale on the horizontal axis is divided into intervals or bins divided by thresholds marked by dotted lines. The cumulative normal probability in the intervals is the probability of the ordinal values.\n\n\n\n\nIn conducting analyses of ordinal data with ordinal models, we often fit models that describe the cumulative probability that a rating response is located at some value (typically, understood in terms of threshold) on an underlying latent continuum. In ordinal models, we do not assume that the ordinal responses map to equally spaced intervals on the latent scale: the values or thresholds at which the continuum are split are to be estimated.\nIn applying metric models to ordinal data, we do assume that intervals are equal though this assumption is unlikely to be true or, at least, is unlikely to be verifiable. This faulty assumption has consequences because the mis-application of metric models (e.g. ANOVA, linear models) to ordinal data is both commonplace and risky. As Liddell and Kruschke (2018) demonstrate, mis-applying metric models to ordinal data can result in false positives (detecting a difference when none is present), false negatives (missing a difference that is present) and inversions (swapping the difference so that it appears to be positive instead of negative or vice versa). These kinds of misrepresentions cannot be avoided and are not fixed by, for example, averaging ratings scales data together.\n\n\n\n\nUnderstand practically the reasons for using ordinal models when we analyze ordinal outcome variables, ?@sec-ordinal-practical-understanding.\nPractice running ordinal models with varying random effects structures.\nPractice reporting the results of ordinal models, including through the use of prediction plots.\n\n\n\n\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Chapter: 05-ordinal\n1.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to work with ordered categorical outcomes i.e. ordinal data using ordinal models.\n1.2. The practical elements include data tidying, visualization and analysis steps.\n1.3. You can read the chapter and run the code to gain experience and encounter code you can adapt for your own purposes.\n\nRead in the example dataset.\nExperiment with the .R code used to work with the example data.\nRun ordinal models of demonstration data.\nRun ordinal models of alternate data sets (see links in Section 1.9).\nReview the recommended readings (Section 1.13).\n\n2. Practical materials\n2.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning. I set out the data tidying, analysis and visualization steps you can follow, working with the example dataset, described next.\n\n\n\nWe will be working, at first, with a sample of data collected as part of the Clearly understood: health comprehension project (Davies, Ratajczak, Gillings, Chadwick & Gold). These data are unpublished.\n\n\n\n\nOur interest, in conducting the project, lies in identifying what factors make it easy or difficult to understand written health information. In part, we are concerned about the processes that health providers or clinicians apply to assure the effectiveness of the text they produce to guide patients or carers, for example, in taking medication, in making treatment decisions, or in order to follow therapeutic programmes.\nIt is common, in the quality assurance process in the production of health information texts, that text producers ask participants in patient review panels to evaluate draft texts. In such reviews, a participant may be asked a question like “How well do you understand this text?” This kind of question presents a metacognitive task: we are asking a participant to think about their thinking. But it is unclear that people can do this well or, indeed, what factors determine the responses to such questions (Dunlosky and Lipko 2007).\nFor these reasons, we conducted studies in which we presented adult participants with sampled health information texts (taken from health service webpages) and, critically, asked them to respond to the question:\n\nHow well do you think you have understood this text? (Please check one response)\n\nFor each text, in response to this question, participants were asked to click on one option from an array of response options ranging from (1) Not well at all to (9) Extremely well. The data we collected in this element of our studies comprise, clearly, ordinal responses. Thus, we may use these data to address the following research question.\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\n\n\n\nWe will work with a sample of participant data drawn from a series of Lancaster University undergraduate dissertation studies connected to the Clearly understood project. In these studies, we collected data from 202 participants on a series of measures (Section 1.5.1.3) of vocabulary knowledge, health literacy, reading strategy, as well as responses to health information texts. The distributions of participants’ scores on each of a range of attribute variables\n\n\n\n\n\nFigure 2: Grid of plots showing the distribution of participant attributes. The grid includes histograms of the distributions of: self-rated accuracy; vocabulary (SHIPLEY); health literacy (HLVA); reading strategy (FACTOR3); and age (years). We also see dot plots presenting counts of numbers of participants of different self-reported gender, education, and ethnicity categories.\n\n\n\n\nThe plots indicate:\n\nmost self-rated accuracy scores are high (over 6);\nmany participants with vocabulary scores greater than 30, a few present lower scores;\nhealth literacy scores centered on 8 or some, with lower and higher scores;\na skewed distribution of reading strategy scores, with many around 20-40, and a tail of higher scores;\nmost participants are 20-40 years of age, some older;\nmany more female than male participants, very few non-binary reported;\nmany more participants with higher education than further, very few with secondary;\nand many White participants (Office of National Statistics categories), far fewer Asian or Mixed or Black ethnicity participants.\n\n\n\n\nWe collected data through an online survey administered through Qualtrics.\nWe used the Shipley vocabulary sub-test (Shipley et al. 2009) to estimate vocabulary knowledge.\nWe used the Health Literacy Vocabulary Assessment (HLVA, Ratajczak, 2020; adapted for online presentation, Chadwick, 2020) to estimate health literacy.\nWe used an instrument drawn from unpublished work by Calloway (2019) to assess the approach participants took to reading and understanding written information.\nWe presented participants with a sample of 20 health information texts. In the data collection process for this dataset, participants were recruited in multiple different studies. In each study, any one participant was presented with a randomly selected subset of the total of 20 texts.\nWe asked participants to rate their level of understanding of the health-related texts that we presented in the study. We used a nine-point judgment scales because they have been found to outperform alternative scales with fewer categories in terms of criterion validity, internal consistency, test-retest reliability, and discriminating power (Preston and Colman 2000).\nWe recorded participants’ demographic characteristics: gender (coded: Male, Female, non-binary, prefer not to say); education (coded: Secondary, Further, Higher); and ethnicity (coded: White, Black, Asian, Mixed, Other).\n\n\n\n\nYou can download the 2021-22_PSYC304-health-comprehension.csv file holding the data we analyse in this chapter by clicking on the link.\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read the data file into R.\n\nhealth &lt;- read_csv(\"2021-22_PSYC304-health-comprehension.csv\", \n                                 na = \"-999\",\n                                 col_types = cols(\n                                   ResponseId = col_factor(),\n                                   rating = col_factor(),\n                                   GENDER = col_factor(),\n                                   EDUCATION = col_factor(),\n                                   ETHNICITY = col_factor(),\n                                   NATIVE.LANGUAGE = col_factor(),\n                                   OTHER.LANGUAGE = col_factor(),\n                                   text.id = col_factor(),\n                                   text.question.id = col_factor(),\n                                   study = col_factor()\n                                 )\n                               )\n\nNotice that we use col_types = cols(...) to require read_csv() to class some columns as factors.\nImportantly, we ask R to treat the rating variable as a factor with rating = col_factor().\n\n\n\n\n\n\nTip\n\n\n\nIn the practical work we do, we will be using functions from the {ordinal} library to model ordinal data.\n\nIn using these functions, we need ask R to treat the ordinal outcome variable as a factor.\n\n\n\n\n\n\nIt is always a good to inspect what you have got when you read a data file in to R. Here, what may most concern us is the distribution of observed responses on the rating scale (responses to the “How well do you understand?” question). Figure 3 is a dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.\n\nhealth &lt;- health %&gt;% mutate(rating = fct_relevel(rating, sort))\n\nhealth %&gt;%\n  group_by(rating) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = rating, y = count, colour = rating)) + \n  geom_point(size = 3) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank()  # No horizontal grid lines\n  ) +\n  coord_flip()\n\n\n\n\nFigure 3: Dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.\n\n\n\n\nThe plot indicates that most participants chose response options 5-9, while very few rated their understanding at the lowest levels (options 1-4). Interestingly, many ratings responses around 7-8 were recorded: many more than responses at 5-6.\nIn analyzing these data, we will seek to estimate what information available to us can be used to predict whether a participant’s rating of their understanding is more likely to be, say, 1 or 2, 2 or 3 … 7 or 8, 8 or 9.\n\nOne practical way to think about the estimation problem when working with ratings-style ordinal data is this:\n\nWhat factors move or how do influential factors move the probability that the ordinal response is a relatively low or relatively high order response option?\nIn doing this, we do not have to assume that rating scale points map to equal sized intervals on the underlying latent scale where the scale may be an unobserved psychological continuum (like understanding).\n\n\nHere, we mostly have information on participant attributes and some information on text properties to do our prediction analyses. In other studies, we may be using information about experimental conditions, or selected groups of participants to estimate effects on variation in ratings responses.\n\n\n\n\nThe Clearly understood health comprehension project dataset is tidy (?@sec-intro-mixed-data-tidy):\n\nEach variable has its own column.\nEach observation has its own row.\nEach value has its own cell.\n\nHowever, there are aspects of the data structure or properties of the dataset variables that will cause inefficiencies or problems in later data analysis if we do not fix them first.\nYou can see what we have if you look at the results we get from using summary() and str() to inspect the dataset.\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n\n\nYou should be used to seeing the summary() of a dataset, showing summary statistics of numeric variables and counts of the numbers of observations of data coded at different levels for each categorical or nominal variable classed as a factor.\nUsing the str() function may be new to you and, as you can see, the output from the function call gives you a bit more information on how R interprets the data in the variable columns. You can see that each variable is listed alongside information about how the data in the column are interpreted (as Factor or num numeric). Where we have columns holding information on factors there we see information about the levels.\nRecall that for a categorical or nominal variable e.g. ETHNICITY, provided R interprets the variable as a factor, each data value in the column is coded as corresponding to one level i.e. group or class or category (e.g., we have ETHNICITY classes \"Asian\" etc.) Recall, also, that at the data read-in stage, we instructed R how we wanted it to interpret each column using col_types = cols().\n\nstr(health)\n\ntibble [4,040 × 17] (S3: tbl_df/tbl/data.frame)\n $ ResponseId         : Factor w/ 202 levels \"R_sKW4OJnOlidPxrH\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ AGE                : num [1:4040] 20 20 20 20 20 20 20 20 20 20 ...\n $ GENDER             : Factor w/ 3 levels \"Female\",\"Male\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ EDUCATION          : Factor w/ 3 levels \"Further\",\"Higher\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ ETHNICITY          : Factor w/ 4 levels \"Asian\",\"White\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ NATIVE.LANGUAGE    : Factor w/ 2 levels \"English\",\"Other\": 1 1 1 1 1 1 1 1 1 1 ...\n $ OTHER.LANGUAGE     : Factor w/ 17 levels \"NA\",\"Catonese\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ ENGLISH.PROFICIENCY: chr [1:4040] \"NA\" \"NA\" \"NA\" \"NA\" ...\n $ SHIPLEY            : num [1:4040] 26 26 26 26 26 26 26 26 26 26 ...\n $ HLVA               : num [1:4040] 8 8 8 8 8 8 8 8 8 8 ...\n $ FACTOR3            : num [1:4040] 59 59 59 59 59 59 59 59 59 59 ...\n $ rating             : Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n $ response           : num [1:4040] 1 1 1 1 1 1 1 0 1 1 ...\n $ RDFKGL             : num [1:4040] 10.61 10.61 10.61 10.61 8.12 ...\n $ study              : Factor w/ 4 levels \"cs\",\"jg\",\"ml\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ text.id            : Factor w/ 20 levels \"studyone.TEXT.105\",..: 1 1 1 1 2 2 2 2 3 3 ...\n $ text.question.id   : Factor w/ 80 levels \"studyone.TEXT.105.CQ.1\",..: 1 2 3 4 5 6 7 8 9 10 ...\n\n\nOur specific concern, here, is that the rating response variable is treated as a factor because the {ordinal} library we are going to use to do the modeling must find the outcome variable is a factor.\nWe can focus str() on the rating variable. We see that it is being treated as a factor.\n\nstr(health$rating)\n\n Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n\n\nHowever, we also need to make sure that the rating outcome variable is being treated as an ordered factor.\nWe can perform a check as follows. (I found how to do this here)\n\nis.ordered(factor(health$rating))\n\n[1] FALSE\n\n\nWe can see that the variable is not being treated as an ordered factor. We need to fix that.\nThe ordinal model estimates the locations (thresholds) for where to split the latent scale (the continuum underlying the ratings) corresponding to different ratings values. If we do not make sure that the outcome factor variable is split as it should be then there is no guarantee that {ordinal} functions will estimate the thresholds in the right order (i.e., 1,2,3 ... rather than 3,2,1...).\nWe can make sure that the confidence rating factor is ordered precisely as we wish using the ordered() function.\n\nhealth$rating &lt;- ordered(health$rating,\n                         levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"))\n\nWe can then do a check to see that we have got what we want. We do not wantrating to be treated as numeric, we do want it to be treated as an ordered factor.\n\nis.numeric(health$rating)\n\n[1] FALSE\n\nis.factor(health$rating)\n\n[1] TRUE\n\nstr(health$rating)\n\n Ord.factor w/ 9 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 8 8 8 8 7 7 7 7 7 7 ...\n\nis.ordered(health$rating)\n\n[1] TRUE\n\n\nIt is.\nNext, before doing any modelling, it will be sensible to standardize potential predictors\n\nhealth &lt;- health %&gt;% \n  mutate(across(c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL), \n                scale, center = TRUE, scale = TRUE,\n                .names = \"z_{.col}\"))\n\nYou can see that in this chunk of code, we are doing a number of things:\n\nhealth &lt;- health %&gt;% recreates the health dataset from the following steps.\nmutate(...) do an operation which retains the existing variables in the dataset, to change the variables as further detailed.\nacross(...) work with the multple column variables that are named in the c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL) set.\n...scale, center = TRUE, scale = TRUE... here is where we do the standardization work.\n\nWhat we are asking for is that R takes the variables we name and standardizes each of them.\n\n.names = \"z_{.col}\") creates the standardized variables under adapted names, adding z_ to the original column name so that we can distinguish between the standardized and original raw versions of the data columns.\n\nNote that the across() function is a useful function for applying a function across multiple column variables see information here There is a helpful discussion on how we can do this task here\nWe can then check that we have produced the standardized variables as required.\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id       z_AGE.V1      \n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86    Min.   :-0.9826247  \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86    1st Qu.:-0.8620352  \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86    Median :-0.4399723  \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86    Mean   : 0.0000000  \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86    3rd Qu.: 0.9468060  \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86    Max.   : 2.8159420  \n (Other)         :2096   (Other)              :3524                        \n    z_SHIPLEY.V1          z_HLVA.V1          z_FACTOR3.V1    \n Min.   :-3.294105   Min.   :-2.6074887   Min.   :-4.205936  \n 1st Qu.:-0.543723   1st Qu.:-0.7330662   1st Qu.:-0.529155  \n Median : 0.189713   Median : 0.2041450   Median :-0.003900  \n Mean   : 0.000000   Mean   : 0.0000000   Mean   : 0.000000  \n 3rd Qu.: 0.739789   3rd Qu.: 0.6727506   3rd Qu.: 0.783981  \n Max.   : 1.289866   Max.   : 2.0785675   Max.   : 1.834490  \n                                                             \n     z_RDFKGL.V1     \n Min.   :-1.4644660  \n 1st Qu.:-0.6814594  \n Median : 0.0807363  \n Mean   : 0.0000000  \n 3rd Qu.: 0.6430616  \n Max.   : 2.3187650  \n                     \n\n\n\n\n\nIn our first analysis, we can begin by assuming no random effects. We keep things simple at this point so that we can focus on the key changes in model coding.\nThe model is conducted to examine what shapes the variation in rating responses that we see in Figure 3.\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\nIn our analysis, the outcome variable is the ordinal response variable rating. The predictors consist of the variables we standardized earlier. We use the clm() function from the {ordinal} library to do the analysis. I will give information in outline here, the interested reader can see more detailed information in Rune Haubo Bojesen Christensen (2022) and R. H. B. Christensen (2015). You can also find the manual for the {ordinal} library functions here\nWe code the model as follows.\n\nhealth.clm &lt;- clm(rating ~\n                    \n                    z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n                  Hess = TRUE, link = \"logit\",\n                  data = health)\n\nsummary(health.clm)\n\nThe code works as follows.\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\nclm() the function name changes because now we want a cumulative link model of the ordinal responses.\n\nThe model specification includes information about the fixed effects, the predictors: z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL.\nSecond, we have the bit that is specific to cumulative link models fitted using the clm() function.\n\nHess = TRUE is required if we want to get a summary of the model fit; the default is TRUE but it is worth being explicit about it.\nlink = \"logit\" specifies that we want to model the ordinal responses in terms of the log odds (hence, the probability) that a response is a low or a high rating value (compare ?@sec-glmm-practical-understanding).\n\n\n\nIf you run the model code, it may take a few seconds to run. Then you will get the results shown in the output.\n\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6880.78 13787.55 5(0)  9.26e-07 7.3e+01\n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.17719    0.02966  -5.975 2.30e-09 ***\nz_SHIPLEY  0.34384    0.03393  10.135  &lt; 2e-16 ***\nz_HLVA     0.16174    0.03265   4.954 7.28e-07 ***\nz_FACTOR3  0.74535    0.03145  23.699  &lt; 2e-16 ***\nz_RDFKGL  -0.27220    0.02892  -9.412  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.65066    0.12978 -35.836\n2|3 -4.13902    0.10395 -39.817\n3|4 -3.26845    0.07390 -44.228\n4|5 -2.56826    0.05804 -44.248\n5|6 -1.69333    0.04437 -38.160\n6|7 -0.87651    0.03671 -23.876\n7|8  0.24214    0.03402   7.117\n8|9  1.63049    0.04252  38.346\n\n\nThe summary() output for the model is similar to the outputs you have seen for other model types.\n\nWe first get formula: information about the model you have specified.\nR will tell us what data: we are working with.\nWe then get Coefficients: estimates.\n\nThe table summary of coefficients arranges information in ways that will be familiar you:\n\nFor each predictor variable, we see ’Estimate, Std. Error, z value, and Pr(&gt;|z|)` statistics.\nThe Pr(&gt;|z|) p-values are based on Wald tests of the null hypothesis that a predictor has null impact.\nThe coefficient estimates can be interpreted based on whether they are positive or negative.\n\nA positive coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of higher rating values. A negative coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of lower rating values.\n\nWe then get Threshold coefficients: indicating where the model fitted estimates the threshold locations: where the latent scale is cut, corresponding to different rating values.\n\n\n\n\n\n\n\nTip\n\n\n\nIn reporting ordinal (e.g., cumulative link) models, we typically focus on the coefficient estimates for the predictor variables.\n\n\n\n\n\n\nIn our analysis, we begn by assuming no random effects. However, this is unlikely to be appropriate given the data collection process deployed in the Clearly understood projects, where:\n\na sample of participants were asked to respond to a sample of texts;\nwe have multiple observations of responses for each participant;\nwe have multiple observations of responses for each stimulus text;\nparticipants were assigned to groups, and within a group all participants were asked to respond to the same stimulus texts.\n\nThese features ensure that the data have a multilevel structure and this structure requires us to fit a Cumulative Link Mixed-effects Model (CLMM).\nWe keep things simple at this point so that we can focus on the key changes in model coding. We can code a Cumulative Link Mixed-effects Model as follows.\n\nhealth.clmm &lt;- clmm(rating ~\n                      \n                      z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +\n                      \n                      (1|ResponseId),\n                    \n                    Hess = TRUE, link = \"logit\",\n                    data = health)\n\nsummary(health.clmm)\n\nIf you inspect the code chunk, you can see that we have made two changes.\nFirst, we have changed the function.\n\nclmm() the function name changes because now we want a Cumulative Linear Mixed-effects Model.\n\nSecondly, the model specification includes information about fixed effects and now about random effects.\n\nWith (1 | Participant) we include random effects of participants on on intercepts.\n\n\n\nIf you run the model code, you will see that the model may take several seconds, possibly a minute or two to complete. We will then get the results shown in the output.\n\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +  \n    (1 | ResponseId)\ndata:    health\n\n link  threshold nobs logLik   AIC     niter       max.grad cond.H \n logit flexible  4040 -4978.08 9984.16 1480(19002) 3.93e-03 3.5e+02\n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ResponseId (Intercept) 9.825    3.134   \nNumber of groups:  ResponseId 202 \n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.44313    0.23483  -1.887  0.05916 .  \nz_SHIPLEY  0.77130    0.26514   2.909  0.00363 ** \nz_HLVA     0.20809    0.25617   0.812  0.41663    \nz_FACTOR3  1.68342    0.23836   7.063 1.63e-12 ***\nz_RDFKGL  -0.44345    0.03677 -12.059  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -9.3297     0.3453 -27.019\n2|3  -8.1413     0.2948 -27.618\n3|4  -6.5243     0.2632 -24.786\n4|5  -5.2159     0.2491 -20.938\n5|6  -3.4668     0.2371 -14.623\n6|7  -1.8481     0.2316  -7.981\n7|8   0.2965     0.2296   1.292\n8|9   3.0417     0.2347  12.959\n\n\nYou can see that the output summary presents the same structure. If you compare the output you see in Section 1.7.1, however, you will notice some similarities and some differences:\n\nIf you focus first on the estimates of the coefficients for the predictor variables, you will see that the estimates have the same sign (positive or negative) as they had before.\nHowever, you will see that the estimates have different magnitudes.\nYou will also see that the p-values are different.\n\nStudents often focus on p-values in reading model summaries. This is mistaken for multiple reasons. The p-values correspond to the probabilities associated with the null hypothesis significance test: the test of the hypothesis that the effect of the predictor is null (i.e. the predictor has no impact). This null assumption is made whichever model we are looking at. The p-values do not indicate whether an effect is more or less probable. But you do get such posterior probabilities in Bayesian analyses. So it does not really mean much, though it is common, to talk about effects being highly significant. Thus it should not worry us too much if the p-values are significant in one analysis but not significant in another.\nThat said, it is interesting, perhaps, that once we include random effects of participants on intercepts in our analysis then the effects of z_AGE and z_HLVA are no longer significant. I would be tempted to ask if the previously significant effects of these variables owed their impact to random differences between participants in their average or overall level of rating response.\n\n\n\nIt will be helpful for the interpretation of the estimates of the coefficients of these predictor variables if we visualize the predictions we can make, about how rating values vary, given differences in predictor variable values, given our model estimates. We can do this using functions from the {ggeffects} library. You can read more about the {ggeffects} library here where you will see a collection of articles explaining what you can do, and why, as well as technical information including some helpful tutorials.\nThe basic model prediction coding looks like this.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n\nFigure 4 shows you the marginal effect of variation in the reading strategy attribute, i.e., the effect of differences between individuals in how they score on the FACTOR3 measure of reading strategy. Note that the variable is listed as z_FACTOR3 because, as you will recall, we standardized numeric predictor variables before entering them in our model.\nThese kinds of plots are understood to present what are variously called conditional effects, or adjusted predictions or marginal effects. You can find a discussion of marginal effects in the context of working with the {ggeffects} library here and here. You can find an extensive, helpful (with examples) discussion of marginal effects by Andrew Heiss here.\nIn short, what we want to do is to take the model coefficient estimates, and generate predictions with these estimates, given different values of the predictor variable, while holding the other predictor variables at some constant or some level (or some series of values).\nIf you look at the code chunk, you can see that we first:\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\n\n\nIn this line, we use ggpredict() to work with some model information, assuming we previously fitted a model and gave it a name (here, health.clmm).\n\nNote that if you fit the model and call it health.clmm, as we did in Section 1.8, then an object of that name is created in the R workspace or environment. If you click on that object name in the environment window in R-Studio, you will see that there is a list of pieces of information about the model, including the coefficient estimates, the model formula etc. associated with that name.\n\nSo when we use ggpredict(), we ask R to take that model information and, for the term we specify, here, specify using terms=\"z_FACTOR3 [all]\", we ask R to generate some predictions.\ndat &lt;- ggpredict(...) asks R to put those predictions in an object called dat.\n\nIf you click on that object name in the environment window in R-Studio, you will see that it comprises a dataset. The dataset includes the columns:\n\nx giving different values of the predictor variable. ggpredict() will choose some ‘representative’ values for you but you can construct a set of values of the predictor for which you want predictions.\npredicted holds predicted values, given different predictor x values.\n\nIf you then run the line plot(dat) you can see what this gets us for these kinds of models. Figure 4 presents a grid of plots showing the model-predicted probabilities that a rating response will have one value for each of the 1-9 rating response values that are possible given the Likert rating scale used in data collection. In the grid, a different plot is shown for each possible response value, indicating how the probability varies that the rating response will take that value.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n\n\n\n\nFigure 4: A grid of plots showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable.\n\n\n\n\nIf you examine Figure 4, you can recognize that we have one plot for each different value of the response options available for the Likert-scale rating items: 1-9. You can also see that in each plot we get a curve. In some cases – for rating response values 1-4 – the curve is flat or flattens very quickly, for higher levels of the z_FACTOR3 variable. In some cases – for rating response values 5-9 – the curve is more obvious, and resembles a normal distribution curve.\nIf you think about it, what these plots indicate are the ways in which the probability that a rating response is a low value (e.g., a rating of 1) or a high value (e.g., a rating of 9) rises or falls. Each possible rating response is associated with a probability distribution. For example, look at the plot labelled 6: that shows you the probability distribution indicating how the probability varies that a response will take the value 6. We can see that the distribution is normal in shape, a bell-shaped curve. We can see that the peak of the curve is over the z_FACTOR3 score (shown on the x-axis) of about 1.5. We can see that the probability represented by the height of the line showing the curve is lower for z_FACTOR3 scores lower than the score under the peak (e.g. scores less than z_FACTOR3 \\(=2\\)). The probability represented by the height of the line showing the curve is lower for z_FACTOR3 scores higher than the score under the peak (e.g. scores greater than z_FACTOR3 \\(=1\\)).\nWe can see that the peak of the normal curve, in the case of rating response values 5-9, is located at different places on the horizontal axis. Look at each of the plots labelled 5-9. Notice how the horizontal location of the curves shifts as z_FACTOR3 scores increase. If you go from left to right, i.e. from low to high values of z_FACTOR3, on each plot then you will see that the peak of the curve is located in different places: going from plot 5 to plot 9 the peak of the curve moves rightwards. These curves show how the probability that a rating response takes a high value (e.g. 9 instead of 8 or 8 instead of 7 etc.) is higher for higher values of z_FACTOR3. This idea might be a bit clearer if we draw the plot in a different way.\nFigure 5 shows the same model predictions but plots the predictions of the way that probability changes, for each rating response, by superimposing the plots for each response value, one on top of the other. I have drawn each probability curve in a different colour, and these colours match those used to present the counts of different response values shown in Figure 3.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nggplot(dat, aes(x, predicted, \n                colour = response.level)) + \n  geom_line(size = 1.5) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + \n  labs(x = \"Reading strategy (z_FACTOR3)\", y = \"Predicted probability of a rating\") +\n  guides(colour = guide_legend(title = \"Rating\")) +\n  ylim(0, 1) +\n  theme_bw()\n\n\n\n\nFigure 5: A plot showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable\n\n\n\n\nYou can read Figure 5 by observing that:\n\nFor low value ratings e.g. for rating responses from 1-4, there is not much predicted probability that a response with such a value will be made (flat lines) but if they are going to be made they are likely to be made by people with low scores on the z_FACTOR3.\n\nYou can see this because you can see how the curves peak around low values of z_FACTOR3. This should make sense: people with low scores on reading strategy are maybe not doing reading effectively, are maybe as a result not doing well in understanding the texts they are given to read, and thus are not confident about their understanding. (This is a speculative causal theory but it will suffice for now.)\nRecall, also, that as Figure 3 indicated, in the Clearly understood health comprehension dataset, we saw that few rating responses were recorded for low value ratings of understanding. Few people in our sample made rating responses by choosing ratings of 1 or 2 to indicate low levels of understanding.\nFigure 5 also suggests that:\n\nFor higher value rating responses – responses representing ratings from 5 to 9 – there is variation in the probability that responses with such values will be made.\nThat variation in probability is shown by the probability distribution curves.\nFor these data, and this model, we can see that the probability shifts suggesting that participants in our sample were more likely to choose a higher value rating if they were also presenting high scores on the z_FACTOR3 measure of reading strategy.\n\n\n\n\n\nAs the review reported by Liddell and Kruschke (2018) suggests, we may have many many studies in which ordinal outcome data are analysed but very few published research reports that present analyses of ordinal data using ordinal models.\nYou can see two examples in the papers published by Ricketts, Dawson, and Davies (2021) and by Rodríguez-Ferreiro, Aguilera, and Davies (2020). These papers are both published open accessible, so that they are freely available, and they are both associated with accessible data repositories.\n\nYou can find the repository for Ricketts, Dawson, and Davies (2021) here.\nYou can find the repository for Rodríguez-Ferreiro, Aguilera, and Davies (2020) here.\n\nThe Rodríguez-Ferreiro, Aguilera, and Davies (2020) shares a data .csv only.\nThe Ricketts, Dawson, and Davies (2021) repository shares data and analysis code as well as a fairly detailed guide to the analysis methods. Note that the core analysis approach taken in Ricketts, Dawson, and Davies (2021) is based on Bayesian methods but that we also conduct clmm() models using the {ordinal} library functions discussed here; these models are labelled frequentist models and can be found under sensitivity analyses.\nFor what it’s worth, the Ricketts, Dawson, and Davies (2021) is much more representative of the analysis approach I would recommend now.\nWhatever the specifics of your research question, dataset, analysis approach or model choices, I would recommend the following for your results report.\n\nExplain the model – the advice extended by Meteyard and Davies (2020) still apply: the reader will need to know:\n\n\nThe identity of the outcome and predictor variables;\nThe reason why you are using an ordinal approach, explaining the ordinal (ordered, categorical) nature of the outcome;\nThe structure of the fixed effects part of the model, i.e. the effects, in what form (main effects, interactions) you are seeking to estimate;\nAnd the structure of the random effects part of the model, i.e. what grouping variable (participants? items?), whether they encompass random intercepts or random slopes or covariances.\n\nYou can report or indicate some of this information by presenting a table summary of the effects estimated in your model (e.g., see Table 5, Rodríguez-Ferreiro, Aguilera, and Davies 2020; see tables 2 and 3, Ricketts, Dawson, and Davies 2021). Journal formatting restrictions or other conventions may limit what information you can present.\nNotice that I do not present information on threshold estimates.\n\nExplain the results – I prefer to show and tell.\n\n\nPresent conditional or marginal effects plots (see figures 2 and 3, Ricketts, Dawson, and Davies 2021) to indicate the predictions you can make given your model estimates.\nAnd explain what the estimates or what the prediction plots appear to show.\n\n\n\n\n\n\nAs I hint, when we discuss the concept that ordinal responses may map somehow to a latent unobserved underlying continuum (see Figure 1), there are other ways to think about ordinal data. Rather, there are other ways to think about the psychological mechanisms or the data generating mechanisms that give rise to the ordinal responses we analyse.\nIn Ricketts, Dawson, and Davies (2021), we explain:\n\nIn the semantic post-test, participants worked their way through three steps, only progressing from one step to the next step if they provided an incorrect response or no response. Given the sequential nature of this task, we analysed data using sequential ratio ordinal models (Bürkner & Vuorre, 2019). In sequential models, we account for variation in the probability that a response falls into one response category (out of k ordered categories), equal to the probability that it did not fall into one of the foregoing categories, given the linear sum of predictors. We estimate the k-1 thresholds and the coefficients of the predictors.\n\nWhat this explanation refers to is the fact that, in our study:\n\nThe semantic post-test assessed knowledge for the meanings of newly trained words. We took a dynamic assessment or cuing hierarchy approach (Hasson & Joffe, 2007), providing children with increasing support to capture partial knowledge and the incremental nature of acquiring such knowledge (Dale, 1965). Each word was taken one at a time and children were given the op- portunity to demonstrate knowledge in three steps: definition, cued definition, recognition.\n\nWe follow advice set out by Bürkner and Vuorre (2019) in modeling the ordered categorical (i.e. ordinal) responses using a sequential ratio approach.\n\n\n\n\nYou will have noticed that the mixed-effects model coded in Section 1.8 incorporates a relatively simple random effect: a term specified to estimate the variance associated with the random effect of differences between participants in intercepts.\nAs we we have seen, more complex random effects structures may be warranted Matuschek et al. (2017). When we attempt to fit models with more complex structures, as we have discussed, for example, in ?@sec-dev-mixed-convergence-problems and ?@sec-glmm-bad-signs, we may run into convergence problems. (Such convergence problems are one reason why I tend to favour Bayesian methods; see, for exampe, the discussions in Bürkner and Vuorre (2019) and Liddell and Kruschke (2018).) There are ways to resolve these problems by changing the control parameters of the {ordinal} functions (see e.g. this discussion or see the information here) or by simplfying the model.\n\n\n\nWe discussed ordinal data and the reasons why we are motivated to analyze ordinal data using ordinal models.\nWe examine the coding required to fit ordinal models.\nWe look at the results outputs from ordinal models, and visualizations representing the predictions that can be generated given ordinal model estimates.\nWe consider the kinds of information that results reports should include.\nWe examine possible extensions to ordinal models.\n\n\nWe used two functions from the {ordinal} library to fit and evaluate ordinal models.\n\nWe used clm() to fit an ordinal model without random effects.\nWe used clmm() to fit an ordinal mixed-effects model with fixed effects and random effects.\n\n\n\n\n\nThe published example studies referred to in this chapter are published in (Ricketts, Dawson, and Davies 2021; Rodríguez-Ferreiro, Aguilera, and Davies 2020).\nLiddell and Kruschke (2018) present a clear account of the problems associated with treating ordinal data as metric, and explain how we can better account for ordinal data.\nBürkner and Vuorre (2019) present a clear tutorial on cumulative and sequential ratio models.\nBoth Liddell and Kruschke (2018) and Bürkner and Vuorre (2019) work from a Bayesian perspective but the insights are generally applicable.\nGuides to the {ordinal} model functions clm() and clmm() are presented in (Rune Haubo Bojesen Christensen 2022; R. H. B. Christensen 2015)."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-motivations",
    "href": "PSYC402/Week20.html#sec-ordinal-motivations",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "Ordinal data are very common in psychological science. Often, we will encounter ordinal data recorded as responses to Likert-style items in which the participant is asked to indicate a response on an ordered scale ranging between two end points (Bürkner and Vuorre 2019; Liddell and Kruschke 2018). An example of a Likert question item might be: How well do you think you have understood this text? (Please check one response) where the participant must respond by checking an option, given 5 options ranging from 1 (not well at all) to 5 (very well). The critical characteristics of such responses are that:\n\nThe responses are ordered, as indicated by the number labels;\nResponse types are categorical or qualitative, not numeric.\n\nWe will be working with study data in which the outcome that is the target for our analyses comprise responses to questions designed to elicit ratings. Ordinal data may, however, also derive from situations in which ordered categorical responses do not derive from ratings items (we will look briefly at sequential responses, Bürkner and Vuorre 2019).\nThe challenge we face is that we will aim to develop skills in using ordinal models when, in contrast, most psychological research articles will report analyses of ordinal data using conventional methods like ANOVA or linear regression. We will work to understand why ordinal models are better. We will learn that applying conventional methods to ordinal data will, in principle, involve a poor account of the data and, in practice, will create the risk of producing misleading results. And we will learn how to work with and interpret the results from ordinal models with or without random effects.\nIn our work in this chapter, we will rely extensively on the ideas set out by Liddell and Kruschke (2018), see Section 1.13."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-ideas",
    "href": "PSYC402/Week20.html#sec-ordinal-ideas",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "Important\n\n\n\nOrdinal responses are labelled with numbers but ordinal data are not numeric.\n\n\nOrdinal responses are coded with numeric labels. These number labels may indicate order but we do not know that the difference between e.g. response options 1 versus 2 is the same as the difference between 2 versus 3 or 3 versus 4. Ordinal data contrast with metric data (Liddell and Kruschke 2018) which are recorded on scales for which we assume both order and equal intervals. When researchers apply metric models to ordinal data, they incorrectly assume that the response options e.g. in ratings are separated by equal intervals. Yet, in a review of the 68 recently articles that mentioned the term “Likert” in a sample of highly ranked Psychology journals, Liddell and Kruschke (2018) found that ordinal data were treated as metric and the articles presented results from metric models.\nOne way to think about ordinal data is that often (but not always) ratings may be understood to come from psychological processes in which the participant, in response to the Likert question, divides some latent (unobserved) psychological continuum or scale into categories in order to select a response option.\nImagine, for example, that you have been asked the question “How well do you understand this text? (on a scale from 1-5)”. Presumably, to answer this question, you will have to choose a response based on where you think you are on your unobserved measure of your understanding. You may be able to evaluate the cohesion, or some other internal measure, of your understanding of the text. Simplifying a bit, we might assume that your internal measure of understanding is associated with a normal probability distribution so that it peaks over some value (e.g., 3) of the strength of understanding though other values are possible. As Figure 1 suggests, a participant in this common situation will have to map the internal measure (the latent scale, e.g., of understanding) to a number from the response options you are given (e.g., rating scale values ranging 1-5). But there is no reason to suppose that your internal measure of your understanding is divided into an ordered metric scale.\n\n\n\n\n\nFigure 1: A latent scale on the horizontal axis is divided into intervals or bins divided by thresholds marked by dotted lines. The cumulative normal probability in the intervals is the probability of the ordinal values.\n\n\n\n\nIn conducting analyses of ordinal data with ordinal models, we often fit models that describe the cumulative probability that a rating response is located at some value (typically, understood in terms of threshold) on an underlying latent continuum. In ordinal models, we do not assume that the ordinal responses map to equally spaced intervals on the latent scale: the values or thresholds at which the continuum are split are to be estimated.\nIn applying metric models to ordinal data, we do assume that intervals are equal though this assumption is unlikely to be true or, at least, is unlikely to be verifiable. This faulty assumption has consequences because the mis-application of metric models (e.g. ANOVA, linear models) to ordinal data is both commonplace and risky. As Liddell and Kruschke (2018) demonstrate, mis-applying metric models to ordinal data can result in false positives (detecting a difference when none is present), false negatives (missing a difference that is present) and inversions (swapping the difference so that it appears to be positive instead of negative or vice versa). These kinds of misrepresentions cannot be avoided and are not fixed by, for example, averaging ratings scales data together."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-targets",
    "href": "PSYC402/Week20.html#sec-ordinal-targets",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "Understand practically the reasons for using ordinal models when we analyze ordinal outcome variables, ?@sec-ordinal-practical-understanding.\nPractice running ordinal models with varying random effects structures.\nPractice reporting the results of ordinal models, including through the use of prediction plots."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-study-guide",
    "href": "PSYC402/Week20.html#sec-ordinal-study-guide",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "I have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Chapter: 05-ordinal\n1.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to work with ordered categorical outcomes i.e. ordinal data using ordinal models.\n1.2. The practical elements include data tidying, visualization and analysis steps.\n1.3. You can read the chapter and run the code to gain experience and encounter code you can adapt for your own purposes.\n\nRead in the example dataset.\nExperiment with the .R code used to work with the example data.\nRun ordinal models of demonstration data.\nRun ordinal models of alternate data sets (see links in Section 1.9).\nReview the recommended readings (Section 1.13).\n\n2. Practical materials\n2.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning. I set out the data tidying, analysis and visualization steps you can follow, working with the example dataset, described next."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-data",
    "href": "PSYC402/Week20.html#sec-ordinal-data",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "We will be working, at first, with a sample of data collected as part of the Clearly understood: health comprehension project (Davies, Ratajczak, Gillings, Chadwick & Gold). These data are unpublished.\n\n\n\n\nOur interest, in conducting the project, lies in identifying what factors make it easy or difficult to understand written health information. In part, we are concerned about the processes that health providers or clinicians apply to assure the effectiveness of the text they produce to guide patients or carers, for example, in taking medication, in making treatment decisions, or in order to follow therapeutic programmes.\nIt is common, in the quality assurance process in the production of health information texts, that text producers ask participants in patient review panels to evaluate draft texts. In such reviews, a participant may be asked a question like “How well do you understand this text?” This kind of question presents a metacognitive task: we are asking a participant to think about their thinking. But it is unclear that people can do this well or, indeed, what factors determine the responses to such questions (Dunlosky and Lipko 2007).\nFor these reasons, we conducted studies in which we presented adult participants with sampled health information texts (taken from health service webpages) and, critically, asked them to respond to the question:\n\nHow well do you think you have understood this text? (Please check one response)\n\nFor each text, in response to this question, participants were asked to click on one option from an array of response options ranging from (1) Not well at all to (9) Extremely well. The data we collected in this element of our studies comprise, clearly, ordinal responses. Thus, we may use these data to address the following research question.\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\n\n\n\nWe will work with a sample of participant data drawn from a series of Lancaster University undergraduate dissertation studies connected to the Clearly understood project. In these studies, we collected data from 202 participants on a series of measures (Section 1.5.1.3) of vocabulary knowledge, health literacy, reading strategy, as well as responses to health information texts. The distributions of participants’ scores on each of a range of attribute variables\n\n\n\n\n\nFigure 2: Grid of plots showing the distribution of participant attributes. The grid includes histograms of the distributions of: self-rated accuracy; vocabulary (SHIPLEY); health literacy (HLVA); reading strategy (FACTOR3); and age (years). We also see dot plots presenting counts of numbers of participants of different self-reported gender, education, and ethnicity categories.\n\n\n\n\nThe plots indicate:\n\nmost self-rated accuracy scores are high (over 6);\nmany participants with vocabulary scores greater than 30, a few present lower scores;\nhealth literacy scores centered on 8 or some, with lower and higher scores;\na skewed distribution of reading strategy scores, with many around 20-40, and a tail of higher scores;\nmost participants are 20-40 years of age, some older;\nmany more female than male participants, very few non-binary reported;\nmany more participants with higher education than further, very few with secondary;\nand many White participants (Office of National Statistics categories), far fewer Asian or Mixed or Black ethnicity participants.\n\n\n\n\nWe collected data through an online survey administered through Qualtrics.\nWe used the Shipley vocabulary sub-test (Shipley et al. 2009) to estimate vocabulary knowledge.\nWe used the Health Literacy Vocabulary Assessment (HLVA, Ratajczak, 2020; adapted for online presentation, Chadwick, 2020) to estimate health literacy.\nWe used an instrument drawn from unpublished work by Calloway (2019) to assess the approach participants took to reading and understanding written information.\nWe presented participants with a sample of 20 health information texts. In the data collection process for this dataset, participants were recruited in multiple different studies. In each study, any one participant was presented with a randomly selected subset of the total of 20 texts.\nWe asked participants to rate their level of understanding of the health-related texts that we presented in the study. We used a nine-point judgment scales because they have been found to outperform alternative scales with fewer categories in terms of criterion validity, internal consistency, test-retest reliability, and discriminating power (Preston and Colman 2000).\nWe recorded participants’ demographic characteristics: gender (coded: Male, Female, non-binary, prefer not to say); education (coded: Secondary, Further, Higher); and ethnicity (coded: White, Black, Asian, Mixed, Other).\n\n\n\n\nYou can download the 2021-22_PSYC304-health-comprehension.csv file holding the data we analyse in this chapter by clicking on the link.\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read the data file into R.\n\nhealth &lt;- read_csv(\"2021-22_PSYC304-health-comprehension.csv\", \n                                 na = \"-999\",\n                                 col_types = cols(\n                                   ResponseId = col_factor(),\n                                   rating = col_factor(),\n                                   GENDER = col_factor(),\n                                   EDUCATION = col_factor(),\n                                   ETHNICITY = col_factor(),\n                                   NATIVE.LANGUAGE = col_factor(),\n                                   OTHER.LANGUAGE = col_factor(),\n                                   text.id = col_factor(),\n                                   text.question.id = col_factor(),\n                                   study = col_factor()\n                                 )\n                               )\n\nNotice that we use col_types = cols(...) to require read_csv() to class some columns as factors.\nImportantly, we ask R to treat the rating variable as a factor with rating = col_factor().\n\n\n\n\n\n\nTip\n\n\n\nIn the practical work we do, we will be using functions from the {ordinal} library to model ordinal data.\n\nIn using these functions, we need ask R to treat the ordinal outcome variable as a factor.\n\n\n\n\n\n\nIt is always a good to inspect what you have got when you read a data file in to R. Here, what may most concern us is the distribution of observed responses on the rating scale (responses to the “How well do you understand?” question). Figure 3 is a dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.\n\nhealth &lt;- health %&gt;% mutate(rating = fct_relevel(rating, sort))\n\nhealth %&gt;%\n  group_by(rating) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = rating, y = count, colour = rating)) + \n  geom_point(size = 3) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank()  # No horizontal grid lines\n  ) +\n  coord_flip()\n\n\n\n\nFigure 3: Dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.\n\n\n\n\nThe plot indicates that most participants chose response options 5-9, while very few rated their understanding at the lowest levels (options 1-4). Interestingly, many ratings responses around 7-8 were recorded: many more than responses at 5-6.\nIn analyzing these data, we will seek to estimate what information available to us can be used to predict whether a participant’s rating of their understanding is more likely to be, say, 1 or 2, 2 or 3 … 7 or 8, 8 or 9.\n\nOne practical way to think about the estimation problem when working with ratings-style ordinal data is this:\n\nWhat factors move or how do influential factors move the probability that the ordinal response is a relatively low or relatively high order response option?\nIn doing this, we do not have to assume that rating scale points map to equal sized intervals on the underlying latent scale where the scale may be an unobserved psychological continuum (like understanding).\n\n\nHere, we mostly have information on participant attributes and some information on text properties to do our prediction analyses. In other studies, we may be using information about experimental conditions, or selected groups of participants to estimate effects on variation in ratings responses."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-data-tidy",
    "href": "PSYC402/Week20.html#sec-ordinal-data-tidy",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "The Clearly understood health comprehension project dataset is tidy (?@sec-intro-mixed-data-tidy):\n\nEach variable has its own column.\nEach observation has its own row.\nEach value has its own cell.\n\nHowever, there are aspects of the data structure or properties of the dataset variables that will cause inefficiencies or problems in later data analysis if we do not fix them first.\nYou can see what we have if you look at the results we get from using summary() and str() to inspect the dataset.\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n\n\nYou should be used to seeing the summary() of a dataset, showing summary statistics of numeric variables and counts of the numbers of observations of data coded at different levels for each categorical or nominal variable classed as a factor.\nUsing the str() function may be new to you and, as you can see, the output from the function call gives you a bit more information on how R interprets the data in the variable columns. You can see that each variable is listed alongside information about how the data in the column are interpreted (as Factor or num numeric). Where we have columns holding information on factors there we see information about the levels.\nRecall that for a categorical or nominal variable e.g. ETHNICITY, provided R interprets the variable as a factor, each data value in the column is coded as corresponding to one level i.e. group or class or category (e.g., we have ETHNICITY classes \"Asian\" etc.) Recall, also, that at the data read-in stage, we instructed R how we wanted it to interpret each column using col_types = cols().\n\nstr(health)\n\ntibble [4,040 × 17] (S3: tbl_df/tbl/data.frame)\n $ ResponseId         : Factor w/ 202 levels \"R_sKW4OJnOlidPxrH\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ AGE                : num [1:4040] 20 20 20 20 20 20 20 20 20 20 ...\n $ GENDER             : Factor w/ 3 levels \"Female\",\"Male\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ EDUCATION          : Factor w/ 3 levels \"Further\",\"Higher\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ ETHNICITY          : Factor w/ 4 levels \"Asian\",\"White\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ NATIVE.LANGUAGE    : Factor w/ 2 levels \"English\",\"Other\": 1 1 1 1 1 1 1 1 1 1 ...\n $ OTHER.LANGUAGE     : Factor w/ 17 levels \"NA\",\"Catonese\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ ENGLISH.PROFICIENCY: chr [1:4040] \"NA\" \"NA\" \"NA\" \"NA\" ...\n $ SHIPLEY            : num [1:4040] 26 26 26 26 26 26 26 26 26 26 ...\n $ HLVA               : num [1:4040] 8 8 8 8 8 8 8 8 8 8 ...\n $ FACTOR3            : num [1:4040] 59 59 59 59 59 59 59 59 59 59 ...\n $ rating             : Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n $ response           : num [1:4040] 1 1 1 1 1 1 1 0 1 1 ...\n $ RDFKGL             : num [1:4040] 10.61 10.61 10.61 10.61 8.12 ...\n $ study              : Factor w/ 4 levels \"cs\",\"jg\",\"ml\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ text.id            : Factor w/ 20 levels \"studyone.TEXT.105\",..: 1 1 1 1 2 2 2 2 3 3 ...\n $ text.question.id   : Factor w/ 80 levels \"studyone.TEXT.105.CQ.1\",..: 1 2 3 4 5 6 7 8 9 10 ...\n\n\nOur specific concern, here, is that the rating response variable is treated as a factor because the {ordinal} library we are going to use to do the modeling must find the outcome variable is a factor.\nWe can focus str() on the rating variable. We see that it is being treated as a factor.\n\nstr(health$rating)\n\n Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n\n\nHowever, we also need to make sure that the rating outcome variable is being treated as an ordered factor.\nWe can perform a check as follows. (I found how to do this here)\n\nis.ordered(factor(health$rating))\n\n[1] FALSE\n\n\nWe can see that the variable is not being treated as an ordered factor. We need to fix that.\nThe ordinal model estimates the locations (thresholds) for where to split the latent scale (the continuum underlying the ratings) corresponding to different ratings values. If we do not make sure that the outcome factor variable is split as it should be then there is no guarantee that {ordinal} functions will estimate the thresholds in the right order (i.e., 1,2,3 ... rather than 3,2,1...).\nWe can make sure that the confidence rating factor is ordered precisely as we wish using the ordered() function.\n\nhealth$rating &lt;- ordered(health$rating,\n                         levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"))\n\nWe can then do a check to see that we have got what we want. We do not wantrating to be treated as numeric, we do want it to be treated as an ordered factor.\n\nis.numeric(health$rating)\n\n[1] FALSE\n\nis.factor(health$rating)\n\n[1] TRUE\n\nstr(health$rating)\n\n Ord.factor w/ 9 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 8 8 8 8 7 7 7 7 7 7 ...\n\nis.ordered(health$rating)\n\n[1] TRUE\n\n\nIt is.\nNext, before doing any modelling, it will be sensible to standardize potential predictors\n\nhealth &lt;- health %&gt;% \n  mutate(across(c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL), \n                scale, center = TRUE, scale = TRUE,\n                .names = \"z_{.col}\"))\n\nYou can see that in this chunk of code, we are doing a number of things:\n\nhealth &lt;- health %&gt;% recreates the health dataset from the following steps.\nmutate(...) do an operation which retains the existing variables in the dataset, to change the variables as further detailed.\nacross(...) work with the multple column variables that are named in the c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL) set.\n...scale, center = TRUE, scale = TRUE... here is where we do the standardization work.\n\nWhat we are asking for is that R takes the variables we name and standardizes each of them.\n\n.names = \"z_{.col}\") creates the standardized variables under adapted names, adding z_ to the original column name so that we can distinguish between the standardized and original raw versions of the data columns.\n\nNote that the across() function is a useful function for applying a function across multiple column variables see information here There is a helpful discussion on how we can do this task here\nWe can then check that we have produced the standardized variables as required.\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id       z_AGE.V1      \n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86    Min.   :-0.9826247  \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86    1st Qu.:-0.8620352  \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86    Median :-0.4399723  \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86    Mean   : 0.0000000  \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86    3rd Qu.: 0.9468060  \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86    Max.   : 2.8159420  \n (Other)         :2096   (Other)              :3524                        \n    z_SHIPLEY.V1          z_HLVA.V1          z_FACTOR3.V1    \n Min.   :-3.294105   Min.   :-2.6074887   Min.   :-4.205936  \n 1st Qu.:-0.543723   1st Qu.:-0.7330662   1st Qu.:-0.529155  \n Median : 0.189713   Median : 0.2041450   Median :-0.003900  \n Mean   : 0.000000   Mean   : 0.0000000   Mean   : 0.000000  \n 3rd Qu.: 0.739789   3rd Qu.: 0.6727506   3rd Qu.: 0.783981  \n Max.   : 1.289866   Max.   : 2.0785675   Max.   : 1.834490  \n                                                             \n     z_RDFKGL.V1     \n Min.   :-1.4644660  \n 1st Qu.:-0.6814594  \n Median : 0.0807363  \n Mean   : 0.0000000  \n 3rd Qu.: 0.6430616  \n Max.   : 2.3187650"
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-working-models-clm",
    "href": "PSYC402/Week20.html#sec-ordinal-working-models-clm",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "In our first analysis, we can begin by assuming no random effects. We keep things simple at this point so that we can focus on the key changes in model coding.\nThe model is conducted to examine what shapes the variation in rating responses that we see in Figure 3.\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\nIn our analysis, the outcome variable is the ordinal response variable rating. The predictors consist of the variables we standardized earlier. We use the clm() function from the {ordinal} library to do the analysis. I will give information in outline here, the interested reader can see more detailed information in Rune Haubo Bojesen Christensen (2022) and R. H. B. Christensen (2015). You can also find the manual for the {ordinal} library functions here\nWe code the model as follows.\n\nhealth.clm &lt;- clm(rating ~\n                    \n                    z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n                  Hess = TRUE, link = \"logit\",\n                  data = health)\n\nsummary(health.clm)\n\nThe code works as follows.\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\nclm() the function name changes because now we want a cumulative link model of the ordinal responses.\n\nThe model specification includes information about the fixed effects, the predictors: z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL.\nSecond, we have the bit that is specific to cumulative link models fitted using the clm() function.\n\nHess = TRUE is required if we want to get a summary of the model fit; the default is TRUE but it is worth being explicit about it.\nlink = \"logit\" specifies that we want to model the ordinal responses in terms of the log odds (hence, the probability) that a response is a low or a high rating value (compare ?@sec-glmm-practical-understanding).\n\n\n\nIf you run the model code, it may take a few seconds to run. Then you will get the results shown in the output.\n\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6880.78 13787.55 5(0)  9.26e-07 7.3e+01\n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.17719    0.02966  -5.975 2.30e-09 ***\nz_SHIPLEY  0.34384    0.03393  10.135  &lt; 2e-16 ***\nz_HLVA     0.16174    0.03265   4.954 7.28e-07 ***\nz_FACTOR3  0.74535    0.03145  23.699  &lt; 2e-16 ***\nz_RDFKGL  -0.27220    0.02892  -9.412  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.65066    0.12978 -35.836\n2|3 -4.13902    0.10395 -39.817\n3|4 -3.26845    0.07390 -44.228\n4|5 -2.56826    0.05804 -44.248\n5|6 -1.69333    0.04437 -38.160\n6|7 -0.87651    0.03671 -23.876\n7|8  0.24214    0.03402   7.117\n8|9  1.63049    0.04252  38.346\n\n\nThe summary() output for the model is similar to the outputs you have seen for other model types.\n\nWe first get formula: information about the model you have specified.\nR will tell us what data: we are working with.\nWe then get Coefficients: estimates.\n\nThe table summary of coefficients arranges information in ways that will be familiar you:\n\nFor each predictor variable, we see ’Estimate, Std. Error, z value, and Pr(&gt;|z|)` statistics.\nThe Pr(&gt;|z|) p-values are based on Wald tests of the null hypothesis that a predictor has null impact.\nThe coefficient estimates can be interpreted based on whether they are positive or negative.\n\nA positive coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of higher rating values. A negative coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of lower rating values.\n\nWe then get Threshold coefficients: indicating where the model fitted estimates the threshold locations: where the latent scale is cut, corresponding to different rating values.\n\n\n\n\n\n\n\nTip\n\n\n\nIn reporting ordinal (e.g., cumulative link) models, we typically focus on the coefficient estimates for the predictor variables."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-working-models-clmm",
    "href": "PSYC402/Week20.html#sec-ordinal-working-models-clmm",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "In our analysis, we begn by assuming no random effects. However, this is unlikely to be appropriate given the data collection process deployed in the Clearly understood projects, where:\n\na sample of participants were asked to respond to a sample of texts;\nwe have multiple observations of responses for each participant;\nwe have multiple observations of responses for each stimulus text;\nparticipants were assigned to groups, and within a group all participants were asked to respond to the same stimulus texts.\n\nThese features ensure that the data have a multilevel structure and this structure requires us to fit a Cumulative Link Mixed-effects Model (CLMM).\nWe keep things simple at this point so that we can focus on the key changes in model coding. We can code a Cumulative Link Mixed-effects Model as follows.\n\nhealth.clmm &lt;- clmm(rating ~\n                      \n                      z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +\n                      \n                      (1|ResponseId),\n                    \n                    Hess = TRUE, link = \"logit\",\n                    data = health)\n\nsummary(health.clmm)\n\nIf you inspect the code chunk, you can see that we have made two changes.\nFirst, we have changed the function.\n\nclmm() the function name changes because now we want a Cumulative Linear Mixed-effects Model.\n\nSecondly, the model specification includes information about fixed effects and now about random effects.\n\nWith (1 | Participant) we include random effects of participants on on intercepts.\n\n\n\nIf you run the model code, you will see that the model may take several seconds, possibly a minute or two to complete. We will then get the results shown in the output.\n\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +  \n    (1 | ResponseId)\ndata:    health\n\n link  threshold nobs logLik   AIC     niter       max.grad cond.H \n logit flexible  4040 -4978.08 9984.16 1480(19002) 3.93e-03 3.5e+02\n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ResponseId (Intercept) 9.825    3.134   \nNumber of groups:  ResponseId 202 \n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.44313    0.23483  -1.887  0.05916 .  \nz_SHIPLEY  0.77130    0.26514   2.909  0.00363 ** \nz_HLVA     0.20809    0.25617   0.812  0.41663    \nz_FACTOR3  1.68342    0.23836   7.063 1.63e-12 ***\nz_RDFKGL  -0.44345    0.03677 -12.059  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -9.3297     0.3453 -27.019\n2|3  -8.1413     0.2948 -27.618\n3|4  -6.5243     0.2632 -24.786\n4|5  -5.2159     0.2491 -20.938\n5|6  -3.4668     0.2371 -14.623\n6|7  -1.8481     0.2316  -7.981\n7|8   0.2965     0.2296   1.292\n8|9   3.0417     0.2347  12.959\n\n\nYou can see that the output summary presents the same structure. If you compare the output you see in Section 1.7.1, however, you will notice some similarities and some differences:\n\nIf you focus first on the estimates of the coefficients for the predictor variables, you will see that the estimates have the same sign (positive or negative) as they had before.\nHowever, you will see that the estimates have different magnitudes.\nYou will also see that the p-values are different.\n\nStudents often focus on p-values in reading model summaries. This is mistaken for multiple reasons. The p-values correspond to the probabilities associated with the null hypothesis significance test: the test of the hypothesis that the effect of the predictor is null (i.e. the predictor has no impact). This null assumption is made whichever model we are looking at. The p-values do not indicate whether an effect is more or less probable. But you do get such posterior probabilities in Bayesian analyses. So it does not really mean much, though it is common, to talk about effects being highly significant. Thus it should not worry us too much if the p-values are significant in one analysis but not significant in another.\nThat said, it is interesting, perhaps, that once we include random effects of participants on intercepts in our analysis then the effects of z_AGE and z_HLVA are no longer significant. I would be tempted to ask if the previously significant effects of these variables owed their impact to random differences between participants in their average or overall level of rating response.\n\n\n\nIt will be helpful for the interpretation of the estimates of the coefficients of these predictor variables if we visualize the predictions we can make, about how rating values vary, given differences in predictor variable values, given our model estimates. We can do this using functions from the {ggeffects} library. You can read more about the {ggeffects} library here where you will see a collection of articles explaining what you can do, and why, as well as technical information including some helpful tutorials.\nThe basic model prediction coding looks like this.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n\nFigure 4 shows you the marginal effect of variation in the reading strategy attribute, i.e., the effect of differences between individuals in how they score on the FACTOR3 measure of reading strategy. Note that the variable is listed as z_FACTOR3 because, as you will recall, we standardized numeric predictor variables before entering them in our model.\nThese kinds of plots are understood to present what are variously called conditional effects, or adjusted predictions or marginal effects. You can find a discussion of marginal effects in the context of working with the {ggeffects} library here and here. You can find an extensive, helpful (with examples) discussion of marginal effects by Andrew Heiss here.\nIn short, what we want to do is to take the model coefficient estimates, and generate predictions with these estimates, given different values of the predictor variable, while holding the other predictor variables at some constant or some level (or some series of values).\nIf you look at the code chunk, you can see that we first:\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\n\n\nIn this line, we use ggpredict() to work with some model information, assuming we previously fitted a model and gave it a name (here, health.clmm).\n\nNote that if you fit the model and call it health.clmm, as we did in Section 1.8, then an object of that name is created in the R workspace or environment. If you click on that object name in the environment window in R-Studio, you will see that there is a list of pieces of information about the model, including the coefficient estimates, the model formula etc. associated with that name.\n\nSo when we use ggpredict(), we ask R to take that model information and, for the term we specify, here, specify using terms=\"z_FACTOR3 [all]\", we ask R to generate some predictions.\ndat &lt;- ggpredict(...) asks R to put those predictions in an object called dat.\n\nIf you click on that object name in the environment window in R-Studio, you will see that it comprises a dataset. The dataset includes the columns:\n\nx giving different values of the predictor variable. ggpredict() will choose some ‘representative’ values for you but you can construct a set of values of the predictor for which you want predictions.\npredicted holds predicted values, given different predictor x values.\n\nIf you then run the line plot(dat) you can see what this gets us for these kinds of models. Figure 4 presents a grid of plots showing the model-predicted probabilities that a rating response will have one value for each of the 1-9 rating response values that are possible given the Likert rating scale used in data collection. In the grid, a different plot is shown for each possible response value, indicating how the probability varies that the rating response will take that value.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n\n\n\n\nFigure 4: A grid of plots showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable.\n\n\n\n\nIf you examine Figure 4, you can recognize that we have one plot for each different value of the response options available for the Likert-scale rating items: 1-9. You can also see that in each plot we get a curve. In some cases – for rating response values 1-4 – the curve is flat or flattens very quickly, for higher levels of the z_FACTOR3 variable. In some cases – for rating response values 5-9 – the curve is more obvious, and resembles a normal distribution curve.\nIf you think about it, what these plots indicate are the ways in which the probability that a rating response is a low value (e.g., a rating of 1) or a high value (e.g., a rating of 9) rises or falls. Each possible rating response is associated with a probability distribution. For example, look at the plot labelled 6: that shows you the probability distribution indicating how the probability varies that a response will take the value 6. We can see that the distribution is normal in shape, a bell-shaped curve. We can see that the peak of the curve is over the z_FACTOR3 score (shown on the x-axis) of about 1.5. We can see that the probability represented by the height of the line showing the curve is lower for z_FACTOR3 scores lower than the score under the peak (e.g. scores less than z_FACTOR3 \\(=2\\)). The probability represented by the height of the line showing the curve is lower for z_FACTOR3 scores higher than the score under the peak (e.g. scores greater than z_FACTOR3 \\(=1\\)).\nWe can see that the peak of the normal curve, in the case of rating response values 5-9, is located at different places on the horizontal axis. Look at each of the plots labelled 5-9. Notice how the horizontal location of the curves shifts as z_FACTOR3 scores increase. If you go from left to right, i.e. from low to high values of z_FACTOR3, on each plot then you will see that the peak of the curve is located in different places: going from plot 5 to plot 9 the peak of the curve moves rightwards. These curves show how the probability that a rating response takes a high value (e.g. 9 instead of 8 or 8 instead of 7 etc.) is higher for higher values of z_FACTOR3. This idea might be a bit clearer if we draw the plot in a different way.\nFigure 5 shows the same model predictions but plots the predictions of the way that probability changes, for each rating response, by superimposing the plots for each response value, one on top of the other. I have drawn each probability curve in a different colour, and these colours match those used to present the counts of different response values shown in Figure 3.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nggplot(dat, aes(x, predicted, \n                colour = response.level)) + \n  geom_line(size = 1.5) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + \n  labs(x = \"Reading strategy (z_FACTOR3)\", y = \"Predicted probability of a rating\") +\n  guides(colour = guide_legend(title = \"Rating\")) +\n  ylim(0, 1) +\n  theme_bw()\n\n\n\n\nFigure 5: A plot showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable\n\n\n\n\nYou can read Figure 5 by observing that:\n\nFor low value ratings e.g. for rating responses from 1-4, there is not much predicted probability that a response with such a value will be made (flat lines) but if they are going to be made they are likely to be made by people with low scores on the z_FACTOR3.\n\nYou can see this because you can see how the curves peak around low values of z_FACTOR3. This should make sense: people with low scores on reading strategy are maybe not doing reading effectively, are maybe as a result not doing well in understanding the texts they are given to read, and thus are not confident about their understanding. (This is a speculative causal theory but it will suffice for now.)\nRecall, also, that as Figure 3 indicated, in the Clearly understood health comprehension dataset, we saw that few rating responses were recorded for low value ratings of understanding. Few people in our sample made rating responses by choosing ratings of 1 or 2 to indicate low levels of understanding.\nFigure 5 also suggests that:\n\nFor higher value rating responses – responses representing ratings from 5 to 9 – there is variation in the probability that responses with such values will be made.\nThat variation in probability is shown by the probability distribution curves.\nFor these data, and this model, we can see that the probability shifts suggesting that participants in our sample were more likely to choose a higher value rating if they were also presenting high scores on the z_FACTOR3 measure of reading strategy."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-reporting-results",
    "href": "PSYC402/Week20.html#sec-ordinal-reporting-results",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "As the review reported by Liddell and Kruschke (2018) suggests, we may have many many studies in which ordinal outcome data are analysed but very few published research reports that present analyses of ordinal data using ordinal models.\nYou can see two examples in the papers published by Ricketts, Dawson, and Davies (2021) and by Rodríguez-Ferreiro, Aguilera, and Davies (2020). These papers are both published open accessible, so that they are freely available, and they are both associated with accessible data repositories.\n\nYou can find the repository for Ricketts, Dawson, and Davies (2021) here.\nYou can find the repository for Rodríguez-Ferreiro, Aguilera, and Davies (2020) here.\n\nThe Rodríguez-Ferreiro, Aguilera, and Davies (2020) shares a data .csv only.\nThe Ricketts, Dawson, and Davies (2021) repository shares data and analysis code as well as a fairly detailed guide to the analysis methods. Note that the core analysis approach taken in Ricketts, Dawson, and Davies (2021) is based on Bayesian methods but that we also conduct clmm() models using the {ordinal} library functions discussed here; these models are labelled frequentist models and can be found under sensitivity analyses.\nFor what it’s worth, the Ricketts, Dawson, and Davies (2021) is much more representative of the analysis approach I would recommend now.\nWhatever the specifics of your research question, dataset, analysis approach or model choices, I would recommend the following for your results report.\n\nExplain the model – the advice extended by Meteyard and Davies (2020) still apply: the reader will need to know:\n\n\nThe identity of the outcome and predictor variables;\nThe reason why you are using an ordinal approach, explaining the ordinal (ordered, categorical) nature of the outcome;\nThe structure of the fixed effects part of the model, i.e. the effects, in what form (main effects, interactions) you are seeking to estimate;\nAnd the structure of the random effects part of the model, i.e. what grouping variable (participants? items?), whether they encompass random intercepts or random slopes or covariances.\n\nYou can report or indicate some of this information by presenting a table summary of the effects estimated in your model (e.g., see Table 5, Rodríguez-Ferreiro, Aguilera, and Davies 2020; see tables 2 and 3, Ricketts, Dawson, and Davies 2021). Journal formatting restrictions or other conventions may limit what information you can present.\nNotice that I do not present information on threshold estimates.\n\nExplain the results – I prefer to show and tell.\n\n\nPresent conditional or marginal effects plots (see figures 2 and 3, Ricketts, Dawson, and Davies 2021) to indicate the predictions you can make given your model estimates.\nAnd explain what the estimates or what the prediction plots appear to show."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-extensions",
    "href": "PSYC402/Week20.html#sec-ordinal-extensions",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "As I hint, when we discuss the concept that ordinal responses may map somehow to a latent unobserved underlying continuum (see Figure 1), there are other ways to think about ordinal data. Rather, there are other ways to think about the psychological mechanisms or the data generating mechanisms that give rise to the ordinal responses we analyse.\nIn Ricketts, Dawson, and Davies (2021), we explain:\n\nIn the semantic post-test, participants worked their way through three steps, only progressing from one step to the next step if they provided an incorrect response or no response. Given the sequential nature of this task, we analysed data using sequential ratio ordinal models (Bürkner & Vuorre, 2019). In sequential models, we account for variation in the probability that a response falls into one response category (out of k ordered categories), equal to the probability that it did not fall into one of the foregoing categories, given the linear sum of predictors. We estimate the k-1 thresholds and the coefficients of the predictors.\n\nWhat this explanation refers to is the fact that, in our study:\n\nThe semantic post-test assessed knowledge for the meanings of newly trained words. We took a dynamic assessment or cuing hierarchy approach (Hasson & Joffe, 2007), providing children with increasing support to capture partial knowledge and the incremental nature of acquiring such knowledge (Dale, 1965). Each word was taken one at a time and children were given the op- portunity to demonstrate knowledge in three steps: definition, cued definition, recognition.\n\nWe follow advice set out by Bürkner and Vuorre (2019) in modeling the ordered categorical (i.e. ordinal) responses using a sequential ratio approach."
  },
  {
    "objectID": "PSYC402/Week20.html#richly-parameterized-mixed-effects-models",
    "href": "PSYC402/Week20.html#richly-parameterized-mixed-effects-models",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "You will have noticed that the mixed-effects model coded in Section 1.8 incorporates a relatively simple random effect: a term specified to estimate the variance associated with the random effect of differences between participants in intercepts.\nAs we we have seen, more complex random effects structures may be warranted Matuschek et al. (2017). When we attempt to fit models with more complex structures, as we have discussed, for example, in ?@sec-dev-mixed-convergence-problems and ?@sec-glmm-bad-signs, we may run into convergence problems. (Such convergence problems are one reason why I tend to favour Bayesian methods; see, for exampe, the discussions in Bürkner and Vuorre (2019) and Liddell and Kruschke (2018).) There are ways to resolve these problems by changing the control parameters of the {ordinal} functions (see e.g. this discussion or see the information here) or by simplfying the model."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-summary",
    "href": "PSYC402/Week20.html#sec-ordinal-summary",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "We discussed ordinal data and the reasons why we are motivated to analyze ordinal data using ordinal models.\nWe examine the coding required to fit ordinal models.\nWe look at the results outputs from ordinal models, and visualizations representing the predictions that can be generated given ordinal model estimates.\nWe consider the kinds of information that results reports should include.\nWe examine possible extensions to ordinal models.\n\n\nWe used two functions from the {ordinal} library to fit and evaluate ordinal models.\n\nWe used clm() to fit an ordinal model without random effects.\nWe used clmm() to fit an ordinal mixed-effects model with fixed effects and random effects."
  },
  {
    "objectID": "PSYC402/Week20.html#sec-ordinal-recommended-reading",
    "href": "PSYC402/Week20.html#sec-ordinal-recommended-reading",
    "title": "Introduction to Ordinal Models",
    "section": "",
    "text": "The published example studies referred to in this chapter are published in (Ricketts, Dawson, and Davies 2021; Rodríguez-Ferreiro, Aguilera, and Davies 2020).\nLiddell and Kruschke (2018) present a clear account of the problems associated with treating ordinal data as metric, and explain how we can better account for ordinal data.\nBürkner and Vuorre (2019) present a clear tutorial on cumulative and sequential ratio models.\nBoth Liddell and Kruschke (2018) and Bürkner and Vuorre (2019) work from a Bayesian perspective but the insights are generally applicable.\nGuides to the {ordinal} model functions clm() and clmm() are presented in (Rune Haubo Bojesen Christensen 2022; R. H. B. Christensen 2015)."
  },
  {
    "objectID": "PSYC402/index.html",
    "href": "PSYC402/index.html",
    "title": "Analysing and Interpreting Psychological Data I",
    "section": "",
    "text": "Welcome\nThis is filler text introducing students to the course in general. Perhaps what’s expected of them, the course aims, and other information of use.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam in est non nisi eleifend vulputate a et magna. Mauris vulputate felis lacus, ut bibendum lectus sollicitudin vel. Integer vestibulum arcu et risus vestibulum, in ullamcorper magna consectetur. Donec ut tempus enim, at vulputate libero. Integer porta metus eget elit cursus, sit amet hendrerit ipsum facilisis. Maecenas mollis, elit gravida ornare tempus, est lectus mattis velit, et commodo nunc lectus eu enim. Aenean luctus, felis ac sodales accumsan, dolor nunc euismod lorem, vel vulputate ipsum orci quis ipsum. Quisque placerat, velit vitae dictum feugiat, lectus lorem rutrum ligula, at fringilla ex enim consectetur felis. Aliquam erat volutpat. Nunc a nisi eget ex ornare dictum.\n\n\nCourse Contacts\n\n\n\n\nEmail Address\n\n\n\n\nTom Beesley\nt.beesley at lancaster dot ac dot uk\n\n\nJohn Towse\nj.towse at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC402/Week12.html",
    "href": "PSYC402/Week12.html",
    "title": "Categorical predictors",
    "section": "",
    "text": "So far, all the predictors in the models we’ve looked at were continuous variables. What if you wanted to know whether a response differed between two or more discrete groups? Hang on, you might say, that sounds like doing an ANOVA. True, you might have used ANOVA to assess whether group means differed in previous stats courses. ANOVAs—to some degree—are just a special type of regression where you have categorical predictors. This week we’ll look at how to model responses as a function of categorical predictors and we’ll combine categorical predictors to model how a predictor might affect the outcome variable differently across two different groups. For example, we might be interested in whether the amount of time adolescents use digital devices (screen-time) predicts their well-being. Additionally, we might want to know whether well-being is different for adolescent boys and girls and whether the relationship between screen-time and well-being differs for these two groups. By fitting a regression model in which we combine a continuous (screen-time) and a categorical (sex) predictor, we can do exactly that. We’ll be working on that in the lab."
  },
  {
    "objectID": "PSYC402/Week12.html#lectures",
    "href": "PSYC402/Week12.html#lectures",
    "title": "Categorical predictors",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week follows the recommended chapters in Winter (2020) – see under ‘Reading’ below – and is presented in two parts:\n\nCategorical predictors (~17 min)\nInteractions (~18 min)"
  },
  {
    "objectID": "PSYC402/Week12.html#reading",
    "href": "PSYC402/Week12.html#reading",
    "title": "Categorical predictors",
    "section": "Reading",
    "text": "Reading\n\nBlogpost by Professor Dorothy Bishop\nIn this very short blogpost Professor Dorothy Bishop explains the links between ANOVA and Regression.\n\n\nWinter (2020)\nLink\nChapter 7 provides an excellent overview of using categorical predictors in regression models and explains how this is implemented in R.\nChapter 8 explains what interactions are and how to model and interpret them."
  },
  {
    "objectID": "PSYC402/Week12.html#pre-lab-activities",
    "href": "PSYC402/Week12.html#pre-lab-activities",
    "title": "Categorical predictors",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Data-wrangling in R\nThe more you practise coding in R, the easier it will become. The RStudio interactive tutorials I mentioned last week are an excellent place to start if you haven’t engaged with those yet.\n\nThe Basics Start here to learn how to inspect, visualize, subset and transform your data, as well as how to run code.\nWork with Data Learn how to extract values form a table, subset tables, calculate summary statistics, and derive new variables.\nVisualize Data Learn how to use ggplot2 to make any type of plot with your data. The tutorials on Exploratory Data Analysis and Scatterplots are particularly relevant.\n\nIf you feel confident with the material covered in those tutorials the following are useful to try:\n\nSeparating and Uniting Columns Here you will learn to separate a column into multiple columns and to reverse the process by uniting multiple columns into a single column. Then you’ll practise your data wrangling skills on messy real world data.\nJoin Data Sets Learn how to work with relational data. Here you will learn how to augment data sets with information from related data sets, as well as how to filter one data set against another.\n\nPlease note that there are often different ways to do the same or similar things in R. This means you might encounter slightly different functions or styles of coding in different materials. This is not something to worry about. Just make sure you’re clear on what a bit of code achieves and choose the function/style that you feel most comfortable with.\n\n\nPre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 402_week12_forStudents.zip file.\n\n\nRemind yourself of how to access and work with the RStudio Server.\n\nSign in to the RStudio Server, using the login details provided to you via email. Note that when you are not on campus you need to log into the VPN first (look on the portal if you need more information about that).\nCreate a new folder for this week’s work.\nUpload the zip-file to the folder you have created on the RStudio server. Note you can either upload a single file or a zip-file, not a folder with multiple files.\nI highly recommend using R Projects to structure your workflow. You could create an R project for each week of the module. Have a look at section 8 Workflow: projects of R for Data Science by Hadley Wickam and Gareth Grolemund for an introduction."
  },
  {
    "objectID": "PSYC402/Week12.html#lab-activities",
    "href": "PSYC402/Week12.html#lab-activities",
    "title": "Categorical predictors",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nwhen and why to apply multiple regression to answer questions in psychological science\nconducting multiple regression in R when combining continuous and categorical predictors\ninterpreting the R output of multiple linear regression (when combining continuous and categorical predictors)\nreporting results for multiple linear regression (when combining continuous and categorical predictors), following APA guidelines\n\n\nLab activity 1: Combining a continuous and a categorical predictor in a resssion model\n\nBackground: Smartphone screen-time and well-being\nThere is currently much debate (and hype) surrounding smartphones and their effects on well-being, especially with regard to children and teenagers. We’ll be looking at data from this recent study of English adolescents: Przybylski, A. & Weinstein, N. (2017). A Large-Scale Test of the Goldilocks Hypothesis. Psychological Science, 28, 204–215.\nThis was a large-scale study that found support for the “Goldilocks” hypothesis among adolescents: that there is a “just right” amount of screen-time, such that any amount more or less than this amount is associated with lower well-being. This was a huge survey study with data containing responses from over 120,000 participants! Fortunately, the authors made the data from this study openly available, which allows us to dig deeper into their results. And the question we want to expand on in this lab is whether the relationship between screen-time and well-being depends on the partcipant’s (self-reported) sex. In other words, our research question is: Does screen-time have a bigger impact on boys or girls, or is it the same for both?\nThe dependent measure used in the study was the Warwick-Edinburgh Mental Well-Being Scale (WEMWBS). This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70.\nOn Przybylski & Weinstein’s page for this study on the Open Science Framework, you can find the participant survey, which asks a large number of additional questions (see page 14 for the WEMWBS questions and pages 4-5 for the questions about screen-time). Within the same page you can also find the raw data, which some of you might want to consider using for your research report.\nHowever, for the purpose of this lab, you will be using local pre-processed copies of the data (participant_info.csv, screen_time.csv and `wellbeing.csv, which you downloaded as part of the ‘Pre-lab activities’.\nPrzybylski and Weinstein looked at multiple measures of screen-time, but again for the interests of this lab we will be focusing on smartphone use, but do feel free to expand your skills after by looking at different definitions of screen-time. Overall, Przybylski and Weinstein suggested that decrements in well-being started to appear when respondents reported more than one hour of daily smartphone use. So, bringing it back to our additional variable of sex, our research question is now: Does the negative association between hours of smartphone use and well-being (beyond the one-hour point) differ for boys and girls?\nLet’s think about this in terms of the variables. We have:\n\na continuous outcome variable: well-being;\na continuous∗ predictor variable: screen-time;\na categorical predictor variable: sex.\n\nPlease note that well-being and screen-time are technically only quasi-continuous inasmuch as that only discrete values are possible. However, there are a sufficient number of discrete categories in our data that we can treat the data as effectively continuous.\nNow, in terms of analysis, what we are effectively trying to do is to estimate two slopes relating screen-time to well-being, one for adolescent girls and one for adolescent boys, and then statistically compare these slopes. Sort of like running a correlation for boys, a correlation for girls, and comparing the two. Or alternatively, where you would run a regression (to estimate the slopes) but also one where you would need a t-test (to compare two groups). But the expressive power of regression allows us to do this all within a single model. Again, as we have seen building up to this lab, an independent groups t-test is just a special case of ordinary regression with a single categorical predictor; ANOVA is just a special case of regression where all predictors are categorical. But remember, although we can express any ANOVA design using regression, the converse is not true: we cannot express every regression design in ANOVA. As such people like regression, and the general linear model, as it allows us to have any combination of continuous and categorical predictors in the model. The only inconvenience with running ANOVA models as regression models is that you have to take care in how you numerically code the categorical predictors. We will use an approach called deviation coding which we will look at today later in this lab.\nTo complete this lab activity, you can use the R-script (402_wk12_labAct1_template.R) that you downloaded as part of the ‘Pre-lab activities’ as a template. Work through the activity below, adding relevant bits of code to your script as you go along.\n\n\nStep 1: Background and set up\nBefore we get stuck in we need to set up a few things.\n\nTASK: Add code to clear the environment. HINT: rm(list=ls())\n\nNext we need to tell R which libraries to use. We need broom, car and tidyverse.\n\nTASK: Add code to load relevant libraries. HINT: library()\n\nFinally, read in the three data files; call the participant info pinfo; call the screen_time data screen and the well-being data wellbeing.\n\n**TASK*: Add code to read in the three data files. HINT: Use the read_csv() function.\n\n\n\nStep 2: Checking the formatting\nGiven our research question and the information you have about the scores, provided above under ‘Background’ and from the OSF-webpage, is the data ready for use?\n\nTASK: Add code to look at the first few lines of each data frame. HINT: Use the head() function (or tail() function).\n\nQUESTION 1: In which table is the variable corresponding to sex located and what is this variable called?\nThe ‘source and analysis code.sps’ file in the ‘Data and Code’ section on the OSF-webpage tells us how they coded the sex variable: 0 = female indicator and 1 = male indicator. It is worth exploring the OSF-webpage, to get used to foraging other files for these kinds of information, as they are not always clearly explained in a codebook or README. file.\nFor ease, lets recode the sex variable to reflect word labels of ‘female’ and ‘male’. This doesn’t change the order: R will still see female as 0, and male as 1 because female occurs before male in the alphabet. Add the code below to your script to do this. Don’t forget to run it as well.\n\npinfo$sex &lt;- ifelse(pinfo$sex == 1, \"male\", \"female\")\nhead(pinfo)                            \n\nQUESTION 2: In what format is the well-being data (long or wide)? On how many participants does it include observations? And on how many items for each participant?\nQUESTION 3: What is the name of the variable that identifies individual participants in this dataset? It is important to work this out as this variable will allow us to link information across the three data files.\n\n\nStep 3: Data preparation - Aggregating the total well-being scores\nWe need to sum the 14 items of the well-being scale for each participant to create one well-being score per participant.\n\nTASK: To create one well-being score per participant add code to the script to do the following: first, transform the well-being variable from wide to long (using pivot_longer()); then, use group_by() to get scores for each participant and finally use summarise() to calculate a total well-being score, calling the new variable tot_wellbeing. Save all of this to an object called ‘wb_tot’.\n\nIt is useful to calculate some descriptive statistics for the new variable tot_wellbeing.\n\nTASK: Calculate some descriptive statistics for tot_wellbeing. HINT: Use summarise() to calculate the mean, standard deviation, minimum and maximum values.\n\nFinally, let’s get an idea of the distribution of the new variable tot_wellbeing.\n\nTASK: Visualise the distribution in a histogram. HINT: Use ggplot() and geom_historgram().\n\nQUESTION 4: Is the distribution of well-being scores symmetrical, negatively skewed or positively skewed?\n\n\nStep 4: Data preparation - Transforming screen time data\nGreat, so we have the well-being scores sorted out, we now need to think about the screen-time usage data and whether it is being used on a weekday or a weekend. As always, to get an idea of the data, it is often very useful to visualise the variables before proceeding with the analysis.\nBefore we can do this, we’ll need to tidy these data up. Have another look at the screen data by using the head() function. You’ll see that we have Serial in the first column (this is good), but in the following eight columns, we have columns for each type of activity (Comph, Comp, Smart, Watch) and the part of the week it took place (we and wk) combined. Instead, to be able to work with the data, we need two columns: one for the type of activity (we’ll call it variable) and one for the part of the week (we’ll call it day).\nA second issue is that we need to alter the abbreviations Comph, Comp, Smart and Watch to reflect more descriptive text for each in plots.\nBelow are two chunks of code that represent these steps. In the next tasks you’ll practise with taking a set of piped commands apart. The purpose of this is to get you used to “parsing” the code at the right places so that when you see piped commands in other people’s code, you know how to break it down and find the relevant parts that you can use.\n\nTASK: In the code chunk below we use the separate() function to split the character strings already in the dataset. You know that with piped commands, there are chunks of code. Run the code first in its entirety and then pull each line apart to see how each function works on the data. Write a descriptive sentence for each function’s role in the command. Don’t forget to copy the chunk to your script and run it.\n\n\nscreen_long &lt;- screen %&gt;%\n  pivot_longer(-Serial, names_to = \"var\", values_to = \"hours\") %&gt;%\n  separate(var, c(\"variable\", \"day\"), \"_\")                       \n\n\nTASK: In the next code chunk we use the dplyr::recode() function with mutate() to relabel the separated names into understandable names that will be clear in plots. Again, run the code first in its entirety and then pull each line apart to see how each function works on the data. Write a descriptive sentence for each function’s role in the command. Don’t forget to copy the chunk to your script and run it.\n\n\nscreen2 &lt;- screen_long %&gt;%\n  mutate(variable = dplyr::recode(variable,\n                                  \"Watch\" = \"Watching TV\",\n                                  \"Comp\" = \"Playing Video Games\",\n                                  \"Comph\" = \"Using Computers\",\n                                  \"Smart\" = \"Using Smartphone\"),\n         day = dplyr::recode(day,\n                             \"wk\" = \"Weekday\",\n                             \"we\" = \"Weekend\"))                   \n\nThe code above has a new feature: the dplyr::recode part. This syntax – using the double colon – happens when there are many versions of a function with the same name. You can imagine that a function called ‘recode’ is immensely useful at the data wrangling stage of analysis. By using the name of the package, a double set of colons, followed by a function name, you are ensuring that R uses a particular version of the function, at that point only. This avoids having two or more packages loaded in your environment that sometimes do not play nicely together!\nTo be able to monitor that your code is performing as you want it to, you need to have in your mind an idea of how the data should look at the end of a code chunk. So stop a moment and be clear, discuss with your lab-mates if you feel like it and answer the following question.\nQUESTION 5: What are the variables and the levels or conditions within each variable of screen2?\n\nTASK:Now join wb_tot and screen_2 by participant and then group by the variables ‘variable’, ‘day’ and ‘hours’ and then calculate a ‘mean_wellbeing’ variable for each of the grouped levels. Save it in an object called ‘dat_means’.HINT: Write separate lines of code for each action and then, when you know they all work, reformat them as a piped command; look back earlier in this code and last week for the functions that perform these actions.\n\n\nTASK: Now check that you have an object that is 72 observations of 4 variables. You should have a mean wellbeing score for every level of the hours, over weekdays and weekends for each level of the four types of screen time (4 x 2 x 9)\n\nNext, it is a good idea to visualise the mean well-being data as function of hours of screen-time for the different days (weekday vs. weekend) and types of screen (playing video games, using computers, using smartphone and watching tv). This is quite a complex graph. We’ll go through creating it step-by-step, but let’s first look at the end result:\n\nOk, that’s what we are working towards.\n\nTASK: Below, a chunk of code is presented. It is your task to fill in the x and y variables. HINT: Go back to the research question - which variable is for the x axis and for the y axis?\n\n\nggplot(dat_means, aes(x = , y = )) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n\nQUESTION 6a: What research question does this plot describe? Is it appropriate for the levels within the data?\n\nTASK: Now, let’s add a different linetype for each day (weekday vs. weekend). Fill in the blanks in the code below.\n\n\nggplot(dat_means, aes(x = , y = , linetype = )) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n\nQUESTION 6b: What research question does this plot describe? Is it appropriate for the levels within the data?\nStill not quite there.\n\nTASK: Fill in the blanks (for x, y and linetype) as before. Now have a good look at the code below. What has changed? Copy the code to your script and run it. Then, for each line write a sentence as a comment to describe its effect on the plot.\n\n\nggplot(dat_means, aes(x = , y = , linetype = )) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~variable, nrow = 2) +\n  theme_bw()\n\nWe add the facet_wrap() function here. You can check the ?facet_wrap() help page for more information.\nQUESTION 6: c. What does the facet_wrap() function do? Is this plot appropriate for the levels in the data?\n\n\nStep 5: Calculating mean hours per day for smartphone use, for each participant\nAs mentioned at the beginning, in today’s lab we’ll focus on smartphone use. So looking at the bottom left of the figure we could suggest that smartphone use of more than 1 hour per day is associated with increasingly negative well-being the longer screen time people have. This looks to be a similar effect for Weekdays and Weekends, though perhaps overall well-being in Weekdays is marginally lower than in Weekends (the line for Weekday is lower on the y-axis than Weekends). This makes some sense as people tend to be happier on Weekends!\n\nTASK: Below is a set of comments that describe what the chunk of code that you need to write next does: use ‘screen2’ and then filter out the observations for ‘Using Smartphone’, and then group together each participant, and then summarise the mean hours calling it ‘hours_per_day’, save it in an object called ‘smarttot’\n\n\nTASK: Now let’s do it the other way around. Run the code below. Have a look at the structure of ‘smart_wb’ HINT: You can use the str() function. What does the code do?\n\n\nsmart_wb &lt;- smarttot %&gt;%\n  filter(hours_per_day &gt; 1) %&gt;%\n  inner_join(wb_tot, \"Serial\") %&gt;%\n  inner_join(pinfo, \"Serial\") %&gt;%\n  mutate(sex = as.factor(sex)) \n\nQUESTION 7: What does the code do? Write a short paragraph, using the phrase “and then” to represent the pipes.\n\n\nStep 6: More visualisation\nWe are now using only one small part of the data - smartphone use and its relationship with well-being over different durations of time. Before formally testing our research question, we can visualise the data and enquire about sex differences on the same plot - run each chunk of code below:\n\nTASK To further group the data, copy the code below to your script and run it. Look at the ‘smart_wb_gen’ dataframe. What has the code above done? Write a couple of sentences of description\n\n\nsmart_wb_gen &lt;- smart_wb %&gt;%\n  group_by(hours_per_day, sex) %&gt;%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\n\n\nTASK: Let’s visualise these data.\n\n\nggplot(smart_wb_gen, aes(hours_per_day, mean_wellbeing, color = sex)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_discrete(name = \"Sex\", labels = c(\"Girls\", \"Boys\"))+\n  scale_x_continuous(name = \"Total hours smartphone use\") +\n  scale_y_continuous(name = \"Mean well-being score\") +\n  theme_bw()\n\nQUESTION 8: Write an interpretation of the above plot in plain English.\n\n\nStep 7: The regression model\nIn the steps 2 to 6 we’ve covered some pretty heavy-lifting data-wrangling. As it is so often the case that something like this is needed when working with real data, ti is really important to practise this. However, to ensure you also spend time on fitting the regression model and interpreting the output, you can choose to use the data-file smart_wb.csv to get started with that. It contains the data in a format that is the result of all the data-wrangling we did in steps 2 to 6. So, download the smart_sb.csv data-file, put it in the folder that is your working directory and you’re all set for running the regression model.\nIf the smart_wb.csv file that you can download above does not want to upload to the server, try using the code below to read it directly from the web:\n\nsmart_wb &lt;- read_csv(\"https://raw.githubusercontent.com/mg78/lu_psyc402/main/files/week12/smart_wb.csv\")\n\n\nTASK: Let’s run the regression. Write code in your script in which you call your output ‘mod’, and use the data ‘smart_wb’ using the following formula, lm(y ~ x1 + x2 + x1:x2, data) to construct your regression model. Go back to the research question for your outcome and two predictor variables.\n\n\nTASK: Call and save the summary of your model as ‘mod_summary’; then have a look at it.\n\nLet’s first look at the model as a whole:\nQUESTION 9: What is the p-value for the overall model? Is it significant? What does this mean?\nQUESTION 10: To two decimal places, what percentage of the variance in well-being scores does the overall model explain?\nNow, lets look at the coefficients of our predictors:\nQUESTION 11: Are the main effects of smartphone use and sex significant?\nQUESTION 12: Which variable indicates the interaction between smartphone use and sex?\nQUESTION 13: And is the interaction significant?\nQUESTION 14: What is the most reasonable interpretation of these results?\nThe above model uses treatment coding (sometimes called dummy coding), for the sex variable. In a categorical variable with only two levels this means that one level is coded as 0 and the other level is coded as 1. In categorical variables with more than two levels, it works slightly differently.\n\nTASK: We can check that the sex variable is treatment or dummy coded with the following code:\n\n\ncontrasts(smart_wb$sex)\n\nBecause we have not explicitly told R about the labels for the sex variable, it has used level 0 as the reference level, hidden within the intercept term and level sex1 describes the difference (or slope) between level 0 and 1 or in this dataset from female to male.\nQUESTION 15: Is being Male better for a person’s well-being in the context of smartphone use than being Female?\nNow let’s look at deviation coding. There are other ways to code your categorical variables. One of them is deviation coding (also sometimes called sum coding). This effectively divides the difference of the values between your categorical levels by the number of levels so that each level can be compared to one intercept that is central to them all rather than comparing levels to one reference level. It is like centering for a categorical level.\n\nTASK Use the code chunk below to: 1) add a variable to the smart_wb data that is a deviation coding of sex; 2) set the deviation coding (we’ll labe it ‘Sum’ here for easy variable naming); and 3) look at the output for the sum-coded sex variable.\n\n\nsmart_wb &lt;- mutate(smart_wb, sexSum = sex) # add a variable to the smart_wb data that is a deviation coding of sex\ncontrasts(smart_wb$sexSum) &lt;- contr.sum(2) # wet the deviation coding\ncontrasts(smart_wb$sexSum) # look at the output for the sum coded sex variable\n\nNext, we’ll run the regression again, using the sum-coded sex variable and we’ll compare the outputs.\n\n# Run the regression model again, using the sumcoded sex model and compare outputs\nmod_S &lt;- lm(tot_wellbeing ~ hours_per_day + sexSum + hours_per_day:sexSum, smart_wb)\nmod_S_summary &lt;- summary(mod_S)\n\n# Compare the two model summary outputs\nmod_summary\nmod_S_summary\n\n‘sexSum1’ is now the coefficient for sex and represents the change from the intercept value which now lies between the values for being Female and Male. Note how this coefficient is a negative.\nThe earlier model had a positive coefficient because the intercept described the reference group of the Girls, who on average begin at a lower well-being level than Boys (refer back to the scatterplot to verify this). Because the sum-coding has moved the intercept to a point that is the center of the difference between Boys and Girls, sexSum1 now describes the distance between the centre and a level of Sex.\nValues for well-being in Girls are thus: Intercept + sexSum+1 49.74 + (-1.61)(+1)\nValues for well-being in Boys are thus: Intercept + sexSum-1 49.74 + (-1.61)(-1)\nwith the Boys being higher in well-being…(remember a negative number multiplied by a negative number produces a positive number and a negative number multiplied by a positive number produces a negative number).\nThe interpretation of both model effects is the same, and if you look at the summary statistics, they are identical. Deviation coding effectively centers your categorical variables and helps with interpretation of interaction terms.\n\n\nStep 8: Checking assumptions\nNow that we’ve fit a model, let’s check whether it meets the assumptions of linearity, normality and homoscedasticity. With regression models, you do this after you’ve actually fit the model.\nLinearity Unlike when we did simple regression we can’t use crPlots() to test for linearity when there is an interaction, but we know from looking at the grouped scatterplot that this assumption has been met.\nNormality Normally we would test for normality with a QQ-plot and a Shapiro-Wilk test. However, because this dataset is so large, the Shapiro-Wilk is not appropriate (if you try to run the test it will produce a warning telling you that the sample size must be between 3 and 5000). This is because with extremely large sample sizes the Shapiro-Wilk test will find that any deviation from normality is significant. Therefore we should judge normality based upon the QQ-plot.\n\nTASK: Create a QQ-plot to check the residuals are normally distributed HINT: You can use the qqPlot() function. The residuals are stored in the ‘mod’ object you created earlier.\n\nQUESTION 16: What do you conclude from the QQ-plot?\nHomoscedasticity Here we have the same problem as with testing for normality: with such a large sample the ncvTest() will produce a significant result for any deviation from homoscedasticity. So we need to rely on plots again. To check for homoscedasticity we can use plot() from Base R that will produce a bunch of helpful plots (more information here:.\n\nTASK: Copy the code chunk below to your script and run it.\n\n\npar(mfrow=c(2,2))             # 4 charts in 1 panel\nplot(mod)                     # this may take a few seconds to run\n\nThe residuals vs leverage plot shows a flat red line so, whilst it isn’t perfect, we can assume that with such a large sample size regression is still an appropriate analysis.\nMulti-collinearity Finally, lets check for multicollinearity using the vif() function. Essentially, this function estimates how much the variance of a coefficient is “inflated” because of linear dependence with other predictors, i.e., that a predictor isn’t actually adding any unique variance to the model, it’s just really strongly related to other predictors. Thankfully, vif is not affected by large samples like the other tests. There are various rules of thumb, but most converge on a VIF of above 2 to 2.5 for any one predictor being problematic.\n\nTASK: Copy the code chunk below to your script and run it.\n\n\nvif(mod)                      # Check for multi-collinearity\n\nQUESTION 17: Do any of the predictors show evidence of multicollinearity?\n\n\nStep 9: Write up\nQUESTION 18: How would you write up the results following APA guidance? You can choose whether you do so for the model using treatment coding or for the model using deviation coding."
  },
  {
    "objectID": "PSYC402/Week12.html#answers",
    "href": "PSYC402/Week12.html#answers",
    "title": "Categorical predictors",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\nYou can download the R-script that includes the relevant code here: 402_wk12_labAct1_withAnswers.R.\n\nLab activity 1: Combining a continuous and a categorical predictor in a resssion model\n\nIn which table is the variable corresponding to sex located and what is this variable called? In pinfo; the variable is called sex.\nIn what format is the well-being data (long or wide)? On how many participants does it include observations? And on how many items for each participant? The well-being data are in ‘wide’ format. It contains observations on 102580 participants, on 14 items.\nWhat is the name of the variable that identifies individual participants in this dataset? It is important to work this out as this variable will allow us to link information across the three data files. Serial\nIs the distribution of well-being scores symmetrical, negatively skewed or positively skewed? Negatively skewed\nWhat are the variables and the levels or conditions within each variable of screen2?\n\nParticipants plus: Levels: variables = 4: watching tv, playing video games, using computers, using smartphone day = 2 = weekdays, weekends hours = 9 = 0, 0.5, 1 - 7\n\n\n\n\n\nWhat research question does this plot describe? Is it appropriate for the levels within the data? The plot describes how hours of use impact upon well-being? No, it is too broad a research question.\n\n\nWhat research question would fit this visualisation?Is it appropriate for the levels in the data? The hours are now displayed by weekdays and weekends. How do hours of screen time impact on well-being on weekdays and at the weekend? No, it is still too broad.\n\n\nWhat does the facet_wrap() function do? Is this plot appropriate for the levels in the data? The facet_wrap() function has split the types of screen time and shown how hours of use across weekdays and weekends impact upon well-being. This captures the levels of information within the dataset.\n\n\n\nSee the script\nWrite an interpretation of the above plot in plain English. Something along the lines of: Adolescent girls show lower overall well-being compared to adolescent boys. In addition, the slope for girls appears more negative than that for boys; the one for boys appears relatively flat. This suggests that the negative association between well-being and smartphone use is stronger for girls.\nWhat is the p-value for the overall model? Is it significant? What does this mean? The p-value for the overall model fit is &lt; 2.2e-16. This significant. It means that together the predictors describe the variance in well-being better than a model without the predictors (the null model). So knowing something about smartphone use and sex of participants will allow us to predict their well-being to a degree.\nTo two decimal places, what percentage of the variance in well-being scores does the overall model explain? 9.38%\nAre the main effects of smartphone use and sex significant? Yes.\nWhich variable indicates the interaction between smartphone use and sex? The interaction is indicated by the variable hours_per_day:sex.\nAnd is the interaction significant? Yes.\nWhat is the most reasonable interpretation of these results? Smartphone use was more negatively associated with well-being for girls than for boys.\nIs being Male better for a person’s well-being in the context of smartphone use than being Female? Yes.\nWhat do you conclude from the QQ-plot? The residuals are normally distributed.\nDo any of the predictors show evidence of multicollinearity? Yes, Boys and the interaction do. We’ll talk about that more, later in the module.\nWrite up\n\nTreatment / dummy coded model Treatment coding was used for categorical predictors with the Girls level acting as the reference group. The results of the regression indicated that the model significantly predicted well-being (F(3, 71029) = 2450.89, p &lt; .001, Adjusted R2 = 0.09), accounting for 9% of the variance.Total hours of smart phone use was a significant negative predictor of well-being scores (β = -0.77, p &lt; .001, as was sex (β = 3.22, p &lt; .001), with girls having lower well-being scores than boys. Importantly, there was a significant interaction between screen time and sex (β = 0.45, p &lt; .001): smartphone use was more negatively associated with well-being for girls than for boys.\nDeviation / sum coded model Deviation coding was used for categorical predictors. The results of the regression indicated that the model significantly predicted well-being (F(3, 71029) = 2450.89, p &lt; .001, Adjusted R2 = 0.09), accounting for 9% of the variance. Total hours of smart phone use was a significant negative predictor of well-being scores (β = -0.55, p &lt; .001, as was sex (β = -1.61, p &lt; .001), with girls having lower well-being scores than boys. Importantly, there was a significant interaction between screen time and sex (β = -0.22, p &lt; .001), indicating that smartphone use was more negatively associated with well-being for girls than for boys."
  },
  {
    "objectID": "PSYC402/Week13.html",
    "href": "PSYC402/Week13.html",
    "title": "More on interactions",
    "section": "",
    "text": "Interactions are ubiquitous in psychological science, which is why we’ll spend some more time building models that include interaction terms. Last week we modelled well-being as a function of screen-time (a continuous predictor) and biological sex (a categorical predictor) and their interaction. This week we’ll look at interactions between continuous predictors."
  },
  {
    "objectID": "PSYC402/Week13.html#lectures",
    "href": "PSYC402/Week13.html#lectures",
    "title": "More on interactions",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week follows the recommended chapters in Winter (2020) – see under ‘Reading’ below – and is presented below:\n\nMore on interactions (~18 min)"
  },
  {
    "objectID": "PSYC402/Week13.html#reading",
    "href": "PSYC402/Week13.html#reading",
    "title": "More on interactions",
    "section": "Reading",
    "text": "Reading\n\nWinter (2020)\nLink\nChapter 8 explains what interactions are and how to model and interpret them."
  },
  {
    "objectID": "PSYC402/Week13.html#pre-lab-activities",
    "href": "PSYC402/Week13.html#pre-lab-activities",
    "title": "More on interactions",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Data-wrangling in R\nThe more you practise coding in R, the easier it will become. The RStudio interactive tutorials I mentioned in weeks 11 and 12 are an excellent place to start if you haven’t engaged with those yet.\n\nThe Basics Start here to learn how to inspect, visualize, subset and transform your data, as well as how to run code.\nWork with Data Learn how to extract values form a table, subset tables, calculate summary statistics, and derive new variables.\nVisualize Data Learn how to use ggplot2 to make any type of plot with your data. The tutorials on Exploratory Data Analysis and Scatterplots are particularly relevant.\nSeparating and Uniting Columns Here you will learn to separate a column into multiple columns and to reverse the process by uniting multiple columns into a single column. Then you’ll practise your data wrangling skills on messy real world data.\nJoin Data Sets Learn how to work with relational data. Here you will learn how to augment data sets with information from related data sets, as well as how to filter one data set against another.\n\nIf you feel confident with the material covered in those tutorials the following is useful to try:\n\nProgramming Basics R is easiest to use when you know how the R language works. This tutorial will teach you the implicit background knowledge that informs every piece of R code.\n\nPlease note that there are often different ways to do the same or similar things in R. This means you might encounter slightly different functions or styles of coding in different materials. This is not something to worry about. Just make sure you’re clear on what a bit of code achieves and choose the function/style that you feel most comfortable with.\n\n\nPre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 402_week13_forStudents.zip file and upload it into a new folder in RStudio Server.\n\n\nRemind yourself of how to access and work with the RStudio Server.\n\nSign in to the RStudio Server, using the login details provided to you via email. Note that when you are not on campus you need to log into the VPN first (look on the portal if you need more information about that).\nCreate a new folder for this week’s work.\nUpload the zip-file to the folder you have created on the RStudio server. Note you can either upload a single file or a zip-file, not a folder with multiple files.\nI highly recommend using R Projects to structure your workflow. You could create an R project for each week of the module. Have a look at section 8 Workflow: projects of R for Data Science by Hadley Wickam and Gareth Grolemund for an introduction."
  },
  {
    "objectID": "PSYC402/Week13.html#lab-activities",
    "href": "PSYC402/Week13.html#lab-activities",
    "title": "More on interactions",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nwhen and why to apply multiple regression to answer questions in psychological science\nconducting multiple regression in R including interaction between continuous predictors\ninterpreting the R output of multiple linear regression (when including an interaction between continuous predictors)\nreporting results for multiple linear regression (when including an interaction between continuous predictors), following APA guidelines\n\n\nBackground\nToday, we’ll be working with a dataset from the following paper: Hamermesh, D. S. and Parker, A. (2005). Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity. Economics of Education Review, 24(4), 369 – 376.\nThe abstract of their paper is below or see here for the paper itself.\nAbstract: Adjusted for many other determinants, beauty affects earnings; but does it lead directly to the differences in productivity that we believe generate earnings differences? We take a large sample of student instructional ratings for a group of university teachers and acquire six independent measures of their beauty, and a number of other descriptors of them and their classes. Instructors who are viewed as better looking receive higher instructional ratings, with the impact of a move from the 10th to the 90th percentile of beauty being substantial. This impact exists within university departments and even within particular courses, and is larger for male than for female instructors. Disentangling whether this outcome represents productivity or discrimination is, as with the issue generally, probably impossible.\nOur research question: Do professors’ beauty score and age predict how students evaluate their teaching?\nTo complete this lab activity, you can use the R-script (402_wk13_labAct1_template.R) that you downloaded as part of the ‘Pre-lab activities’ as a template. Work through the activity below, adding relevant bits of code to your script as you go along.\nQUESTION 1: Do you notice anything about the name of one of the variables and the name of the data table?\nThe table contains, amongst other things, the following characteristics of the professors\n\n‘beauty’ - beauty score per professor\n‘eval’ - teaching evaluation score per professor\n‘age’ - age of the professor\n\nQUESTION 2: Go back to the research question (see under ‘Background’ above), which of these three variables is the outcome variable? Which ones are the predictors?\n\n\nStep 2: Descriptive statistics and distributions\n\nTASK: Calculate some descriptive statistics for the variables of interest (eval, beauty and age). HINT: You can use summarise() to calculate the mean, sd, min and max values.\n\nNow that we have the descriptive statistics, let’s get further information about the distribution of the variables by plotting histograms.\n\nTASK: Visualise the distributions of the variables of interest in histograms. HINT: Use ggplot() and geom_historgram()\n\n\n\nStep 3: Center and standardise\nAs mentioned before, it will make it easier to interpret regression models with multiple predictors if we center and standardise our predictors. Before we go any further, we’ll do that.\n\nTASK: Center and standardise the predictor variables. HINT: Centering involves subtracting the mean; standardising involves dividing by the standard deviation.\n\n\n\nStep 4: Scatterplots\nNow let’s have a look at the relationships between variables using scatterplots. To remind yourself of what centering and standardising does, do this for both the raw data and the centered and standardised data.\n\nTASK: Visualise the relationships between the variables of interest in scatterplots. HINT: Create six different scatterplots using ggplot() with geom_point() and geom_smooth().\n\nQUESTION 3: Can you write an interpretation of the above plots in plain English?\nQUESTION 4: What is the difference between the scatterplots plotting the raw data and the ones plotting the centered and standardised data?\nNothing wrong with making the scatterplots individually, but R does have functions that let you make several in one go in what is called a ‘scatterplot matrix’. To be able to do that, we first have to create an object that only includes the variables of interest. Then we need to tell R this is a ‘data frame’ (a specific type of data table). Finally, we use the pairs() function to create the matrix of scatterplots. The code below does these things:\n\nTASK: Add the code below to your script and, in your script, also add comments to each line of code to summarise what that line does.\n\n\nbeauty_matrix &lt;- beauty_z %&gt;%     \n  select(age_z, beauty_z, eval) %&gt;%     \n  as.data.frame()\n\npairs(beauty_matrix)\n\nAlthough handy to get a quick overview, the scatterplots made using ggplot() are often clearer.\nIt is useful to have a quick look at the bivariate correlations between the variables of interest, before you run a regression model. We can easily generate a correlation matrix for these variables.\n\nTASK: Add the code below to your script and check you understand what each line does.\n\n\nintercor_results &lt;- correlate(x = beauty_matrix, # our data\n                              test = TRUE, # compute p-values\n                              corr.method = \"pearson\", # run a spearman test \n                              p.adjust.method = \"bonferroni\") # use the bonferroni correction\nintercor_results\n\nAfter you’ve run this code, look at the output in the console. It creates three tables, one with correlation coefficients, one with p-values for these coefficients and one with sample sizes.\n\n\nStep 5: The regression model\nWe’ve looked at descriptive statistics and distributions of variables and also at relations between variables. This has given us a good idea of what the data look like. Now we’ll construct the regression model to predict ‘evaluation score’ as a function of ‘age’ and ‘beauty score’. We’ll do this in two stages. First we’ll construct a model without an interaction term. Then we’ll construct a model that includes an interaction term betweeen the two predictor variables. Don’t forget to use the standardised data for all this.\n\nTASK: Construct a regression model without an interaction term. HINT: Use the following formula, lm(y ~ x1 + x2, data); go back to the research question for your outcome and predictor variables.\n\n\nTASK: Call and save the summary of your model; then have a look at it.\n\nQUESTION 5: Is the overall model significant?\nQUESTION 6: Are the predictors significant? What does this mean?\n\nTASK: Now create a model that includes an interaction term for the two predictors. Again, use the centered and standardised data. HINT: Use the following formula, lm(y ~ x1 + x2 + x1:x2, data); go back to the research question for your outcome and predictor variables.\n\nQUESTION 7: Is the overall model significant?\nQUESTION 8: Have a good look at the coefficients. Can you interpret each one of them in turn and then formulate an overall interpretation? HINT: Remember that after centering and standardising, the meaning of 0 has changed for both predictor variables.\nInterpretation of coefficients in a multiple regression can be facilitated by ‘added variable’ plots.\n\nTASK: Use the function avPlots() to create ‘added variable’ plots.\n\nCreating a scatterplot with our outcome variable on the y-axis and the significant predictor on the x-axis and then plotting our third variable (age) using different colours gives some information. Do you see how high age scores (light blue + 2 SD) seem to be more frequent in the bottom left corner?\n\nTASK: Use the code below to create the plot.\n\n\nggplot(data = beauty_z, aes(x = beauty_z, y = eval, colour = age_z)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE, colour = 'black') +\n  theme_bw() +\n  labs(x = \"Beauty score\", y = \"Teaching evaluation score\")\n\nBut it might be more useful to plot different regression lines for different values of age. We can do this be transforming age into a categorical variable for plotting purposes. The code below creates three categories, based on eye-balling the histogram for age:\n\nyoungest (40 and younger)\naverage (between 41 and 53)\noldest (54 and older).\n\n\nTASK: Copy the code below to your script and make sure you understand what it does.\n\n\noldest &lt;- beauty_z %&gt;%\n  filter(age &gt;= 54)\n\naverage &lt;- beauty_z %&gt;%\n  filter(age &gt; 40) %&gt;%\n  filter(age &lt; 54)\n\nyoungest &lt;- beauty_z %&gt;%\n  filter(age &lt;= 40)\n\nNow let’s create a single plot with three different lines, one for each of the age groups created above.\n\nTASK: Copy the code below to your script and make sure you understand what it does.\n\n\nggplot() +\n  geom_point(data = oldest, aes(x = beauty_z, y = eval), colour = 'blue') +\n  geom_smooth(data = oldest, aes(x = beauty_z, y = eval), method = \"lm\", se = TRUE, colour = 'blue') +\n  geom_point(data = average, aes(x = beauty_z, y = eval), colour = 'black') +\n  geom_smooth(data = average, aes(x = beauty_z, y = eval), method = \"lm\", se = TRUE, colour = 'black') +\n  geom_point(data = youngest, aes(x = beauty_z, y = eval), colour = 'green') +\n  geom_smooth(data = youngest, aes(x = beauty_z, y = eval), method = \"lm\", se = TRUE, colour = 'green') +\n  theme_bw() +\n  labs(x = \"Beauty score\", y = \"Teaching evaluation score\")\n\nThe line for the oldest participants seems much steeper than for the other two groups, suggesting that the interaction between age and beauty is mostly driven by older participants who have received more extreme beauty scores.\n\n\nStep 6: Checking assumptions\nNow that we’ve fitted a model, let’s check whether it meets the assumptions of linearity, normality and homoscedasticity.\nLinearity Unlike when we did simple regression we can’t use crPlots() to test for linearity when there is an interaction, but we know from looking at the grouped scatterplot that this assumption has been met.\nNormality Normally we would test for normality with a qq-plot and a Shapiro-Wilk test. However, because this dataset is so large, the Shapiro-Wilk is not appropriate (if you try to run the test it will produce a warning telling you that the sample size must be between 3 and 5000). This is because with extremely large sample sizes the Shapiro-Wilk test will find that any deviation from normality is significant. Therefore we should judge normality based upon the qq-plot.\n\nTASK: Create a qq-plot to check the residuals are normally distributed. HINT: Use the qqPlot() function; mind the capital P.\n\nQUESTION 9: What do you conclude from the qq-plot?\nHomoscedasticity Here we have the same problem as with testing for normality: with such a large sample the ncvTest() will produce a significant result for any deviation from homoscedasticity. So we need to rely on plots again.\nTo check for homoscedasticity we can use plot() from Base R that will produce a bunch of helpful plots (more information [here] (https://www.r-bloggers.com/2016/01/how-to-detect-heteroscedasticity-and-rectify-it/).\n\nTASK: Copy the code below to your script and run it to create the plots\n\n\npar(mfrow=c(2,2))                 # 4 charts in 1 panel\nplot(mod_int)                     # this may take a few seconds to run\n\nQUESTION 10: What do you conclude from the residuals vs leverage plot?\nMulti-collinearity Now let’s check for multi-collinearity using the vif() function. Essentially, this function estimates how much the variance of a coefficient is “inflated” because of linear dependence with other predictors, i.e., that a predictor isn’t actually adding any unique variance to the model, it’s just really strongly related to other predictors. Thankfully, the vif() function is not affected by large samples like the other tests. There are various rules of thumb, but most converge on a VIF of above 2 - 2.5 for any one predictor being problematic.\n\nTASK: Use the vif() function to test for multi-collinearity.\n\nQUESTION 11: Do any of the predictors show evidence of multi-collinearity?\nFinally, we need to write up the results.\nQUESTION 12: Can you write up the results of the regression analysis following APA guidelines? HINT: Don’t forget to mention and interpret the interaction effect."
  },
  {
    "objectID": "PSYC402/Week13.html#answers",
    "href": "PSYC402/Week13.html#answers",
    "title": "More on interactions",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\nYou can download the R-script that includes the relevant code here: 402_wk13_labAct1_withAnswers.R.\n\nDo you notice anything about the name of one of the variables and the name of the data table? Both the data table and one of the variables are called ‘beauty’. Not a problem, as such as long as you don’t get confused.\nGo back to the research question (see under ‘Background’ above), which of these three variables is the outcome variable? Which ones are the predictors? The research question is ‘Do professors’ beauty score and age predict how students evaluate their teaching?’ From this we can deduct that the outcome variable is teaching evaluation score and that the predictors are age and beauty score.\nCan you write an interpretation of the above plots in plain English? A moderate negative association seems present between beauty score and age: with increasing age, beauty score decreases. A moderate positive association seems present between beauty score and teaching evaluation: professors with higher beauty scores also receive higher teaching evaluations. Not much of a association seems present between age and teaching evaluation (the line is pretty horizontal).\nWhat is the difference between the scatterplots plotting the raw data and the ones plotting the centered and standardised data? The units of the x-axis have changed from years (for age) and scores (for beauty) to standard units, with zero in the middle.\nIs the overall model significant? Yes, F(2, 460) = 8.53, p = .0002\nAre the predictors significant? What does this mean? The beauty score significantly predicts teaching evaluation score, but age does not. Professors with higher beauty scores, received better teaching evaluations.\nIs the overall model significant? Yes, F(3, 459) = 9.32, p = 5.451e-06\nHave a good look at the coefficients. Can you interpret each one of them in turn and then formulate an overall interpretation? HINT: Remember that after centering and standardising, the meaning of 0 has changed for both predictor variables. The intercept is predicted teaching evaluation score for a professor with average age and average beauty score. The slope of ‘age’ is positive; this means that for higher age, teaching evaluation scores were better. However the coefficient is not significant, therefore has little predictive power.The slope of ‘beauty’ is positive; this means that with higher beauty score, professors receive higher teaching evaluations. This predictor is significant. The slope for the interaction is also positive. This can be read as follows: When age and beauty both increase, teaching evaluation score also increases. The interaction is significant.\nWhat do you conclude from the qq-plot? The residuals are mostly normally distributed. At the top right (quantile + 3), there are some values that don’t quite look normally distributed, this is probably due to fewer data points being available in the highest age bracket. Have a look at a histogram for age. There are a few individuals well above the retirement age, but clearly a lot fewer than in younger age brackets. This basically means that the model does not do a particularly good job for predicting evaluation score at high values of age. As retirement age is quite a natural point to limit the data, you could run the model again, only including people below retirement age, this should give you better behaving residuals. Ideally, you’d pre-register a decision such as this. If you didn’t do this prior to data collection, you could still limit the age range included in the final model, but you would need to be transparent in your reporting.\nWhat do you conclude from the residuals vs leverage plot? **The residuals vs leverage plot shows a flat red line so, whilst it isn’t perfect, we can assume that with regression is still an appropriate analysis.\nDo any of the predictors show evidence of multi-collinearity? No\nCan you write up the results of the regression analysis following APA guidelines? The results of the regression indicated that the model significantly predicted teaching evaluation scores (F(3, 459) = 9.316, p &lt; .001, adjusted R^2 = 0.05), accounting for 5% of the variance. A professor’s beauty score was a significant positive predictor of teaching evaluation score (\\(\\beta\\) = 0.12, p &lt; .001). This effect was moderated by a significant positive interaction between beauty score and age (\\(\\beta\\) = 0.08, p &lt; .001), suggesting that when age and beauty score both increased, teaching evaluation score also increased."
  },
  {
    "objectID": "PSYC402/references.html",
    "href": "PSYC402/references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC402/Week11.html",
    "href": "PSYC402/Week11.html",
    "title": "Recap of the linear model and practising data-wrangling in R",
    "section": "",
    "text": "Before we start covering new material, we want to spent some time on recapping the basic concepts of the linear model (correlation, simple regression, multiple regression). You all come from different educational backgrounds and therefore have vastly different knowledge of, and experience with statistics. Therefore, please follow your own judgement as to whether you feel you want to/need to revisit material outlining the theoretical background to and the practical implementation in R for these topics. Below we provide some guidance as to materials that are relevant. Just to be clear: We don’t expect you to watch and/or read and/or do everything, please have a look at what you feel you need and spend some time with those materials."
  },
  {
    "objectID": "PSYC402/Week11.html#lectures",
    "href": "PSYC402/Week11.html#lectures",
    "title": "Recap of the linear model and practising data-wrangling in R",
    "section": "Lectures",
    "text": "Lectures\nThe linear model was discussed in weeks 6 to 9 of PSYC401, so that is a good place to start.\nAlternatively, if you don’t feel confident about the material, these recorded lectures might help.\n\nThe linear model: theory (~30 min) An introduction to the linear model and linear regression. I follow material as discussed in Chapter 4 of Bodo Winter’s book Statistics for Linguists: An Introduction using R (see below under ‘Reading’).\nHow to build a linear model in R (~30 min) In this video I demonstrate how to build a linear model in R by talking you through a simple linear regression script (you can download it here stats_linearModel_howTo.R). If you are unclear on what different parts of the lm() function do, or how to read the output, this video might help clarify that.\nMultiple regression: theory (~35 min) An introduction to multiple regression. I follow material as discussed in Chapter 5 of Bodo Winter’s book Statistics for Linguists: An Introduction using R (see below under ‘Reading’).\nCentering and standardising (~5 min) Brief explanation of what centering and standardising are."
  },
  {
    "objectID": "PSYC402/Week11.html#reading",
    "href": "PSYC402/Week11.html#reading",
    "title": "Recap of the linear model and practising data-wrangling in R",
    "section": "Reading",
    "text": "Reading\n\nMiller & Haden (2013)\nLink\nChapter 10 gives you a brief overview of what correlation and regression are. Chapter 11 introduces correlation in more detail. Chapters 12 and 14 provide accessible overviews of simple and multiple regression, respectively. All these chapters are really short but provide a good basis to understanding. We consider this the minimum level of understanding you should acquire.\n\n\nWinter (2020)\nLink\nChapter 4 provides and excellent conceptual introduction to the linear model and also explains how this is implemented in R (highly recommended).\nChapter 5 takes a slightly different approach to the one taken in Miller & Haden (2013) to introducing correlation. If you already understand the basic theory behind correlation, this will be an interesting read. Chapter 5 also clearly explains what centering and standardizing are and why you need to bother with these linear transformations.\nChapter 6 provides an excellent overview of multiple regression and also explains how this is implemented in R."
  },
  {
    "objectID": "PSYC402/Week11.html#pre-lab-activities",
    "href": "PSYC402/Week11.html#pre-lab-activities",
    "title": "Recap of the linear model and practising data-wrangling in R",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Visualising the regression line\nHave a look at this visualisation of the regression line by Ryan Safner.\nIn this shiny app, you see a randomly-generated set of data points (within specific parameters, to keep the graph scaled properly). You can choose a slope and intercept for the regression line by using the sliders. The graph also displays the residuals as dashed red lines. Moving the slope or the intercept too much causes the generated line to create much larger residuals. The shiny app also calculates the sum of squared errors (SSE) and the standard error of the regression (SER), which calculates the average size of the error (the red numbers). These numbers reflect how well the regression line fits the data, but you don’t need to worry about those for now.\nIn the app he uses the equation Y = aX + b in which b is the intercept and a is the slope.\nThis is slightly different from the equation you saw during the lecture. There we talked about Y = b0 + b1*X + e. Same equation, just different letters. So b0 in the lecture is equivalent to b in the app and b1 in the lecture is equivalent to a in the app.\nPre-lab activity questions:\n\nChange the slider for the intercept. How does it change the regression line?\nChange the slider for the slope. How does it change the regression line?\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line?\n\n\n\nPre-lab activity 2: Data-wrangling in R\nIn PSYC401, you’ve already learned how to read in data, how to select variables and how to compute summary statistics, so re-visiting the PSYC401 materials is a good place to start.\nRStudio also provides some useful interactive tutorials that take you through the basics:\n\nThe Basics Start here to learn how to inspect, visualize, subset and transform your data, as well as how to run code.\nWork with Data Learn how to extract values form a table, subset tables, calculate summary statistics, and derive new variables.\nVisualize Data Learn how to use ggplot2 to make any type of plot with your data. The tutorials on Exploratory Data Analysis and Scatterplots are particularly relevant.\n\nPlease note that there are often different ways to do the same or similar things in R. This means you might encounter slightly different functions or styles of coding in different materials. This is not something to worry about. Just make sure you’re clear on what a bit of code achieves and choose the function/style that you feel most comfortable with.\n\n\nPre-lab activity 3: Getting ready for the lab class\n\nRemind yourself of how to access and work with the RStudio Server.\n\nRevisit PSYC401 to remind yourself of how to access the RStudio Server.\nI highly recommend using R Projects to structure your workflow. You could create an R project for each week of the module. Have a look at section 8 Workflow: projects of R for Data Science by Hadley Wickam and Gareth Grolemund for an introduction.\n\n\n\nGet your files ready\nDownload the 402_week11_forStudents.zip file and upload it into a new folder in RStudio Server."
  },
  {
    "objectID": "PSYC402/Week11.html#lab-activities",
    "href": "PSYC402/Week11.html#lab-activities",
    "title": "Recap of the linear model and practising data-wrangling in R",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nwhen and why to apply simple and multiple regression to answer questions in psychological science\nconducting multiple regression in R\ninterpreting the R output of simple and multiple linear regression\nreporting results for simple and multiple linear regression following APA guidelines\n\n\nLab activity 1: Interpreting and reporting results\nHave a look at the R output below.\nR Output 1\n\n\nWhat is the outcome or dependent variable?\nWhat is the predictor or independent variable?\nIs the overall model significant?\nHow much variance does the model account for?\n\nThinking about assumptions, what do you conlcude from the plots and output below?\n\nDoes the relationship appear linear?\nDo the residuals show normality and homoscedasticity?\n\nScatterplot\n\nQQ-plot\n\nR Output 2\n\n\n\nLab activity 2: Conducting simple and multiple regression\n\nBackground\nToday, to help get a practical understanding of regression, you will be working with real data and using regression to explore the question of whether there is a relationship between voice acoustics and ratings of perceived trustworthiness.\n\nThe Voice\nThe prominent theory of voice production is the source-filter theory (Fant, 1960) which suggests that vocalisation is a two-step process: air is pushed through the larynx (vocal chords) creating a vibration, i.e. the source, and this is then shaped and moulded into words and utterances as it passes through the neck, mouth and nose, and depending on the shape of those structures at any given time you produce different sounds, i.e. the filter. One common measure of the source is pitch (otherwise called Fundamental Frequency or F0 (F-zero)) (Titze, 1994), which is a measure of the vibration of the vocal chords, in Hertz (Hz); males have on average a lower pitch than females for example. Likewise, one measure of the filter is called formant dispersion (measured again in Hz), and is effectively a measure of the length of someone’s vocal tract (or neck). Height and neck length are suggested to be negatively correlated with formant dispersion, so tall people tend to have smaller formant dispersion. So all in, the sound of your voice is thought to give some indication of what you look like.\nMore recently, work has focussed on what the sound of your voice suggests about your personality. McAleer, Todorov and Belin (2014) suggested that vocal acoustics give a perception of your trustworthiness and dominance to others, regardless of whether or not it is accurate. One extension of this is that trust may be driven by malleable aspects of your voice (e.g. your pitch) but not so much by static aspects of your voice (e.g. your formant dispersion). Pitch is considered malleable because you can control the air being pushed through your vocal chords (though you have no conscious control of your vocal chords), whereas dispersion may be controlled by the structure of your throat which is much more rigid due to muscle, bone, and other things that keep your head attached. This idea of certain traits being driven by malleable features and others by static features was previously suggested by Oosterhof and Todorov (2008) and has been tested with some validation by Rezlescu, Penton, Walsh, Tsujimura, Scott and Banissy (2015).\nSo, the research question today is: Can vocal acoustics, namely pitch and formant dispersion, predict perceived trustworthiness from a person’s voice? We will only look at male voices today, but you have the data for female voices as well should you wish to practice (note that in the field, tendency is to analyse male and female voices separately as they are effectively sexually dimorphic). As such, we hypothesise that a linear combination of pitch and dispersion will predict perceived vocal trustworthiness in male voices. This is what we will analyse.\nTo complete this lab activity, you can use the R-script (402_wk11_labAct2.R) that you downloaded as part of the ‘pre-lab activities’ as a template. Work through the activity below, adding relevant bits of code to your script as you go along.\n\n\n\nStep 1: Background and set up\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one.\n\nTASK: Use the code snippet below to clear the environment. TIP: If you hover your mouse over the box that includes the code snippet, a ‘copy to clipboard’ icon will appear in the top right corner of the box. Click that to copy the code. Now you can easily paste it into your script. \n\n\nrm(list=ls())                            \n\nBefore we can get started we need to tell R which libraries to use. For this analysis we’ll need broom, car, pwr and tidyverse.\n\nTASK: Load the relevant libraries. HINT: Use the library() function.\n\nIn this lab, we are setting out to test whether a linear combination of pitch and dispersion will predict perceived vocal trustworthiness in male voices. We’ll be working with two data files:\n\nvoice_acoustics.csv - shows the VoiceID, the sex of the voice, and the pitch and dispersion values\nvoice_ratings.csv - shows the VoiceID and the ratings of each voice by 28 participants on a scale of 1 to 9 where 9 was extremely trustworthy and 1 was extremely untrustworthy.\n\n\nTASK: Read in both files (using the read_csv() function), have a look at the layout of the data and familiarise yourself with it. The ratings data is rather messy and in a different layout to the acoustics but can you tell what is what?\n\nQUESTION 1 How are the acoustics data and the ratings data organised (wide or long)? Are both data files ‘tidy’? If you need more info on what that means, have a look here.\n\n\nStep 2: Restructuring the ratings data\nWe are going to need to do some data-wrangling before we do any analysis! Specifically, we need the change the ratings data to the long format.\nHere we’ll use the pivot_longer() function (see here or type ?pivot_longer in the Console for more info) to restructure the ratings data from wide to long and store the resulting table as ‘ratings_tidy’.\n\nTASK: Use the code snippet below to restructure the data.\n\n\nratings_tidy &lt;- pivot_longer(\n  data = ratings,    # the data you want to restructure\n  cols = P1:P28,     # columns you want to restructure\n  names_to = \"participant\", # variable name that captures whatever is across the columns\n  # (in this case P1 to P28 for the 28 different participants)\n  values_to = \"rating\") # variable name that captures whatever is in the cells\n  # (in this case numbers for ratings)\n\n\n\nStep 3: Calculate mean trustworthiness rating for each voice\nNow that we have the ratings data into a tidy format, the next step is to calculate the mean rating for each voice. Remember that each voice is identified by the ‘VoiceID’ variable.\n\nTASK: Calculate the mean rating for each voice and store the resulting table in a variable named ‘ratings_mean’. HINT: Use group_by() and summarise(). Are you using the tidy data? Also, remember that if there are any missing values (NAs) then na.rm = TRUE would help.\n\n\n\nStep 4: Join the data together\nOk, before we get ahead of ourselves, in order to perform the regression analysis we need to combine the data from ‘ratings_mean’ (the mean ratings) with ‘acoustics’ (the pitch and dispersion ratings). Also, as we said, we only want to analyse male voices today.\n\nTASK: Join the two tables and keep only the data for the male voices, call the resulting table ‘joined’. HINT: Use the inner_join() function (making use of the variable that is common across both tables) to join. See here or type ?inner_join in the Console for more info. Use the filter() function to only keep male voices. Remember that the Boolean operator for exactly equal is ==.\n\n\n\nStep 5: Spreading the data\nOk so we are starting to get an understanding of our data and we want to start thinking about the regression. However, the regression would be easier to work with if Pitch and Dispersion were in separate columns. This can be achieved using the pivot_wider() function (see here or type ?pivot_wider in the Console for more info). This is basically the inverse of pivot_longer(). It increases the number of columns and decreases the number of rows.\n\nTASK: Use the code snippet below to spread the data.\n\n\njoined_wide &lt;- joined %&gt;%\n  pivot_wider(\n    names_from = measures, # name of the categorical column to spread\n    values_from = value) # name of the data to spread\n\nQUESTION 2 Why do we not need to specify within the pivot_wider() function which data to use?\n\n\nStep 6: Visualising the data\nAs always, it is a good idea to visualise your data.\n\nTASK: Now that we have all the variables in one place, make two scatterplots, one of mean trustworthiness rating with dispersion and one for mean trustworthiness rating and pitch. HINT: For this you’ll need the ggplot() function together with geom_point() and geom_smooth(). Make sure to give your axes some sensible labels.\n\nQUESTION 3 According to the scatterplots, how would you decribe the relationships between trustworthiness and dispersion and trustworthiness and pitch in terms of direction and strength? Which one of the two seems stronger?\n\n\nStep 7: Conducting and interpreting simple regression\nWith all the variables in place, we’re ready now to start building two simple linear regression models:\n\nPredicting trustworthiness mean ratings from Pitch\nPredicting trustworthiness mean ratings from Dispersion\n\n\nTASK: Use the lm() function to run the following two regression models and use the summary() function to look at the output of each model. Store the first model in a table called ‘mod_pitch’ and store the second model in ‘mod_disp’. HINT: lm(dv ~ iv, data = my_data)\n\nQUESTION 4 What do you conclude from the output of these models? Which model is significant? Which predictors are significant? How much variance does each model describe?\n\n\nStep 8: Conducting and interpreting multiple regression\nNow let’s look at both predictors in the same model. Before we do this, it is sensible to center and standardise the predictors.\nLook at the code below. Can you follow how the predictors are first centered (_c) and then standardised (_z)?\nHere I do this by hand because I think it makes it clearer, even though there are functions that do this in one step (scale()).\n\njoined_wide &lt;- mutate(joined_wide,\n                      Dispersion_c = Dispersion - mean(Dispersion),\n                      Dispersion_z = Dispersion_c / sd(Dispersion_c),\n                      Pitch_c = Pitch - mean(Pitch),\n                      Pitch_z = Pitch_c / sd(Pitch_c))\n\n\nTASK: Now use the centered and standardised data for the multiple regression. Use the lm() function to run a model for predicting trustworthiness mean ratings from Pitch and Dispersion, and store the model in ‘mod_pitchdisp_z’. Use the ‘summary()’ function to look at the output. HINT: lm(dv ~ iv1 + iv2, data = my_data)\n\nQUESTION 5 What do you conclude from the output of this model? Is the overall model significant? Which predictors are significant? How much variance does the model describe? Which model would you say is best for predicting ratings of trustworthiness, the Pitch only, the Dispersion only or the Pitch+Dispersion model?\n\n\nStep 9: Checking assumptions\nNow that we’ve established which model best fits the data, let’s check whether it meets the assumptions of linearity, normality and homoscedasticity.\n\nTASK: Check the assumptions of linearity, normality and homoscedasticity. HINT: crPlots() to check linearity qqPlot() and shapiro.test() to check normality of the residuals residualPlot() and nvcTest() to check homoscedasticity of the residuals\n\nQUESTION 6 What do you conclude from the graphs and output? Should we also check for collinearity?\n\n\nStep 10: Writing up the results\n\nTASK: Write up the results following APA guidelines. HINT: The Purdue writing lab website is helpful for guidance on punctuating statistics. The APA Style 7th Edition Numbers and Statistics Guide is also useful."
  },
  {
    "objectID": "PSYC402/Week11.html#answers",
    "href": "PSYC402/Week11.html#answers",
    "title": "Recap of the linear model and practising data-wrangling in R",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\nLab activity 1: Interpreting and reporting results\n\nWhat is the outcome or dependent variable? Word reading\nWhat is the predictor or independent variable? Non-word reading\nIs the overall model significant? Yes, F(1,50) = 69.03, p &lt; .001\nHow much variance does the model account for? 58%\nDoes the relationship appear linear? Yes. The dots and the pink line assemble quite closely on the dashed line.\nDo the residuals show normality and homoscedasticity? The qq-plot suggest that the residuals are normally distributed as the dots fall close to the solid blue line and within the range of the dashed blue lines. The Shapiro-Wilk test of normality confirms this (it is not significant). Similarly, the output of the non-constant variance score tests is not significant suggesting that the residuals are homoscedastic.\n\n\n\nLab activity 2: Conducting simple and multiple regression\nYou can download the R-script that includes the relevant code and answers to the questions here: 402_wk11_labAct2_withAnswers.R."
  },
  {
    "objectID": "PSYC402/Week14.html",
    "href": "PSYC402/Week14.html",
    "title": "Logistic regression",
    "section": "",
    "text": "All of the models considered up to this point dealt with continuous response variables. Previously we looked at categorical predictors, but what if the response itself is categorical? For instance, whether the participant has made an accurate or inaccurate selection or whether a job candidate gets hired or not. Another common type of data is count data, where values are also discrete. Often with count data, the number of opportunities for something to occur is not well-defined. For instance, the number of speech error in a corpus, the number of turn shifts between speakers in a conversation or the number of visits to the doctor. Logistic regression allows us to model a categorical response variable."
  },
  {
    "objectID": "PSYC402/Week14.html#lectures",
    "href": "PSYC402/Week14.html#lectures",
    "title": "Logistic regression",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week follows the recommended chapters in Winter (2020) – see under ‘Reading’ below – and is presented below:\n\nLogistic regression (~38 min)"
  },
  {
    "objectID": "PSYC402/Week14.html#reading",
    "href": "PSYC402/Week14.html#reading",
    "title": "Logistic regression",
    "section": "Reading",
    "text": "Reading\n\nBarr (2020)\nLink\nThis online textbook provides a useful overview of logistic regression. It does at some point talk about modelling multi-level data and random effects. Don’t worry about that for now, those will be covered in the second half of 402. This week we’ll focus on ‘single-level’ data.\n\n\nWinter (2020)\nLink\nChapter 12 provides a comprehensive introduction to logistic regression and its implementation in R."
  },
  {
    "objectID": "PSYC402/Week14.html#pre-lab-activities",
    "href": "PSYC402/Week14.html#pre-lab-activities",
    "title": "Logistic regression",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Getting ready\n\nGet your files ready\nDownload the 402_week14_forStudents.zip file and upload it into a new folder in RStudio Server.\n\n\nRemind yourself of how to access and work with the RStudio Server.\n\nSign in to the RStudio Server, using the login details provided to you via email. Note that when you are not on campus you need to log into the VPN first (look on the portal if you need more information about that).\nCreate a new folder for this week’s work.\nUpload the zip-file to the folder you have created on the RStudio server. Note you can either upload a single file or a zip-file, not a folder with multiple files.\nI highly recommend using R Projects to structure your workflow. You could create an R project for each week of the module. Have a look at section 8 Workflow: projects of R for Data Science by Hadley Wickam and Gareth Grolemund for an introduction.\n\n\n\n\nPre-lab activity 2: Rainy days\nTry running the code mentioned in the online textbook by Barr. If you find it easier, use the rainy_days.R script (in the ‘402_week14_forStudents folder you were asked to download in ’Pre-lab activity 1’). It illustrates the point that for discrete data, the variance is often not independent from the mean. In addition, it introduces some very useful R functions: What do the rep() function, the c() function and the facet_wrap() function do? Remember, you can type ?function name() (e.g., ?rep()) in the Console to get more information about a function. Finally, can you add a graph for rainy days in Lancaster?\n\n\nPre-lab activity 3:Gesture perception\nPlease go through the example described in section 12.6 of the chapter on logistic regression in Bodo Winter’s book (link under ‘Reading’). Read the section and (simultaneously) work through the script (chapter12_6.R; in the ‘402_week14_forStudents folder you were asked to download in ’Pre-lab activity 1’). We’ll be working more with this dataset during the lab, so it is helpful if you get a feel for it now."
  },
  {
    "objectID": "PSYC402/Week14.html#lab-activities",
    "href": "PSYC402/Week14.html#lab-activities",
    "title": "Logistic regression",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nwhen and why to apply logistic regression to answer questions in psychological science\nconducting logistic regression in R\ninterpreting the R output of logistic regression\nreporting results for logistic regression following APA guidelines\n\n\nLab activity 1: More work on gesture perception\nThe dataset we’ll be working with is described in section 12.6 of the chapter on logistic regression in Bodo Winter’s book (link under ‘Reading’). In the pre-lab activity, we explored the dataset and fitted a first logistic regression model assessing whether participants’ perception of a gesture (expressed as a categorical decision between a ‘shape’ vs. a ‘height’ interpretation of the gesture) was affected by the extent of ‘pinkie curl’. In this lab activity, we’ll be building on that analysis by: 1) Repeating the analysis with a centered pinkie curl variable, and 2) by adding a second predictor: index_curve.\nTo complete this lab activity, please open the R-script 402_wk14_labAct1.R in R Studio and work your way through it. All instructions, hints and questions are contained in the script."
  },
  {
    "objectID": "PSYC402/Week14.html#answers",
    "href": "PSYC402/Week14.html#answers",
    "title": "Logistic regression",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\nYou can download the R-script that includes the relevant code here: 402_wk14_labAct1_withAnswers.R."
  },
  {
    "objectID": "PSYC402/Week15.html",
    "href": "PSYC402/Week15.html",
    "title": "Poisson regression",
    "section": "",
    "text": "Previously, we looked at logistic regression in the context of a binomial outcome variable, that is, a two-level variable such as correct vs. incorrect, or looking to the left vs. the right. Poisson regression is another type of generalized linear model that is particularly useful for count data."
  },
  {
    "objectID": "PSYC402/Week15.html#lectures",
    "href": "PSYC402/Week15.html#lectures",
    "title": "Poisson regression",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week follows the recommended chapters in Winter (2020) – see under ‘Reading’ below – and is presented below:\n\nPoisson regression (~28 min)"
  },
  {
    "objectID": "PSYC402/Week15.html#reading",
    "href": "PSYC402/Week15.html#reading",
    "title": "Poisson regression",
    "section": "Reading",
    "text": "Reading\n\nWinter (2020)\nLink\nChapter 13 provides a clear introduction to Poisson regression and its implementation in R."
  },
  {
    "objectID": "PSYC402/Week15.html#pre-lab-activities",
    "href": "PSYC402/Week15.html#pre-lab-activities",
    "title": "Poisson regression",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Getting a feel for Poisson data\nTo get a feel for Poisson data, we’ll use the rpois() function to generate random data that is Poisson-distributed. rpois() needs two bits of information: lambda, and how many numbers you want to generate.\nAs usual, before we get stuck in we need to set up a few things.\n\nTASK: Add code to clear the environment. HINT: rm(list=ls())\n\nNext we need to tell R which libraries to use. For this pre-lab activity, we just need the tidyverse library.\n\nTASK: Add code to load relevant libraries. HINT: library()\n\nOk, now let’s play around with different lambdas to get a feel for the Poisson distribution.\n\nTASK: Copy the code below to your script and run it. Then change the value of lambda in the rpois() function and see how the distribution changes.\n\n\nlambda2 &lt;- rpois(n = 1000, lambda = 2)\n\nlambda2 &lt;- as.data.frame(lambda2)\n\nggplot(data = lambda2, mapping = aes(x = lambda2)) +\n  geom_bar()\n\nQUESTION: What do you notice about the Poisson distribution if you choose a high value for lambda?\n\n\nPre-lab activity 2: Getting ready\n\nGet your files ready\nDownload the 402_week15_forStudents.zip file and upload it into a new folder in RStudio Server.\n\n\nRemind yourself of how to access and work with the RStudio Server.\n\nSign in to the RStudio Server, using the login details provided to you via email. Note that when you are not on campus you need to log into the VPN first (look on the portal if you need more information about that).\nCreate a new folder for this week’s work.\nUpload the zip-file to the folder you have created on the RStudio server. Note you can either upload a single file or a zip-file, not a folder with multiple files.\nI highly recommend using R Projects to structure your workflow. You could create an R project for each week of the module. Have a look at section 8 Workflow: projects of R for Data Science by Hadley Wickam and Gareth Grolemund for an introduction."
  },
  {
    "objectID": "PSYC402/Week15.html#lab-activities",
    "href": "PSYC402/Week15.html#lab-activities",
    "title": "Poisson regression",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nwhen and why to apply Poisson regression to answer questions in psychological science\nconducting Poisson regression in R\ninterpreting the R output of Poisson regression\nreporting results for Poisson regression following APA guidelines\n\n\nLab activity 1: Visual dominance\nWinter et al. (2018) showed that, on average, English words that were rated as strongly associated with the visual modality are more frequent than words more strongly associated with other sensory modalities. In this week’s lab activity we will retrace that analysis focusing on the subset of adjectives (the paper also included verbs and nouns). We’ll use sensory modality ratings as reported by Lynott and Connell (2009; see here for more info; data file: lynott_connell_2009_modality.csv) and word frequencies as reported by the English Lexicon Project (data file: ELP_full_length_frequency.csv). The research question is: Do English speakers use ‘visual’ adjectives more frequently than adjectives more strongly associated with other sensory modalities?\n\nStep 1: Set up\nBefore we get stuck in we need to set up a few things.\n\nTASK: Add code to clear the environment. HINT: rm(list=ls())\n\nNext we need to tell R which libraries to use. We need broom, tidyverse, MASS and pscl.\n\nTASK: Add code to load relevant libraries. HINT: library()\n\nFinally, read in the two data files (lynott_connell_2009_modality.csv and ELP_full_length_frequency.csv) and have a look at them.\n\nTASK: Add code to read in the two data files and have a look at them. HINT: Use the read_csv() and head() functions.\n\nQUESTION 1: Which variables do you need to address the research question?\n\n\nStep 2: A bit of data wrangling\nWe need to combine the information in the data files to be able to do any analyses. We can use a ‘join’ to do this. Have a look at the online book by Hadley Wickam and Gareth Grolemund (here) to remind yourself what a ‘join’ is. In particular, have a look at the inner_join() and the left_join().\nQUESTION 2: Which ‘join’ is most appropriate, the inner_join() or the left_join()? Also, does it matter which datafile you specify as x and which one as y? If so, why does it matter?\n\nTASK: Add code to join the two data files and store the resulting table in an object called both. Try out the different joins and use head() to inspect the result. HINT: You should end up with a table that has 423 observations of at least 8 variables.\n\nNext, we want to select only the variables we need. We want to use the select() function from dplyr. Because the MASS library is also loaded and that library also contains a select() function, we need to tell R specifically to use the one from dplyr. You can do this by using dpply::, like this:\n\nboth &lt;- dplyr::select(Word, DominantModality:Smell, Log10Freq)\n\n\nTASK: Add the code above to your script and run it.\n\nFinally, to apply Poisson regression, we need the frequency variable as positive integers.\n\nTASK: Use the code below to transform the frequency variable to raw values. Don’t forget to add it to your script and run it.\n\n\nboth &lt;- mutate(both, Freq = 10 ^ Log10Freq)\n\nQUESTION 3: What does this line of code do. Write a comment to summarise its function.\n\n\nStep 3: Visualise the data\nTo get a better feel for the data, let’s make some scatterplots.\n\nTASK: Add code to make scatterplots with Freq on the y axis and each of the sensory modality ratings on the respective x axis. To be able to see more easily what is going on, limit the y-axis to values between 0 and 20000. HINT: Make 5 different scatterplots using ggplot() withgeom_point() and geom_smooth(). You can use ylim() to limit the values on the y-axis.\n\nQUESTION 4: What do you conclude from the scatterplots?\n\n\nStep 4: The regression model\nWe are going to fit a Poisson regression model with Taste, Smell, Touch, Sight and Sound as predictors (all of these are continuous rating scales).\n\nTASK: Fit a Poisson regression model for ‘Freq’ as a function of ‘Taste’, ‘Smell’, ‘Touch’, ‘Sight’ and ‘Sound’. HINT: Use the glm() function with family = poisson.\n\nQUESTION 5: How do you interpret the output of the Poisson regression?\n\n\nStep 5: Overdispersion\nIn the lecture we saw that it is possible that the variance is larger than theoretically expected for a given lambda. If this happens, we are dealing with what’s called ‘overdispersion’. You can compensate for this by using a variant of Poisson regression that is called ‘negative binomial regression’. In negative binomial regression the variance is uncouples from the mean.\n\nTASK: Fit a negative binomial regression model for ‘Freq’ as a function of ‘Taste’, ‘Smell’, ‘Touch’, ‘Sight’ and ‘Sound’. HINT: Use the glm.nb() function.\n\nNext, check whether there is significant overdispersion by performing a likelihood ratio test, comparing the likelihood of the negative binomial model against the likelihood of the corresponding Poisson model.\n\nTASK: Use the odTest() function to perform an ‘overdispersion’ test.\n\nQUESTION 6: What do you conclude from the results of the overdispersion test?\nQUESTION 7: How do you interpret the negative binomial regression output? Do English speakers use visual adjectives more frequently? What about smell adjectives in comparison?"
  },
  {
    "objectID": "PSYC402/Week15.html#answers",
    "href": "PSYC402/Week15.html#answers",
    "title": "Poisson regression",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\nYou can download the R-script that includes the relevant code here: 402_wk15_labAct1_withAnswers.R.\n\nWhich variables do you need to address the research question? Our dependent variable is word frequency, so we need a variable that tells us how frequent words are. This information is contained in the English Lexicon Project data file, variable Log10Freq. The sensory modality ratings as reported in the in the data file supplied by Lynott and Connell (2009) are our predictors. That means we need the variables ‘Sight’, ‘Touch’, ‘Sound’, ‘Taste’, and ‘Smell’. You also need a variable that is common across the two files to help with merging them together (i.e., by  = ), this would be ‘Word’.\nWhich ‘join’ is most appropriate, the inner_join() or the left_join()? Also, does it matter which datafile you specify as x and which one as y? If so, why does it matter? A left_join() keeps all observations in table x, adding the information from table y for those observations. That means that it does matter which table you specify as x and which one as y. ELP contains word frequencies for many more words than we have sensory modality ratings for (in lyn). When specifying ELP as x, you keep 33075 rows, adding a lot of NAs for words we do not have sensory modality ratings for. Therefore it makes more sense to specify lyn as x, and add the word frequency information for the words we have sensory modality ratings for. An inner_join() matches pairs of observations whenever their keys are equal, and unmatched rows are not included in the result. This means that generally inner joins are not appropriate because it is too easy to lose observations.\nWhat does this line of code do. Write a comment to summarise its function.\n\n\nboth &lt;- mutate(both, Freq = 10 ^ Log10Freq)\n\nUse the mutate() function to add a variable Freq to the table both. Freq is derived from the variable Log10Freq by taking it to the power of 10. We do this because that reverses the log transformation and will therefore give us the raw values, all positive integers that we need for Poisson regression.\n\nWhat do you conclude from the scatterplots? Sight: moderate positive association - the visual rating is higher for more frequent words. Smell: weak negative association - when word frequency goes up, the smell rating goes down. There are a* lot of low smell ratings for adjectives (lots of dots clode to 0). Sound: weak positive association - when word frequency goes up, so does the sound rating. Touch: weak positive association - when word frequency goes up, so does the touch rating. Taste: no assocation - line is pretty much horizontal; similar to smell, there are a lot of low taste ratings (cluster of rating scores around 0).\nHow do you interpret the output of the Poisson regression? Estimate for Sight is indeed positive and largest (compared to the other predictors), while the estimate for Smell is negative and the estimate for Taste is very close to 0. All predictors are significant.\nWhat do you conclude from the results of the overdispersion test? The result of the overdispersion test is significant (p &lt; 2.2e-16), indicating that there is significant overdispersion. The negative binomial regression model is therefore more appropriate.\nHow do you interpret the negative binomial regression output? Do English speakers use visual adjectives more frequently? What about smell adjectives in comparison? The estimates for all predictors have now changed and not all of them are now significant. Also note that the standard errors of the estimates have increased substantially. Sight still shows a significant positive relationship with word frequency and has by far the largest estimate suggesting it contributes most. So, yes, English speakers use visual adjectives more frequently. For smell adjectives the estimate is negative and significant, suggesting that for more frequent words, the Smell ratings are smaller."
  },
  {
    "objectID": "PSYC402/Week17.html",
    "href": "PSYC402/Week17.html",
    "title": "Introduction to linear mixed-effects models",
    "section": "",
    "text": "In ?@sec-intro-multilevel, we looked at a multilevel structured dataset in which there were observations about children’s grades, and it became evident that those children can be grouped by or under classes. As we discussed, this kind of data structure will come from studies with a very common design in which, for example, the researcher records observations about a sample of children who are members of a sample of classes. In working with these kind of data, it is common to say that the observations of children’s grades are nested within classes in a hierarchy.\nMany Psychologists conduct studies where observations are properly understood to be structured in groups of some form but where, nevertheless, it is inappropriate to think of the observations as being nested [@baayen2008]. We are talking, here, about repeated-measures designs where the experimenter presents a sample of multiple stimuli for response to each participant in a sample of multiple participants. This is another very common experimental design in psychological science.\nStudies with repeated-measures designs will produce data with a structure that, also, requires the use of mixed-effects models but, as we shall see, the way we think about the structure will be a bit more complicated. We could say that observations of the responses made by participants to each stimulus can be grouped by participant: each person will tend to respond in similar ways to different stimuli. Or, we could say that observations of responses can be grouped by stimulus because each stimulus will tend to evoke similar kinds of responses in different people. Or, we could say that both forms of grouping should be taken into account at the same time.\nWe shall take the third position and this chapter will concern why, and how we will adapt our thinking and practice."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-motivations",
    "href": "PSYC402/Week17.html#sec-intro-motivations",
    "title": "Introduction to linear mixed-effects models",
    "section": "",
    "text": "In ?@sec-intro-multilevel, we looked at a multilevel structured dataset in which there were observations about children’s grades, and it became evident that those children can be grouped by or under classes. As we discussed, this kind of data structure will come from studies with a very common design in which, for example, the researcher records observations about a sample of children who are members of a sample of classes. In working with these kind of data, it is common to say that the observations of children’s grades are nested within classes in a hierarchy.\nMany Psychologists conduct studies where observations are properly understood to be structured in groups of some form but where, nevertheless, it is inappropriate to think of the observations as being nested [@baayen2008]. We are talking, here, about repeated-measures designs where the experimenter presents a sample of multiple stimuli for response to each participant in a sample of multiple participants. This is another very common experimental design in psychological science.\nStudies with repeated-measures designs will produce data with a structure that, also, requires the use of mixed-effects models but, as we shall see, the way we think about the structure will be a bit more complicated. We could say that observations of the responses made by participants to each stimulus can be grouped by participant: each person will tend to respond in similar ways to different stimuli. Or, we could say that observations of responses can be grouped by stimulus because each stimulus will tend to evoke similar kinds of responses in different people. Or, we could say that both forms of grouping should be taken into account at the same time.\nWe shall take the third position and this chapter will concern why, and how we will adapt our thinking and practice."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-key",
    "href": "PSYC402/Week17.html#sec-intro-key",
    "title": "Introduction to linear mixed-effects models",
    "section": "The key idea to get us started",
    "text": "The key idea to get us started\n\n\n\n\n\n\nImportant\n\n\n\nLinear mixed-effects models and multilevel models are basically the same.\n\n\nThis week, we again look at data with multilevel structure. But we are looking at data where participants were asked to respond to a set of stimuli (here, words) so that our observations consist of recordings made of the response made by each child to each stimulus. We use the same procedure we did for multilevel data but with one significant change which we shall identify and explain."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-targets",
    "href": "PSYC402/Week17.html#sec-intro-targets",
    "title": "Introduction to linear mixed-effects models",
    "section": "Targets",
    "text": "Targets\nOur learning objectives again include the development of both concepts and skills.\n\nskills – practice how to tidy experimental data for mixed-effects analysis.\nconcepts – begin to develop an understanding of crossed random effects of participants and stimuli.\nskills and concepts – practice fitting linear mixed-effects models incorporating random effects of participants and stimuli."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-guide",
    "href": "PSYC402/Week17.html#sec-intro-mixed-guide",
    "title": "Introduction to linear mixed-effects models",
    "section": "Study guide",
    "text": "Study guide\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 8 minutes\nPart 2 – about 21 minutes\nPart 3 – about 15 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter. The lectures provide a summary of the main points.\n2. Chapter: 02-mixed\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data with crossed random effects.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example CP reading study datasets.\nIdentify how the data are structured by both participant and stimulus differences.\nUse visualizations to explore the impact of the structure.\nRun analyses using linear mixed-effects models involving multiple random effects.\nReview the recommended readings (Section 14).\n\n3. Practical workbook materials\n3.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-data",
    "href": "PSYC402/Week17.html#sec-intro-mixed-data",
    "title": "Introduction to linear mixed-effects models",
    "section": "The data we will work with: CP reading study",
    "text": "The data we will work with: CP reading study\nIn this chapter, we will be working with the CP reading study dataset. CP tested 62 children (aged 116-151 months) on reading aloud in English. In the experimental reading task, she presented 160 words as stimuli. The same 160 words were presented to all children. The words were presented one at a time on a computer screen. Each time a word was shown, each child had to read the word out loud and their response was recorded. Thus, the CP reading study dataset comprised observations about the responses made by 62 children to 160 words.\nIn addition to the reading task, CP administered tests of reading skill [TOWRE sight word and phonemic tests, @torgesen1999towre], reading experience [CART, @stainthorp1997children], the Spoonerisms sub-test of the Phonological Awareness test Battery [PhAB, @Frederickson1997a], and an orthographic choice test measure of orthographic knowledge. She also recorded the gender and the handedness of the children.\nWe are going to use the CP study data to examine the answers to a research question similar to the question CP investigated:\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\n\n\n\nWe can look at the answers to this question while also taking into account the impacts of random differences – between sampled participants or between sampled words – using mixed-effects models.\nUltimately, the CP dataset were incorporated in an analysis of the impact of age on reading skills over the life-span, reported by @davies2017. You can find more details on the data and the methods in that paper. (Data and analysis code are shared through the journal article webpage [paywalled] here, and a preprint version of the article can be accessed here.)\nThe CP study resembles many studies in psychological science. The critical features of the study are that:\n\nWe have an outcome measure – the reading response – observed multiple times.\n\n\nWe have multiple responses recorded for each participant: they make one response to each stimulus (here, each stimulus word), for the multiple stimuli that they see in the experimental reading task.\nAnd we have multiple responses recorded for each stimulus: one response is made to each stimulus by each participant, for all the participants who completed the task, in a sample of multiple participants.\n\nThe presence of these features is the reason why we need to use mixed-effects models in our analysis. These features are common across a range of study designs so the lessons we learn will apply frequently in psychological research. This is the reason why it is important we teach and learn how to use mixed-effects models.\n\nLocate and download the data file\nYou can download the data-02-mixed.zip files folder to get the data you need for the practical work we will be doing for this chapter.\nWe will be working with multiple data files located in a .zip folder called data-02-mixed. In this folder, we have got four files that we will need to import or read in to R:\n\nCP study word naming rt 180211.dat\nCP study word naming acc 180211.dat\nwords.items.5 120714 150916.csv\nall.subjects 110614-050316-290518.csv\n\nThe words.items file holds information about the 160 stimulus words presented in the experimental reading (word naming) task. The all.subjects file holds information about the 62 participants who volunteered to take part in the experiment. These .csv files are comma separated values files.\nThe .dat files are tab delimited files holding behavioural data: the latency or reaction time rt (in milliseconds) and the accuracy acc of response made by each participant to each stimulus.\nIn the following, I will describe a series of steps through which we get the data ready for analysis. However, as we shall see, you can avoid these steps by using the pre-tidied dataset:\n\nlong.all.noNAs.csv\n\nThe data files are collected together with the .R scripts:\n\n02-mixed-workbook.R the workbook you will need to do the practical exercises.\n02-mixed-workbook-answers.R with answers to questions and code for exercises.\n\nBefore we do anything else, we need to talk about the messiness of real Psychological data and how we deal with it."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-data-untidy",
    "href": "PSYC402/Week17.html#sec-intro-mixed-data-untidy",
    "title": "Introduction to linear mixed-effects models",
    "section": "The challenges of working with real (untidy) experimental data",
    "text": "The challenges of working with real (untidy) experimental data\nOrdinarily, textbooks and guides to data analysis give you the data ready for analysis but this situation will never be true for your professional practice (at least, not at first). Instead of pretending that data arrive ready for analysis, we are going to look at the process of data tidying, step-by-step. This will help you to get ready for the same process when you have to develop and use it in your own research.\nWe are going to spend a bit of time looking at the data tidying process. This process involves identifying and resolving a series of challenges, in order. Looking at the tidying process will give you a concrete sense of the structure in the data. You should also take this opportunity to reflect on the nature of the process itself – what we have to do and why, in what order and why – so that you can develop a sense of the process you might need to build when the time comes for you to prepare your own data for analysis.\nThe time that we spend looking at data tidying is an investment in learning that will save you time later, in your professional work. If, however, you want to skip it, go to section Section 8.\n\nThe data we need to use for analysis are not all in the same file\nIn analyzing psychological data, the first step is usually to collect the data together. In psychological research, the data may exist, at first, in separate files. For the CP study, we have separate files for each of the pieces of information we need to use in our analyses:\n\nParticipant attributes: information about participants’ age, gender, identifier code, and abilities on various measures.\nStimulus attributes: information about stimulus items, e.g., the word, its item number, its value on each variable in a set of psycholinguistic properties (like word length, frequency).\nBehaviour: behavioural observations e.g. reaction time or accuracy of responses made by each participant to each stimulus word.\n\nOften, we need all these kinds of information for our analyses but different pieces of information are produced in separate ways and come to us in separate files. For example, we may collect experimental response data using software like PsychoPy, E-Prime, Qualtrics or DMDX. We may collect information about participant characteristics using standardized measures, or by asking participants to complete a set of questions on their age, gender, and so on.\n\n\nThe data we need to use are untidy\nOften, the files we get are untidy: not in a useful or tidy format. For example, if you open the file CP_study_word_naming_rt_180211.dat (a .dat or tab delimited file) in Excel, you will see a spreadsheet that looks like Figure 1.\n\n\n\nFigure 1: CP study RTs .dat file\n\n\nTypical of the output from data collection software, we can see a data table with:\n\nin the top row, column header labels item_name, AislingoC, AllanaD ...;\nin the first (leftmost) column, row labels item_name, act, ask, both ...;\nfor each row, we see values equal to the reaction time (RT) observed for the response made to each stimulus (listed in the row labels);\nfor each column, we see values equal to the RTs observed for each person (listed in the column labels);\nand at each intersection of row and column (for each cell), we see the RT observed for a response made by a participant to a stimulus.\n\nData laid out like this are sometimes said to be in wide format. You can see that the data are wide because at least one variable – here, reading reaction time – is held not in one column but spread out over several columns, side-by-side. Thus, the dataset is wide with fewer rows and many columns.\nWe want the data in what is called the tidy format.\n\nHow tidy data are tidy\nThere are three inter-related rules which make data tidy [@grolemund].\n\n\n\n\n\n\nImportant\n\n\n\nIn tidy data:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\nYou can read more about tidy data here.\nFor our purposes, the reason we want the data in tidy format is that it is required for the functions we are going to use for mixed-effects modelling. However, in general, tidy format is maximally flexible, and convenient, for use with different R functions."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-tidy-data",
    "href": "PSYC402/Week17.html#sec-intro-mixed-tidy-data",
    "title": "Introduction to linear mixed-effects models",
    "section": "Tidy the data",
    "text": "Tidy the data\nTo answer our research question, we will need to combine the behavioural data with information about the participants (age, gender …) and about the words (word, frequency …) We will need to ensure that the dataset we construct will be in tidy format. We will need to select variables (columns) to get just those required for our later analyses. And we will need to filter cases (rows), excluding errors or outliers.\nWe shall need to do this work in a series of processing steps:\n\nImport the data or read the data into R, see Section 7.1\nRestructure the data, see Section 7.3\nSelect or transform variables, see Section 7.7\nFilter observations, see Section 7.8\n\nWe will use {tidyverse} library functions from the beginning, starting with the import stage.\n\nlibrary(tidyverse)\n\n(Every step can also be done in alternative processing steps with the same result using base R code.)\n\nRead in the data files by using the read_csv() and read_tsv() functions\nI am going to assume you have downloaded the data files, that they are all in the same folder, and that you know where they are on your computer or server. We need to use different versions of the read_ function to read all four files into R.\n\nbehaviour.rt &lt;- read_tsv(\"CP study word naming rt 180211.dat\", na = \"-999\")\nbehaviour.acc &lt;- read_tsv(\"CP study word naming acc 180211.dat\", na = \"-999\")\nsubjects &lt;- read_csv(\"all.subjects 110614-050316-290518.csv\", na = \"-999\")\nwords &lt;- read_csv(\"words.items.5 120714 150916.csv\", na = \"-999\")\n\nThese different versions respect the different ways in which the .dat and .csv file formats work. We need read_tsv() when data files consist of tab separated values. We need read_csv() when data files consist of comma separated values.\nYou can read more about the {tidyverse} {readr} library of helpful functions here\nIt is very common to get experimental data in all sorts of different formats. Learning to use tidyverse functions will make it easier to cope with this when you do research.\n\n\n\n\n\n\nTip\n\n\n\nWe use the read_ function to read in the data\n\nentering (here, at least) arguments – inside the brackets after the function name – to tell R what file we need and how missing values (NAs) are coded.\n\n\n\nIt will help your understanding to examine an example. Take a look at what this line of code includes, element by element.\n\nbehaviour.rt &lt;- read_tsv(\"CP study word naming rt 180211.dat\", na = \"-999\")\n\n\nWe write behaviour.rt &lt;- read_tsv(...) to create an object in the R environment, which we call behaviour.rt: the object with this name is the dataset we read into R using read_tsv(...).\nWhen we write the function read_tsv(...) we include two arguments inside it.\nread_tsv(\"CP study word naming rt 180211.dat\", ... first, the name of the file, given in quotes \"\" and then a comma.\nread_tsv(..., na = \"-999\") second, we tell R that there are some missing values na which are coded with the value \"-999\".\n\n\n\nA quick lesson about missing value codes\nIn the datasets – typically, the spreadsheets – we create in our research, we will have values missing for different reasons. Take another look at the data spreadsheet you saw earlier, Figure 1.\n\n\n\n\n\n\nTip\n\n\n\nIn R, a missing value is said to be “not available”: NA.\n\n\nYou should be able to see that the spreadsheet holds information, as explained, about the RTs of the responses made by each child to each stimulus word. Each of the cells in the spreadsheet (i.e. the box where a column intersects with a row) includes a number value. Most of the values are positive numbers like 751.3: the reaction time of a response, recorded in milliseconds. The values have to be positive because they represent the length of time between the moment the stimulus word is presented on the test computer screen and the moment the child’s spoken word response has begun to be registered by the computer microphone and sound recording software.\nSome of the cells hold the value -999, however. Obviously, we cannot have negative RT. The value represents the fact that we have no data. Take a look at Figure 1: we have a -999 where we should have a RT for the response made by participant AllanaD to the word broad. This -999 is there because, for some reason, we did not record an RT or a response for that combination of participant and stimulus.\nWe can choose any value we like, as researchers, to code for missing data like this. Some researchers choose not to code for the absence of a response recording or leave the cell in a spreadsheet blank or empty where data are missing. This is bad practice though it is common.\nThere are a number of reasons why it is bad practice to just leave a cell empty when it is empty because no observation is to be recorded.\n\nData may be missing for different reasons: maybe a child did not make any response to a stimulus (often called a “null response”); or maybe a child made a response but there was a microphone or other technical fault; or maybe a child made a response but it was an error and (here) the corresponding performance measure (RT) cannot be counted.\nIf you do not code for missingness in the data then the software you use will do it for you, but you may not know how it does so, or where.\nIf you have missing data, you ought to be able to identify where the data are missing.\n\nI use -999 to code for missing values because you should never see a value like that in real reading RT data. You can use whatever value you like but you should make sure you do code for missing data somehow.\n\n\nReshape the data from wide to long using the pivot_longer() function\nWe are going to need to restructure these data from a wide format to a longer format. We need to restructure both behavioural datasets, accuracy and RT. We do this using the pivot_longer() function.\n\nrt.long &lt;- behaviour.rt %&gt;%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nacc.long &lt;- behaviour.acc %&gt;%\n              pivot_longer(2:62, names_to = \"subjectID\", values_to = \"accuracy\")\n\nResearchers used to have to do this sort of thing by hand, using copying and pasting, in Excel or SPSS. Doing the process by hand takes many hours or days. And you always make errors.\n\n\n\n\n\n\nTip\n\n\n\nDoing dataset construction programmatically, using R functions, is generally faster and more reliable than doing it by hand.\n\n\nHere, we use a function you may have seen before: pivot_longer(). It will help your understanding to examine the code carefully.\n\nrt.long &lt;- behaviour.rt %&gt;%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nThe name of the function comes from the fact that we are starting with data in wide format e.g. behaviour.rt where we have what should be a single variable of observations (RTs) arranged in a wide series of multiple columns, side-by-side (one column for each participant). But we want to take those wide data and lengthen the dataset, increasing the number of rows and decreasing the number of columns.\nLet’s look at this line of code bit by bit. It includes a powerful function that accomplishes a lot of tasks, so it is worth explaining this function in some detail.\n\nrt.long &lt;- behaviour.rt %&gt;%\n\n\nAt the start, I tell R that I am going to create a new longer dataset (more rows, fewer columns) that I shall call rt.long.\nI will create this longer dataset from &lt;- the original wide dataset behaviour.rt.\nand I will create the new longer dataset by taking the original wide dataset and piping it %&gt;% to the pivot function coded on the next line:\n\n\npivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\n\nOn this next line, I tell R how to do the pivoting by using three arguments.\n\n\npivot_longer(2:62...)\n\n\nFirst, I tell R that I want to re-arrange all the columns that can be found in the dataset from the second column to the sixty-second column.\nIn a spreadsheet, we have a number of columns.\nColumns can be identified by their position in the spreadsheet.\nThe position of a column in a spreadsheet can be identified by number, from the leftmost column (column number 1) to the rightmost column (here, column number 62) in our dataset.\nSo this argument tells R exactly which columns I want to pivot.\n\n\npivot_longer(..., names_to = \"subjectID\", ...)\n\n\nSecond, I tell R that I want it to take the column labels and put them into a new column, called subjectID.\nIn the wide dataset behaviour.rt, each column holds a list of numbers (RTs) but begins with a word in the topmost cell, the name code for a participant, in the column label position.\nWe want to keep the information about which participant produces which response when we pivot the wide data to a longer structure.\nWe do this by asking R to take the column labels (the participant names) and listing them in a new column, called subjectID which now holds the names as participant ID codes.\n\n\npivot_longer(...values_to = \"RT\")\n\n\nThird, we tell R that all the RT values should be put in a single column.\nWe can understand that this new column RT will hold RT observations in a vertical stack, one cell for each response by a person to a word, with rows ordered by subjectID.\n\nThere are 61 columns of data listed by participant though 62 children were tested because we lost one child’s data through an administrative error. As a result, in the wide data sets there are 62 columns, with the first column holding item_name data.\nYou can find more information about pivoting data here\nAnd you can find more information specifically about the pivot_longer() operation here\n\n\nWhy we restructure the data\nAs I noted, one problem with the wide format is that the data are structured so that the column names are not names of variables. In our example wide format dataset, behaviour.rt, the columns are headed by a participant identity code or name but a participant code is not the name of a variable, it is a value of the variable I call subjectID.\nIn the design of the CP reading study, we want to take into account the impact of differences between participants on response RT (so, we need to identify which participant makes which response). But we do not see the responses made by a participant as a predictor variable.\nA second problem is that, in a wide format file like behaviour.rt, information about the responses made to each stimulus word is all on the same row (that seems good) but in different columns. Each person responded to all the words. But the response made to a word e.g. act made by one participant is in a different column (e.g., 594.8ms, for AislingoC) from the response made to the same word by a different participant (e.g., 586ms, for AlexB). This means that information about the responses made to each stimulus word are spread out as values across multiple columns.\nYou can see this for yourself if you inspect the source rt data using head() to view the top four rows of the dataset.\n\n\n\n\n\nitem_name\nAislingoC\nAlexB\nAllanaD\nAmyR\nAndyD\nAnnaF\nAoifeH\nChloeBergin\nChloeF\nChloeS\nCianR\nConorF\nDavidL\nDillonF\nDJHerlihy\nEamonD\nEimearK\nEllenH\nEoinL\nGrainneH\nJackBr\nJackK\nJackS\nJamesoC\nJenniferoS\nKateF\nKayleighMc\nKenW\nKevinL\nKieranF\nKillianB\nKirstyC\nLeeJ\nMarkC\nMatthewC\nMeganoB\nMichaelaoD\nNataliaR\nNiallG\nNiallGavin\nNiallW\nOisinN\nOlaA\nOwenD\nPalomaM\nPauricT\nPerryD\nRachelD\nRebeccaGr\nRebeccaM\nRebeccaR\nRoisinF\nRonanT\nSarahP\nShaunaBr\nSiobhanR\nTaraB\nTeeTeeOj\nThomasK\nTristianT\nZainab\n\n\n\n\nact\n594.8\n586.0\nNA\n693.0\n597\n627.0\n649.0\n1081.0\n642.0\n622.7\n701.0\n686.0\n951.0\n661.0\n692.0\n670.0\n502.4\n578.4\n651.8\n441.6\n895.2\n529.0\n639.0\n809\n676.9\n568.8\n586.1\n591.3\n587\n586\n723.9\n1428.0\n557.0\n639.4\n676\n714.8\n623.3\n615.0\n796.0\n568.4\n800.9\n595\nNA\n574.9\n628\n797.0\n652\n757.0\n631.0\n520.5\n640.0\n733.0\n566\n758.0\n670.0\n532.4\n615.7\n540.0\n1390.0\n747\n651.0\n\n\nask\n481.5\n864.0\n1163.0\n694.4\n616\n631.0\n538.0\n799.3\n603.0\n526.0\n591.5\n699.6\n827.2\n635.0\n654.0\n508.0\n564.0\n822.0\n479.7\n600.0\n617.6\n555.9\n654.6\n765\n856.0\n576.7\n690.3\n501.9\n634\n626\n523.3\n640.7\n516.0\n625.7\n561\n698.2\n685.0\n606.0\n793.0\n551.9\n668.7\n722\n868.0\n633.0\n578\n660.2\n851\n640.3\n630.0\n535.0\n568.0\n579.7\n562\n747.0\n602.4\n558.4\n691.0\n580.8\n590.9\n795\n740.5\n\n\nboth\n457.5\n670.0\n1114.3\n980.0\n1019\n796.1\n545.2\nNA\n581.0\n568.4\n665.0\n751.0\n917.0\n808.1\n737.6\n597.0\n475.0\n608.0\n699.2\n600.6\n527.3\n601.0\n982.1\n917\n854.4\n571.6\n825.3\n584.0\n720\n571\n624.0\n853.4\nNA\n703.0\n781\n1065.8\n591.5\n559.2\n837.6\n612.0\n743.4\n743\n919.0\n883.0\n616\n734.6\n658\n574.4\n634.9\n559.7\n689.0\n997.8\n648\n753.4\n548.0\n530.5\n625.4\n552.0\n985.0\n640\n764.0\n\n\nbox\n546.0\n748.6\n975.0\n678.0\n589\n604.0\n574.0\n658.0\n688.7\n492.0\n641.0\n699.0\n824.0\n731.6\n634.0\n557.5\n520.1\n581.0\n1191.0\n612.0\n540.0\n540.0\n810.0\n885\n573.8\n614.9\n722.0\n594.6\n632\n549\n564.0\n632.6\n668.6\n627.0\n577\n785.9\n649.0\n616.0\n928.0\n608.7\n690.0\n857\n886.6\n544.0\n863\n726.0\n812\n640.5\n613.8\n395.1\n656.7\n604.7\n530\n667.4\n547.7\n570.0\n585.6\n532.0\n634.0\n764\n912.0\n\n\n\n\n\n\n\nThis structure is a problem for visualization and for analysis because the functions we will use require us to specify single columns for an outcome variable like reaction time.\nWe are looking at the process of tidying data because untidiness is very common. Learning how to deal with it will save you a lot of time and grief later.\nYou should check for yourself how subjectID and RT or accuracy scores get transposed from the old wide structure to the new long structure.\n\n# RT data\nhead(behaviour.rt)\nhead(rt.long)\n\n# accuracy data\nhead(behaviour.acc)\nhead(acc.long)\n\nIf you compare the rt.long or acc.long data with the data in the original wide format then you can see how – in going from wide – we have re-arranged the data to a longer and narrower set of columns:\n\none column listing each word;\none column for subjectID;\nand one column for RT or accuracy.\n\nWhat a check will show you is that we have multiple rows for responses to each item so that the item is repeated multiple times in different rows.\nThese data are now tidy.\n\nEach column has information about one variable\nAnd each row has information about one observation, here, the response made by a participant to a word\n\nThis is a big part of data tidying now done. However, these data are incomplete. Next we shall combine behavioural observations with data about stimulus words and about participants.\n\n\nThe tidyverse evolves\nOver the years, different ways of reshaping data have evolved. This reflects how important and common the task is. An older way to do the same operation uses the function gather().\nYou can read more about gather() here\nIn {tidyverse} the functions designed to enable you to restructure data have evolved through a series of different forms. This change is one of the real benefits of using open software like R. In my experience, the newer functions can be useful for really untidy data. I expect things will continue to evolve and improve over time.\n\n\nMerging data from different datasets using _join() functions\nTo answer our research question, we next need to combine the RT with the accuracy data, and then the combined behavioural data with participant information and stimulus information. This is because, as we have seen, information about behavioural responses, about participant attributes or stimulus word properties, are located in separate files.\nMany researchers have completed this kind of operation by hand. This involves copying and pasting bits of data in a spreadsheet. It can take hours or days. I know because I have done it, and I have seen others do it.\n\n\n\n\n\n\nWarning\n\n\n\nPlease do not try to combine datasets through manual operations e.g. in Excel. I guarantee that:\n\nYou will make mistakes.\nYou will not know when or where those mistakes are in your data.\n\nThere are better ways to spend your time.\n\n\nWe can combine the datasets, in the way that we need, using the {tidyverse} full_join() function. This gets the job done quickly, and accurately.\nFirst, we join RT and accuracy data together.\n\nlong &lt;- rt.long %&gt;% \n          full_join(acc.long)\n\nThen, we join subject and item information to the behavioural data.\n\nlong.subjects &lt;- long %&gt;% \n                   full_join(subjects, by = \"subjectID\")\n\nlong.all &lt;- long.subjects %&gt;%\n              full_join(words, by = \"item_name\")\n\nNotice, we can let R figure out how to join the pieces of data together. If we were doing this by hand then we would need to check very carefully the correspondences between observations in different datasets.\nHere, in a series of steps, we take one dataset and join it (merge it) with the second dataset. Let’s look at an example element by element to better understand how this is accomplished.\n\nlong &lt;- rt.long %&gt;% \n           full_join(acc.long)\n\nThe code work as follows.\n\nlong &lt;- rt.long %&gt;%\n\n\nWe create a new dataset we call long.\nWe do this by taking one original dataset rt.long and %&gt;% piping it to the operation defined in the second step.\n\n\nfull_join(acc.long)\n\n\nIn this second step, we use the function full_join() to add observations from a second original dataset acc.long to those already from rt.long\n\nThe addition of observations from one database joining to those from another happens through a matching process.\n\nR looks at the datasets being merged.\nIt identifies if the two datasets have columns in common. Here, the datasets have subjectID and item_name in common).\nR can use these common columns to identify rows of data. Here, each row of data will be identified by both subjectID and item_name i.e. as data about the response made by a participant to a word.\nR will then do a series of identity checks, comparing one dataset with the other and, row by row, looking for matching values in the columns that are common to both datasets.\nIf there is a match then R joins the corresponding rows of data together.\nIf there isn’t a match then it creates NAs where there are missing entries in one row for one dataset which cannot be matched to a row from the joining dataset.\n\n\n\n\n\n\n\nTip\n\n\n\nNote that in one example, the example of code I discuss here, I did not specify identifying columns in common, allowing the function to do the work. In the other code chunks I did: long.all &lt;- long.subjects %&gt;% full_join(words, by = \"item_name\") using the by = ... argument.\n\nSometimes, you can vary in how you employ a _join() function.\nIt may help to specify the identifying column if you want to make explicit (to yourselves and others) how the process is to be completed.\n\n\n\n\nRelational data\nIn the {tidyverse} family of dplyr functions, when you work with multiple datasets (tables of data), we call the datasets relational data.\nThere are three families of functions (like verbs) designed to work with relational data:\n\nMutating joins, which add new variables to one data frame from matching observations in another.\nFiltering joins, which filter observations from one data frame based on whether or not they match an observation in the other table.\nSet operations, which treat observations as if they were set elements.\n\nWe can connect datasets – relate them – according to shared variables like subjectID, item_name (for our data). In {tidyverse}, the variables that connect pairs of tables are called keys where, and this is what counts, key(-s) are variable(-s) that uniquely identify an observation.\nFor the experimental reading data, we have observations about each response made by a participant (one of 61 subjects) to an item (one of 160 words). For these data, we can match up a pair of RT and accuracy observations for each (unique) subjectID-item_name combination. If you reflect, we could not combine the RT and accuracy data correctly if we did not have both identifying variables in both datasets, because both column variables are required to uniquely identify each observation.\nFurther, we could not combine the RT and accuracy data correctly if there were mismatches in values of the identifying variable. Sometimes, I have done this operation and it has gone wrong because a subjectID has been spelled one way in one dataset e.g. hugh and another way in the other dataset e.g. HughH. This leads me to share some advice.\n\n\n\n\n\n\nTip\n\n\n\n\nBe careful about spelling identifiers.\nAlways check your work after merger operations.\n\nYou can check your work by calculating dataset lengths to ensure the number of rows in the new dataset matches your expectations, given the study design and data collection procedure.\n\n\n\n\n_join functions\nWe used the full_join() function.\nThere are three kinds of joins.\n\nA left join keeps all observations in x.\nA right join keeps all observations in y.\nA full join keeps all observations in x and y.\n\nI used full_join() because I wanted to retain all observations from both datasets, whether there was a match (as assumed) or not, in the identifying variables, between observations in each dataset.\n\n\nExercise\n\nBreak the join: You can examine how the full_join() works by experimenting with stopping it from working.\n\nAs I discuss, you need to have matches in values on key (common) variables. If the subjectID is different on different datasets, you will lose data that would otherwise be merged to form the merged or composite dataset.\nCheck what happens if you deliberately misspell one of the subjectID values in one of the original source wide behavioural data files.\nTo be safe, you might want to do this exercise with copies of the source files kept in a folder you create for this purpose. If it goes wrong, you can always re-access the source files and read them in again.\nYou can check what happens before and after you break the match by counting the number of rows in the dataset that results from the merger. We can count the number of rows in a dataset with:\n\nlength(long.all$RT)\n\n[1] 9762\n\n\nThis bit of code takes the length of the vector (i.e. variable column RT in dataset long.all), thus counting the number of rows in the dataset.\n\n\n\nSelect or transform the variables\nOK, now we have all the data about everything all in one big, long and wide, dataset. But we do not actually require all of the dataset for the analyses we are going to do.\nWe next need to do two things. First, we need to get rid of variables we will not use: we do that by using select(). Then, we need to remove errors and outlying short RT observations: we do that by using filter() in Section Section 7.8.\nWe are going to select just the variables we need using the select() function.\n\nlong.all.select &lt;- long.all %&gt;% \n                        select(item_name, subjectID, RT, accuracy, \n                               Lg.UK.CDcount, brookesIMG, AoA_Kup_lem, \n                               Ortho_N, regularity, Length, BG_Mean, \n                               Voice,   Nasal,  Fricative,  Liquid_SV,\n                               Bilabials,   Labiodentals,   Alveolars,\n                               Palatals,    Velars, Glottals, \n                               age.months, TOWREW_skill, TOWRENW_skill, \n                               spoonerisms, CART_score)\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that these variables do not have reader-friendly names: but naming things is important.\n\nCheck out the ever-useful Jenny Bryan’s advice.\n\n\n\nThe names we have in the CP study data were fine for internal use within my research group but we should be careful to ensure that variables have names that make sense to others and to our future selves. We can adjust variable names using the rename() function but I will leave that as an exercise for you to do.\n\nExercise\n\nSelect different variables: You could analyze the CP study data for a research report.\n\nWhat if you wanted to analyze a different set of variables, could you select different variables?\n\n\n\nFilter observations\nWe now have a tidy dataset long.all.select with 26 columns and 9762 rows.\nThe dataset includes missing values, designated NA. Here, every error (coded 0, in accuracy) corresponds to an NA in the RT column.\nThe dataset also includes outlier data. In this context, \\(RT &lt; 200\\) are probably response errors or equipment failures. We will want to analyse accuracy later, so we shall need to be careful about getting rid of NAs.\nAt this point, I am going to exclude two sets of observations only.\n\nobservations corresponding to correct response reaction times that are too short: \\(RT &lt; 200\\).\nplus observations corresponding to the word false which (because of stupid Excel auto-formatting) dropped item attribute data.\n\nWe can do this using the filter() function, setting conditions on rows, as arguments.\n\n# step 1\nlong.all.select.filter &lt;- long.all.select %&gt;% \n                            filter(item_name != 'FALSE')\n\n# step 2\nlong.all.select.filter &lt;- long.all.select.filter %&gt;%\n                            filter(RT &gt;= 200)\n\nHere, I am using the function filter() to …\n\nCreate a new dataset long.all.select.filter &lt;- ... by\nUsing functions to work on the data named immediately to the right of the assignment arrow: long.all.select\nAn observation is included in the new dataset if it matches the condition specified as an argument in the filter() function call, thus:\n\n\nfilter(item_name !='FALSE') means: include in the new dataset long.all.select.filter all observations from the old dataset long.all.select that are not != (! not = equal to) the value FALSE in the variable item_name,\nthen recreate the long.all.select.filter as a version of itself (with no name change) by including in the new version only those observations where RT was greater than or equal to 200ms using RT &gt;= 200.\n\n\n\n\n\n\n\nWarning\n\n\n\nThe difference between = and ==\nYou need to be careful to distinguish these signs.\n\n= assigns a value, so x = 2 means “x equals 2”\n== tests a match so x == 2 means: “is x equal to 2?”\n\n\n\n\nUsing multiple arguments in filtering\nYou can supply multiple arguments to filter() and this may be helpful if (1.) you want to filter observations according to a match on condition-A and condition-B (logical “and” is coded with &) or (2.) you want to filter observations according to a match on condition-A or condition-B (logical “or” is coded |).\nYou can read more about using multiple arguments to filter observations here.\n\n\nExercise\n\nVary the filter conditions: in different ways\n\n\nChange the threshold for including RTs from RT &gt;= 200 to something else\nCan you assess what impact the change has? Note that you can count the number of observations (rows) in a dataset using e.g. length(data.set.name$variable.name)\n\nFiltering or re-coding observations is an important element of the research workflow in psychological science, as I discuss in ?@sec-multiversedata. How we do or do not remove observations from original data may have an impact on our results (as explored by, e.g., Steegen et al., 2014). It is important, therefore, that we learn how to do this reproducibly using R scripts that we can share with our research reports.\nYou can read further information about filtering here.\n\n\n\nRemove missing values\nWe will be working with the long.all.select.filter.csv dataset collated from the experimental, subject ability scores, and item property data collected for the CP word naming study.\nFor convenience, I am going to remove missing values before we go any further, using the na.omit() function.\n\nlong.all.noNAs &lt;- na.omit(long.all.select.filter)\n\n\n\n\n\n\n\nTip\n\n\n\nThe na.omit() function is powerful.\n\nIn using this function, I am asking R to create a new dataset long.all.noNAs from the old dataset long.all.select.filter in a process in which the new dataset will have no rows in which there is a missing value NA in any column.\nYou need to be reasonably sure, when you use this function, where your NAs may be because, otherwise, you may end the process with a new filtered dataset that has many fewer rows in it than you expected.\n\n\n\n\n\nNow we have some tidy data\n\nhead(long.all.noNAs, n = 10)\n\n# A tibble: 10 × 26\n   item_n…¹ subje…²    RT accur…³ Lg.UK…⁴ brook…⁵ AoA_K…⁶ Ortho_N regul…⁷ Length\n   &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 act      Aislin…  595.       1    4.03       4    6.42       5       1      3\n 2 act      AlexB    586        1    4.03       4    6.42       5       1      3\n 3 act      AmyR     693        1    4.03       4    6.42       5       1      3\n 4 act      AndyD    597        1    4.03       4    6.42       5       1      3\n 5 act      AnnaF    627        1    4.03       4    6.42       5       1      3\n 6 act      AoifeH   649        1    4.03       4    6.42       5       1      3\n 7 act      ChloeB… 1081        1    4.03       4    6.42       5       1      3\n 8 act      ChloeF   642        1    4.03       4    6.42       5       1      3\n 9 act      ChloeS   623.       1    4.03       4    6.42       5       1      3\n10 act      CianR    701        1    4.03       4    6.42       5       1      3\n# … with 16 more variables: BG_Mean &lt;dbl&gt;, Voice &lt;dbl&gt;, Nasal &lt;dbl&gt;,\n#   Fricative &lt;dbl&gt;, Liquid_SV &lt;dbl&gt;, Bilabials &lt;dbl&gt;, Labiodentals &lt;dbl&gt;,\n#   Alveolars &lt;dbl&gt;, Palatals &lt;dbl&gt;, Velars &lt;dbl&gt;, Glottals &lt;dbl&gt;,\n#   age.months &lt;dbl&gt;, TOWREW_skill &lt;dbl&gt;, TOWRENW_skill &lt;dbl&gt;,\n#   spoonerisms &lt;dbl&gt;, CART_score &lt;dbl&gt;, and abbreviated variable names\n#   ¹​item_name, ²​subjectID, ³​accuracy, ⁴​Lg.UK.CDcount, ⁵​brookesIMG,\n#   ⁶​AoA_Kup_lem, ⁷​regularity\n\n\nIf we inspect long.all.noNAs, we can see that we have now got a tidy dataset with all the data we need for our analyses:\n\nOne observation per row, corresponding to data about a response made by a participant to a stimulus in an experimental trial\nOne variable per column\nWe have information about the speed and accuracy of responses\nAnd we have information about the children and about the words.\n\nWe have removed the missing values and we have filtered outliers.\n\n\nWe can output the data as a .csv file\nHaving produced the tidy dataset, we may wish to share it, or save ourselves the trouble of going through the process again. We can do this by creating a .csv file.\n\nwrite_csv(long.all.noNAs, \"long.all.noNAs.csv\")\n\nThis function will create a .csv file from the dataset you name long.all.noNAs which R will put in your working directory.\n\n\nData tidying – conclusions\nMost research work involving quantitative evidence requires a big chunk of data tidying or processing before you get to the statistics. Most of the time, this is work you will have to do. The lessons you can learn about the process will generalize to many future research scenarios.\n\nIt is a mistake to think of data tidying or wrangling as an inconvenience or as an extra task or something you need to do to get to the ‘good stuff’ (your results).\n\nAll analysis results follow from and thus are determined by the data processing steps that precede analysis.\nAnalysis results can and do vary, perhaps critically, depending on different processing decisions, and reasonable people may differ on key processing decisions.\nThe process of data tidying is frequently instructive of your data recording quality: you find things out about your field measurements or your instrument integrity or quality of your recordings, when you pay attention, when you process your data.\n\n\nIt is wise to see data tidying or processing as a key part of the data analysis workflow because, as I discuss in ?@sec-multiversedata, the choices you make or the integrity or quality of the actions you take, will have consequences for your analysis results or, more generally, for the quality of the evidence you share with others.\nHere is a nice substack post that links to some scholarly writing and makes some excellent points.\n\n\n\n\n\n\nTip\n\n\n\nA key recommendation is that you write code to tidy or process data, thus creating a self-documented auditable data tidying process in your analysis workflow.\n\nThis is simple to do in R and that is one important reason why we use and teach R."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-crossed-random",
    "href": "PSYC402/Week17.html#sec-intro-mixed-crossed-random",
    "title": "Introduction to linear mixed-effects models",
    "section": "Repeated measures designs and crossed random effects",
    "text": "Repeated measures designs and crossed random effects\nOur focus in this chapter is on analyzing data that come from studies with repeated-measures designs where the experimenter presents multiple stimuli for response to each participant.\nIn our working example, the CP reading study, CP asked all participants in her study to read a selection of words. All participants read the same selection of words, and every person read every word. For each participant, we have multiple observations and these (within-participant) observations will not be independent of each other. One participant will tend to be slower or less accurate compared to another participant, on average. Likewise, one participant’s responses will reveal a stronger (or weaker) impact of the effect of an experimental variable than another participant. These between-participant differences will tend to be apparent across the sample of participants.\nYou could say that the lowest trial-level observations can be grouped with respect to participants, that observations are nested within participant. But the data can also be grouped by stimuli. Remember that in the CP study, all participants read the same selection of words, and every person read every word. This means that for each stimulus word, there are multiple observations because all participants responded to each word, and these (within-item) observations will not be independent of each other. One word may prove to be more challenging compared to another, eliciting slower or less accurate responses, on average. Likewise, participants’ responses to a word will reveal a stronger (or weaker) impact of the effect of an experimental variable than the responses to another word. Again, these between-stimulus differences will tend to be apparent when you examine observations of responses across the sample of words.\nUnder these circumstances, are observations about the responses made by different participants nested under words, or are observations about the responses to different words nested under participants? We do not have to make a decision.\nGiven this common repeated-measures design, we can analyze the outcome variable in relation to:\n\nfixed effects: the impact of independent variables like participant reading skill or word frequency\nrandom effects: the impact of random or unexplained differences between participants and also between stimuli\n\nIn this situation, we can say that the random effects are crossed [@baayen2008]. When multilevel models require the specification of crossed random effects, they tend to be called mixed-effects models."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-working-models",
    "href": "PSYC402/Week17.html#sec-intro-mixed-working-models",
    "title": "Introduction to linear mixed-effects models",
    "section": "Working with mixed-effects models",
    "text": "Working with mixed-effects models\nTo illustrate the approach, we examine observations from the CP study. We begin, as we did previously, by ignoring differences due to grouping variables (like participant or stimulus). We pretend that all observations are independent. In this fantasy situation, we address our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\n\n\n\n\nLoad the data if you need to\nIf you have not completed the process of tidying the CP study data then you can import the pre-tidied data here.\n\nlong.all.noNAs &lt;- read_csv(\"long.all.noNAs.csv\", \n                  col_types = cols(\n                    subjectID = col_factor(),\n                    item_name = col_factor()\n                    )\n                  ) \n\nNotice that I am using read_csv() with an additional argument col_types = cols(...).\n\nHere, I am requesting that read_csv() treats subjectID and item_name as factors.\n\n\n\n\n\n\n\nTip\n\n\n\nWe can use col_types = cols(...) to control how read_csv() interprets specific column variables in the data.\n\n\nControlling the way that read_csv() handles variables is a very useful capacity, and a more efficient way to work than, say, first reading in the data and then using coercion to ensure that variables are assigned appropriate types. You can read more about it here.\n\n\nLinear model for multilevel data – ignoring the hierarchical structure\nWe begin our data analysis by asking if reading reaction time (RT) varies in association with word frequency. A scatterplot shows that response latencies decrease with increasing word frequency (Figure 2).\n\nlong.all.noNAs %&gt;%\nggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .2) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 1.5, colour=\"red\") + \n  theme_bw() + \n  xlab(\"Word frequency: log context distinctiveness (CD) count\")\n\n\n\n\nFigure 2: Reading reaction time compared to word frequency, all data\n\n\n\n\nIn the plot, we see that the best fit line drawn with geom_smooth() trends downward for higher values of word frequency. This means that Figure 2 suggests that RT decreases with increasing word frequency. (I know there is a weird looking line of points around 0 but we can ignore that here.)\nWe can estimate the relationship between RT and word frequency using a linear model in which we ignore the possibility that there may be differences (between subjects, or between items) in the intercept or (between subjects) in the slope of the frequency effect. This simplified model can be stated as:\n\\[\nY_{ij} = \\beta_0 + \\beta_1X_j + e_{ij}\n\\]\n\nwhere \\(Y_{ij}\\) is the value of the observed outcome variable, the RT of the response made by the \\(i\\) participant to the \\(j\\) word;\n\\(\\beta_1X_j\\) refers to the fixed effect of the explanatory variable (here, word frequency), where the frequency value \\(X_j\\) is different for different words \\(j\\), and \\(\\beta_1\\) is the estimated coefficient of the effect due to the relationship between response RT and word frequency;\n\\(e_{ij}\\) is the residual error term, representing the differences between observed \\(Y_{ij}\\) and predicted values (given the model).\n\nThe linear model can be fit in R using the lm() function, as we have done previously.\n\n# label: lm-all-freq\nlm.all.1 &lt;- lm(RT ~  Lg.UK.CDcount, data = long.all.noNAs)\n\nsummary(lm.all.1)\n\n\nCall:\nlm(formula = RT ~ Lg.UK.CDcount, data = long.all.noNAs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-346.62 -116.03  -38.37   62.05 1981.58 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    882.983     11.901   74.19   &lt;2e-16 ***\nLg.UK.CDcount  -53.375      3.067  -17.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185.9 on 9083 degrees of freedom\nMultiple R-squared:  0.03227,   Adjusted R-squared:  0.03216 \nF-statistic: 302.8 on 1 and 9083 DF,  p-value: &lt; 2.2e-16\n\n\nIn the estimates from this linear model, we see an approximate first answer to our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\nResult: Our analysis shows that the estimated effect of word frequency is \\(\\beta = -53.375\\). This means that, according to the linear model, RT decreases by about 54 milliseconds for each unit increase in log word frequency.\n\n\n\nNotice that, here, word frequency information is located in the Lg.UK.CDcount variable. In a common move for reading data analyses, we transformed the frequency estimate to the Log base 10 of the word frequency values, prior to analysis, in part because word frequency estimates are usually highly skewed.\nThe model does not explain much variance, as \\(Adjusted \\space R^2 = .03\\) but, no doubt due to the large sample, the regression model is significant overall \\(F(1,9083) = 302.8, p &lt; .001\\).\n\nExercise\n\nVary the linear model: using different outcomes or different predictor variables.\n\nThe CP study dataset is rich with possibility. It would be useful to experiment with it.\n\nChange the predictor from frequency to something else: what do you see when you visualize the relationship between variables using scatterplots?\nSpecify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?\n\n\n\n\nCan we ignore the hierarchical structure?\nThe problem is that, as we have discussed, we assume that observations are independent for the linear model yet we can suppose in advance that that assumption of independence will be questionable given the expectation that participants’ responses will differ in predictable ways: one participant’s responses will perhaps be slower or less accurate than another, perhaps more or less affected by word frequency than another.\nWe can examine that variation by estimating the intercept and the slope of the frequency effect separately using the data for each participant alone. We can start by visualizing the frequency effect for each child in a grid of plots, with each plot representing the \\(\\text{RT} \\sim \\text{frequency}\\) relationship for the data for a child (Figure 3).\nWe looked at how the plotting code works in ?@sec-multi-plot-lattice.\n\nlong.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n    geom_point(alpha = .2) + \n    geom_smooth(method = \"lm\", se = FALSE, size = 1.25, colour = \"red\") + \n    theme_bw() + \n    xlab(\"Word frequency (log10 UK SUBTLEX CD count)\") + \n    facet_wrap(~ subjectID)\n\n\n\n\nFigure 3: RT vs. word frequency, considered separately for data for each child\n\n\n\n\nFigure 3 shows how, on average, more frequent words are associated with shorter reaction time: faster responses. The plot further shows, however, that the effect of frequency varies considerably between children.\n\nSome children show little or no effect; the best fit line is practically level.\nSome children show a marked effect, with a steep fit line indicating a strong frequency effect.\n\nWe can get more insight into the differences between children if we plot the estimated intercept and frequency effect coefficients for each child directly. This allows more insight because it focuses the eye on the differences between children in the estimates. We do this next: see Figure 4. (I work through the code for generating the plot in: 02-mixed-workbook-with-answers.R)\n\n\n\n\n\nFigure 4: Estimated intercepts and frequency effect slopes calculated for each child’s data, with child data analysed separately for all children. Points represent estimates. Lines represent standard errors for estimates. Point estimates are presented in order of size.\n\n\n\n\nFigure 4 presents two plots showing the estimates of the intercept and the coefficient of the effect of word frequency on reading RT, calculated separately for each child. This means that we fitted a separate linear model, for the association between RT and frequency, using the data for just one child, for each child in our sample, for all the children.\nThe estimate for each child is shown as a black dot. The standard error of the estimate is shown as a black vertical line, shown above and below a point. You can say that where there is a longer line there we have more uncertainty about the location of the estimate.\nThe estimates calculated for each child are shown ordered from left to right in the plot by the size of the estimate. This adjustment to the plot reveals how the estimates of both the intercept and the slope of the frequency effect vary substantially between children.\n\n\n\n\n\n\nImportant\n\n\n\nThe first key observation is that if there is an average intercept for everyone in the sample or, better, an intercept we could estimate for everyone in the population, then the different intercepts we have estimated for each child would be distributed around that population-level average:\n\nSome children will have slower (here, larger) intercepts\nand other children will have faster (shorter) intercepts.\n\n\n\nHere, the intercept can be taken to be the average RT when all other effects in the model are set to zero. RT varies for this sample around somewhere like \\(\\beta_0 = 883ms\\) so a slower larger intercept might be e.g. \\(\\beta_0 = 1000ms\\).\n\n\n\n\n\n\nImportant\n\n\n\nThe second key observation is that if there is an average slope for the frequency effect, an effect of frequency on reading RT, averaged across everyone in the population, then, again, the different slopes we have estimated for each child would be distributed around that population-level effect.\n\nSome children will have larger (here, more negative) frequency effects\nand other children will have smaller (less negative) frequency effects.\n\n\n\nHere, the frequency effect is associated with a negative coefficient e.g. \\(\\beta_1 = -53\\) so a larger frequency effect will be a bigger negative number e.g. \\(\\beta_1 = -100\\).\n\n\nMultilevel – here, more appropriately known as – mixed-effects models\nIn a mixed-effects model, we account for this variation: the differences between participants in intercepts and slopes.\nWe do this by modeling the intercept as two terms:\n\\[\n\\beta_{0i} = \\gamma_0 + U_{0i}\n\\]\n\nwhere \\(\\gamma_0\\) is the average intercept and \\(U_{0i}\\) is the difference for each \\(i\\) child between their intercept and the average intercept.\n\nWe model the frequency effect as two terms:\n\\[\n\\beta_{1i} = \\gamma_1 + U_{1i}\n\\]\n\nwhere \\(\\gamma_1\\) is the average slope and \\(U_{1i}\\) represents the difference for each \\(i\\) child between the slope of their frequency effect and the average slope.\n\nWe can then incorporate in a single model the fixed effects due to the average intercept and the average frequency effect, as well as the random effects, error variance due to unexplained differences between participants in intercepts and frequency effects:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n\\]\n\nwhere the outcome \\(Y_{ij}\\) is related to …\nthe average intercept \\(\\gamma_0\\) and differences between \\(i\\) children in the intercept \\(U_{0i}\\);\nthe average effect of the explanatory variable frequency \\(\\gamma_1X_j\\) and differences between \\(i\\) participants in the slope \\(U_{1i}X_j\\);\nin addition to residual error variance \\(e_{ij}\\).\n\n\nWhat are we doing with these random effects terms?\nIn sections Section 9.7 and Section 11, we look at what exactly is captured in these random effects terms \\(U_{0i}, U_{1i}\\). Let’s first look at the practicalities of analysis then come back to deepen our understanding a bit more.\nRight now, it is important to understand that in our analysis we do not care about the differences between specific children. We care that there are differences. And we care how widely spread are the differences between child A and the average intercept (or slope), or between child B and the average intercept (or slope), or between child C … (you get the idea). Therefore, in our analysis, we estimate the spread of the differences as a variance term. We can see this when we look at the results of the mixed-effects model we specify, next.\n\n\n\nFitting a mixed-effect model using the lmer() function\nWe can fit a mixed-effects model of the \\(\\text{RT} \\sim \\text{frequency}\\) relationship, taking into account the random differences between participants. I first go through the model fitting code bit by bit. (I go through the output, the results, in Section 9.6.)\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + \n                         (Lg.UK.CDcount + 1||subjectID),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.1)\n\nYou have seen the lmer() function code before but practice makes perfect so we shall go through the code step by step, as we did previously.\nFirst, we have a chunk of code mostly similar to what we do when we do a regression analysis.\n\nlmer.all.1 &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nRT ~  Lg.UK.CDcount is a formula expressing the model in which we estimate the fixed effect on the outcome or dependent variable RT (reaction time, in milliseconds) as predicted \\(\\sim\\) by the independent or predictor variable Lg.UK.CDcount (word frequency).\n...(..., data = long.all.noNAs) specifies the dataset in which you can find the variables named in the model fitting code.\nsummary(lmer.all.1) gets a summary of the fitted model object, showing you the results.\n\nSecond, we have the bit that is specific to multilevel or mixed-effects models.\n\nWe add (...||subjectID) to tell R about the random effects corresponding to random differences between sample groups (here, observations grouped by child) that are coded by the subjectID variable.\n(...1 ||subjectID) says that we want to estimate random differences between sample groups (observations by child) in intercepts, where the intercept is coded by 1.\n(Lg.UK.CDcount... ||subjectID) adds random differences between sample groups (observations by child) in slopes of the frequency effect coded using the Lg.UK.CDcount variable name.\n\n\nExercise\nIt will help your learning if you now go back and compare this model with the model you saw in ?@sec-multi-lmer.\n\nIdentify what is different: dataset, variable names, and model formula.\nIdentify what stays the same: function name … the specification of both model and random effects.\n\nIf you can see what is different versus what stays the same then you learn what you can change when the time comes for your analysis with your data.\n\n\n\n\n\n\nTip\n\n\n\nLearning to look at example code so that you can identify how to adapt it for your own purposes is a key skill in psychological data science.\n\n\n\n\nWhat does || mean?\nBefore we move on, I want you to notice something that looks like nothing much: ||.\nWe are going to need to defer until later a (necessary) discussion of exactly why we need the two double lines. In short, the use of || asks R to fit a model in which we estimate random effects associated with:\n\nvariance due to differences in intercepts\nvariance due to differences in slopes\nbut not covariance between the two sets of differences\n\nI do this because otherwise the model I specify will not converge.\nWe shall need to discuss these things: convergence, and failures to converge; as well as random effects specification and simplification. We will discuss random effects covariance in Section 11. For now, the most important lesson is learnt by seeing how the analysis approach we saw last week can be extended to examining the effects of experimental variables in data from repeated measures design studies.\n\n\n\nReading the lmer() results\nThe lmer() model code we discussed in Section 9.5 gives us the following output.\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID))\n   Data: long.all.noNAs\n\nREML criterion at convergence: 117805.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7839 -0.5568 -0.1659  0.3040 12.4850 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n subjectID   (Intercept)   87575    295.93  \n subjectID.1 Lg.UK.CDcount  2657     51.55  \n Residual                  23734    154.06  \nNumber of obs: 9085, groups:  subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    950.913     39.216  24.248\nLg.UK.CDcount  -67.980      7.092  -9.586\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.093\n\n\nWe discussed the major elements of the results output last week. We expand on that discussion, a little, here.\nThe output from the model summary first gives us information about the model.\n\nFirst, we see information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID).\nThen, we see REML criterion at convergence about the model fitting process, which we can usually ignore.\nThen, we see information about the distribution of the model residuals.\nWe then see information listed under Random effects: this is where you can see information about the error variance terms estimated by the model.\n\n\nThe information is listed in four columns: 1. Groups; 2. Name; 3. Variance; and 4. Std.Dev.\n\nWe have discussed how observations can be grouped by participant (because we have multiple response observations for each person in the study) just as previously we identified how observations could be grouped by class (because we saw that children were nested under class). That is what we mean when we refer to Groups: we are identifying the grouping variables that give hierarchical structure to the data.\n\nThe Name lists whether the estimate we are looking at corresponds to, here, random differences between participants in intercepts (listed as (Intercept)), or in slopes (listed as Lg.UK.CDcount).\n\nAs we discuss later, in Section 11, mixed-effects models estimate the spread in random differences. We are not interested in the specific differences in intercept or slope between specific individuals. What we want is to be able to take into account the variance associated with those differences.\nThus, we see in the Random Effects section, the variances associated with:\n\nsubjectID Intercept) 87575, differences between participants in the intercepts;\nsubjectID.1 Lg.UK.CDcount 2657, differences between participants in the slopes of the frequency effect;\nAlongside Residual  23734, residuals where, just like a linear model, we have variance associated with differences between model estimates and observed RT, here, at the trial level.\n\nWe do not usually discuss the specific variance estimates in research reports. However, the relative size of the variances does provide useful information [@meteyard2020a], as we shall see when we discuss the different estimates we get when we include a random effect due to differences between items (Section 10.2).\n\nLastly, we see estimates of the coefficients (of the slopes) of the fixed effects.\n\nIn this model, we see estimates of the fixed effects of the intercept and the slope of the RT ~ Lg.UK.CDcount model. We discuss these estimates next.\n\nIs there a difference between linear model and linear mixed-effects model results?\nRecall that the linear model yields the estimate for the frequency effect on reading RT such that RT decreases by about 53 ms for unit increase in log word frequency (\\(\\beta = -53.375\\)). Now, when we have taken random differences between participants into account, we see that the estimate of the effect for the mixed-effects model is \\(\\beta = -67.980\\). Taking into account random differences clearly has an impact on results.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\nResult: The mixed-effect analysis shows that RT decreases by about \\(68\\) milliseconds for each unit increase in log word frequency.\n\n\n\nWhich coefficient estimate should you trust? Well, it is obvious that the linear model and the linear mixed-effects model estimate are relatively similar. However, it is also obvious that the linear model makes an assumption – the assumption of independence of observations – that does not make sense theoretically (because reading responses will be similar for each child) and does not make sense empirically (because responses will differ between children, see Figure 4). Thus, we have good grounds for supposing that the linear mixed-effects model estimate for the frequency effect is likely to be closer to the true underlying population effect.\nIt is important to remember, however, that whatever estimate we can produce is the estimate given the sample of words we used, our information about word frequency, and our measurement of reading RT responses. How far our estimate actually generalizes to the wider population is not something we can judge in the context of a single study.\nFurther, we have not finished in our consideration of the random effects that the account should include. We need to do more work by thinking about the differences between stimuli (Section 10).\n\n\n\n\n\n\nWarning\n\n\n\nWhy aren’t there p-values?\nWe will come back to this (?@sec-dev-mixed-discussion-p-values) but note that if \\(t &gt; 2\\) we can suppose that an effect is significant at the \\(.05\\) significance level.\n\n\n\n\n\nWhat we estimate when we estimate random effects\nWe have said that we can incorporate, in a mixed-effects model, fixed effects (e.g., the average frequency effect) and random effects, error variance due to unexplained differences between participants in intercepts and in frequency effects:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n\\]\nSo we distinguish:\n\nthe average intercept \\(\\gamma_0\\) and differences between \\(i\\) children in the intercept \\(U_{0i}\\);\nthe average effect of the explanatory variable frequency \\(\\gamma_1X_j\\) and differences between \\(i\\) participants in the slope \\(U_{1i}X_j\\).\n\nWhen we think about the differences between participants (or between the units of any grouping variable), in intercepts or in slopes, we typically assume that the differences are:\n\nrandom;\nshould be normally distributed;\nand are distributed around the population or average fixed effects.\n\nWe can say that the mixed-effects model sees the differences between participants relative to the fixed effect intercept or slope, that is, relative to the population level or average effects.\nWe can illustrate this by plotting, in Figure 5, the differences as estimated – technically, predicted – by the mixed-effects model that we have been examining. (The code for producing the plot can be found in 02-mixed-workbook-answers.R.)\n\n\n\n\n\nFigure 5: Plot showing histograms indicating the distribution of participant adjustments to account for between-child differences in intercept or slope (the Best Linear Unbiased Predictions).\n\n\n\n\nWhat you can see in Figure 5 are distributions, presented using histograms. The centers of the distributions are located at zero (shown by a red line). For each distribution (a. and b.), that is where the model estimate of the intercept or the slope of the frequency effect is located. Spread around that central point, you see the adjustments the model makes to account for differences between participants.\nNotice how, in Figure 5 (a.):\n\nSome children have intercepts that are smaller than the population-level or average intercept – so their adjustments are negative (to decrease their intercepts).\nSome children have intercepts that are larger than the population-level or average intercept – so their adjustments are positive (to increase their intercepts).\n\n\nStrikingly, you can see that a few children have intercepts that are as much as 1000ms larger than the population-level or average intercept: the bars representing the estimates for these children are far out on the right of the x-axis in Figure 5 (a.).\n\nNow notice how, in Figure 5 (b.):\n\nSome children have frequency effects (coefficients) that are smaller than the population-level or average frequency effect – so their adjustments are positive (to decrease their frequency effect, by making it less negative).\nSome children have frequency effects that are larger than the population-level or average frequency effect – so their adjustments are negative (to increase their frequency effect, by making it more negative).\n\n(When you look at Figure 5 (b.), remember that the estimated \\(\\beta\\) coefficient for the frequency effect is negative because higher word frequency is associated with smaller RT.)\n\nStrikingly, you can see that a few children have frequency effects that are as much as 200ms larger (see plot (b.) around \\(x = -200\\)) than the population-level or average effect.\n\nWhen a mixed-effects model is fitted to a dataset, its set of estimated parameters includes the coefficients for the fixed effects as well as the standard deviations for the random effects [@baayen2008]. If you read the literature on mixed-effects models, you will see that the adjustments are called Best Linear Unbiased Predictors (BLUPs).\n\nExercise: Random effects in mixed-effects models\nMixed-effects modeling is hard to get used to at first. A bit more practice helps to show you how the different parts of the model work. We again focus on the random effects.\nIn the model we have seen so far, we specify (Lg.UK.CDcount + 1||subjectID)\n\nWe can change this part – and only this part – to see what happens to the results. Do it: rerun the model code, having changed the random effects part:\n\n\nlmer(RT ~  Lg.UK.CDcount + (1|subjectID)...) gives us a random intercepts model accounting for just random differences between participants in the intercept\nlmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 0|subjectID)...) gives us a random slope model accounting for just random differences between participants in the slope of the frequency effect\nlmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID)...) gives us a random intercepts and slopes model accounting for both random differences between participants in the intercept and in the slope, as well as covariance in these differences.\n\n\n\n\n\n\n\nTip\n\n\n\nTry out these variations and look carefully at the different results. Look, especially, at what happens to the Random effects part of the summary.\n\n\nThis will be an important and revealing exercise.\nWe can visualize the differences between the models in a plot showing the different predictions that the different models give us. Figure 6 shows what a mixed-effects model predicts are the effects of frequency on RT for different children in the CP study.\n\nThe predictions vary depending on the nature of the random effects we specify in the model.\n\n(Note that I figured out how to produce the plot from the information here.)\n\n\n\n\n\nFigure 6: Plot showing model predictions of the effect, for each individual, of word frequency on reading reaction time – predictions vary between models incorporating (a.) random effect of participants on intercepts only; (b.) random effect of participants on slopes only and (c.) random effect of participants on intercepts and on slopes.\n\n\n\n\nWe can see that:\n\nIf the model includes the random effect of participants on intercepts only then all the slopes are the same (the lines in the figure are parallel) because this model assumes that the only differences between participants are differences in the intercepts.\nIf the model includes the random effect of participants on slopes only then the slopes vary but they all have the same intercept. The plot does not show this but you can see how all the slopes are converging on one point somewhere on the left. This happens because this model assumes that the only differences between participants are differences in the slopes.\nIf the model includes the random effect of participants on intercepts and on slopes then we can see how the intercepts and the slopes vary. Given what we saw when we looked at the relation between frequency and RT for each participant considered separately we might argue that this model is much more realistic about the data.\n\n\n\nExercise: Error messages for mixed-effects models\n\n\n\n\n\n\nImportant\n\n\n\nIt is key to your skills development that you learn to make effective use of the warnings and error messages R can produce.\n\n\nYou do not have to just believe me when I say that || is in the model code to stop a problem appearing.\n\nExperiment – and see what happens when you change the code. Try this.\n\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.1)\n\nDo you get an error message?\nA very useful trick is to learn to copy the error message you get into a search engine on your web browser. Do this and you will find useful help, as here"
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-fixed-effect-fallacy",
    "href": "PSYC402/Week17.html#sec-intro-mixed-fixed-effect-fallacy",
    "title": "Introduction to linear mixed-effects models",
    "section": "Variation between stimuli: the “language as fixed-effect fallacy”",
    "text": "Variation between stimuli: the “language as fixed-effect fallacy”\n\n\n\n\n\n\nImportant\n\n\n\nExperimental psychologists will often collect data in studies where they present some stimuli to a sample of participants.\n\n\n@clark1973 showed that the appropriate analysis of experimental effects for such data requires the researcher to take into account the error variance due to unexplained or random differences between sampled participants and also to random differences between sampled stimuli. This is true in the context of psycholinguistics but it is also true in the context of work in any field where the presented stimuli can be understood to constitute a sample from a wider population of potential stimuli [e.g., stories about social situations, @judd2012].\nIf we were to estimate the average latency of the responses made by different children to each word, in the CP study data, we would see that there is considerable variation between words. We do this in Figure 7. (I work through the code for producing the plot in 02-mixed-workbook-answers.R.)\n\nSome words elicit slower and some elicit faster responses on average.\nWe can also see that there is, again, variation in the uncertainty of estimates, as reflected in differences in the lengths of the error bars corresponding to the standard errors of the estimates.\n\n\n\n\n\n\nFigure 7: Estimated intercepts (with SEs) calculated for each stimulus word, with coefficients ordered by average latency for each word\n\n\n\n\nIn general, psychologists have been aware since @clark1973 (if not earlier) that responses to experimental stimuli can vary because of random or unexplained differences between the stimuli: whether the stimuli are words, pictures or stories, etc. And researchers have been aware that if we did not take such variation into account, we might mistakenly detect an experimental effect, for example, as a significant difference between mean response in different conditions, simply because different stimuli presented in different conditions varied in some unknown way, randomly.\nFor many years, psychologists tried to take random differences between stimuli into account, alongside random differences between participants, using a variety of strategies with important limitations (see @baayen2008, for discussion). @clark1973 suggested that researchers could calculate \\(minF'\\) (not F) when doing Analyses of Variance of experimental data\nThis involves a series of steps.\n\nYou start by aggregating your data\n\n\nBy-subjects data – for each subject, take the average of their responses to all the items\nBy-items data – for each item, take the average of all subjects’ responses to that item\n\n\nYou do separate ANOVAs, one for by-subjects (F1) data and one for by-items (F2) data\nYou put F1 and F2 together, calculating minF’\n\nAveraging data by-subjects or by-items is relatively simple. It is very common to see, in the literature, psychological reports in which F1 and F2 analysis results are presented (that is why I am explaining this).\nCalculating \\(minF'\\) is also relatively simple:\n\\[\nminF' = \\frac{MS_{effect}}{MS_{\\text{random-subject-effects}} + MS_{\\text{random-word-differences}}} = \\frac{F_1F_2}{F_1 + F_2}\n\\]\nHowever, after a while, psychologists stopped doing the extra step of the \\(minF'\\) calculation [@raaijmakers1999]. They carried on calculating and reporting F1 and F2 ANOVA results but, as @baayen2008 discuss, that approach risks a high false positive error rate.\nPsychologists also found that while the \\(minF'\\) approach allowed them to take into account between-participant and between-stimulus differences it could not be applied where ANOVA could not be used. This stopped researchers from taking a comprehensive approach to error variance where they wanted to conduct multiple regression analyses.\nIn the psychological literature, you will often see multiple regression analyses of by-items data, where a sample of participants has been asked to respond to a sample of stimuli, and the analysis is of the effects of stimulus properties on outcomes averaged (over participants’ responses) to the mean outcome by item. The problem is that analyzing data only by-items ensures that we lose track of participant differences.\n@lorch1990 warn that analyzing only by-items mean RTs just assumes wrongly that subjects are a fixed effect. This approach, again, risks a higher rate of false positive errors.\n\nInclude the random effect of stimulus\nThe good thing is that, thanks to the advent of mixed-effects models, we now no longer need to tolerate these problems.\nIn the context of our working example, in our analysis of the CP study data, we can build up our mixed-effects model by adding a random effect to capture the impact of unexplained differences between stimuli.\nWe model the random effect of items on intercepts by modeling the intercept as two terms:\n\\[\n\\beta_{0j} = \\gamma_0 + W_{0j}\n\\]\n\nwhere \\(\\gamma_0\\) is the average intercept and \\(W_{0j}\\) represents the deviation, for each word, between the average intercept and the per-word intercept.\n\nOur model can now incorporate the additional random effect of items on intercepts:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + W_{0j} + e_{ij}\n\\]\nIn this model, the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and the word frequency effect \\(\\gamma_1X_j\\)\nplus random effects due to unexplained differences between participants in intercepts \\(U_{0i}\\) and the slope of the frequency effect \\(U_{1i}X_j\\)\nas well as random differences between items in intercepts \\(W_{0j}\\),\nin addition to the residual term \\(e_{ij}\\).\n\n\n\n\n\n\n\nWarning\n\n\n\nWhat about random effects associated with differences between stimulus items in the slopes of effects?\n\nJust as we may expect there to be between-participant differences in the slope of the word frequency effect, we may expect there to be between-stimulus differences in the slope of the effect of, e.g., participant age.\nRest assured, we will look at this question.\n\n\n\n\n\nFitting a mixed-effect model – now with random effects of subjects and items\nWe can fit a mixed-effects model of the \\(\\text{RT} \\sim \\text{frequency}\\) relationship, taking into account the random differences between participants and now also the random differences between stimulus words.\n\nlmer.all.2 &lt;- lmer(RT ~  Lg.UK.CDcount + \n                         (Lg.UK.CDcount + 1||subjectID) +\n                         (1|item_name),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID)) + (1 | item_name)\n   Data: long.all.noNAs\n\nREML criterion at convergence: 116976.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1795 -0.5474 -0.1646  0.3058 12.9485 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n item_name   (Intercept)     3397    58.29  \n subjectID   Lg.UK.CDcount   3624    60.20  \n subjectID.1 (Intercept)   112314   335.13  \n Residual                   20704   143.89  \nNumber of obs: 9085, groups:  item_name, 159; subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)     971.07      51.87  18.723\nLg.UK.CDcount   -72.33      10.79  -6.703\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.388\n\n\nThis is the same mixed-effects model as the one we discussed in Section 9.5 and Section 9.6 but with one important addition.\n\nWe add (1|item_name) to take into account random differences between between words in intercepts.\n\n\nReading the results\nTake a look at the model results. You should notice three changes.\n\nYou can see that the estimate for the effect of word frequency on reading reaction time has changed again, it is now \\(\\beta = -72.33\\)\nitem_name   (Intercept)     3397 there is now an additional term in the list of random effects, giving the model estimate for variance associated with random differences between words in intercepts\nAnd you can see that the residual variance has changed. In the first model lmer.all.1 it was 23734, now it is 20704\n\n\n\n\n\n\n\nTip\n\n\n\nThe reduction in residual variance is one way in which we can judge how good a job the model is doing in accounting for the variance in the outcome, observed response reaction time.\n\n\nWe can see that by adding a term to account for differences between items we can reduce the amount by which the model estimates deviate from observed outcomes. This difference in error variance is, essentially, one basis for estimating how well the model fits the data, and a basis for estimating the variance explained by a model in terms of the \\(R^2\\) statistic you have seen before.\nWe will come back to this."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-variance-covariance",
    "href": "PSYC402/Week17.html#sec-intro-mixed-variance-covariance",
    "title": "Introduction to linear mixed-effects models",
    "section": "Variances and covariances of random effects",
    "text": "Variances and covariances of random effects\nAs I have said, we usually do not aim to examine the specific deviation from the average intercept or the average fixed effect slope for a participant or stimulus. We estimate just the spread of deviations by-participants or by-items.\nA mixed-effects model like our final model includes fixed effects corresponding to the intercept and the slope of the word frequency effect plus the variances:\n\n\\(var(U_{0i})\\) variance of deviations by-participants from the average intercept;\n\\(var(U_{1i}X_j)\\) variance of deviations by-participants from the average slope of the frequency effect;\n\\(var(W_{0j})\\) variance of deviations by-items from the average intercept;\n\\(var(e_{ij})\\) residuals, at the response level, after taking into account all other terms.\n\nBecause we have variances, we may expect the random effects of participants or items to covary, e.g., participants who are slow to respond may also be more susceptible to the frequency effect, as can be seen in Figure 8.\n\n\n\n\n\nFigure 8: Scatterplot showing the relationship between estimated coefficients for the intercept and for the frequency effect, for each child analysed separately\n\n\n\n\nThis is why it would often make sense to specify, among the random effects of the model, terms corresponding to the covariance of the random effects:\n\n\\(covar(U_{0i}, U_{1i}X_j)\\)\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember we excluded random effects covariance {#sec-intro-mixed-exclude-covariance}\n\n\nIn Section 9.5.2, I noted how we used the || notation to stop the model estimating the covariance between differences between participants in intercepts and in slopes. The reason I did this is that if I had requested that the model estimate the covariance the model would have failed to converge. What this means depends on understanding how mixed-effects models are estimated. We shall have to return to a development of that understanding later. For now, it is enough to note that mixed-effects models fitted with lmer() often have more difficulty with random effects covariance estimates."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-lmer-results-reporting",
    "href": "PSYC402/Week17.html#sec-intro-mixed-lmer-results-reporting",
    "title": "Introduction to linear mixed-effects models",
    "section": "Reporting the results of a mixed-effects model",
    "text": "Reporting the results of a mixed-effects model\nThere is no official convention on what or how to report the results of a mixed-effects model. Lotte Meteyard and I suggest what psychologists should report in an article [@meteyard2020a] that has been downloaded a few thousand times so, maybe, our advice will help to influence practice.\n\n\nExplain what you did, and why.\nExplain what you found, not just whether effects are significant or not.\n\n\n\nWe argue that researchers should explain what analysis they have done and, where space allows, should report both the estimates of the fixed effects and the estimates of the random effects.\nWe think you can report the model code (maybe in an appendix, maybe in a note under a tabled summary of results).\n\nA table summary presenting model results can look like this.\n\n\n\nCoefficients\nEstimate\nSE\nt\n\n\n\n\n\n(Intercept)\n971.1\n51.9\n18.7\n\n\n\nFrequency effect\n-72.3\n10.8\n-6.7\n\n\n\n\n\n\n\nGroups\nName\nVariance\nSD\n\n\n\n\n\nitem\n(Intercept)\n3397\n58.3\n\n\n\nparticipant\n(Intercept)\n112314\n335.1\n\n\n\nparticipant\nFrequency\n3624\n60.2\n\n\n\nresidual\n\n20704\n143.9\n\n\n\n\nNote: lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID) + (1|item_name)\n@Barr2013a argued that choices about random effects structure affect the generalizability of the estimates of fixed effects. In particular, it seems sensible to examine the possibility that the slope of the effect of an explanatory variable may vary at random between participants or between stimuli. Correspondingly, researchers should report and explain their decisions about the inclusion of random effects.\n\n\n\n\n\n\nTip\n\n\n\nResearchers should report their modelling in sufficient detail that their results can be reproduced by others.\n\n\nIt is normal practice in psychology to report the p-values associated with null hypothesis significance tests of effects when reporting analysis. Performing hypothesis tests using t- or F-distributions depends on the calculation of degrees of freedom yet it is uncertain how degrees of freedom should be counted when analyzing multilevel data (@baayen2008). In most software applications, however, p-values associated with fixed effects may be calculated using an approximation for denominator degrees of freedom.\nWe will come back to how we should report the results of mixed-effects models because, here, too, in learning about writing, we can benefit by developing our approach, in depth, step by step."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-conclusions",
    "href": "PSYC402/Week17.html#sec-intro-mixed-conclusions",
    "title": "Introduction to linear mixed-effects models",
    "section": "Conclusions",
    "text": "Conclusions\nA large proportion of psychological studies involves scenarios in which the researcher samples both participants and some kind of stimuli. Often, the researcher will present the stimuli to the participants for response in some version of a range of possible designs:\n\nall participants see and respond to all stimuli;\nparticipants respond to different sub-sets of stimuli in different conditions (or in different groups) but they see and respond to all stimuli in a sub-set;\nparticipants are allocated to respond to stimulus sub-sets according to a counter balancing scheme (e.g., through the use of Latin squares).\n\nWhatever version of this scenario, if participants are responding to multiple stimuli and if multiple partcipants respond to each stimulus, then the data will have a multilevel structure such that each observation can be grouped both by participant and by stimulus.\nWe are interested in taking into account the random effects associated with unexplained or random differences between participants or between stimuli. We often discuss the accounting of these effects in terms of the estimation of error variances associated with the random differences, calling the effects of the differences random effects. Where we have to deal with both samples of participants and samples of stimuli, we can talk about crossed random effects.\nThe terms are not that important. The insight is.\n\n\n\n\n\n\nImportant\n\n\n\nIn general, in experimental psychological science, when we do data analysis, if we want to estimate effects of experimental variables more accurately then our models need to incorporate terms to capture the impact on observed outcomes of sampled participants and sampled stimuli.\n\n\nHistorically, we have, as a field, learned to take into account these sampling effects. Now, and most likely, more and more commonly in the future, we are learning to use multilevel or mixed-effects models to do this.\n\nSummary\n\nSummary: concepts\nWe discussed the way that data are structured when they come from studies with repeated measures designs. Critically, we examined data from a common study design where a sample of stimulus items are presented for response to members of a participant sample. This means that each observation can be grouped by participant and, also, by stimulus. The possibility that observations can be grouped means that the data have a multilevel structure.\nThe multilevel structure requires the use of linear mixed-effects models when we seek to estimate the effects of experimental variables. The fact that data can be grouped both by participant and by stimulus means that the model can incorporate random effects to capture random between-participant differences as well as between-stimulus differences.\nThe use of mixed-effects models has meant that psychologists no longer need to adopt compromise solutions which have important limitations, like by-items and by-subjects analyses.\n\n\nSummary: skills\nWe reviewed the ways that experimental data can be untidy. And we outlined the steps that may be required to process untidy data into a tidy format suitable for analysis. As is typical for the data analysis we need to do for experimental psychological science, getting data ready for analysis requires a series of steps including: access; import; restructure; select variables; and filter observations.\nWe then developed a mixed-effects model to answer the question:\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\n\n\n\nOur analysis focused on the relationship between reading response reaction time (RT, in ms) and the predictor word frequency. We examined how the effect of word frequency was estimated in a linear model ignoring the multilevel structure and then in mixed-effects models which incorporated terms to capture:\n\nvariance associated with random differences between participants in intercepts or in the slope of the frequency effect,\nvariance associated with random differences between items in intercepts.\n\nWe saw that estimates of the frequency effect differed between different models.\n\n\n\nGlossary: useful functions\nWe used a number of functions to tidy and visualize the CP study data.\n\nread_csv() and read_csv() to load source data files into the R workspace\npivot_longer() to restructure data from wide to long\nfull_join() to put together data from separate datasets; in our example, from datasets holding information about participant attributes, stimulus word properties, and participant behaviours\nselect() to select the variables we need\nfilter() to filter observations based on conditions\nna.omit() to remove missing values\nFor visualisation, we used facet_wrap() to show plots of the relationship between outcome and predictor variables separately for different groups (by participant, or by item)\n\nFor our analyses:\n\nWe used lmer() to fit a multilevel model.\n\nWe used the summary() function to get model results for both linear models and for the mulilevel or liner mixed-effects model."
  },
  {
    "objectID": "PSYC402/Week17.html#sec-intro-mixed-recommended-reading",
    "href": "PSYC402/Week17.html#sec-intro-mixed-recommended-reading",
    "title": "Introduction to linear mixed-effects models",
    "section": "Recommended reading",
    "text": "Recommended reading\n[@baayen2008; see also @Barr2013a; @judd2012] discuss mixed-effects models with crossed random effects in a variety of contexts in psychological science. The explanations are clear and the examples are often helpful.\nI wrote a tutorial article on mixed-effects models with Lotte Meteyard [@meteyard2020a]. We discuss how important the approach now is for psychological science, what researchers worry about when they use it, and what they should do and report when they use the method."
  },
  {
    "objectID": "PSYC402/Week16.html",
    "href": "PSYC402/Week16.html",
    "title": "Introduction to multilevel data",
    "section": "",
    "text": "In this chapter, we shall start to develop skills in using a method or approach that is essential in modern data analysis: multilevel modeling. We are going to invest four weeks in working on this approach. This investment is designed to give you a specific, important, advantage in your work as a psychologist, or as someone who produces or consumes psychological research.\n\n\n\n\n\n\nNote\n\n\n\nMultilevel models: why do we need to do this? - Four weeks is a lot of time to spend on one method.\n\n\nIt is now clear that someone who works in psychological research has to know about multilevel or hierarchically structured data, and has to know how to apply multilevel models (or mixed-effects models). Growth in the popularity of these kinds of analysis has been very very rapid, as can be seen in Figure 1. It is now, effectively, the standard or default method for professional data analysis in most areas of psychological and other social or clinical sciences (or it soon will be). There are good reasons for this [@Baayen2008].\n\n\n\nFigure 1: Number of Pubmed citations for ‘Linear Mixed Models’ by year. Generated using the tool available at http://dan.corlan.net/medline-trend.html, entering “Linear Mixed Models” as the phrase search term and using data from 2000 to 2018, from Meteyard and Davies (2020) – used without permission\n\n\nWe continue to teach ANOVA and multiple regression (linear models) in our courses because the research literature is full of the results of analyses done using these methods and because many psychologists continue to use these methods in their research. However, there is increasingly wide-spread recognition that these classical methods have serious problems when applied to data with hierarchical structure. Because most psychological data (not all) will have hierarchical structure, this makes learning about multilevel or mixed-effects methods a key learning objective.\nBut, because it is relatively new, many professional psychologists struggle to understand why or how to use these methods effectively. This means that students who acquire the skill graduate with a clear employability advantage. It also means that we have to take seriously the challenge of learning about these methods. This is why we will spend a bit of time on them. In my experience, in over a decade of teaching multilevel models, in working with both students and professionals, we shall need to develop understanding and skills gradually. We will work patiently, so that we can secure understanding by building our learning through a series of practical examples, increasing the scope of our practical skills, and developing the sophistication of our understanding, step-by-step, as we go.\n\n\nMultilevel models are also known as hierarchical models or linear mixed-effects models or random effects models. People use these terms interchangeably. They also use the abbreviations LMMs or LMEs. Sorry about that: humans make methods, and the names we use for things do vary.\nI will only use the terms multilevel or linear mixed-effects models.\n\n\n\n\n\n\nWarning\n\n\n\n\nIn this chapter, we emphasize the multilevel perspective but, to anticipate future development, we will come to think in terms of mixed-effects models."
  },
  {
    "objectID": "PSYC402/Week16.html#sec-multi-motivations",
    "href": "PSYC402/Week16.html#sec-multi-motivations",
    "title": "Introduction to multilevel data",
    "section": "",
    "text": "In this chapter, we shall start to develop skills in using a method or approach that is essential in modern data analysis: multilevel modeling. We are going to invest four weeks in working on this approach. This investment is designed to give you a specific, important, advantage in your work as a psychologist, or as someone who produces or consumes psychological research.\n\n\n\n\n\n\nNote\n\n\n\nMultilevel models: why do we need to do this? - Four weeks is a lot of time to spend on one method.\n\n\nIt is now clear that someone who works in psychological research has to know about multilevel or hierarchically structured data, and has to know how to apply multilevel models (or mixed-effects models). Growth in the popularity of these kinds of analysis has been very very rapid, as can be seen in Figure 1. It is now, effectively, the standard or default method for professional data analysis in most areas of psychological and other social or clinical sciences (or it soon will be). There are good reasons for this [@Baayen2008].\n\n\n\nFigure 1: Number of Pubmed citations for ‘Linear Mixed Models’ by year. Generated using the tool available at http://dan.corlan.net/medline-trend.html, entering “Linear Mixed Models” as the phrase search term and using data from 2000 to 2018, from Meteyard and Davies (2020) – used without permission\n\n\nWe continue to teach ANOVA and multiple regression (linear models) in our courses because the research literature is full of the results of analyses done using these methods and because many psychologists continue to use these methods in their research. However, there is increasingly wide-spread recognition that these classical methods have serious problems when applied to data with hierarchical structure. Because most psychological data (not all) will have hierarchical structure, this makes learning about multilevel or mixed-effects methods a key learning objective.\nBut, because it is relatively new, many professional psychologists struggle to understand why or how to use these methods effectively. This means that students who acquire the skill graduate with a clear employability advantage. It also means that we have to take seriously the challenge of learning about these methods. This is why we will spend a bit of time on them. In my experience, in over a decade of teaching multilevel models, in working with both students and professionals, we shall need to develop understanding and skills gradually. We will work patiently, so that we can secure understanding by building our learning through a series of practical examples, increasing the scope of our practical skills, and developing the sophistication of our understanding, step-by-step, as we go.\n\n\nMultilevel models are also known as hierarchical models or linear mixed-effects models or random effects models. People use these terms interchangeably. They also use the abbreviations LMMs or LMEs. Sorry about that: humans make methods, and the names we use for things do vary.\nI will only use the terms multilevel or linear mixed-effects models.\n\n\n\n\n\n\nWarning\n\n\n\n\nIn this chapter, we emphasize the multilevel perspective but, to anticipate future development, we will come to think in terms of mixed-effects models."
  },
  {
    "objectID": "PSYC402/Week16.html#sec-multi-challenges",
    "href": "PSYC402/Week16.html#sec-multi-challenges",
    "title": "Introduction to multilevel data",
    "section": "Challenges",
    "text": "Challenges\nThe key challenges for learning should be explained at the start so that we know what we shall have to do to overcome them.\n\nEven though most psychological data has some sort of multilevel or hierarchical structure, we are not used to recognizing it. This is because the structure has often been hidden or ignored in the education and practice of traditional research methods in Psychology.\n\nIn time, you will come to see multilevel structure everywhere [@kreft1998]. But first you will need to get some practice so that you can become familiar with the idea and learn to recognize what it looks like when data have a multilevel or hierarchical structure. This is why we will examine multilevel structured data across a range of different kinds of experiments or surveys, over a series of weeks.\nWe will learn to identify and understand hierarchical structure in psychological data by just looking at datasets, by producing visualizations, by doing analyses, and by trying to explain to ourselves and each other what we think we see.\n\nThe ideas that support an understanding of why and how we use multilevel models can be intimidating when we first encounter them. The mathematics behind how the models work is both profound and sophisticated. But the good news is that we can practice the application of the analysis method while talking and thinking about the critical ideas using just words or plots.\n\nWe cannot or should not avoid engaging with the ideas – because we have to be able to explain what we are doing – but there are many routes to an effective understanding. For those who want to develop a more mathematically-based perspective, I will provide references to important texts in the literature on multilevel models (see Section 11)."
  },
  {
    "objectID": "PSYC402/Week16.html#the-key-idea-to-get-us-started",
    "href": "PSYC402/Week16.html#the-key-idea-to-get-us-started",
    "title": "Introduction to multilevel data",
    "section": "The key idea to get us started",
    "text": "The key idea to get us started\n\n\n\n\n\n\nImportant\n\n\n\nMultilevel models are a general form of linear model.\n\n\nAnother way of saying this is: linear models are a special form of multilevel models.\nThis is because linear models assume that observations are independent. We often cannot make this assumption, as we shall see. More generally, then, we do not assume that observations are independent and so we use multilevel models."
  },
  {
    "objectID": "PSYC402/Week16.html#sec-multi-approach",
    "href": "PSYC402/Week16.html#sec-multi-approach",
    "title": "Introduction to multilevel data",
    "section": "The approach we take",
    "text": "The approach we take\nWe are not going to take a mathematical approach to learning about multilevel models. We do not have to. The approach we are going to take is:\n\nverbal – We will talk about the main ideas in words. Sometimes, we will present formulas but that is just to save having to use too many words.\nvisual – We will show ourselves and each other what multilevel structure in data looks like, and what that structure means for our analyses of behaviour.\npractical – We will use R to complete analyses, so we will learn about coding models in practice. Fortunately, through coding we can get a clear idea of what we want the models to do."
  },
  {
    "objectID": "PSYC402/Week16.html#sec-multi-targets",
    "href": "PSYC402/Week16.html#sec-multi-targets",
    "title": "Introduction to multilevel data",
    "section": "Targets",
    "text": "Targets\nOur learning objectives include the development of key concepts and skills.\n\nconcepts – how data can have multilevel structures and what this requires in models\nskills – where skills comprise the capacity to:\n\n\nuse visualization to examine observations within groups\nrun linear models over all data and within each class\nuse the lmer() function to fit models of multilevel data\n\nWe are just getting started. Our plan will be to build depth and breadth in understanding as we progress over the next few weeks."
  },
  {
    "objectID": "PSYC402/Week16.html#sec-multi-guide",
    "href": "PSYC402/Week16.html#sec-multi-guide",
    "title": "Introduction to multilevel data",
    "section": "Study guide",
    "text": "Study guide\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 16 minutes\nPart 2 – about 13 minutes\nPart 3 – about 13 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter. The lectures provide a summary of the main points.\n2. Chapter: 01-multilevel\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example data BAFACALO_DATASET.RData and identify how the data are structured at multiple levels.\nUse visualizations to explore the impact of the structure.\nRun analyses using linear models (revision) and linear mixed-effects models (extension) code.\nReview the recommended readings (Section 11).\n\n3. Practical workbook materials\n3.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning."
  },
  {
    "objectID": "PSYC402/Week16.html#sec-multi-data-intro",
    "href": "PSYC402/Week16.html#sec-multi-data-intro",
    "title": "Introduction to multilevel data",
    "section": "The data we will work with: Brazilian school children",
    "text": "The data we will work with: Brazilian school children\nIn this chapter, we will be working with data taken from a study on education outcomes in Brazilian children, reported by @golino2014. First, we will progress through the steps required to download and prepare the files for analysis in R.\nThe BAFACALO_DATASET.RData data were collected and shared online by @golino2014. Information about the background motivating the study, the methods of data collection, along with the dataset itself, can be found here.\n@golino2014 collected school end-of-year subject grades for a sample of 292 children recruited from multiple classes in a school in Brazil. Here, each subject is a theme or course that children studied in school, e.g., Physics or English language, and for which each child was awarded a grade at the end of the school year. So, we have information on different children, and information about their subject grades. Children were taught in different classes but the classes appear to be units of the school organization (the information is not quite clear) not subject or course groupings. Thus, we also have information on which classes children were in when data about them were collected.\n\nLocate and download the data file\nYou can access the data from the link associated with the @golino2014 article here.\nOr you can download the data-01-multilevel.zip files folder for this chapter.\nThe data were compiled to an .RData format file.\n.RData is R’s own file format so the code you use to load and access the data for analysis is a bit simpler than you are used to. (You will have used read.csv() or read_csv() previously.)\nIn my own practice, I prefer to keep files in formats like .csv which can be opened and read in other software applications (like Excel), so using an .RData is an exception in these materials.\nThe file is located in a .zip folder called data-01-multilevel. The data file is collected together with the .R scripts:\n\n01-multilevel-workbook.R the workbook you will need to do the practical exercises.\n01-multilevel-workbook-answers.R with answers to questions and code for exercises.\n\n\n\nRead in the data file using the load() function\nYou can read the BAFACALO_DATASET.RData file into the R workspace or environment using the following code.\n\nload(\"BAFACALO_DATASET.RData\")\n\n\n\nInspect the data\nThe dataset consists of rows and columns. Take a look. If you have successfully loaded the dataset into the R environment, then you should be able to view it.\nYou could look at the dataset by using the View() function.\n\nView(BAFACALO_DATASET)\n\nOr you can use the head() function to see the top few rows of the dataset.\n\nhead(BAFACALO_DATASET)\n\nYou can check for yourself that each row holds data about a child, including their participant identity code, as well as information about their parents, household, gender, age, school class, and their grades on end-of-year subject attainment (e.g., how well they did in English language).\nThere are many more variables in the dataset than we need for our exercises, and a summary would fill pages. You can see for yourself if you inspect the dataset using\n\nsummary(BAFACALO_DATASET)"
  },
  {
    "objectID": "PSYC402/Week16.html#sec-multi-data-tidy",
    "href": "PSYC402/Week16.html#sec-multi-data-tidy",
    "title": "Introduction to multilevel data",
    "section": "Tidy the data",
    "text": "Tidy the data\nWhen you inspect the file, you will see that it includes a large number of variables, but we only really care about those we will use in our exercises:\n\nparticipant_id gives the participant identity code for each child;\nclass_number gives the class identity code for the school class for each child;\nand values in portuguese, english, math, and physics columns give the score for each child in subject class attainment measures.\n\nYou can see that I am not explaining the variables in great depth. For our aims, we do not need more detailed information but please do read the data article if you wish to find out more.\nWe need to tidy the data before we can get to the analysis parts of this chapter. We are going to:\n\nSelect the variables we want to work with;\nFilter out missing values, if any are present;\nAnd make sure R knows how we want each variable to be identified; what type the variable should have.\n\nWe shall need the tidyverse library of functions.\n\nlibrary(tidyverse)\n\n\nSelect the variables\nWe can start by selecting the variables we want, which include those named here, ignoring all the rest. We take the BAFACALO_DATASET. We then use the select() function to select the variables we want.\n\nbrazil &lt;- BAFACALO_DATASET %&gt;%\n                    select(\n                      class_number, participant_id,\n                      portuguese, english, math, physics\n                      )\n\n\n\n\n\n\n\nTip\n\n\n\nNotice the code is written to create the new selected dataset and, at the same time, gave it a more usable (shorter) name, with brazil &lt;- BAFACALO_DATASET ...\n\n\nInspect the data to see what you have.\n\nsummary(brazil)\n\n  class_number participant_id     portuguese     english         math    \n M11    : 21   Min.   :  1.00   60     : 10   100    : 17   60     : 13  \n M15    : 20   1st Qu.: 73.75   76     :  9   79     : 11   69     :  9  \n M14    : 19   Median :146.50   65     :  8   96     :  8   70     :  9  \n M36    : 19   Mean   :146.50   73     :  8   81     :  7   62     :  8  \n M18    : 18   3rd Qu.:219.25   74     :  7   89     :  7   71     :  7  \n (Other):133   Max.   :292.00   (Other):188   (Other):180   (Other):184  \n NA's   : 62                    NA's   : 62   NA's   : 62   NA's   : 62  \n    physics   \n 60     : 16  \n 60.1   :  5  \n 0      :  4  \n 61.8   :  4  \n 66     :  4  \n (Other):197  \n NA's   : 62  \n\n\n\n\nRemove missing values\nIf you look at the results of the selection, you can see that there are missing values, written as e.g. NA's 62 at the bottom of each summary of each variable (if a variable column includes missing values).\n\n\n\n\n\n\nTip\n\n\n\nRemember that in R NA means “not available” i.e. missing.\n\n\nWe will need to get rid of the missing values. It is simpler to do this at the start rather than wait for an error message, later, when some arithmetic function tells us it cannot give us a result because there are NAs present.\nWe can get rid of the missing values using na.omit()\n\nbrazil &lt;- na.omit(brazil)\n\nIf you then look at a summary of the data again then you will see that the NAs are gone.\n\nsummary(brazil)\n\n  class_number participant_id    portuguese     english         math    \n M11    : 21   Min.   :  3.0   60     : 10   100    : 17   60     : 13  \n M15    : 20   1st Qu.: 77.5   76     :  9   79     : 11   69     :  9  \n M14    : 19   Median :144.5   65     :  8   96     :  8   70     :  9  \n M36    : 19   Mean   :146.6   73     :  8   81     :  7   62     :  8  \n M18    : 18   3rd Qu.:222.8   74     :  7   89     :  7   71     :  7  \n M21    : 17   Max.   :291.0   82     :  7   86     :  6   66     :  6  \n (Other):116                   (Other):181   (Other):174   (Other):178  \n    physics   \n 60     : 16  \n 60.1   :  5  \n 0      :  4  \n 61.8   :  4  \n 66     :  4  \n 74.2   :  4  \n (Other):193  \n\n\n\n\nGetting R to treat a variable as an object of the type required using the as...() family of functions\nBut if you look closely at the output from summary(brazil) you will see that the portuguese and english variables are summarized in the way that R summarizes factors.\nWhen you ask R to summarize factors, R gives you a count of the number of observations associated with each factor level, that is, each category in each variable. Here, it is treating a grade score like \\(100\\) in english as a category (like you might treat \\(dog\\) as a category of pets), and you can see that the count shows you that 17 children were recorded as having scored 100 in their class. We do not want numeric variables like subject grades (e.g. children’s grades in English) treated as categorical variables, factors.\nYou can also see that R gives you a numeric summary of the recorded values in the participant_id variable. This makes no sense because the identity code numbers are (presumably) assigned at random so identity numbers provide no useful numeric information for us. We do not want this either.\nWe want R to treat the educational attainment scores as numbers. We can do this using the as.numeric() function. We want R to treat the class and participant identity numbers as factors (categorical variables). We can do this using the as.factor() function.\nWe could do this one variable at a time.\n\nbrazil$portuguese &lt;- as.numeric(brazil$portuguese)\nbrazil$english &lt;- as.numeric(brazil$english)\nbrazil$math &lt;- as.numeric(brazil$math)\nbrazil$physics &lt;- as.numeric(brazil$physics)\n\nbrazil$class_number &lt;- as.factor(brazil$class_number)\nbrazil$participant_id &lt;- as.factor(brazil$participant_id)\n\nBut it is simpler and more efficient in tidyverse style. (You can see a discussion here that helped me to figure this out.)\n\nbrazil &lt;- brazil %&gt;%\n  mutate(across(c(portuguese, english, math, physics), as.integer),\n         across(c(class_number, participant_id), as.factor)) \n\nIf you now look at the summary of the data, you can see that R will give you mean etc. for the subject class score variables e.g. english, showing that it is now treating them as numeric variables. In comparison, R gives you counts of the numbers of observations for each level (category) of categorical or nominal variables like participant_id.\n\nsummary(brazil)\n\n  class_number participant_id   portuguese       english           math       \n M11    : 21   3      :  1    Min.   : 1.00   Min.   : 1.00   Min.   :  1.00  \n M15    : 20   4      :  1    1st Qu.:26.25   1st Qu.:29.00   1st Qu.: 39.00  \n M14    : 19   5      :  1    Median :42.00   Median :55.00   Median : 58.00  \n M36    : 19   6      :  1    Mean   :43.55   Mean   :50.25   Mean   : 58.17  \n M18    : 18   7      :  1    3rd Qu.:58.75   3rd Qu.:73.00   3rd Qu.: 79.75  \n M21    : 17   9      :  1    Max.   :83.00   Max.   :93.00   Max.   :104.00  \n (Other):116   (Other):224                                                    \n    physics      \n Min.   :  1.00  \n 1st Qu.: 34.00  \n Median : 71.50  \n Mean   : 71.57  \n 3rd Qu.:107.75  \n Max.   :148.00  \n                 \n\n\n\nCoercion: a quick lesson\nR treats things like variables as vectors. A vector can be understood to be a set or list of elements: things like numbers of words.\nR gives each vector a type (factor, numeric etc.) which helps to inform different functions how to handle that vector. Usually, R assigns type correctly but sometimes it does not. We can use what is called coercion to force R to assign the correct type to a variable.\nIt is more efficient to do this at the start of an analysis workflow.\nNormally, I would use read_csv() from tidyverse and assign type to variable using col_types() specification. (See here for more information and an example.) But it is useful to learn what you need to do if you need to change the way a variable is treated after you have got the data into the R environment.\n\n\nExercise – experiment with coercion\nIn R, there are a family of functions that work together. You can test whether a variable (a vector, technically) is or is not a certain type using the is.[something] function. For example:\n\nis.factor(brazil$english)\nis.numeric(brazil$english)\nis.character(brazil$english)\n\nAnd you can coerce variables so that they are treated as having certain types.\n\nbrazil$english &lt;- as.factor(brazil$english)\nbrazil$english &lt;- as.numeric(brazil$english)\nbrazil$english &lt;- as.character(brazil$english)\n\nNow try it out.\n\nTest out type for different variables using is...() for some of the variables.\nTest out coercion – and its results – using as...() for some of the variables.\nLook at the results using summary()."
  },
  {
    "objectID": "PSYC402/Week16.html#sec-multilevel-ideas-intro",
    "href": "PSYC402/Week16.html#sec-multilevel-ideas-intro",
    "title": "Introduction to multilevel data",
    "section": "Introduction to thinking about multilevel models",
    "text": "Introduction to thinking about multilevel models\n\nMain ideas – Phenomena and data sets in the social sciences often have a multilevel structure\nWe (Psychologists) often adopt Repeated Measures or Clustered designs in our studies, and these designs yield data that have a multilevel structure. Examples of research which results in data with a multilevel structure include:\n\nStudies where we test the same people multiple times, maybe in developmental or longitudinal investigations;\nIntervention, learning or treatment studies where we need to make pre- and post-treatment comparisons;\nStudies where we present multiple stimuli and everyone sees the same stimuli;\nStudies that involve multi-stage sampling e.g. selecting a sample of classes or schools then testing a sample of children within each classes or within each school.\n\nThe key insight to keep in mind when considering the analysis of such data is that observations are clustered and are not independent: they are correlated. What is correlated with what?\nImagine testing a number of subjects by giving them all the same test. In that test, you might present them all with the same stimuli over a series of trials, so that everybody sees the same set of stimuli. Do you think an observed response recorded for any one individual will be uncorrelated i.e. independent of that person’s other responses?\nPeople are different and one usually finds that a slow or inaccurate subject is slow or inaccurate for most of their responses. That means that if you have information about one of their responses you can predict, in part, what the time or accuracy of one of their other responses would be. That capacity to predict one response from another is what we mean when we talk about a lack of independence.\nAlternatively, imagine going to test children in a school. Will the children in one class be more like each other than they are like children in other classes? In other words, is there an effect of class – maybe due to the approach of the teacher, the effect of the class environment etc. – so that outcomes for children in a class are correlated with each other?\n\n\n\n\n\n\nImportant\n\n\n\nWe are talking, here, about a really quite general property of data collected in certain, very widely used, designs in Psychology: the clustering or hierarchical ordering of data.\n\n\nThis week, we will see that we must deal with the dependence of observations within a class, where the observed responses were made about the different pupils tested in a class, for a number of different classes.\n\n\nMultilevel models – why they are more used and more useful than traditional methods\nThe utility of multilevel models to analyze multilevel data or hierarchically structured data is well established in education. In educational research, we often need to think about effects of interventions or correlations in the context of observing children in classes, schools or districts, perhaps over time or at different time points. This means that many of the critical textbooks present examples that are based on educational research data [@raudenbush2002hierarchical; @Goldstein1995a; @kreft1998; @Snijders2004a].\nMultilevel models are growing in popularity in Psychology as well as in Education because they can be used to account for systematic and random sources of variance in observed outcomes when data are hierarchically structured. A hierarchical structure is present in data when a researcher: tests participants who belong to different groups like classes, clinics or schools; presents a sample of stimuli to each member of a sample of participants; or makes repeated observations for each participant over a series of test occasions.\nIn these circumstances, the application of traditional analytic methods has typically required the researcher to aggregate their data (e.g., averaging the responses made by a participant to different stimuli) or to ignore the hierarchical structure in their data, (e.g., analyzing the responses made by some pupils while ignoring the fact that the pupils were tested in different classes). But the application of traditional analysis approaches (e.g., regression, ANOVA) to multilevel structured data extracts scientific costs [@Baayen2008; @barr2013].\nIgnoring structure by ignoring or averaging over sources of variability like differences between classes, participants, or stimulus items can mean that analyses are less sensitive because they fail to fully account for error variance. Where differences between classes, participants or stimuli include variation in the impact of experimental variables, e.g., individual differences in response to an experimental manipulation, the application of traditional methods can be associated with an increased risk of false positives in discovery. Yet these costs need no longer be suffered because the capacity to perform multilevel modeling is now readily accessible.\n\n\nPractical applications: children sampled within classes\nIf a researcher tests participants belonging to different groups, e.g., records the educational attainment of different children recruited from different classes in a school, the test scores for the participants are observations that occupy the lowest level of a hierarchy (see Figure 2). In multilevel modeling, those observations are understood to be nested within higher-level sampling units, here, the classes. We can say that the children are sampled from the population of children. And the classes are sampled from the population of classes. Critically, we recognize that the children’s test scores are nested within the classes. This multi-stage sampling has important consequences, as we shall see.\n\n\n\n\n\n\n\n\nD\n\n  \n\nA\n\n A   \n\nB1\n\n B1   \n\nA-&gt;B1\n\n    \n\nB2\n\n B2   \n\nA-&gt;B2\n\n    \n\nC1\n\n C1   \n\nB1-&gt;C1\n\n    \n\nC2\n\n C2   \n\nB1-&gt;C2\n\n    \n\nC3\n\n C3   \n\nB1-&gt;C3\n\n    \n\nC4\n\n C4   \n\nB2-&gt;C4\n\n    \n\nC5\n\n C5   \n\nB2-&gt;C5\n\n    \n\nC6\n\n C6   \n\nB2-&gt;C6\n\n   \n\n\nFigure 2: Multilevel or hierarchically structured data\n\n\n\n\nWe are going to be working with the school data collected by @golino2014 in Brazil, so let’s take another look at that dataset. The data extract, following, presents the first 25 rows of the selected-variables brazil dataset. (I have arranged the rows by class number ID, and you may need to make sure the browser window is wide enough to see the data properly.) If you examine the extract, you can see that there are multiple rows of data for each class_number, one row for each child, with multiple children (you can see different participant_id numbers). This presentation of the dataset illustrates in practical terms – what you can actually see when you look at your data – what multilevel structured data can look like.\n\nbrazil %&gt;%\n       arrange(desc(class_number)) %&gt;%\n                 head(n = 25)\n\n   class_number participant_id portuguese english math physics\n1           M36             11         80      55   60      81\n2           M36             30         48      55   43      57\n3           M36             93         83      75   55      80\n4           M36             95         80      66   55      66\n5           M36            111         80      57   99     129\n6           M36            117         27      76   83      89\n7           M36            153          2      66   89     141\n8           M36            163         68      88   74     137\n9           M36            176         80      75   60     127\n10          M36            216         75      79   64     124\n11          M36            223         81      56   58      78\n12          M36            234         81      75   49      74\n13          M36            242         81      67   45      77\n14          M36            254         76      51   33     103\n15          M36            260         81      59   39      82\n16          M36            271         64      69   82     138\n17          M36            278         80      57   56     107\n18          M36            280         83      64   56      84\n19          M36            286         80      91   64      98\n20          M35             20         42      61   97      92\n21          M35            118         33      59   33      80\n22          M35            140         28      40   33      58\n23          M35            179         38      69   56     145\n24          M35            247         65      51   64      97\n25          M33             15         76      70   73      85\n\n\n\n\nTo understand the application of multilevel models: first, we ignore the multilevel structure in the data\nRecognizing that the children’s scores data are observed within classes means that, when we examine the factors that influence variance in observed outcomes, we need to take into account the fact that the children can be grouped by (or under) the class they were in when their grades were recorded. We can develop an understanding of what this means by moving through a series of steps.\nTo illustrate the understanding we need to develop, we analyze the end-of-year school subject grades for the sample of 292 children studied by @golino2014. With these data, we can examine whether differences between children in their Portuguese language grades predicts differences in their English language grades. (We do not have a theoretical reason to make this prediction though it does not seem unreasonable.) Let’s make this our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: Does Portuguese grade predict English grade?\n\n\n\nNow, the first step we take in our development of understanding will be to first ignore the importance of the potential differences between classes.\nWe can begin our analysis in order to address the research question by plotting the possible association between Portuguese and English grades. We shall create a scatterplot to do this, and we create the plot by running the following code and ignoring group (class) membership. In the following chunk of code, I pipe the brazil data using %&gt;% to ggplot() and then create the plot step-by-step, with each ggplot() step separated by a + except for the last step.\n\nbrazil %&gt;%\nggplot(aes(x = portuguese, y = english)) +\n  geom_point(colour = \"black\", size = 3, alpha = .5) +\n  geom_smooth(method = \"lm\", size = 2, se = FALSE, colour = \"red\") +\n  xlab(\"Portuguese\") + ylab(\"English\") +\n  theme_bw()\n\n\n\n\nFigure 3: Portuguese compared to English grades: each point represents the scores for one child\n\n\n\n\nRemember that each child ID is associated with a pair of grades: their grade in English and their grade in Portuguese. Each point in Figure 3 represents the paired grade data for one child.\nBecause I say that we are interested in predicting English grades then, by convention, we map English grades to the height of the points (i.e. English grade differences are shown as differences on the y-axis). Because we are using Portuguese grades to do the predicting, by convention, we map Portuguese grades to the horizontal position of the points (i.e. Portuguese grade differences are shown as differences on the x-axis).\nI have added a line using geom_smooth() in red to indicate the trend of the potential association between variation in Portuguese grades and variation in English grades.\nFigure 3 suggests that children with higher grades in Portuguese tend, on average, to also have higher grades in English.\n\nExercise – edit plots\nNotice that when we write the code to produce the plot, we add arguments to the geom_point() and geom_smooth() function calls to adjust the appearance of the points and the smoother line. Notice, also, that we adjust the labels for the x-axis and y-axis and, finally, that we determine the overall appearance of the plot using the theme_bw() function call.\nDo the following exercises to practice your ggplot() skills\n\nChange the x and y variables to math and physics\nChange the theme from theme_bw() to something different\nChange the appearance of the points, try different colours, shapes or sizes.\n\nFurther information to help you try out coding options can be found here, on scatterplots, and here, on themes.\n\n\n\nA linear model ignores the multilevel structure in the data\nOur plot indicates the relationship between English and Portuguese language grades, in Figure 3, ignoring the fact that children in the sample belonged to different classes when they were tested. The plot shows us only information about the children and grades, with each point representing the observed English and Portuguese grades for each \\(i\\) child.\nCan we predict variation in English grades given information about child Portuguese grades? Are English grades related to Portuguese grades? We can estimate the relationship between English and Portuguese grades using a linear model in which the English grades variable is the outcome variable (or the dependent variable) and the Portuguese grades variable is the predictor or independent variable:\n\\[\ny_i = \\beta_0 + \\beta_1X_i + e_i\n\\]\n\nwhere \\(y_i\\) represents the English grade for each \\(i\\) child;\n\\(\\beta_0\\) represents the intercept, the outcome value obtained if values of the explanatory variable are zero;\n\\(\\beta_1\\) represents the effect of variation in \\(X_i\\) the Portuguese grade for each child, with the effect of that variation estimated as the the rate of change in English grade for unit change in Portuguese grade;\n\\(e_i\\) represents differences, for each child, between the observed English grade and the English grade predicted by the relationship with Portuguese grades.\n\nAs you have seen, the code for running a linear model corresponds transparently to the statistical model given in the formula.\n\nsummary(lm(english ~ portuguese, data = brazil))\n\n\nCall:\nlm(formula = english ~ portuguese, data = brazil)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-64.909 -17.573   2.782  20.042  53.292 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.77780    3.81426   7.807 2.11e-13 ***\nportuguese   0.47001    0.07897   5.952 9.91e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.02 on 228 degrees of freedom\nMultiple R-squared:  0.1345,    Adjusted R-squared:  0.1307 \nF-statistic: 35.43 on 1 and 228 DF,  p-value: 9.906e-09\n\n\nThe linear model yields the estimate that for unit increase in Portuguese grade there is an associated increase of about .47 in English grade, on average (for the model, \\(F(1, 228) = 35, p &lt; .001; adj. R^2 = .13\\)). So, we have a preliminary answer to our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: Does Portuguese grade predict English grade?\nResult: Our analysis shows that children who score one grade higher in their Portuguese class (e.g., 61 compared to 60) tended to score .47 of a grade higher in their English class.\n\n\n\nWe shall see that we will need to revise this estimate when we do an analysis that does take school class differences into account.\n\n\n\n\n\n\nTip\n\n\n\nNotice: here, I have dispensed with the creation of a model object by name. The model is estimated anyway, and I have embedded the lm() function call within a summary() function so that I am asking R to do two things:\n\nlm() fit a linear model\nsummary() print out a summary of the fitted model object\n\nI do this to give an example of the way in which code can be varied. Also, to show you how one function can be embedded inside another.\n\n\n\nExercise – fit a different linear model\nDo the following exercises to practice your lm() skills\n\nChange the outcome and predictor variables to math and physics\nWhat do the results tell you about the relationship between maths and physics ability?\n\n\n\n\nNotation\nIn the following discussion, I will present some model formulas as equations. I am not doing that because the discussion is going to consider the modeling in terms of the underlying mathematics. I am doing it with the aim of clarifying how quantities – observed, estimated or predicted – add up, in terms of the linear and then the linear mixed-effects model.\n\nI am going to refer to the dependent or outcome variable (e.g., school grade) as \\(y\\),\nand to explanatory or experimental or independent variables (e.g., language skill) as \\(X\\).\nModels shall be fitted to estimate the coefficients, written \\(\\beta\\), of the effects of the explanatory variables on the outcome variable\nTo distinguish the different coefficients of the different effects, I am going to number the coefficients so …\n\n\n\\(\\beta_0\\) is the coefficient of the intercept;\n\\(\\beta_1\\) will be the coefficient of the effect of a first explanatory variable: in the Brazilian schools example, the coefficient of the effect of variation in Portuguese language skill on variation in English language scores.\n\nTo make it clear that each observation can be understood as part of a complex multilevel or crossed random effects structure, I am going to use indices as subscripts for variables.\n\nI will, here, index individual participants (children) using \\(i\\);\nI will index index classes in which children were tested using \\(j\\).\n\nThus, in the Brazilian schools example, we shall see that we are concerned with observations about children’s school grades, where children are sampled as individuals nested in samples of classes. We will examine how an outcome variable (English language grade) is related to a predictor variable (Portuguese language grade) such that:\n\n\\(y_{ij}\\) is the outcome English language grade recorded for each child \\(i\\) in each class \\(j\\);\nwhile \\(X_{ij}\\) represents the explanatory variable, see following, the Portuguese language grade.\n\n\\(X_{ij}\\) is subscripted \\(_{ij}\\) because values of the variable depend upon child identity, and children are identified as child \\(i\\) in class \\(j\\) to represent the multilevel structure of the data\n\n\nCan we really ignore the multilevel structure?\nThe linear model ignores the higher-level structure, the distinction between classes: does this matter?\nWe can see the answer to that question if we inspect Figure 4. We create the plot using the following chunk of code; we discuss that later, first reflecting on what the plot shows us.\n\nbrazil %&gt;%\nggplot(aes(x = portuguese, y = english)) +\ngeom_point(colour = \"darkgrey\") +\n  geom_smooth(method = \"lm\", se = FALSE, colour = \"black\") +\n  facet_wrap(~ class_number) +\n  xlab(\"Portuguese\") + ylab(\"English\") +\n  theme_bw() +\n  scale_x_continuous(breaks=c(25,50,75)) + scale_y_continuous(breaks=c(0,50,100))\n\n\n\n\nFigure 4: Plot of child grades, comparing English with Portuguese grades, shown separately for each school class\n\n\n\n\nFigure 4 presents a grid of scatterplots, with a different scatterplot to show the relationship between children’s Portuguese and English grades for the children in each different class. We can see that the relationship between Portuguese and English grades is (roughly) similar across classes: in general, children with higher Portuguese grades also tend to have higher English grades. However, Figure 4 makes it obvious that there are important differences between classes.\nWe can see that the slope of the best fit line (shown in black) varies between classes. And we can see that the intercept (where the line meets the y-axis) also varies between classes. Further, we can see that in some classes, there is no relationship or a negative relationship between Portuguese and English grades.\nCritically, we can see that classes differ in how much data we have for each. For some classes, we have many observations (e.g., M11) and for other classes we have few or one observation (e.g., M22, with one child). We know, in advance, that variation in sample size will be associated with variation in the uncertainty we have about the estimated relationship between Portuguese and English grades. You will remember that the Central Limit Theorem allows us to calculate the standard error of an estimate like the mean, given the sample size, and that the standard error is an index of our uncertainty of the estimate.\n\n\n\n\n\n\nTip\n\n\n\nFacetting – notice that, in the plotting code, the key expression is facet_wrap(~ class_number) This means:\n\nThe class_number variable is a factor: we ask R to check the summary for the dataframe, and that factor codes what class a child is in.\nThe facet_wrap(...) function then asks R to produce separate plots for each facet for the data – the word facet means face or aspect.\nWe use the formula facet_wrap(~ ...) to ask R to split the data up by using the classification information in the named variable, here class_number.\n\n\n\nThe production of a grid or lattice of plots is a useful method for comparing patterns between data sub-sets or groups. You can see more information about facet_wrap() here\n\n\n\n\n\n\nTip\n\n\n\nAdjusting scales – notice that in these plots I modified the axes to show x-axis and y-axis ticks at specific locations using scale functions. The tick is the notch or short line where we show the numbers on the axes.\n\nscale_x_continuous(breaks=c(25,50,75)) means: set the x-axis ticks at 25, 50 and 75, defined using a vector c() of values\nscale_y_continuous(breaks=c(0,50,100)) means: set the y-axis ticks at 0, 50, 100, defined using a vector of values.\n\n\n\nIt can be useful to adjust the ticks on axes, where other plot production factors cause crowding of labels.\nYou can see more information on scales here.\n\nExercise – edit plots\nDo the following exercises to practice your facet_wrap() skills\n\nChange the x and y variables to math and physics.\nExperiment with showing the differences between classes in a different way: instead of using facet_wrap() in aes() add colour = class_number: what happens?\n\n\n\n\nLinear models for multilevel data – dealing with the hierarchical structure\nFigure 4 shows that there is variation between classes in both the average English grade, shown as differences in the y-axis intercept, and the ‘effect’ of Portuguese language skill, shown as differences in the slope of the best fit line for the predicted relationship between Portuguese and English grades.\nWe put ‘effect’ in quotes to signal the fact that we do not wish to assert a causal relation. The interpretation of the results of the linear model assumes those results are valid provided that the observations are independent, among other things. We can see that we cannot make the assumption of independence because individuals in classes with high average English grades are more likely to have higher English grades.\nWe can represent in our analysis the information we have about hierarchical structure in the data (child within class) by allowing the regression coefficients to vary between groups. We therefore modify the subscripting to take into account the fact that we must distinguish which child \\(i\\) and which class \\(j\\) we are examining, adapting our model to:\n\\[\ny_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij} + e_{ij}\n\\]\n\nwhere \\(y_{ij}\\) represents the outcome measure, the English grade for each \\(i\\) child in each \\(j\\) class;\n\\(\\beta_{0j}\\) represents the average grade, different in different classes;\n\\(\\beta_{1j}X_{ij}\\) represents the variation in \\(X\\) the Portuguese grade for each \\(i\\) child in \\(j\\) class, with the effect of that variation estimated as the coefficient \\(\\beta_{1j}\\), different in different classes;\nand \\(e_{ij}\\) represents differences between observed and predicted English grades for each \\(i\\) child in \\(j\\) class.\n\n\n\nTwo-step or slopes-as-outcomes linear models as approximations to the Linear Mixed-effects or Multilevel modeling approach\nIn practice, we could capture the variation between classes by performing a two-step analysis.\n\nFirst, we estimate the coefficient of the ‘Portuguese’ effect for each class separately. We do multiple (per-class) analyses. In each of these analyses, we estimate the coefficient looking only at the data for one class.\nSecond, we can take those per-class coefficients as the outcome variable in a ‘slopes-as-outcomes analysis’ to examine if the per-class estimates of the experimental effect are reliably different from zero or, more usefully, if the per-class estimates vary in relation to some explanatory variable like teacher skill.\n\nThe problem with the approach is apparent in Figure 5: the figure shows the estimated intercept and coefficient of the slope of the ‘Portuguese’ effect for each class, when we have analyzed the data for each class in a separate linear model.\nThe estimate for each class is shown as a black dot. The standard error of the estimate is shown as a black vertical line, shown above and below a point.\nYou can say that where there is a longer line there we have more uncertainty about the location of the estimate. Notice that the standard errors vary a lot between classes. In some classes, the standard error is small (the black line is short) so we can maybe have more certainty over the estimated intercept or slope for those classes. In other classes, the standard error is large (the black line is long) so we can maybe have less certainty over the estimated intercept or slope for those classes.\n\n\n\n\n\n\nImportant\n\n\n\nThe key idea here is that standard errors vary widely between classes but the two-step modeling approach, while it can take into account the between-class differences in estimates, cannot account for the variation in the standard errors about those estimates.\n\n\n\n\n\n\n\nFigure 5: Plot showing the estimated intercept and coefficient of the slope of the Portuguese effect for each class analysed separately\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that the code I used fits a separate model for each class, and then plots the per-class estimates of intercepts and slopes of the english ~ portuguese relationship.\n\n\n(In case you want to see how this plot is produced, I work through the code for drawing Figure 5 in the workbook .R.)\n\nDiscussion: slopes-as-outcomes analyses as a common approach\nThe ‘slopes-as-outcomes’ approach is quite common and can be found in a number of papers in the psychological and educational research literatures. An influential example can be found in the report by @balota2004 of their analysis of psycholinguistic effects in older and younger adults. @balota2004 wanted to examine if or how effects of variables like word frequency, on reading response latencies, were different in different age groups. To do this, they first estimated the effect of the (word-level) psycholinguistic variables in separate linear model (multple regressions) for each adult. They then took the estimated coefficients as the dependent variable (slopes-as-outcomes) for a second analysis in which they tested the effect of age group on variation in the estimated coefficients (see @Lorch1990a for a discussion of the approach).\nYou may be asked to do this. I would advise you against it because there is a better way, as we see next.\n\n\n\nMultilevel models\nMultilevel models incorporate estimates of the intercept and the effect of independent (experimental or correlational) variables plus estimates of the random variation between classes in intercepts and slopes. Multilevel models are also known as mixed-effects models because they involve the estimation of fixed effects (effects due to independent variables) and random effects (effects due to random differences between groups).\nWe model the intercept (varying between classes) as:\n\\[\n\\beta_{0j} = \\gamma_0 + U_{0j}\n\\]\n\nwhere \\(\\beta_{0j}\\) values equal \\(\\gamma_0\\) the overall average intercept,\nplus \\(U_{0j}\\) the differences, for each class, between that average intercept and the intercept for that class.\n\nWe model the coefficient of the (Portuguese language) skill effect as:\n\\[\n\\beta_{1j} = \\gamma_1 + U_{1j}\n\\]\n\nwhere \\(\\beta_{1j}\\) values equal \\(\\gamma_1\\) the average slope,\nplus \\(U_{1j}\\) the differences, for each class, between that average slope and the slope for that class.\n\nThese models can then be combined:\n\\[\ny_{ij} = \\gamma_0 + \\gamma_1X_{ij} + U_{0j} + U_{1j}X_{ij} + e_{ij}\n\\]\nsuch that the English grade observed for each child \\(y_{ij}\\) can be predicted given:\n\nthe average grade overall \\(\\gamma_0\\) plus\nthe average relationship between English and Portuguese language skills \\(\\gamma_1\\) plus\nadjustments to capture\n\n\nthe difference between the average grade overall and the average grade for the child’s class \\(U_{0j}\\),\nas well as the difference between the average slope of the Portuguese effect and the slope of that effect for their class \\(U_{1j}X_{ij}\\),\nplus any residual differences between the observed and the model predicted English grade \\(e_{ij}\\).\n\n\n\nHow should we think about the differences between the classes?\nIf you have had some experience analyzing psychological data, then there is a potential approach to thinking about the effect of the differences between classes that would seem natural. This approach has, in fact, been proposed (see e.g. Lorch & Myers, 1990) and applied in the literature. I would not recommend using it but thinking about it helps to develop our understanding of what we are doing with multilevel models\nWe could seek to estimate (1.) the relationship of interest – here, the association between English and Portuguese grades, while (2.) also estimating the relationship between (outcome) English grades and the impact made by what class a child is in.\nIn this approach, we would construct a model in which we have English grades as the outcome variable, Portuguese grades as one predictor variable, and then add a variable to code for class identity. The ‘effect’ of the class variable would then capture the differences in intercepts between classes. You could add a further variable to code for differences between classes in the slope of the relationship between English and Portuguese grades. This would allow for the fact that the relationship is positive or negative, stronger or weaker, in different classes.\nIf we added that further variable to code for differences between classes in the English-Portuguese relationship, then we would be estimating those differences as interactions between (1.) the predictive ‘effect’ of Portuguese grades on English grades and (2.) the class effect. An interaction effect is what we have when the effect of one variable (the predictive ‘effect’ of Portuguese grades) is different for different levels of the other variable (the predictive ‘effect’ of Portuguese grades is different for different classes).\nThinking about this approach helps us to think about what we are doing when we are working with multilevel structured data. But the problem with the approach is that it does not allow us to generalize beyond the sample we have. The estimates we would have, given a model in which we code directly for class, would tell us only about the classes in our sample.\nMost of the time, we would prefer to do analyses whose results could be generalized: to other children, to other classes, etc.. For this reason, it makes more sense to suppose that \\(U_{0j} + U_{1j}\\), the class-level deviations, are unexplained differences drawn at random from a population of potential class differences.\nWhat does this mean? Think back to your understanding of the linear model. You have learnt that when we fit a linear model like \\(y_i = \\beta_0 + \\beta_1X_i + e_i\\) we include a term \\(e_i\\) to represents the differences, for each child, between the observed (outcome) English grade and the English grade predicted by the relationship with Portuguese grades. Those differences between observed and predicted outcomes are called residuals. We assume that the direction and size of any one residual, for any one child, is randomly determined because we typically have no idea why there might be a big difference between the predicted and observed grade for one child but a small residual difference for another.\nNow, we can imagine that there will be many classes in many schools, and we can surely expect that there will be differences between the classes. These differences will result from plenty of factors we do not measure or cannot explain. Indeed, we have seen that there are differences between the average (outcome) English grade i.e. the predicted intercept and the observed intercept for each class, and we have seen that there are differences between the average slope of the English-Portuguese grades relationship i.e. the predicted slope and the slope for each class. These differences would, in effect, be random differences. And we can see how the variation in these differences are, for us, just a kind of random error variance, which we can see as class-level residuals.\nWe suppose, technically, that the differences between classes, controlling for the effect of the explanatory variable, are exchangeable (it does not matter which class is which), with classes varying at random. Multilevel models incorporate estimation of the explanatory variables effects \\(\\gamma_0 + \\gamma_1\\) accounting for group-level error variance \\(U_{0j} + U_{1j}\\).\nThe difference between the two-step approach and the multilevel modelling approach is this:\n\nIn the two-step approach, seen earlier, we estimate – separately, for each class – the intercept and the slope of the Portuguese effect, using just the data for that class and ignoring the data for other classes.\nIn the multilevel model, in contrast, we use all the observations, estimating the average intercept and the average slope of the Portuguese effect plus the variance due to the difference for each class (1.) between the average intercept and the class intercept or (2.) between the average slope and the class slope.\nWe can understand these differences between the average (intercept or class) and the class differences as class-level residuals \\(U_{0j} + U_{1j}\\) in addition to the child-level residuals \\(e_{ij}\\).\n\nThis leads us to a conclusion that represents a critical way to understand multilevel models.\n\n\n\n\n\n\nImportant\n\n\n\nThe multilevel model is a linear model but with multiple random effects terms to take into account the hierarchical structure in the data.\n\n\n\n\nFitting a multilevel model using the lmer() function\nWe can fit a multilevel model of the english ~ portuguese relationship, taking into account the fact that the pupils tested in the study were recruited from different classes, using the convenient and powerful lmer() function.\nHappily, the code we use to define a model in lmer() is much like the code we use to define a model in lm() with one important difference which I shall explain. Let’s try it out.\nTo use the lmer() function you need to make the lme4 library available.\n\nlibrary(lme4)\n\nThe model you are going to code will correspond to the statistical model that we have been discussing:\n\\[\ny_{ij} = \\gamma_0 + \\gamma_1X_{ij} + U_{0j} + U_{1j}X_{ij} + e_{ij}\n\\]\nAnd the code is written as follows.\n\nporto.lmer1 &lt;- lmer(english ~ portuguese +\n\n                              (portuguese + 1|class_number),\n\n                              data = brazil)\n\nYou can see that the lmer() function call is closely similar to the lm() function call, with one critical exception, as I explain next.\nFirst, we have a chunk of code mostly similar to what we do when we do a regression analysis.\n\nporto.lmer1 &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nenglish ~ portuguese is a formula expressing the model in which we estimate the fixed effect on the outcome or dependent variable english (English grades) predicted \\(\\sim\\) by the independent or predictor variable portuguese (Portuguese grades).\nIf there were more terms in the model, the terms would be added in series separated as variable names separated by + sum symbols.\n...(..., data = brazil) specifies the dataset in which you can find the variables named in the model fitting code.\nsummary(porto.lmer1) gets a summary of the fitted model object.\n\nSecond, we have a bit that is specific to multilevel or mixed-effects models.\n\nCritically, we add (...|class_number) to tell R about the random effects corresponding to random differences between sample groups (classes) coded by the class_number variable.\n(...1 |class_number) says that we want to estimate random differences between sample groups (classes) in intercepts coded 1.\n(portuguese... |class_number) adds random differences between sample groups (classes) in slopes of the portuguese effect coded by using the portuguese variable name.\n\nIf you run the model code as written – see the .R workbook file for an example of the code – and it works then the code will be shown in the console window in R-Studio. To show the model results, you need to get a summary of the model, using the model name.\n\nsummary(porto.lmer1)\n\n\nExercise – fitting linear mixed-effects models\nMixed-effects model code is hard to get used to at first. A bit of practice helps to show you which bits of code are important, and which bits you will change for your own analyses\n\nChange the outcome (from english) and the predictor (from portuguese): this is about changing the fixed effect part of the model.\nVary the random effects part of the model.\n\n\nChange it from (portuguese + 1 | class_number) to (1 | class_number) what you are doing is asking R to ignore the differences in the slope of the effect of Portuguese grades.\nChange it from (portuguese + 1 | class_number) to (portuguese + 0 | class_number what you are doing is asking R to ignore the differences in the intercept.\n\nTry out these variations and look carefully at the different results.\n\n\n\nReading the lmer results\nNow let’s take a look at the results.\n\nsummary(porto.lmer1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (portuguese + 1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2104.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.81321 -0.59584  0.04359  0.60018  2.23722 \n\nRandom effects:\n Groups       Name        Variance Std.Dev. Corr \n class_number (Intercept) 341.4803 18.479        \n              portuguese    0.3295  0.574   -0.98\n Residual                 493.1009 22.206        \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  25.2837     6.2669   4.034\nportuguese    0.6590     0.1729   3.811\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.943\n\n\nNotice that the output has a number of elements.\n\nFirst, we see information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula english ~ portuguese + (portuguese + 1 | class_number).\nThen, we see `REML criterion at convergence: about the model fitting process, which we can usually ignore.\nThen, we see information about the distribution of the model residuals.\nThen, we see information about the error terms estimated (technically, predicted) by the model.\n\n\nResiduals, just like a linear model plus error terms specific to multilevel or mixed-effects models, group-level residuals:\ndifferences between the average intercept and, here, the intercept (average `english: score) per class;\nand differences between the average slope capturing the \\(english \\sim portuguese\\) relationship and the slope (of the effect) per class.\n\n\nThen, just as for linear models, we see estimates of the coefficients (of the slopes) of the fixed effects, the intercept and the slope of the \\(english \\sim portuguese\\) relationship.\n\nNote that we see coefficient estimates like in a linear model summary but no p-values. We will come back to p-values later but note that their absence is not a bug. Note also that we do not see an \\(R^2\\) estimate. We will come back to that too.\n\nExercise – How do we report mixed-effects models results?\nThere is no convention, yet, on how to report the results of these models. Lotte Meteyard and I argue for a set of conventions that will help researchers to understand each others’ results better.\n\nIn this exercise, go read the bit where we advise Psychologists how to write about the results, in our paper [@meteyard2020a].\n\n\n\n\nFixed and random effects\nYou will have noticed the reference to fixed and random effects in the discussion in this chapter, and the use of fixed and random effects as titles for sections of the model output. The terms fixed effects and random effects are not used consistently in the statistical literature [@Gelman2007g]. I am going to use these terms because they are helpful, at first, and because they are widely used, not least in the results of our analyses in R.\nIt is common in the psychological literature to refer to the effects on outcomes of experimental manipulations (e.g., the effect on outcomes of differences between experimental conditions) or to the effects of correlated variables of theoretical or explanatory interest (e.g., the effect of differences in Portuguese language skill) as fixed effects. Typically, we are aiming to get estimates of the coefficients of these effects. And, much like we would do when we use linear models, we expect that these coefficients represent an estimate of the effects of these variables, on average, across the population. (Hence, some analysts prefer to talk about these effects as population average effects).\nIn comparison, when we are thinking about the effects on outcomes of what we understand to be the random differences between sampled children (e.g., the child-level residuals) or the random differences between sampled classes (e.g., the class-level residuals) then we refer to these effects as random effects. As we have seen, we usually estimate random effects as variances and can estimate them as random error variances. While we may care to estimate how differences in, say, Portuguese language score, is associated with English grade, we typically do not care about the impact of the specific difference between any two classes in English grade.\nHowever, if you take your education in this area further, you will find that the way that fixed and random effects are talked about in the statistical literature can be, at best, inconsistent. And, ultimately, you might ask yourself if there are principled distinctions between fixed and random effects. We can leave these problems aside, here, because they do not influence how we shall learn and practice using multilevel models in the early to medium term in our development of skills and understanding.\n\n\nIs there a difference between linear model and linear mixed-effects model results?\nRecall that the linear model yields the estimate that for unit increase in Portuguese grade there is an associated .5 increase in English grade, on average (\\(F(1, 228) = 35, p &lt; .001; adj. R^2 = .13\\)). We can see that the estimate of the effect for the mixed-effects model is \\(\\beta = .659\\) which is somewhat different from the effect estimate we got from the linear model.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: Does Portuguese grade predict English grade?\nResult: Our analysis shows that children who score one grade higher in their Portuguese class (e.g., 61 compared to 60) tended to score about \\(.66\\) of a grade higher in their English class."
  },
  {
    "objectID": "PSYC402/Week16.html#sec-multi-conclusions",
    "href": "PSYC402/Week16.html#sec-multi-conclusions",
    "title": "Introduction to multilevel data",
    "section": "Conclusions",
    "text": "Conclusions\nPsychological studies frequently result in hierarchically structured data. The structure can be understood in terms of the grouping of observations, as when there are multiple observations per group, participant or stimulus. The existence of such structure must be taken into account in analyses. Multilevel or mixed-effects models can be specified by the researcher to include random effects parameters that capture unexplained differences between participants or other sampling units in the intercepts or the slopes of explanatory variables. Where a sample of participants is asked to respond to a sample of stimuli, structure relating both to participants and to stimuli can be incorporated.\nMany researchers will be aware of concerns over the non-replication of published effects in psychological science (see ?@sec-context-replication). As @gelman2014b discusses, non-replication of results may arise if effects vary between contexts groups while traditional analytic methods assume that effects are constant. Psychological researchers can expect average outcomes and the effects of independent variables to vary among sampling units, whether their investigations involve multiple observations per child or stimulus, or children sampled within classes, clinics or schools. However, traditional analytic methods often require us to ignore this variation by averaging, say, responses over multiple trials for each child, to collapse an inherently multilevel data structure into a single level of observations that can be analysed using regression or ANOVA. By using multilevel models, researchers will, instead, be able to properly estimate the effects of theoretical interest, and better understand how those effects vary.\n\nSummary\n\nWe outlined the features of a multilevel structure dataset.\nWe then discussed visualizing and modeling the relationship between outcome and predictor variables when you ignore the multilevel structure.\nWe examined why ignoring the structure is a bad idea.\nWe considered how slopes-as-outcomes analyses are an approximate method to take the structure into account.\nWe dmonstrated how multilevel modeling is a better method to estimate the relationship between outcome and predictor variables\n\n\n\nGlossary: useful functions\nWe used functions to read-in the libraries of functions we needed.\n\nlibrary(tidyverse)\nlibrary(lme4)\n\nWe used some new functions, or focused on functions we have seen before but not discussed.\n\nWe used load() to load an .RData file into R workspace.\nFor visualization, we used facet_wrap() to show plots of the relationship between English and Portuguese scores in each class separately.\nWe used lmer() to fit a multilevel model.\n\nWe used the summary() function to get model results for both linear models and for the multilevel or linear mixed-effects model."
  },
  {
    "objectID": "PSYC402/Week16.html#sec-multi-reading",
    "href": "PSYC402/Week16.html#sec-multi-reading",
    "title": "Introduction to multilevel data",
    "section": "Recommended reading",
    "text": "Recommended reading\n@Snijders2004a present a helpful overview of multilevel modelling. Readers familiar with the book will see that I rely on it to construct the formal presentation of the models.\n@Gelman2007g present a more in-depth treatment of multilevel models."
  },
  {
    "objectID": "Staff/index.html",
    "href": "Staff/index.html",
    "title": "Staff Information",
    "section": "",
    "text": "Hi. This will have some documents/information on how to set up the materials for Quarto and have it push to the website.\nIt’ll have a .zip for the basic folder structure required for easy integration of the materials into the rest.\nMay even have the instructor files? Although obviously students will click a “For Staff” button, so maybe it needs to be on an invisible link.\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC214/Week8.html#learning-objectives",
    "href": "PSYC214/Week8.html#learning-objectives",
    "title": "8. Two-Factor Mixed and Within-Participants ANOVA",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this week’s lecture, we considered the procedures involved in performing a two-factor mixed and fully-within participants ANOVA. Using the hypothetical data on the Stroop effect presented in the lecture, in today’s lab session we will demonstrate how to analyse such ANOVA designs in R, including the calculation of the simple main effects and follow up procedures for breaking down simple main effects of factors with three or more levels. We will also demonstrate how to write up the results of two-factor mixed and fully within-participants designs.\nIf you get stuck at any point, be proactive and ask for help from one of the GTAs."
  },
  {
    "objectID": "PSYC214/Week8.html#getting-started",
    "href": "PSYC214/Week8.html#getting-started",
    "title": "8. Two-Factor Mixed and Within-Participants ANOVA",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started, we first need to log into the R Studio Server.\nYou can access Lancaster Universities RStudio Server at http://psy-rstudio.lancaster.ac.uk. At present, you will need to be on campus, or connected to the VPN to access this. If you do not yet have Eduroam (the university WiFi) available on your personal device, please follow the instructions from the PSYC214 Announcement Page https://modules.lancaster.ac.uk/mod/forum/discuss.php?d=388256\n\nIf you are on Eduroam (or VPN if off campus) and have accessed the RStudio Server from the URL above, you will now see a login screen (see above). Please use your normal Lancaster University username (e.g., bloggsj). Your own individual RStudio Server password was sent in an email, prior to the first lab, by Kay Rawlins: email header ‘R Studio Server login details’. Please make sure you have this.\nOnce you are logged into the server, create a folder for today’s session. Navigate to the bottom right panel and under the Files option select the New Folder option. Name the new folder psyc214_lab_8. Please ensure that you spell this correctly otherwise when you set the directory using the command given below it will return an error.\nSe we can save this session on the server, click File on the top ribbon and select New project. Next, select existing directory and name the working directory ~/psyc214_lab_8 before selecting create project.\nFinally, open a script for executing today’s coding exercises. Navigate to the top left pane of RStudio, select File -&gt; New File -&gt; R Script. Working from a script will make it easier to edit your code and you will be able to save your work for a later date.\n\nLet’s set our working directory:\n\nsetwd(\"~/psyc214_lab_8\")\n\nNow that you have created a folder for today’s session, it’s time to add the Week 8 files. Head on over to the PSYC214 Moodle page, access the lab folder for Week 8, and download the files StroopMixed.csv, StroopWithin.csv, and simple.R to your desktop. Next, in the RStudio Server open your new psyc214_lab_8 folder. When in the new folder, select the Upload tab. This will present a box that will ask where the data is that you want to upload. Click on Browse, find the StroopMixed.csv file on your desktop and click OK, then repeat these steps for the StroopWithin.csv and the simple.R files.\nBefore moving on, let’s load the relevant libraries/functions that we will be using in today’s session.\n\nlibrary(\"tidyverse\")  # For data storage and manipulation\nlibrary(\"tidyr\")      # For tidy data\nlibrary(\"rstatix\")    # For descriptives statistics, outlier detection, running the ANOVAs etc.\nsource(\"simple.R\")    # Custom function for generating the simple main effects\n\nNote that the custom function simple.R that we use to calculate the simple main effects must be present in your directory whenever you seek to use it."
  },
  {
    "objectID": "PSYC214/Week8.html#analysing-hypothetical-data-for-the-mixed-design-stroop-experiment",
    "href": "PSYC214/Week8.html#analysing-hypothetical-data-for-the-mixed-design-stroop-experiment",
    "title": "8. Two-Factor Mixed and Within-Participants ANOVA",
    "section": "Analysing hypothetical data for the mixed-design Stroop experiment",
    "text": "Analysing hypothetical data for the mixed-design Stroop experiment\nWe begin by analysing the mixed-design Stroop experiment described in the lecture. Recall that in this experiment, a researcher wants to test the hypothesis that response inhibition—the ability to suppress task-irrelevant information—is impaired in patients with schizophrenia. She predicts that if this is true, then a group of patients with schizophrenia will show a larger Stroop effect than a group of healthy controls. Our researcher administers a multi-trial Stroop task to two groups of participants in a 2 (group: healthy vs. schizophrenia) \\(\\times\\) 2 (trial type: congruent vs. incongruent) mixed design, where group is a between-participants factor and trial type is a within-participants factor.\nThe data set contains four columns:\n\nParticipant: represents the participant number, which ranges from 1–80, with \\(N\\) = 40 participants in each of the four conditions resulting from the combination of our two factors.\nGroup: represents whether the participant belongs to the healthy control group (Healthy) or the schizophrenia group (Schizophrenia).\nCongruent: represents the mean response time, averaged across trials, for congruent trials in which the colour word and the ink it is presented in are the same.\nIncongruent: represents the mean response time, averaged across trials, for incongruent trials in which the colour word and the ink it is presented in are different.\n\n\nImport data, set variables as factors, generate descriptive statistics, and perform assumption checks\nThe first thing you need to do is load the data into RStudio. Make sure that you name your data frame as stroopMixed.\n\n# *** ENTER YOUR OWN CODE HERE TO IMPORT THE DATA ***\n\n\n\n# A tibble: 80 × 4\n   Participant Group   Congruent Incongruent\n         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1           1 Healthy       586         906\n 2           2 Healthy       608         773\n 3           3 Healthy       609         887\n 4           4 Healthy       621         775\n 5           5 Healthy       642         699\n 6           6 Healthy       593         707\n 7           7 Healthy       616         850\n 8           8 Healthy       721         763\n 9           9 Healthy       631         760\n10          10 Healthy       639         740\n# … with 70 more rows\n\n\nThe next thing we need to do is convert the Participant variable into a factor. We can do this with the following code:\n\n# Convert \"Participant into factor\nstroopMixed$Participant = factor(stroopMixed$Participant)\n\nOnce you have done this, you need to convert the Group variable into a factor. We’ll let you do this yourself using your own code:\n\n# *** ENTER YOUR OWN CODE HERE TO CONVERT \"GROUP\" INTO A FACTOR ***\n\nThe next thing we need to do is convert our data from wide format into long format. Richard discussed the distinction between these two formats in his Week 4 lab session and we covered it again in our Week 5 lab session. In brief, we need to create a new variable called TrialType that contains the different levels of this factor, congruent and incongruent, which are currently located in different columns. We can do this using the following piece of code which users the gather() function encountered previously:\n\n# Convert data into long format\nstroopMixedLong = gather(stroopMixed,TrialType,RT,Congruent:Incongruent,factor_key = TRUE)\n\nLet’s unpack what this is doing. The first argument to gather(), stroopMixed, is the name of our current data set in wide format. The second argument tells R we want a new column called TrialType. The third argument, RT, tells R the name of our dependent variable. The fourth argument tells R to take the columns Congruent through to Incongruent and combine them into a column labeled TrialType (our second argument). The final argument, factor_key = TRUE, tells R that we want this new column to be a factor. The results of this transformation are allocated to a new data frame called stroopMixedLong. This contains our data in long format and we will be using this version of the data set from henceforth.\nNext, we will generate some descriptive statistics (mean, standard deviation, and confidence intervals). You did this for a between-participants factorial design in last week’s seminar, so you can once again generate the code for this yourself.\n\n# *** ENTER YOUR OWN CODE HERE TO GENERATE DESCRIPTIVE STATISTICS ***\n\nIf you have executed the code correctly, then you should see the following output:\n\n\n# A tibble: 4 × 7\n  Group         TrialType   variable     n  mean    sd    ci\n  &lt;fct&gt;         &lt;fct&gt;       &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Healthy       Congruent   RT          40  623.  39.9  12.7\n2 Healthy       Incongruent RT          40  770.  51.4  16.4\n3 Schizophrenia Congruent   RT          40  625.  50.8  16.2\n4 Schizophrenia Incongruent RT          40  889.  44.3  14.2\n\n\nAt this stage, we would ordinarily perform various checks including identifying possible outliers and checking that our data satisfy the normality assumption. We won’t perform those checks today, not because they are not important (they most certainly are!), but rather because we don’t have the time to do so. There is one check that we will perform and that is to see if the homogeneity of variance assumption has been violated. In a mixed design like ours, the homogeneity of variance assumption only applies to the between-participants factors, but not the within-participants factors.\nOur between-participants factor is Group (healthy vs. schizophrenia) but remember that each group receives each level of our within-participants factor TrialType (congruent vs. incongruent). So, we need to test whether the variances are equal or not for the healthy and schizophrenia groups for congruent and incongruent trials separately. The following code will generate what we need:\n\nstroopMixedLong %&gt;% \n  # Organise the output by the \"TrialType\" factor\n  group_by(TrialType) %&gt;%\n  # Generate Levene's test on the \"Group\" factor\n  levene_test(RT ~ Group)\n\n# A tibble: 2 × 5\n  TrialType     df1   df2 statistic      p\n  &lt;fct&gt;       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 Congruent       1    78     3.95  0.0505\n2 Incongruent     1    78     0.747 0.390 \n\n\nLevene’s test is non-significant for congruent trials, p = .051 (although notice that it is only marginally short of significance), and also for incongruent trials, p = .390. So, we can safely assume that the homogeneity of variance assumption has been satisfied.\n\n\nRunning the mixed ANOVA\nTo run our ANOVA, we are going to use the anova_test function from the rstatix package. This is the function that I recommend you use for mixed or fully-within participants factorial designs. The code required to run the ANOVA is given below:\n\n# Create the mixed design ANOVA model\nstroopMixedModel = anova_test(data = stroopMixedLong, dv = RT, wid = Participant, between = Group, within = TrialType, detailed = TRUE)\n# Print the model summary\n(stroopMixedModel)\n\nANOVA Table (type II tests)\n\n           Effect DFn DFd        SSn      SSd         F         p p&lt;.05   ges\n1     (Intercept)   1  78 84525386.6 136336.6 48358.096 1.06e-110     * 0.996\n2           Group   1  78   147440.3 136336.6    84.353  4.74e-14     * 0.301\n3       TrialType   1  78  1692705.3 205941.9   641.108  2.27e-39     * 0.832\n4 Group:TrialType   1  78   137886.3 205941.9    52.224  2.91e-10     * 0.287\n\n\nTo create the model, the first argument we supplied to anova_test was the name of our data, stroopMixedLong. The second argument we supplied was our dependent variable, RT. The third argument we supplied was Participant, which is the column containing the individuals/participants identifier. The fourth argument we supplied was our between-participants factor, Group. The fourth argument we supplied was our within-participants factor, TrialType.\nNotice that the resulting ANOVA table is different in format to those given in the lecture. The format given in the lecture follows a standard convention, but anova_test frustratingly uses a different format. The main effects of Group, TrialType, and the Group:TrialType interaction are each given on separate rows. The row corresponding to each effect contain the between-group degrees of freedom (DFn), the error degrees of freedom (DFd), the between-group sums of squares (SSn), the error sums of squares (SSd), the \\(F\\) ratio (F), and the p (p) value. On the bright side, this organisation makes it easier to locate the correct degrees of freedom when reporting the different outcomes.\nLooking at our ANOVA table, we can see that there is a significant main effect Group, p \\(&lt;\\) .001. Inspecting our descriptive statistics, we can see that this arose because mean response times are longer for the schizophrenia group than the healthy group. There is also a significant main effect of Trial Type, p \\(&lt;\\) .001, which arose because mean response times are longer for incongruent than congruent trials. Finally, there is a significant interaction between the two factors, p \\(&lt;\\) .001. We will defer interpretation of this interaction until later, once we have calculated the simple main effects and plotted the data. By the way, you can just ignore the first row of the table with the outcome named “intercept”.\nBefore we calculate the simple main effects, notice that the ANOVA table generated by anova_test does not give us the between-group mean squares or the error mean squares that are used to calculate the \\(F\\) ratios. This is a rather silly omission, and I’m not sure why the creators of the rstatix package thought it would be wise to exclude these. Now, you don’t need these values to interpret and report your ANOVA, but I do need to extract these values, so I can show you which error term is being used to test the different ANOVA outcomes and the simple main effects that we will calculate shortly.\nSo, we are going to calculate the mean squares for ourselves. We will first calculate the between-group mean squares. The between-group mean square is simply the between-group sums of squares (SSn) divided by its corresponding degrees of freedom (DFn). We can calculate this as follows:\n\n# Calculate the between-group mean sum of squares\nmixedMeanSquareBetween = stroopMixedModel$SSn/stroopMixedModel$DFn\n# Exclude the intercept (row 1 of 4) from the results\nmixedMeanSquareBetween[2:4]\n\n[1]  147440.3 1692705.3  137886.3\n\n\nThe first value is the between-group mean square for the main effect of Group (147440.3), the second value is the between-group mean square for the main effect of Trial Type (1692705.3), and the third value is the between-group mean square for the interaction (137886.3).\nNext, we will calculate the error mean squares. The error mean square is simply the error sums of squares (SSd) divided by its corresponding degrees of freedom (DFd). We can calculate this as follows:\n\n# Calculate the error mean sum of squares\nmixedMeanSquareError = stroopMixedModel$SSd/stroopMixedModel$DFd\n# Exclude the intercept (row 1 of 4) from the results\nmixedMeanSquareError[2:4]\n\n[1] 1747.906 2640.281 2640.281\n\n\nThe first value is the error mean square for the main effect of Group (1747.906), the second value is the error mean square for the main effect of Trial Type (2640.281), and the third value is the error mean square for the interaction (2640.281). Recall that in the lecture, we saw that in a mixed design, the error term for the within-participants factor is used to test both the main effect of that factor and any interaction involving that factor. Sure enough, we can see that the error term for testing the main effect of our within-participants factor, Trial Type, is the same as the one used to test the interaction (i.e., 2640.281).\nRemember that the \\(F\\) ratio for each outcome is calculated by dividing its between-group mean square by its error mean square. So, let’s calculate the \\(F\\) ratios for ourselves and check they align with what has been given to us in our ANOVA table. We can do that with the following code:\n\n# Calculate F ratios\nmixedFRatios = mixedMeanSquareBetween/mixedMeanSquareError\n# Print the results, excluding the intercept\n(mixedFRatios[2:4])\n\n[1]  84.35256 641.10811  52.22411\n\n\nThe first \\(F\\) ratio is for the main effect of Group (84.353), the second is for the main effect of Trial Type (641.108), and the third is for the interaction (52.224). If you compare these \\(F\\) values with those given in the ANOVA table, you will see that they are identical.\nTo be clear, when conducting a mixed ANOVA in the future, you don’t need to go through the steps of calculating the mean squares. I have gone through these steps with you because I need to make it clear what error terms we are using to test the different ANOVA outcomes and our simple main effects.\n\n\nSimple main effects analysis\nSince the interaction is significant, we need to calculate the simple main effects. Last week, when calculating the simple main effects for a fully-between participants design, we used the testInteractions() function in the phia package. Unfortunately, this package cannot be used with mixed and fully within-participants designs. Accordingly, I have created a custom function called simple() that will compute the simple main effects for you.\nRecall from the lecture that for mixed and within-participants factorial designs, we use an approach to calculating the simple main effects known as the pooled error terms approach. What this means is that the simple main effects of each factor are calculated using the same error term. However, the error term we use will differ depending on the factor for which we are calculating the simple main effects (contrast this approach with that we used for calculating the simple main effects in a fully between-participants design, where we used the same error term to calculate the simple main effects of all factors).\nBefore we can calculate the simple main effects, there are a few things we need to do. First, we need to store our ANOVA table in a dataframe:\n\n# Get the mixed ANOVA table\nmixedAnovaTable = get_anova_table(stroopMixedModel)\n\nNext, we need to calculate the cell totals for each of the four conditions and the number of observations (i.e., scores) in each cell:\n\n# Get cell totals and counts\nmixedCellTotals = stroopMixedLong %&gt;%\n  # Organise the output by the\"Group\" and \"TrialType\" factors\n  group_by(TrialType, Group) %&gt;%\n  # Request cell totals and number of observations (i.e., scores)\n  summarise(sum = sum(RT),n = n())\n  # Print the results\n  (mixedCellTotals)\n\n# A tibble: 4 × 4\n# Groups:   TrialType [2]\n  TrialType   Group           sum     n\n  &lt;fct&gt;       &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Congruent   Healthy       24919    40\n2 Congruent   Schizophrenia 24999    40\n3 Incongruent Healthy       30799    40\n4 Incongruent Schizophrenia 35576    40\n\n\nThen, we need to specify which simple main effects we want to generate. We are first going to calculate the simple main effects of the factor Group at Trial Type. This means, we are going to:\n\nTest the difference between the healthy and schizophrenia groups for congruent trials only.\nTest the difference between the healthy and schizophrenia groups for incongruent trials only.\n\nTo do this, we need to declare Group as the “fixed” factor (we are always comparing the healthy and schizophrenia groups) and Group as the “across” factor (the comparison between the healthy and schizophrenia groups occurs “across” the congruent and incongruent levels of the Trial Type factor):\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"Group\"\nacross = \"TrialType\"\n\nTo get the simple main effects, we pass the cell totals, ANOVA table, and the fixed and across factors to our function simple():\n\n# Simple main effects of Group at Trial Type\nmixedSmeGroup = simple(mixedCellTotals,mixedAnovaTable,fixed,across)\n(mixedSmeGroup)\n\n       Levels Sum of Squares Degrees of Freedom Mean Square            F\n1   Congruent           80.0                  1      80.000   0.04576906\n2 Incongruent       285246.6                  1  285246.612 163.19337388\n3  Error term       136336.6                 78    1747.906   0.00000000\n             P\n1 8.311544e-01\n2 8.245446e-21\n3 0.000000e+00\n\n\nAs described in the lecture, to calculate the simple main effects of a given factor in a mixed design, we use the mean square error for the main effect of that factor from the original ANOVA table as the error term. The mean square error in the simple main effects table is given in column four (Mean Square) of row three (Error term). You will notice that this is the error mean square for the main effect of Group that we calculated earlier from our initial ANOVA table (i.e., 1747.906). If you divide the between-group mean squares for the effect of Group at congruent and incongruent trials by this value, it will give you the \\(F\\) ratios shown in the simple main effects table.\nLooking at the simple main effects, we can see that the simple main effect of Group at congruent trials is not significant, \\(p\\) = 0.831. This indicates that mean response times for congruent trials did not differ between the healthy and schizophrenia groups. However, the simple main effect of Group at incongruent trials is significant, \\(p\\) &lt; .001. Looking at the descriptive statistics we generated earlier, we can see that this is because mean response times are longer on incongruent trials for the schizophrenia group than the healthy group.\nNext, we are going to calculate the simple main effects of the factor Trial Type at Group. This means, we are going to:\n\nTest the difference between congruent and incongruent trials for the healthy group only.\nTest the difference between congruent and incongruent trials for the schizophrenia group only.\n\nTo do this, we now need to declare Trial Type as the “fixed” factor and Group as the “across” factor:\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"TrialType\"\nacross = \"Group\"\n\nWe then generate the simple main effects of Trial Type with the following:\n\n# Simple main effects of Trial Type at Group\nmixedSmeTrialType = simple(mixedCellTotals,mixedAnovaTable,fixed,across)\n(mixedSmeTrialType)\n\n         Levels Sum of Squares Degrees of Freedom Mean Square        F\n1       Healthy       432180.0                  1  432180.000 163.6871\n2 Schizophrenia      1398411.6                  1 1398411.612 529.6451\n3    Error term       205941.9                 78    2640.281   0.0000\n             P\n1 7.609756e-21\n2 1.632592e-36\n3 0.000000e+00\n\n\nRemember, to calculate the simple main effects of a given factor in a mixed design, we use the mean square error for the main effect of that factor from the original ANOVA table as the error term. The mean square error in the simple main effects table is given in column four (Mean Square) of row three (Error term). You will notice that this is the error mean square for the main effect of Trial Type that we calculated earlier from our initial ANOVA table (i.e., 2640.281). If you divide the between-group mean squares for the effect of Trial Type at healthy and schizophrenia groups by this value, it will give you the \\(F\\) ratios shown in the simple main effects table.\nLooking at the simple main effects, we can see that the simple main effect of Trial Type at healthy is significant, \\(p\\) \\(&lt;\\) .001. Inspecting our descriptive statistics, we can see that this is because for the healthy group, mean response times are longer for incongruent than congruent trials. The simple main effect of Trial Type at schizophrenia is also significant, \\(p\\) \\(&lt;\\) .001. Looking at our descriptive statistics, we can see that this is because for the schizophrenia group, mean response times are also longer for incongruent than congruent trials. Thus, both the healthy and schizophrenia groups show a Stroop effect (longer response times for incongruent than congruent trials), but the effect is larger for the schizophrenia group, which has a much smaller \\(p\\) value (and a correspondingly larger \\(F\\) ratio).\n\n\nWriting up the results\n\n\n\nFigure 1. Mean response times as a function of the group and trial type manipulations. Error bars represent 95% confidence intervals.\n\n\nFigure 1 shows mean response times as a function of the group and trial type manipulations. These data were subjected to a 2 (group: healthy vs. schizophrenia) \\(\\times\\) 2 (trial type: congruent vs. incongruent) mixed Analysis of Variance. There was a significant main effect of group, F(1, 78) = 84.35, p \\(&lt;\\) .001, with longer response times in the schizophrenia group than the healthy group, a significant main effect of trial type, F(1, 78) = 641.11, p &lt; .001, with longer response times for incongruent than congruent trials, and a significant interaction between the two factors, F(1, 78) = 52.22, p &lt; .001.\nTo scrutinise the interaction, a simple main effects analysis was undertaken. For the simple main effects of group at trial type, response times on congruent trials did not differ significantly between the healthy and schizophrenia groups, F(1, 78) = .05, p = .831, whereas response times were significantly longer on incongruent trials for the schizophrenia group compared to the healthy group, F(1, 78) = 163.19, p \\(&lt;\\) .001. For the simple main effects of trial type at group, response times were significantly longer on incongruent trials than congruent trials for the healthy group, F(1, 78) = 163.69, p \\(&lt;\\) .001, and for the schizophrenia group, F(1, 78) = 529.65, p \\(&lt;\\) .001, although the effect was larger for the schizophrenia group.\nHence, the interaction arose because the schizophrenia group demonstrated a larger Stroop effect than the healthy group and this was due to longer response times on incongruent, but not congruent, trials."
  },
  {
    "objectID": "PSYC214/Week8.html#analysing-hypothetical-data-for-the-fully-within-participants-stroop-experiment",
    "href": "PSYC214/Week8.html#analysing-hypothetical-data-for-the-fully-within-participants-stroop-experiment",
    "title": "8. Two-Factor Mixed and Within-Participants ANOVA",
    "section": "Analysing hypothetical data for the fully within-participants Stroop experiment",
    "text": "Analysing hypothetical data for the fully within-participants Stroop experiment\nWe turn now to an analysis of the fully within-participant Stroop experiment described in the lecture. In this experiment, a researcher wants to examine whether the magnitude of the Stroop effect decreases with practice at the task. The Stroop task is administered over three successive blocks of trials and the expectation is that the magnitude of the Stroop effect will decrease gradually across blocks. She administers a multi-trial Stroop task to a single group of participants in a 2 (trial type: congruent vs. incongruent) \\(\\times\\) 3 (block: block 1 vs. block 2 vs. block 3) fully within-participants design.\nThe data set contains seven columns:\n\nParticipant: represents the participant number, which ranges from 1–40, with \\(N\\) = 40 participants.\nCongruent_Block1: represents the mean response time, averaged across trials, for congruent trials in the first block of trials.\nCongruent_Block2: represents the mean response time, averaged across trials, for congruent trials in the second block of trials.\nCongruent_Block3: represents the mean response time, averaged across trials, for congruent trials in the third block of trials.\nIncongruent_Block1: represents the mean response time, averaged across trials, for incongruent trials in the first block of trials.\nIncongruent_Block2: represents the mean response time, averaged across trials, for incongruent trials in the second block of trials.\nIncongruent_Block3: represents the mean response time, averaged across trials, for incongruent trials in the third block of trials.\n\n\nImport data, set variables as factors, and generate descriptive statistics\nThere’s a bit to get through in this analysis, so I am going to supply the code for everything that follows. We begin, as always, by loading our data set:\n\n# Import data\nstroopWithin = read_csv(\"data/StroopWithin.csv\")\n(stroopWithin)\n\n# A tibble: 40 × 7\n   Participant Congruent_Block1 Congruent_Block2 Congr…¹ Incon…² Incon…³ Incon…⁴\n         &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1           1              627              687     563     863     742     653\n 2           2              558              681     674     870     687     647\n 3           3              635              709     575     794     728     722\n 4           4              626              673     663     872     644     686\n 5           5              610              610     643     813     729     579\n 6           6              568              655     715     856     703     745\n 7           7              635              641     573     901     695     643\n 8           8              530              677     545     818     737     672\n 9           9              613              621     638     799     699     630\n10          10              542              607     636     829     705     702\n# … with 30 more rows, and abbreviated variable names ¹​Congruent_Block3,\n#   ²​Incongruent_Block1, ³​Incongruent_Block2, ⁴​Incongruent_Block3\n\n\nAs is the case for a fully within-participants design, the data are in entirely wide format and we need to get them into long format for the analysis. We are going to start by grouping the columns Congruent_Block1 through to Incongruent_Block3 into a new variable called Group using the gather function used previously:\n\n# Convert data into long format\nstroopWithinLong = stroopWithin %&gt;%\n  gather(Group,RT,Congruent_Block1:Incongruent_Block3,factor_key = TRUE)\n(stroopWithinLong)\n\n# A tibble: 240 × 3\n   Participant Group               RT\n         &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1           1 Congruent_Block1   627\n 2           2 Congruent_Block1   558\n 3           3 Congruent_Block1   635\n 4           4 Congruent_Block1   626\n 5           5 Congruent_Block1   610\n 6           6 Congruent_Block1   568\n 7           7 Congruent_Block1   635\n 8           8 Congruent_Block1   530\n 9           9 Congruent_Block1   613\n10          10 Congruent_Block1   542\n# … with 230 more rows\n\n\nThe first argument to gather() tells R we want to create a variable called Group. The second argument tells R that RT is the dependent variable. The third argument, Congruent_Block1:Incongruent_Block3, tells R that we want to bundle the columns Congruent_Block1 through to Incongruent_Block3 into the new variable, Group. The fourth argument, factor_key = TRUE, tells R that we want to make this variable a factor. The results of this transformation have been assigned to a new data frame called stroopWithinLong.\nLooking at the new data frame we have created, we can see that it is not exactly what we want. Our new variable Group actually contains both of our independent variables. What we want is to separate these independent variables into two separate columns. We can do that with the separate function:\n\n# Divide Group into seperate columns for Trial Type and Block\nstroopWithinLongSep = stroopWithinLong %&gt;%\n  separate(Group, c(\"TrialType\",\"Block\"))\n(stroopWithinLongSep)\n\n# A tibble: 240 × 4\n   Participant TrialType Block     RT\n         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n 1           1 Congruent Block1   627\n 2           2 Congruent Block1   558\n 3           3 Congruent Block1   635\n 4           4 Congruent Block1   626\n 5           5 Congruent Block1   610\n 6           6 Congruent Block1   568\n 7           7 Congruent Block1   635\n 8           8 Congruent Block1   530\n 9           9 Congruent Block1   613\n10          10 Congruent Block1   542\n# … with 230 more rows\n\n\nThis bit of code tells R to separate the variable Group into two new variables, one called TrialType and one called Block. The results are stored in a new data frame called stroopWithinLongSep, which is the data frame we will be using henceforth. Looking at this data frame we can see that our transformation has had the desired effect—we now have two new columns corresponding to each of our two independent variables.\nHowever, our two new variables are currently stored as characters (they have the labels &lt;chr&gt; beneath the variable names) and we need to convert them to factors. We also need to convert the Participant variable into a factor. So, let’s do that next:\n\n# Convert variables into factors\nstroopWithinLongSep$Participant = factor(stroopWithinLongSep$Participant)\nstroopWithinLongSep$TrialType   = factor(stroopWithinLongSep$TrialType)\nstroopWithinLongSep$Block       = factor(stroopWithinLongSep$Block)\n\nRight, now is as good a time as any to get some descriptive statistics:\n\n# Get descriptive statistics\nwithinDescriptives = stroopWithinLongSep %&gt;%\n  # Organise the output by the\"TrialType\" and \"Block\" factors\n  group_by(TrialType, Block) %&gt;%\n  # Request means, standard deviations, and confidence intervals\n  get_summary_stats(RT, show = c(\"mean\",\"sd\",\"ci\"))\n  # Print the results\n  (withinDescriptives)\n\n# A tibble: 6 × 7\n  TrialType   Block  variable     n  mean    sd    ci\n  &lt;fct&gt;       &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Congruent   Block1 RT          40  615.  48.9  15.6\n2 Congruent   Block2 RT          40  631.  50.8  16.3\n3 Congruent   Block3 RT          40  632.  53.8  17.2\n4 Incongruent Block1 RT          40  832.  43.7  14.0\n5 Incongruent Block2 RT          40  723.  41.2  13.2\n6 Incongruent Block3 RT          40  650.  49.1  15.7\n\n\nTime is limited, so we are once again going to skip over the usual checks, which you should otherwise always perform.\nHowever, because one of our within-participants factors contains three levels (i.e., Block) there is one check that will be performed when we run the ANOVA. Specifically, this is a check to establish whether the sphericity assumption has been violated. Richard introduced you to this assumption when he discussed single-factor within-participants ANOVA, so I will only describe it briefly. The sphericity assumption states that for a within-participants design with three or more levels, the variance of the difference scores between one pair of levels should not differ significantly from the variances of the difference scores for every other possible pair of levels.\nThe assumption is tested using Mauchly’s test of sphericity, which is applied to any outcome involving a factor with three or more levels. If the test result is significant, then the assumption has been violated. If this occurs, then we must adopt the Greenhouse-Geisser correction for violations of this assumption.\n\n\nRunning the mixed ANOVA\nTo run our ANOVA, we are once again going to use the anova_test function from the rstatix package. The code required to run the ANOVA is given below:\n\n# Create the within-participants design ANOVA model\nstroopWithinModel = anova_test(data = stroopWithinLongSep, dv = RT, wid = Participant, within = c(TrialType, Block), detailed = TRUE)\n(stroopWithinModel)\n\nANOVA Table (type III tests)\n\n$ANOVA\n           Effect DFn DFd         SSn      SSd         F        p p&lt;.05   ges\n1     (Intercept)   1  39 111105237.6  77509.9 55903.885 3.54e-63     * 0.995\n2       TrialType   1  39    713623.2  84476.3   329.457 1.28e-20     * 0.568\n3           Block   2  78    272694.5 221792.5    47.951 2.63e-14     * 0.335\n4 TrialType:Block   2  78    403873.6 158005.4    99.687 3.25e-22     * 0.427\n\n$`Mauchly's Test for Sphericity`\n           Effect     W     p p&lt;.05\n1           Block 0.949 0.368      \n2 TrialType:Block 0.952 0.394      \n\n$`Sphericity Corrections`\n           Effect   GGe      DF[GG]    p[GG] p[GG]&lt;.05   HFe   DF[HF]    p[HF]\n1           Block 0.951   1.9, 74.2 1.01e-13         * 0.999  2, 77.9 2.72e-14\n2 TrialType:Block 0.954 1.91, 74.44 2.61e-21         * 1.002 2, 78.17 3.25e-22\n  p[HF]&lt;.05\n1         *\n2         *\n\n\nTo create the model, the first argument we supplied to anova_test was the name of our data, stroopWithinLongSep. The second argument we supplied was our dependent variable, RT. The third argument we supplied was Participant, which is the column containing the individuals/participants identifier. The fourth argument we supplied was our two within-participants factors, TrialType and Block.\nThe ANOVA output now contains three tables. The first is the main ANOVA table, which is organised in the same format as that produced for our mixed design example from earlier.\nThe second table presents the results of Mauchly’s test of sphericity, which has been applied to the factor Block, which has three levels, and the interaction between Trial Type and Block. In both instances, we can see that the test result is non-significant, which means the sphericity assumption has been met.\nThe third table gives the Greenhouse-Geisser corrected ANOVA table for the main effect of Block and the interaction between Trial Type and Block. Had we violated the sphericity assumption, it is these values that we would report when writing up our ANOVA, instead of the values taken from the original ANOVA table (the main effect of Trial Type would be drawn from the original ANOVA table as it only has two levels, and therefore the sphericity assumption does not apply to this outcome). Because we did not violate the sphericity assumption on this occasion, we can focus solely on the original ANOVA table.\nYou may at this point be wondering why Mauchly’s test of sphericity and the Greenhouse-Geisser correction were not produced when we ran the ANOVA for our mixed design example. The answer is simple; for that design our within-participants factor only contained two levels, so the sphericity assumption did not apply. The anova_test function only generates these tests and corrections when at least one of the within-participants factors has three or more levels.\nLet’s redirect our attention to the original ANOVA table. We can see that there is a significant main effect Trial Type, p \\(&lt;\\) .001. Looking at our descriptive statistics, we can see that this arose because response times are longer for incongruent than congruent trials. There is also a significant main effect of Block, p \\(&lt;\\) .001, which arose because response times get quicker across blocks as participants obtain more experience with the task. There is also a significant interaction between the two factors, p \\(&lt;\\) .001. We will defer interpretation of this interaction until later, once we have calculated the simple main effects and plotted the data.\nAs for our mixed design example, the ANOVA table does not include the mean square values for the outcomes. Previously, we calculated the between-group and error mean squares for ourselves, and verified that these produced the obtained \\(F\\) ratios when we divided the former by the latter. This time, we will just generate the error mean squares. Again, this is so I can show you which error term is being used to test each ANOVA outcome and the simple main effects of each factor.\n\n# Calculate the error mean sum of squares\nwithinMeanSquareError = stroopWithinModel$ANOVA$SSd/stroopWithinModel$ANOVA$DFd\n# Exclude the intercept (row 1 of 4) from the results\nwithinMeanSquareError[2:4]\n\n[1] 2166.059 2843.493 2025.710\n\n\nThe first value is the error mean square for the main effect of Trial Type (2166.059), the second value is the error mean square for the main effect of Block (2843.493), and the third value is the error mean square for the interaction (2025.710). Recall that in the lecture, we saw that in a fully within-participants design the two main effects and the interaction each have their own error terms, and sure enough we can see that the error term is different for each outcome.\n\n\nSimple main effects\nSince our interaction is once again significant, we need to proceed to calculate the simple main effects. Before we can calculate the simple main effects, there are a few things we need to do. First, we need to store our ANOVA table in a dataframe:\n\n# Get ANOVA table\nwithinAnovaTable = get_anova_table(stroopWithinModel)\n\nNext, we need to calculate the cell totals for each of the six conditions and the number of observations (i.e., scores) in each cell:\n\n# Calculate cell totals and counts\nwithinCellTotals = stroopWithinLongSep %&gt;%\n  # Organise the output by the TrialType\" and \"Block\" factors\n  group_by(TrialType, Block) %&gt;%\n  # Request cell totals and number of observations (i.e., scores)\n  summarise(sum = sum(RT),n = n())\n  # Print the results\n  (mixedCellTotals)\n\n# A tibble: 4 × 4\n# Groups:   TrialType [2]\n  TrialType   Group           sum     n\n  &lt;fct&gt;       &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Congruent   Healthy       24919    40\n2 Congruent   Schizophrenia 24999    40\n3 Incongruent Healthy       30799    40\n4 Incongruent Schizophrenia 35576    40\n\n\nThen, we need to specify which simple main effects we want to generate. We are first going to calculate the simple main effects of the factor Trial Type at Block. This means, we are going to:\n\nTest the difference between congruent and incongruent trials for Block 1 only.\nTest the difference between congruent and incongruent trials for Block 2 only.\nTest the difference between congruent and incongruent trials for Block 3 only.\n\nNotice that because Trial Type has only two levels, the simple main effects of this factor involve only pairwise comparisons. To generate these simple main effects, we need to declare Trial Type as the “fixed” factor (we are always comparing the congruent and incongruent trials) and Block as the “across” factor (the comparison between the congruent and incongruent trials occurs “across” the Block 1, Block 2, and Block 3 levels of the Block factor):\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"TrialType\"\nacross = \"Block\"\n\nTo get the simple main effects of Trial Type, we pass the cell totals, ANOVA table, and the fixed and across factors to our function simple():\n\n# Simple main effects of Trial Type at Block\nwithinSmeTrialType = simple(withinCellTotals,withinAnovaTable,fixed,across)\n(withinSmeTrialType)\n\n      Levels Sum of Squares Degrees of Freedom Mean Square          F\n1     Block1     941997.012                  1  941997.012 434.889848\n2     Block2     168820.312                  1  168820.312  77.938931\n3     Block3       6679.512                  1    6679.512   3.083717\n4 Error term      84476.296                 39    2166.059   0.000000\n             P\n1 9.362214e-23\n2 7.711308e-11\n3 8.693084e-02\n4 0.000000e+00\n\n\nAs described in the lecture, to calculate the simple main effects of a given factor in a within-participants design, we use the mean square error for the main effect of that factor from the original ANOVA table as the error term. The mean square error in the simple main effects table is given in column four (Mean Square) of row three (Error term). You will notice that this is the error mean square for the main effect of Trial Type that we calculated earlier from our initial ANOVA table (i.e., 2166.059). If you divide the between-group mean squares for the effect of Trial Type at Block 1, Block 2, and Block 3 by this value, it will give you the \\(F\\) ratios given in the simple main effects table.\nLooking at the simple main effects, we can see that the simple main effect of Trial type at Block 1 is significant, \\(p\\) \\(&lt;\\) .001. Looking at our table of descriptive statistics, we can see that this is because mean response times for incongruent trials were longer than for congruent trials. The simple main effect of Trial Type is also significant at Block 2, \\(p\\) \\(&lt;\\) .001, again reflecting longer mean response times for incongruent than congruent trials. However, the simple main effect of Trial Type at Block 3 is not significant, \\(p\\) = .087. Thus, we obtained a statistically reliable Stroop effect for Blocks 1 and 2, but not for Block 3.\nNext, we are going to calculate the simple main effects of the factor Block at Trial Type. This means, we are going to:\n\nTest the differences between Block 1, Block 2, and Block 3 for congruent trials only.\nTest the differences between Block 1, Block 2, and Block 3 for incongruent trials only.\n\nNotice that because Block has three levels, the simple main effects of this factor involve more than simple pairwise comparisons. To test these simple main effects, we now need to declare Block as the “fixed” factor and Trial Type as the “across” factor:\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"Block\"\nacross = \"TrialType\"\n\nWe then generate the simple main effects of Block with the following:\n\n# Simple main effects of Block at Trial Type\nwithinSmeBlock = simple(withinCellTotals,withinAnovaTable,fixed,across)\n(withinSmeBlock)\n\n       Levels Sum of Squares Degrees of Freedom Mean Square          F\n1   Congruent       7616.717                  2    3808.358   1.339324\n2 Incongruent     668951.450                  2  334475.725 117.628461\n3  Error term     221792.467                 78    2843.493   0.000000\n             P\n1 2.679808e-01\n2 2.828545e-24\n3 0.000000e+00\n\n\nRemember, to calculate the simple main effects of a given factor in a within-participants design, we use the mean square error for the main effect of that factor from the original ANOVA table as the error term. The mean square error in the simple main effects table is given in column four (Mean Square) of row three (Error term). You will notice that this is the error mean square for the main effect of Block that we calculated earlier from our initial ANOVA table (i.e., 2843.493). If you divide the between-group mean squares for the effect of Block at congruent and incongruent trials by this value, it will give you the \\(F\\) ratios given in the simple main effects table.\nLooking at the simple main effects, we can see that the simple main effect of Block at congruent trials is not significant, \\(p\\) = .268. Hence, mean response times do not differ reliably across blocks for congruent trials. However, the simple main effect of Block at incongruent trials is significant, \\(p\\) \\(&lt;\\) .001. Since there are three levels in the factor Block, this significant simple main effect is like the outcome of a significant single-factor ANOVA with three levels—it tells us that there are differences between the group means, but it does not tell us where they are located. To find out, we need to perform some follow up tests. We have a few options available to us.\nSuppose we hypothesised the interaction from the beginning and that we are only interested in the difference between Block 1 and Block 2, and Block 2 and Block 3 for incongruent trials, but we are not interested in the third comparison, which is between Block 1 and Block 3. In this case, we would have specified planned comparisons, because we do not intend to conduct all possible comparisons, only the ones relevant to testing our specific hypotheses. In this case, we could simply run two repeated measures \\(t\\)-tests, one comparing Block 1 and Block 2 at incongruent trials, and one comparing Block 2 and Block 3 at incongruent trials. Should we apply the Bonferroni correction to these comparisons? One rule of thumb is that if the number of comparisons is one less than the number of levels, i.e., (\\(a\\)-1) comparisons where \\(a\\) is the number of levels in the factor, then we do not need to apply the Bonferroni correction. In our case, our simple main effect has three levels and we are conducting two comparisons, which is one less than the number of levels, so we can proceed without using the Bonferroni correction (for those occasions where there are four or more levels, this rule of thumb does not apply and you should definitely use the Bonferroni correction). Alternatively, suppose we did not specify at the outset which comparisons we would perform if the interaction was significant. In this case, we would perform all three comparisons and we would use a post-hoc test, such as the Tukey test.\nFour our purposes, we are going to perform planned comparisons. To perform these comparisons, we first need to filter our data so that they only contain responses on incongruent trials (we are excluding responses on congruent trials). The following code gives us what we want:\n\n# Filter the data so they only contain responses on \"Incongruent\" trials\nfilter4SME = filter(stroopWithinLongSep, TrialType == \"Incongruent\")\n(filter4SME)\n\n# A tibble: 120 × 4\n   Participant TrialType   Block     RT\n   &lt;fct&gt;       &lt;fct&gt;       &lt;fct&gt;  &lt;dbl&gt;\n 1 1           Incongruent Block1   863\n 2 2           Incongruent Block1   870\n 3 3           Incongruent Block1   794\n 4 4           Incongruent Block1   872\n 5 5           Incongruent Block1   813\n 6 6           Incongruent Block1   856\n 7 7           Incongruent Block1   901\n 8 8           Incongruent Block1   818\n 9 9           Incongruent Block1   799\n10 10          Incongruent Block1   829\n# … with 110 more rows\n\n\nThe new data frame filter4SME is a version of our data in which only congruent trials are included. We can then use the pariwise_t_test function that you have used many times to run our \\(t\\)-tests (making sure to specify paired = TRUE as we want repeated measures comparisons!):\n\nfilter4SME %&gt;% \n  # Generate the t-tests for the two comparisons of interest\n  pairwise_t_test(RT ~ Block, paired = TRUE,\n    # Specify the comparisons we want\n    comparisons = list(c(\"Block1\",\"Block2\"),c(\"Block2\",\"Block3\")))\n\n# A tibble: 2 × 10\n  .y.   group1 group2    n1    n2 statistic    df        p    p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 RT    Block1 Block2    40    40      9.90    39 3.38e-12 6.76e-12 ****        \n2 RT    Block2 Block3    40    40      7.03    39 1.96e- 8 1.96e- 8 ****        \n\n\nThe comparison between Block 1 and Block 2 is significant, p \\(&lt;\\) .001, as is the comparison between Block 2 and Block 3, p \\(&lt;\\) .001. Looking at our descriptive statistics, we can see that this is because mean response times are faster for incongruent trials in Block 2 than in Block 1, and mean response times for incongruent trials are faster in turn in Block 3 than in Block 2. Thus, with practice on the Stroop task, participants responses on incongruent trials get gradually quicker.\n\n\nWriting up the results\n\n\n\nFigure 2. Mean response times as a function of the trial type and block manipulations. Error bars represent 95% confidence intervals.\n\n\nFigure 2 shows mean response times as a function of the trial type and block manipulations. These data were subjected to a 2 (trial type: congruent vs. incongruent) \\(\\times\\) 3 (block: block 1 vs. block 2 vs block 3) fully within-participants Analysis of Variance. There was a significant main effect of trial type, F(1, 39) = 329.46, p &lt; .001, with response times being longer for incongruent than congruent trials, a significant main effect of block, F(2, 78) = 47.95, p &lt; .001, with response times getting quicker across blocks, and a significant interaction between the two factors, F(2, 78) = 99.69, p &lt; .001.\nTo scrutinise the interaction, a simple main effects analysis was undertaken. For the simple main effects of trial type at block, response times were significantly longer on incongruent trials than congruent trials in block 1, F(1, 39) = 434.89, p &lt; .001, and block 2, F(1, 39) = 77.94, p &lt; .001, but not in block 3, F(1, 39) = 3.08, p = .087. Turning to the simple main effects of block at trial type, response times did not differ significantly across blocks for congruent trials, F(2, 78) = 1.34, p = .268, but they did differ significantly across blocks for incongruent trials, F(2, 78) = 117.63, p &lt; .001. Planned comparisons revealed that response times on incongruent trials were faster in block 2 than in block 1, t(39) = 9.90, p &lt; .001, and faster in turn in block 3 than in block 2, t(39) = 7.03, p &lt; .001.\nHence, the interaction arose because the magnitude of the Stroop effect decreased across blocks and this was due to the speeding up of responses on incongruent, but not congruent, trials.\n\n\nAdditional tasks\nPhew!!! We have covered a lot of ground in today’s session – well done for making it through the exercises. If you should happen to want more, then you could recreate the plots in Figures 1 and 2 by co-opting the code we used to generate line plots in lab session 6."
  },
  {
    "objectID": "PSYC214/Week9.html#learning-objectives",
    "href": "PSYC214/Week9.html#learning-objectives",
    "title": "9. Three-Factor ANOVA",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this week’s lecture, we introduced the procedures involved in interpreting a three-factor ANOVA. Specifically, what to do in the event that the three-way interaction is significant. We saw that the simplest strategy in this instance is to re-analyse the data as a series of two-factor ANOVAs. In today’s lab session, we will demonstrate how to perform a three-factor fully within-participants and mixed ANOVA in R (using the two hypothetical data sets presented in the lecture), and how to analyze a three-way interaction using the procedures described in the lecture. In this lab session, I am also going to show you a better way of rounding the values in dataframes than the options(digits = ) command used in earlier lab sessions.\nIf you get stuck at any point, be proactive and ask for help from one of the GTAs."
  },
  {
    "objectID": "PSYC214/Week9.html#getting-started",
    "href": "PSYC214/Week9.html#getting-started",
    "title": "9. Three-Factor ANOVA",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started, we first need to log into the R Studio Server.\nYou can access Lancaster Universities RStudio Server at http://psy-rstudio.lancaster.ac.uk. At present, you will need to be on campus, or connected to the VPN to access this. If you do not yet have Eduroam (the university wifi) available on your personal device, please follow the instructions from the PSYC214 Announcement Page https://modules.lancaster.ac.uk/mod/forum/discuss.php?d=388256\n\nIf you are on Eduroam (or VPN if off campus) and have accessed the RStudio Server from the URL above, you will now see a login screen (see above). Please use your normal Lancaster University username (e.g., bloggsj). Your own individual RStudio Server password was sent in an email, prior to the first lab, by Kay Rawlins: email header ‘R Studio Server login details’. Please make sure you have this.\nOnce you are logged into the server, create a folder for today’s session. Navigate to the bottom right panel and under the Files option select the New Folder option. Name the new folder psyc214_lab_9. Please ensure that you spell this correctly otherwise when you set the directory using the command given below it will return an error.\nSe we can save this session on the server, click File on the top ribbon and select New project. Next, select existing directory and name the working directory ~/psyc214_lab_9 before selecting create project.\nFinally, open a script for executing today’s coding exercises. Navigate to the top left pane of RStudio, select File -&gt; New File -&gt; R Script. Working from a script will make it easier to edit your code and you will be able to save your work for a later date.\n\nLet’s set our working directory:\n\nsetwd(\"~/psyc214_lab_9\")\n\nNow that you have created a folder for today’s session, it’s time to add the Week 9 files. Head on over to the PSYC214 Moodle page, access the lab folder for Week 9, and download the files memoryContext.csv, wordPronunciation.csv, and simple.R to your desktop. Next, in the RStudio Server open your new psyc214_lab_9 folder. When in the new folder, select the Upload tab. This will present a box that will ask where the file is that you want to upload. Click on Browse, find the memoryContext.csv file on your desktop and click OK, then repeat these steps for the wordPronunciation.csv and simple.R files.\nBefore moving on, let’s load the relevant libraries that we will be using in today’s session.\n\nlibrary(\"tidyverse\")  # For data storage and manipulation\nlibrary(\"tidyr\")      # For tidy data\nlibrary(\"rstatix\")    # For descriptives statistics, outlier detection etc.\nsource(\"simple.R\")    # Custom function for generating the simple main effects"
  },
  {
    "objectID": "PSYC214/Week9.html#analysing-the-hypothetical-data-for-the-memory-and-context-study",
    "href": "PSYC214/Week9.html#analysing-the-hypothetical-data-for-the-memory-and-context-study",
    "title": "9. Three-Factor ANOVA",
    "section": "Analysing the hypothetical data for the memory and context study",
    "text": "Analysing the hypothetical data for the memory and context study\nA memory researcher wants to know if memory is better when material is tested in the same context it was learned in. The researcher also wants to know whether recall and recognition memory are equally context dependent. The researcher manipulates three factors in a 2 \\(\\times\\) 2 \\(\\times\\) 2 fully within-participants design:\n\nmemory task (recall vs. recognition)\nlearning context (learn underwater vs. learn land)\ntesting context (test underwater vs. test land)\n\nParticipants are given words to remember in a specific learning context (either under water or on land) and are then tested in either the same context (e.g., under water if the words were learned under water) or a different context (e.g., on land if the words were learned under water). Memory is tested using a recall procedure (by asking participants to recall the studied words) or a recognition procedure (by presenting participants with a list of words and asking them to indicate which they had studied previously). The dependent measure is the number of words remembered correctly.\nThe data set contains the following variables:\n\nParticipant: represents the participant number, which ranges from 1–5.\nRecall_Under_Under: the number of words recalled correctly when material was learned under water and tested under water.\nRecall_Under_Land: the number of words recalled correctly when material was learned under water and tested on land.\nRecall_Land_Under: the number of words recalled correctly when material was learned on land and tested under water.\nRecall_Land_Land: the number of words recalled correctly when material was learned on land and tested on land.\nRecognition_Under_Under: the number of words recognised correctly when material was learned under water and tested under water.\nRecognition_Under_Land: the number of words recognised correctly when material was learned under water and tested on land.\nRecognition_Land_Under: the number of words recognised correctly when material was learned on land and tested under water.\nRecognition_Land_Land: the number of words recognised correctly when material was learned on land and tested on land.\n\n\nImport data, set variables as factors, and generate descriptive statistics\nThe first thing you need to do is load the data into RStudio. Make sure that you name your data frame as memoryContext.\n\n# *** ENTER YOUR OWN CODE HERE TO IMPORT THE DATA ***\n\n\n\n# A tibble: 5 × 9\n  Participant Recall_U…¹ Recal…² Recal…³ Recal…⁴ Recog…⁵ Recog…⁶ Recog…⁷ Recog…⁸\n        &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1           1          8       5       3       7       5       5       7       6\n2           2          9       6       3       8       7       6       5       8\n3           3          7       5       4       6       6       7       5       6\n4           4          8       4       4       5       7       5       6       5\n5           5          6       3       3       8       5       4       6       4\n# … with abbreviated variable names ¹​Recall_Under_Under, ²​Recall_Under_Land,\n#   ³​Recall_Land_Under, ⁴​Recall_Land_Land, ⁵​Recognition_Under_Under,\n#   ⁶​Recognition_Under_Land, ⁷​Recognition_Land_Under, ⁸​Recognition_Land_Land\n\n\nThe next thing we need to do is convert our data from wide format into long format. The first thing we need to do is group the columns Recall_Under_Under through to Recognition_Land_Land into a new variable called Group using the gather() function:\n\n# Gather factors into a single column\nmemoryContextLong = memoryContext %&gt;%\n  gather(Group,Accuracy,Recall_Under_Under:Recognition_Land_Land,factor_key = TRUE)\n(memoryContextLong)\n\n# A tibble: 40 × 3\n   Participant Group              Accuracy\n         &lt;dbl&gt; &lt;fct&gt;                 &lt;dbl&gt;\n 1           1 Recall_Under_Under        8\n 2           2 Recall_Under_Under        9\n 3           3 Recall_Under_Under        7\n 4           4 Recall_Under_Under        8\n 5           5 Recall_Under_Under        6\n 6           1 Recall_Under_Land         5\n 7           2 Recall_Under_Land         6\n 8           3 Recall_Under_Land         5\n 9           4 Recall_Under_Land         4\n10           5 Recall_Under_Land         3\n# … with 30 more rows\n\n\nThis function was explained in the previous lab session, so if it is not clear what is going on here, check the Week 8 lab session materials.\nLooking at the new data frame we have created, we can see that it is not exactly what we want. Our new variable Group actually contains three independent variables. What we want is to separate these independent variables into three separate columns: MemoryTask, LearningContext, and TestingContext. We can do that with the separate() function:\n\n# Now separate the variable \"Group\" into separate columns for each factor\nmemoryContextLongSep = memoryContextLong %&gt;%\n  separate(Group, c(\"MemoryTask\",\"LearningContext\",\"TestingContext\"))\n(memoryContextLongSep)\n\n# A tibble: 40 × 5\n   Participant MemoryTask LearningContext TestingContext Accuracy\n         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;             &lt;dbl&gt;\n 1           1 Recall     Under           Under                 8\n 2           2 Recall     Under           Under                 9\n 3           3 Recall     Under           Under                 7\n 4           4 Recall     Under           Under                 8\n 5           5 Recall     Under           Under                 6\n 6           1 Recall     Under           Land                  5\n 7           2 Recall     Under           Land                  6\n 8           3 Recall     Under           Land                  5\n 9           4 Recall     Under           Land                  4\n10           5 Recall     Under           Land                  3\n# … with 30 more rows\n\n\nAgain, this function was explained in the previous lab session, so if it is not clear what is going on here, check the Week 8 lab session materials. The latest version of the data set is named memoryContextLongSep, so make sure you use this from henceforth.\nThe next thing we need to do is convert the variables Participant, MemoryTask, LearningContext, and TestingContext into factors and re-order the levels of the latter two variables:\n\n# Make sure all necessary variables are coded as factors -- re-order the levels of \"LearningContext\" and \"TestingContext\"\nmemoryContextLongSep$Participant = factor(memoryContextLongSep$Participant)\nmemoryContextLongSep$MemoryTask = factor(memoryContextLongSep$MemoryTask)\nmemoryContextLongSep$LearningContext = factor(memoryContextLongSep$LearningContext,levels = c(\"Under\",\"Land\"))\nmemoryContextLongSep$TestingContext = factor(memoryContextLongSep$TestingContext,levels = c(\"Under\",\"Land\"))\n\nNext, we will generate some descriptive statistics (mean and standard deviation):\n\n# Get descriptive statistics\ndescriptives = memoryContextLongSep %&gt;%\n  # Organise the output by the \"MemoryTask\", \"LearningContext\", and \"TestingContext\" factors\n  group_by(MemoryTask, LearningContext, TestingContext) %&gt;%\n  # Request means, standard deviations, and confidence intervals\n  get_summary_stats(Accuracy, show = c(\"mean\", \"sd\"))\n  # Round the statistics to two decimal places\n  descriptives$mean = round(descriptives$mean, 2)\n  descriptives$sd = round(descriptives$sd, 2)\n  # Print the results\n  print.data.frame(descriptives)\n\n   MemoryTask LearningContext TestingContext variable n mean   sd\n1      Recall           Under          Under Accuracy 5  7.6 1.14\n2      Recall           Under           Land Accuracy 5  4.6 1.14\n3      Recall            Land          Under Accuracy 5  3.4 0.55\n4      Recall            Land           Land Accuracy 5  6.8 1.30\n5 Recognition           Under          Under Accuracy 5  6.0 1.00\n6 Recognition           Under           Land Accuracy 5  5.4 1.14\n7 Recognition            Land          Under Accuracy 5  5.8 0.84\n8 Recognition            Land           Land Accuracy 5  5.8 1.48\n\n\nNotice the code we have added to round the descriptive statistics to two-decimal places. We use the function round(), specifying how many decimal places we want to round the values (in this case 2). Inside this function, we place the variable we want to be rounded. Because our data are in a dataframe, we need to specify the name of our dataframe followed by a dollar sign and the name of the variable in the dataframe to be rounded (e.g., descriptives$mean tells R to round the values of the variable mean in the dataframe descriptives). We also need to re-assign the rounded values to the dataframe, so that the variable gets updated (e.g., that’s what the descriptives$mean = bit does).\nNotice also that our standard deviations have been reported to two decimal places, but our means haven’t. Why so? This is because the numbers after the first decimal place in this instance are all zeros, so R doesn’t report them. Bearing in mind that APA style requires we report descriptive statistics to two-decimal places, we would just add a single zero to each of the means at the second decimal place. For example, the first mean in the table, 7.6, would be reported as 7.60.\nAt this stage, we would ordinarily perform various checks including identifying possible outliers and checking that our data satisfy the normality assumption. However, as per last week, time is limited, so we won’t perform those checks today (just remember that ordinarily you should not skip this part!). One assumption that is important in within-participants designs is the sphericity assumption, but remember that this only applies to designs with within-participants factors with three or more levels. All of our factors have two levels, so this assumption is not relevant in this instance (it’s also not relevant for our second data set that we analyse later, for which the within-participants factors also only comprise two levels).\n\n\nRunning the ANOVA, follow-up ANOVAs, and simple main effects\nTo run our ANOVA, we are going to use the anova_test function from the rstatix package. This is the same function that we used in the Week 8 lab session to analyse two-factor fully within-participants and mixed designs. The code required to run the ANOVA is given below:\n\n# Create the fully within-participants design ANOVA model\nmemoryContextModel = anova_test(data = memoryContextLongSep, dv = Accuracy, wid = Participant, within = c(MemoryTask, LearningContext, TestingContext), detailed = TRUE)\n# Round the p values to three decimal places\nmemoryContextModel$p = round(memoryContextModel$p, 3)\n# Print the model summary\n(memoryContextModel)\n\nANOVA Table (type III tests)\n\n                                     Effect DFn DFd      SSn  SSd       F     p\n1                               (Intercept)   1   4 1288.225 10.9 472.743 0.000\n2                                MemoryTask   1   4    0.225  0.9   1.000 0.374\n3                           LearningContext   1   4    2.025  5.1   1.588 0.276\n4                            TestingContext   1   4    0.025  7.1   0.014 0.911\n5                MemoryTask:LearningContext   1   4    3.025  4.1   2.951 0.161\n6                 MemoryTask:TestingContext   1   4    0.625  3.5   0.714 0.446\n7            LearningContext:TestingContext   1   4   30.625  4.5  27.222 0.006\n8 MemoryTask:LearningContext:TestingContext   1   4   21.025  3.1  27.129 0.006\n  p&lt;.05      ges\n1     * 0.970000\n2       0.006000\n3       0.049000\n4       0.000637\n5       0.072000\n6       0.016000\n7     * 0.439000\n8     * 0.349000\n\n\nTo create the model, the first argument we supplied to anova_test was the name of our data, memoryContextLongSep. The second argument we supplied was our dependent variable, Accuracy. The third argument we supplied was Participant, which is the column containing the individuals/participants identifier. The fourth argument we supplied was our within-participants factors, MemoryTask, LearningContext, and TestingContext.\nAs we saw in last week’s lab session, the resulting ANOVA table is different in format to those given in the lecture, which follow a more conventional style. In the ANOVA tables given in the lecture, each outcome (each main effect and interaction) is given on a separate row, with the error term used to test it given in the row directly beneath it. However, anova_test gives each outcome and its associated error term all in the same row. Specifically, the row corresponding to each outcome contains the between-group degrees of freedom (DFn), the error degrees of freedom (DFd), the between-group sums of squares (SSn), the error sums of squares (SSd), the \\(F\\) ratio (F), the p (p) value, and the generalised eta squared (ges) value (a measure of effect size). What anova_test does not give us is the between-group mean squares and the error mean squares that are used to calculate the \\(F\\) ratios. However, I showed you how to calculate these in last week’s lab session if you should ever have need for these (you probably won’t).\nYou might be wondering why anova_test has not given us Mauchly’s test of sphericity and the Greenhouse-Geisser correction. This is because all of our factors have only two levels, so the sphericity assumption does not apply. Remember, the anova_test function only generates these tests and corrections when at least one of the within-participants factors has three or more levels.\nInspecting the ANOVA table, rows two to four give the main effects of Memory Task, Learning Context, and Testing Context; rows five to seven give the Memory Task \\(\\times\\) Learning Context, Memory Task \\(\\times\\) Testing Context, and Learning Context \\(\\times\\) Testing Context two-way interactions; and row eight gives the Memory Task \\(\\times\\) Learning Context \\(\\times\\) Testing Context three-way interaction. Looking at the p values, we can see that there is a significant Learning Context \\(\\times\\) Testing Context two-way interaction, \\(p\\) = .006, and a significant Memory Task \\(\\times\\) Learning Context \\(\\times\\) Testing Context three-way interaction, \\(p\\) = .006.\nBecause the three-way interaction is significant, we need to analyse it further. As explained in the lecture, a significant three-way interaction occurs when there are different two-way interactions between two of the factors according to the levels of the third factor. The simplest way to analyse a signiﬁcant three-way interaction is to re-analyse it as a series of two-factor ANOVAs. To do this, we first need to decide on a factor that we are going to split the analyses by. We can pick any factor we want, but there is usually one factor that stands out as being an obvious choice and in our case it is the memory task factor. So, what we need to do is to perform two, two-factor ANOVAs:\n\na 2 (learning context: learn under water vs. learn land) \\(\\times\\) 2 (testing context: test under water vs. test land) ANOVA for the recall memory test condition only.\na 2 (learning context: learn under water vs. learn land) \\(\\times\\) 2 (testing context: test under water vs. test land) ANOVA for the recognition memory test condition only.\n\nWe will start by running the two-factor ANOVA for the recall memory test condition (ignoring the recognition memory test condition). To do this, we first need to produce a filtered version of our data set called recallOnly that only includes the results for the recall memory test condition. We can create that with the following piece of code:\n\n# Get the data for the \"Recall\" condition only\nrecallOnly = memoryContextLongSep %&gt;%\n  filter(MemoryTask == \"Recall\") \n\nThe command filter(MemoryTask == \"Recall\") tells R that we only want the data for the recall condition of the Memory Task factor.\nNext, we can run our two-factor ANOVA on this filtered data set. The steps are the same as above, except that we need to drop the Memory Task factor included previously (remember, we are only analysing the recall condition of the Memory Task factor).\n\n# Run the two-factor ANOVA for the \"Recall\" condition only\nrecallModel = anova_test(data = recallOnly, dv = Accuracy, wid = Participant, within = c(LearningContext, TestingContext), detailed = TRUE)\n# Round the p values to three decimal places\nrecallModel$p = round(recallModel$p, 3)\n# Print the model summary\n(recallModel)\n\nANOVA Table (type III tests)\n\n                          Effect DFn DFd   SSn SSd       F     p p&lt;.05   ges\n1                    (Intercept)   1   4 627.2 5.3 473.358 0.000     * 0.971\n2                LearningContext   1   4   5.0 5.5   3.636 0.129       0.214\n3                 TestingContext   1   4   0.2 4.3   0.186 0.688       0.011\n4 LearningContext:TestingContext   1   4  51.2 3.3  62.061 0.001     * 0.736\n\n\nThe only thing we are interested in from the ANOVA table is the outcome of the two-way interaction between Learning Context and Testing Context; you can ignore everything else. You can see that the interaction is significant, p = .001, so the next step is to perform a simple main effects analysis to identify the nature of the interaction.\nThe procedure for performing the simple main effects analysis is the same as I demonstrated to you in our Week 8 lab session. That is, we use the pooled error terms approach, which means that the simple main effects of each factor are calculated using the same error term that was used to test the main effect of that factor in the ANOVA that preceded the simple main effects analysis (in this case, our two-factor ANOVA on the recall data only).\nBefore we can calculate the simple main effects, there are a few things we need to do. First, we need to store our ANOVA table in a dataframe:\n\n# Get the recall ANOVA table\nrecallAnovaTable = get_anova_table(recallModel)\n\nNext, we need to calculate the cell totals for each of the four conditions and the number of observations (i.e., scores) in each cell:\n\n# Get cell totals and counts\nrecallCellTotals = recallOnly %&gt;%\n  # Organise the output by the \"LearningContext\" and \"TestingContext\" factors\n  group_by(LearningContext, TestingContext) %&gt;%\n  # Request cell totals and number of observations (i.e., scores)\n  summarise(sum = sum(Accuracy),n = n())\n  # Print the results\n  (recallCellTotals)\n\n# A tibble: 4 × 4\n# Groups:   LearningContext [2]\n  LearningContext TestingContext   sum     n\n  &lt;fct&gt;           &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;\n1 Under           Under             38     5\n2 Under           Land              23     5\n3 Land            Under             17     5\n4 Land            Land              34     5\n\n\nThen, we need to specify which simple main effects we want to generate. We are first going to calculate the simple main effects of the factor Learning Context at Testing Context. This means, we are going to:\n\nTest the difference between learning under water and learning on land when tested under water only.\nTest the difference between learning under water and learning on land when tested on land only.\n\nTo do this, we need to declare Learning Context as the “fixed” factor (we are always comparing learning under water and learning on land) and Testing Context as the “across” factor (the comparison between learning under water and learning on land occurs “across” the test under water and test on land levels of the Testing Context factor):\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"LearningContext\"\nacross = \"TestingContext\"\n\nWe then generate the simple main effects of Learning Context by passing these variables into simple():\n\n# Simple main effects of \"Learning Context\" at \"TestingContext\"\nsmeLearningContext = simple(recallCellTotals,recallAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeLearningContext$P = round(smeLearningContext$P, 3)\n(smeLearningContext)\n\n      Levels Sum of Squares Degrees of Freedom Mean Square        F     P\n1      Under           44.1                  1      44.100 32.07273 0.005\n2       Land           12.1                  1      12.100  8.80000 0.041\n3 Error term            5.5                  4       1.375  0.00000 0.000\n\n\nWe can see that there is a significant simple main effect of Learning Context at test under water, \\(p\\) = .005; when tested under water, recall memory scores are higher when the material was learned under water than when it was learned on land. There is also a significant simple main effect of Learning Context at test on land, \\(p\\) = .041; when tested on land, recall memory scores are higher when the material was learned on land than when it was learned under water. You will need to consult the descriptive statistics to verify this is correct.\nNext, we are going to calculate the simple main effects of the factor Testing Context at Learning Context. This means, we are going to:\n\nTest the difference between testing under water and testing on land when material was learned under water only.\nTest the difference between testing under water and testing on land when material was learned on land only.\n\nTo do this, we now need to declare Testing Context as the “fixed” factor and Learning Context as the “across” factor:\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"TestingContext\"\nacross = \"LearningContext\"\n\nWe then generate the simple main effects of Testing Context with the following:\n\n# Simple main effects of \"Testing Context\" at \"LearningContext\"\nsmeTestingContext = simple(recallCellTotals,recallAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeTestingContext$P = round(smeTestingContext$P, 3)\n(smeTestingContext)\n\n      Levels Sum of Squares Degrees of Freedom Mean Square        F     P\n1      Under           22.5                  1      22.500 20.93023 0.010\n2       Land           28.9                  1      28.900 26.88372 0.007\n3 Error term            4.3                  4       1.075  0.00000 0.000\n\n\nWe can see that there is a significant simple main effect of Testing Context at learn under water, \\(p\\) = .010; when the material is learned under water, recall memory scores are higher when tested under water than when tested on land. There is also a significant simple main effect of Testing Context at learn land, \\(p\\) = .007; when the material is learned on land, recall memory scores are higher when tested on land than when tested under water.\nIn sum, from the simple main effects analysis what we can see is that recall memory is context sensitive; that is, recall memory performance is better when people are tested in the same context that they learned the information than when they are tested in a different context to that which they learned the information.\nWhat about recognition memory?\nThat brings us to our second two-factor ANOVA. For this, we now need to produce a filtered version of our data set called recognitionOnly that only includes the results for the recognition memory task condition. We can create that with the following piece of code:\n\n# Get the data for the \"Recognition\" condition only\nrecognitionOnly = memoryContextLongSep %&gt;%\n  filter(MemoryTask == \"Recognition\") \n\nThe command filter(MemoryTask == \"Recognition\") tells R that we only want the data for the recognition condition of the Memory Task factor.\nNext, we can run our two-factor ANOVA on this filtered data set.\n\n# Run the two-factor ANOVA for the \"Recognition\" condition only\nrecognitionModel = anova_test(data = recognitionOnly, dv = Accuracy, wid = Participant, within = c(LearningContext, TestingContext), detailed = TRUE)\n# Round the p values to three decimal places\nrecognitionModel$p = round(recognitionModel$p, 3)\n# Print the model summary\n(recognitionModel)\n\nANOVA Table (type III tests)\n\n                          Effect DFn DFd    SSn SSd       F     p p&lt;.05   ges\n1                    (Intercept)   1   4 661.25 6.5 406.923 0.000     * 0.970\n2                LearningContext   1   4   0.05 3.7   0.054 0.828       0.002\n3                 TestingContext   1   4   0.45 6.3   0.286 0.621       0.021\n4 LearningContext:TestingContext   1   4   0.45 4.3   0.419 0.553       0.021\n\n\nThe key result is that this time the critical two-way interaction is nonsignificant, p = 0.553. What this indicates is that, unlike recall memory, recognition memory is not context sensitive. This is the reason for the three-way interaction; recall memory is sensitive to the learning and testing context, whereas recognition memory is apparently insensitive to the learning and testing context.\n\n\nWriting up the results\n\n\n\nFigure 1. Memory scores as a function of learning context and testing context for the recall memory task (left panel) and the recognition memory task (right panel).\n\n\nFigure 1 shows memory scores as a function of learning context and testing context for the recall and recognition memory tasks. These data were subjected to a 2 (memory task: recall vs. recognition) \\(\\times\\) 2 (learning context: learn under water vs. learn land) \\(\\times\\) 2 (testing context: test under water vs. test land) within-participants ANOVA. There was no significant main effect of memory task, F(1, 4) = 1.00, p = .374, no significant main effect of learning context, F(1, 4) = 1.59, p = .276, and no significant main effect of testing context, F(1, 4) = 0.01, p = .911. Neither the memory task \\(\\times\\) learning context interaction, F(1, 4) = 2.95, p = .161, nor the memory task \\(\\times\\) testing context interaction, F(1, 4) = 0.71, p = .446, were significant. However, the learning context \\(\\times\\) testing context interaction was significant, F(1, 4) = 27.22, p = .006. Critically, there was also a significant three-way interaction, F(1,4) = 27.13, p = .006.\nTo explore the three-way interaction, two 2 (learning context) \\(\\times\\) 2 (testing context) ANOVAs were conducted; one using the data for the recall memory task only, and the second using the data for the recognition memory task only. For the first ANOVA on the recall memory task data, there was a significant interaction, F(1, 4) = 62.06, p = .001. A simple main effects analysis revealed that when tested under water, recall memory was better when the material was learned under water than when it was learned on land, F(1, 4) = 44.10, p = .005, and when tested on land, recall memory was better when the material was learned on land than when it was learned under water, F(1, 4) = 12.10, p = .041. Mirroring these results, when the material was learned under water, recall memory was better when tested under water than when tested on land, F(1, 4) = 22.50, p = .010, and when the material was learned on land, recall memory was better when tested on land than when tested under water, F(1, 4) = 28.90, p = .007.\nFor the second ANOVA on the recognition memory task data, there was no significant interaction, F(1, 4) = 0.42, p = .553.\nIn brief, the three-way interaction reflects the fact that recall memory is sensitive to the learning and testing context, whereas recognition memory is apparently not sensitive to such contextual factors."
  },
  {
    "objectID": "PSYC214/Week9.html#analysing-the-hypothetical-data-for-the-word-pronunciation-study",
    "href": "PSYC214/Week9.html#analysing-the-hypothetical-data-for-the-word-pronunciation-study",
    "title": "9. Three-Factor ANOVA",
    "section": "Analysing the hypothetical data for the word pronunciation study",
    "text": "Analysing the hypothetical data for the word pronunciation study\nA researcher wants to investigate the development in children’s ability to pronounce regular and irregular words. The researcher adopts a 2 \\(\\times\\) 2 \\(\\times\\) 2 mixed design:\n\nage (7 years old vs. 9 years old) is a between-participants factor\nword frequency (low vs. high) is a within-participants factor\nword type (regular vs. irregular) is also a within-participants factor\n\nParticipants are given 10 words to pronounce in each category (40 words in total) and the dependent measure of interest is the number of pronunciation errors.\nThe data set contains the following variables:\n\nParticipant: represents the participant number, which ranges from 1–10.\nAge: whether the participant is 7-years-old or 9-years-old.\nHigh_Regular: pronunciation errors for high frequency regular words.\nHigh_Irregular: pronunciation errors for high frequency irregular words.\nLow_Regular: pronunciation errors for low frequency regular words.\nLow_Irregular: pronunciation errors for low frequency irregular words.\n\n\nImport data, set variables as factors, and generate descriptive statistics\nThe first thing you need to do is load the data into RStudio. Make sure that you name your data frame as wordPron.\n\n# *** ENTER YOUR OWN CODE HERE TO IMPORT THE DATA ***\n\n\n\n# A tibble: 10 × 6\n   Participant Age         High_Regular High_Irregular Low_Regular Low_Irregular\n         &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n 1           1 7 Year Olds            6              7           5             6\n 2           2 7 Year Olds            7              5           6             7\n 3           3 7 Year Olds            5              6           7             6\n 4           4 7 Year Olds            6              7           5             7\n 5           5 7 Year Olds            6              6           5             7\n 6           6 9 Year Olds            4              4           3             6\n 7           7 9 Year Olds            3              4           4             7\n 8           8 9 Year Olds            4              3           5             9\n 9           9 9 Year Olds            5              5           3             8\n10          10 9 Year Olds            3              4           3             7\n\n\nThe next thing we need to do is convert our data from wide format into long format. The first thing we need to do is group the columns High_Regular through to Low_Irregular into a new variable called Group using the gather() function. We showed you how to do this in the earlier data set, so give this a go for yourself. Make sure that you name the dependent measure Errors and that you call your new data set wordPronLng.\n\n# *** ENTER YOUR OWN CODE HERE TO GATHER THE WITHIN-PARTICIPANTS FACTORS INTO A COMMON GROUP ***\n\nIf you have executed your code correct, you should see the following output:\n\n\n# A tibble: 40 × 4\n   Participant Age         Group        Errors\n         &lt;dbl&gt; &lt;chr&gt;       &lt;fct&gt;         &lt;dbl&gt;\n 1           1 7 Year Olds High_Regular      6\n 2           2 7 Year Olds High_Regular      7\n 3           3 7 Year Olds High_Regular      5\n 4           4 7 Year Olds High_Regular      6\n 5           5 7 Year Olds High_Regular      6\n 6           6 9 Year Olds High_Regular      4\n 7           7 9 Year Olds High_Regular      3\n 8           8 9 Year Olds High_Regular      4\n 9           9 9 Year Olds High_Regular      5\n10          10 9 Year Olds High_Regular      3\n# … with 30 more rows\n\n\nWe have a new variable Group that contains our two independent variables, Frequency and Word Type, and a new variable Errors that contains our dependent measure. The next step is to use the separate() function to divide the variable Group into two new variables, one called Frequency and one called WordType. Again, we gave you an example of this earlier, so try your own code out for this bit. Just make sure you call your new data set wordPronLngSep.\n\n# *** ENTER YOUR OWN CODE HERE TO SEPARATE \"GROUP\" INTO SEPARATE VARIABLES FOR \"FREQUENCY\" AND \"WORDTYPE\" ***\n\nAssuming you have executed your code correctly, you should see the following output:\n\n\n# A tibble: 40 × 5\n   Participant Age         Frequency WordType Errors\n         &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1           1 7 Year Olds High      Regular       6\n 2           2 7 Year Olds High      Regular       7\n 3           3 7 Year Olds High      Regular       5\n 4           4 7 Year Olds High      Regular       6\n 5           5 7 Year Olds High      Regular       6\n 6           6 9 Year Olds High      Regular       4\n 7           7 9 Year Olds High      Regular       3\n 8           8 9 Year Olds High      Regular       4\n 9           9 9 Year Olds High      Regular       5\n10          10 9 Year Olds High      Regular       3\n# … with 30 more rows\n\n\nOur variable Group has now disappeared and in its place we have two new variables: Frequency and WordType. This is the version of the data set we will be using for the rest of the analysis.\nThe next thing we need to do is convert the columns Participant, Age, Frequency, and WordType into factors and re-order the levels of the latter two variables:\n\n# Make sure all necessary variables are coded as factors -- re-order the levels of \"Frequency\" and \"WordType\"\nwordPronLngSep$Participant = factor(wordPronLngSep$Participant)\nwordPronLngSep$Age = factor(wordPronLngSep$Age)\nwordPronLngSep$Frequency = factor(wordPronLngSep$Frequency,levels = c(\"Low\",\"High\"))\nwordPronLngSep$WordType = factor(wordPronLngSep$WordType,levels = c(\"Regular\",\"Irregular\"))\n\nNext, we will generate some descriptive statistics (mean and standard deviation). You can generate the code for this yourself. Make sure that you round the statistics to two-decimal places using the procedure I showed you in the earlier example.\n\n# *** ENTER YOUR OWN CODE HERE TO GENERATE DESCRIPTIVE STATISTICS ***\n\nIf you have executed the code correctly, then you should see the following output:\n\n\n          Age Frequency  WordType variable n mean   sd\n1 7 Year Olds       Low   Regular   Errors 5  5.6 0.89\n2 7 Year Olds       Low Irregular   Errors 5  6.6 0.55\n3 7 Year Olds      High   Regular   Errors 5  6.0 0.71\n4 7 Year Olds      High Irregular   Errors 5  6.2 0.84\n5 9 Year Olds       Low   Regular   Errors 5  3.6 0.89\n6 9 Year Olds       Low Irregular   Errors 5  7.4 1.14\n7 9 Year Olds      High   Regular   Errors 5  3.8 0.84\n8 9 Year Olds      High Irregular   Errors 5  4.0 0.71\n\n\n\n\nRunning the ANOVA, follow-up ANOVAs, and simple main effects\nThe code required to run the ANOVA is given below:\n\n# Create the mixed design ANOVA model\nwordPronModel = anova_test(data = wordPronLngSep, dv = Errors, wid = Participant, between = Age, within = c(Frequency, WordType), detailed = TRUE)\n# Round the p values to three decimal places\nwordPronModel$p = round(wordPronModel$p, 3)\n# Print the model summary\n(wordPronModel)\n\nANOVA Table (type II tests)\n\n                  Effect DFn DFd    SSn SSd        F     p p&lt;.05   ges\n1            (Intercept)   1   8 1166.4 4.5 2073.600 0.000     * 0.981\n2                    Age   1   8   19.6 4.5   34.844 0.000     * 0.467\n3              Frequency   1   8    6.4 8.7    5.885 0.041     * 0.222\n4               WordType   1   8   16.9 3.7   36.541 0.000     * 0.430\n5          Age:Frequency   1   8    6.4 8.7    5.885 0.041     * 0.222\n6           Age:WordType   1   8    4.9 3.7   10.595 0.012     * 0.179\n7     Frequency:WordType   1   8   12.1 5.5   17.600 0.003     * 0.351\n8 Age:Frequency:WordType   1   8    4.9 5.5    7.127 0.028     * 0.179\n\n\nTo create the model, the first argument we supplied to anova_test was the name of our data, wordPronLngSep. The second argument we supplied was our dependent variable, Errors. The third argument we supplied was Participant, which is the column containing the individuals/participants identifier. The fourth argument we supplied was our between-participants factor, Age. The fifth argument we supplied as our within-participants factors, Frequency and WordType.\nAs in our first example, our factors have only two levels, so anova_test does not give us Mauchly’s test of sphericity or the Greenhouse-Geisser correction for the within-participants factors.\nInspecting the ANOVA table, rows two to four give the main effects of Age, Frequency, and Word Type; rows five to seven give the Age \\(\\times\\) Frequency, Age \\(\\times\\) Word Type, and Frequency \\(\\times\\) Word Type two-way interactions; and row eight gives the Age \\(\\times\\) Frequency \\(\\times\\) Word Type three-way interaction. Looking at the \\(p\\) values, we can see that all the main effects, two-way interactions, and the three-way interaction are significant.\nBecause the three-way interaction is significant, we need to analyse it further. As before, to do this we need to re-analyse the data as a series of two-factor ANOVAs. First, we must decide which factor to split the analysis by and the obvious contender is the between-participants factor of age. Accordingly, what we need to do is to perform two, two-factor ANOVAs:\n\na 2 (frequency: low vs. high) \\(\\times\\) 2 (word type: regular vs. irregular) ANOVA for the 7-year-olds only.\na 2 (frequency: low vs. high) \\(\\times\\) 2 (word type: regular vs. irregular) ANOVA for the 9-year-olds only.\n\nWe will start by running the two-factor ANOVA for the 7-year-olds (ignoring the data for the 9-year-olds). To do this, we first need to produce a filtered version of our data set called sevenYearsOnly that only includes the results for the 7-year-old children. We can create that with the following piece of code:\n\n# Get the data for the \"7 Year Olds\" only\nsevenYearsOnly = wordPronLngSep %&gt;%\n  filter(Age == \"7 Year Olds\") \n\nNext, we can run our two-factor ANOVA on this filtered data set. The steps are the same as above, except that we need to drop the Age factor included previously (remember, we are only analysing the data for the 7-year-olds).\n\n# Run the two-factor ANOVA for the \"7 Year Olds\" only\nsevenYearsModel = anova_test(data = sevenYearsOnly, dv = Errors, wid = Participant, within = c(Frequency, WordType), detailed = TRUE)\n# Round the p values to three decimal places\nsevenYearsModel$p = round(sevenYearsModel$p, 3)\n# Print the model summary\n(sevenYearsModel)\n\nANOVA Table (type III tests)\n\n              Effect DFn DFd   SSn SSd        F     p p&lt;.05   ges\n1        (Intercept)   1   4 744.2 0.3 9922.667 0.000     * 0.988\n2          Frequency   1   4   0.0 2.5    0.000 1.000       0.000\n3           WordType   1   4   1.8 2.7    2.667 0.178       0.164\n4 Frequency:WordType   1   4   0.8 3.7    0.865 0.405       0.080\n\n\nThe only thing we are interested in from the ANOVA table is the outcome of the two-way interaction between Frequency and Word Type; you can ignore everything else. You can see that the interaction is nonsignificant in this instance, \\(p\\) = .405. Thus, for 7-year-old children Frequency and Word Type do not combine with one another to influence pronunciation errors.\nWhat about 9-year-old children?\nThat brings us to our second two-factor ANOVA. For this, we now need to produce a filtered version of our data set called nineYearsOnly that only includes the results for the 9-year-old children. We can create that with the following piece of code:\n\nnineYearsOnly = wordPronLngSep %&gt;%\n  filter(Age == \"9 Year Olds\") \n\nThe command filter(Age == \"9 Year Olds\") tells R that we only want the data for the 9-year-old children.\nNext, we can run our two-factor ANOVA on this filtered data set.\n\n# Run the two-factor ANOVA for the \"9 Year Olds\" only\nnineYearsModel = anova_test(data = nineYearsOnly, dv = Errors, wid = Participant, within = c(Frequency, WordType), detailed = TRUE)\n# Round the p values to three decimal places\nnineYearsModel$p = round(nineYearsModel$p, 3)\n# Print the model summary\n(nineYearsModel)\n\nANOVA Table (type III tests)\n\n              Effect DFn DFd   SSn SSd       F     p p&lt;.05   ges\n1        (Intercept)   1   4 441.8 4.2 420.762 0.000     * 0.971\n2          Frequency   1   4  12.8 6.2   8.258 0.045     * 0.492\n3           WordType   1   4  20.0 1.0  80.000 0.001     * 0.602\n4 Frequency:WordType   1   4  16.2 1.8  36.000 0.004     * 0.551\n\n\nThis time the interaction between Frequency and Word Type is significant, p = .004, so we now need to perform a simple main effects analysis to determine why.\nBefore we can calculate the simple main effects, there are a few things we need to do. First, we need to store our ANOVA table in a dataframe:\n\n# Get the 9 year olds ANOVA table\nnineYearsAnovaTable = get_anova_table(nineYearsModel)\n\nNext, we need to calculate the cell totals for each of the four conditions and the number of observations (i.e., scores) in each cell:\n\n# Get cell totals and counts\nnineYearsCellTotals = nineYearsOnly %&gt;%\n  # Organise the output by the \"Frequency\" and \"WordType\" factors\n  group_by(Frequency, WordType) %&gt;%\n  # Request cell totals and number of observations (i.e., scores)\n  summarise(sum = sum(Errors),n = n())\n  # Print the results\n  (recallCellTotals)\n\n# A tibble: 4 × 4\n# Groups:   LearningContext [2]\n  LearningContext TestingContext   sum     n\n  &lt;fct&gt;           &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;\n1 Under           Under             38     5\n2 Under           Land              23     5\n3 Land            Under             17     5\n4 Land            Land              34     5\n\n\nThen, we need to specify which simple main effects we want to generate. We are first going to calculate the simple main effects of the factor Frequency at Word Type. This means, we are going to:\n\nTest the difference between low and high frequency regular words only.\nTest the difference between low and high frequency irregular words only.\n\nTo do this, we need to declare Frequency as the “fixed” factor (we are always comparing low and high frequency words) and Word Type as the “across” factor (the comparison between low and high frequency words occurs “across” the regular and irregular levels of the Word Type factor):\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"Frequency\"\nacross = \"WordType\"\n\nWe then generate the simple main effects of Frequency at Word Type with the following:\n\n# Simple main effects of \"Frequency\" at \"WordType\"\nsmeFrequency = simple(nineYearsCellTotals,nineYearsAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeFrequency$P = round(smeFrequency$P, 3)\n(smeFrequency)\n\n      Levels Sum of Squares Degrees of Freedom Mean Square           F     P\n1    Regular            0.1                  1        0.10  0.06451613 0.812\n2  Irregular           28.9                  1       28.90 18.64516129 0.012\n3 Error term            6.2                  4        1.55  0.00000000 0.000\n\n\nThe simple main effect of Frequency at regular words is nonsignificant, \\(p\\) = .812, indicating that pronunciation errors for regular words do not differ according to whether they are low or high in frequency. However, the simple main effect of Frequency at irregular words is significant, \\(p\\) = .012, indicating that pronunciation errors for irregular words are higher when they are of low frequency than when they are of high frequency (check the descriptive statistics to verify this).\nNow, let’s calculate the simple main effects of Word Type at Frequency. This means, we are going to:\n\nTest the difference between regular and irregular low frequency words only.\nTest the difference between regular and irregular high frequency words only.\n\nTo do this, we need to declare Word Type as the “fixed” factor and Frequency as the “across” factor:\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"WordType\"\nacross = \"Frequency\"\n\nWe then generate the simple main effects of Word Type at Frequency as follows:\n\n# Simple main effects of \"WordType\" at \"Frequency\"\nsmeWordType = simple(nineYearsCellTotals,nineYearsAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeWordType$P = round(smeWordType$P, 3)\n(smeWordType)\n\n      Levels Sum of Squares Degrees of Freedom Mean Square     F     P\n1        Low           36.1                  1       36.10 144.4 0.000\n2       High            0.1                  1        0.10   0.4 0.561\n3 Error term            1.0                  4        0.25   0.0 0.000\n\n\nThe simple main effect of Word Type at low frequency is significant, \\(p\\) \\(&lt;\\) .001, indicating that there are more pronunciation errors for low frequency irregular words than for low frequency regular words. Second, the simple main effect of word type at high frequency is nonsignificant, \\(p\\) = .561, indicating that pronunciation errors for high frequency regular and irregular words do not differ.\nIn short, the three-way interaction arose because pronunciation errors in 7-year-old children are unaffected by word frequency and word type, whereas pronunciation errors in 9-year-old children are influenced by these factors. Specifically, low frequency irregular words are associated with more pronunciation errors than low frequency regular words, but there is no difference between the frequency of pronunciation errors for high frequency irregular and regular words. This pattern can be seen in Figure 2 below which plots the data for the word pronunciation study.\n\n\n\nFigure 2. Pronunciation errors as a function of word frequency and word type for 7-year-old children (left panel) and 9-year-old children (right panel).\n\n\n\n\nWriting up the results\nThe conventions for writing up the results of a mixed three-factor ANOVA are the same as for a fully within-participants three-factor ANOVA (and indeed a fully between-participants three-factor ANOVA), so see my example write-up for the memory and context study.\n\n\n\nAdditional tasks\nPhew!! That’s probably the most we have covered in any of our lab sessions. Well done for making it through to the end!\nHere are some additional tasks you might consider doing:\n\nWrite-up the results of the word pronunciation study.\nGenerate the interaction plots in Figures 1 and 2, but add error bars (confidence intervals) to the data points.\n\nI’ll include the write-up/plot code for these additional tasks in the instructors copy of the lab materials at the end of the week."
  },
  {
    "objectID": "PSYC214/index.html",
    "href": "PSYC214/index.html",
    "title": "Statistics for Group Comparisons",
    "section": "",
    "text": "Welcome\nThis is filler text introducing students to the course in general. Perhaps what’s expected of them, the course aims, and other information of use.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam in est non nisi eleifend vulputate a et magna. Mauris vulputate felis lacus, ut bibendum lectus sollicitudin vel. Integer vestibulum arcu et risus vestibulum, in ullamcorper magna consectetur. Donec ut tempus enim, at vulputate libero. Integer porta metus eget elit cursus, sit amet hendrerit ipsum facilisis. Maecenas mollis, elit gravida ornare tempus, est lectus mattis velit, et commodo nunc lectus eu enim. Aenean luctus, felis ac sodales accumsan, dolor nunc euismod lorem, vel vulputate ipsum orci quis ipsum. Quisque placerat, velit vitae dictum feugiat, lectus lorem rutrum ligula, at fringilla ex enim consectetur felis. Aliquam erat volutpat. Nunc a nisi eget ex ornare dictum.\n\n\nCourse Contacts\n\n\n\n\nEmail Address\n\n\n\n\nTom Beesley\nt.beesley at lancaster dot ac dot uk\n\n\nJohn Towse\nj.towse at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC214/Week7.html#learning-objectives",
    "href": "PSYC214/Week7.html#learning-objectives",
    "title": "7. Two-Factor Between-Participants ANOVA",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this week’s lecture, we demonstrated the steps involved in performing a two-factor between-participants ANOVA by hand using the simplest possible instance of such a design, namely a 2 \\(\\times\\) 2 factorial design. We saw that the analysis entails between 1–2 stages. In the first stage, we perform the ANOVA to establish whether either of the two main effects or interaction is significant. If the interaction is not significant, then the analysis stops here. However, if there is a significant interaction, then we must proceed to a second stage and perform a simple main effects analysis to establish the nature of the significant interaction. In today’s lab session, we will demonstrate how to perform a 2 \\(\\times\\) 2 between-participants factorial ANOVA in R, including the calculation of the simple main effects. We will also demonstrate how to write up the results of a factorial ANOVA."
  },
  {
    "objectID": "PSYC214/Week7.html#getting-started",
    "href": "PSYC214/Week7.html#getting-started",
    "title": "7. Two-Factor Between-Participants ANOVA",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started, we first need to log into the R Studio Server.\nYou can access Lancaster University’s RStudio Server at http://psy-rstudio.lancaster.ac.uk. At present, you will need to be on campus, or connected to the VPN to access this. If you do not yet have Eduroam (the university wifi) available on your personal device, please follow the instructions from the PSYC214 Announcement Page https://modules.lancaster.ac.uk/mod/forum/discuss.php?d=388256\n\nIf you are on Eduroam (or VPN if off campus) and have accessed the RStudio Server from the URL above, you will now see a login screen (see above). Please use your normal Lancaster University username (e.g., bloggsj). Your own individual RStudio Server password was sent in an email, prior to the first lab, by Kay Rawlins: email header ‘R Studio Server login details’. Please make sure you have this.\nOnce you are logged into the server, create a folder for today’s session. Navigate to the bottom right panel and under the Files option select the New Folder option. Name the new folder psyc214_lab_7. Please ensure that you spell this correctly otherwise when you set the directory using the command given below it will return an error.\nSe we can save this session on the server, click File on the top ribbon and select New project. Next, select existing directory and name the working directory ~/psyc214_lab_7 before selecting create project.\nFinally, open a script for executing today’s coding exercises. Navigate to the top left pane of RStudio, select File –&gt; New File –&gt; R Script. Working from a script will make it easier to edit your code and you will be able to save your work for a later date.\n\nLet’s set our working directory:\n\nsetwd(\"~/psyc214_lab_7\")\n\nNow that you have created a folder for today’s session, it’s time to add the Week 7 data file. Head on over to the PSYC214 Moodle page, access the lab folder for Week 7, and download the file COVID19Data.csv to your desktop. Next, in the RStudio Server open your new pscy214_lab_7 folder. When in the new folder, select the Upload tab. This will present a box that will ask where the data is that you want to upload. Click on Browse, find the COVID19Data.csv file on your desktop and click OK.\nBefore moving on, let’s load the relevant libraries that we will be using in today’s session.\n\nlibrary(\"tidyverse\")  # For data storage and manipulation\nlibrary(\"tidyr\")      # For tidy data\nlibrary(\"rstatix\")    # For descriptives statistics, outlier detection etc.\nlibrary(\"effectsize\") # For generating effect sizes for the ANOVA and simple main effects\nlibrary(\"ggpubr\")     # For generating QQ plots\nlibrary(\"phia\")       # For calculating simple main effects"
  },
  {
    "objectID": "PSYC214/Week7.html#analysing-the-hypothetical-covid-19-vaccination-study-data",
    "href": "PSYC214/Week7.html#analysing-the-hypothetical-covid-19-vaccination-study-data",
    "title": "7. Two-Factor Between-Participants ANOVA",
    "section": "Analysing the Hypothetical COVID-19 Vaccination Study Data",
    "text": "Analysing the Hypothetical COVID-19 Vaccination Study Data\nToday, we are going to analyse the hypothetical COVID-19 vaccination study data that we plotted in last week’s lab session. As a reminder, the table below summarises the four different conditions of our 2 \\(\\times\\) 2 between-participant study.\n\nThe data set consists of the following four columns:\n\nPar: represents the participant number, which ranges from 1–200, with \\(N\\) = 50 participants in each of the four conditions resulting from the combination of our two factors, described next.\nFear: represents whether the participant indicated by the row received (Fear Appeal) or did not receive (No Fear Appeal) the fear appeal.\nEfficacy: represents whether the participant indicated by the row received (Efficacy Message) or did not receive (No Efficacy Message) the self-efficacy message.\nLikelihood: represents the vaccination likelihood score of the participant indicated by the row.\n\nThe first thing you need to do is load the data into RStudio. Make sure that you name your data frame as covid19Data. Once you have done this, you need to re-level the factors Fear and Efficacy so that “No Fear Appeal” is the baseline level for the former factor, and “No Efficacy Message” is the baseline level for the latter factor. We showed you how to do this in the Week 6 lab session.\n\n# *** ENTER YOUR OWN CODE HERE IMPORTING THE DATA AND RE-LEVELLING THE FACTORS ***\n\nOnce you have done the above, type (covid19Data) into the console to view the first 10 rows of the data. We want to check that the columns for Fear and Efficacy have the labels &lt;fct&gt; beneath them. This means that R has stored these variables as factors, which is necessary for us to perform the ANOVA. By default, R will import these variables as characters, in which case the variables would have the labels &lt;chr&gt; beneath them. If that is the case for your data, then you will need to modify this before proceeding further.\n\n\n# A tibble: 200 × 4\n     Par Fear           Efficacy            Likelihood\n   &lt;dbl&gt; &lt;fct&gt;          &lt;fct&gt;                    &lt;dbl&gt;\n 1     1 No Fear Appeal No Efficacy Message          6\n 2     2 No Fear Appeal No Efficacy Message          4\n 3     3 No Fear Appeal No Efficacy Message          7\n 4     4 No Fear Appeal No Efficacy Message          8\n 5     5 No Fear Appeal No Efficacy Message          5\n 6     6 No Fear Appeal No Efficacy Message          6\n 7     7 No Fear Appeal No Efficacy Message          2\n 8     8 No Fear Appeal No Efficacy Message          6\n 9     9 No Fear Appeal No Efficacy Message          5\n10    10 No Fear Appeal No Efficacy Message          3\n# … with 190 more rows\n\n\n\nDescriptives statistics and assumption checks\nThe next step is to generate some descriptive statistics. We want the means, standard deviations, and confidence intervals for our four conditions. You have done this before, so you can generate these descriptive statistics for yourselves. When you invoke the group_by() function, remember to include the variables for both factors, (Fear, Efficacy) with a comma seperating them, as I have done here. Make sure you name your descriptive statistics descriptives.\n\n# *** ENTER YOUR OWN CODE FOR GENERATING THE DESCRIPTIVE STATISTICS ***\n\nOnce you have written and executed the code in RStudio, type descriptives in the console to view the descriptive statistics, it should yield the following output:\n\n\n            Fear            Efficacy   variable  n mean    sd    ci\n1 No Fear Appeal No Efficacy Message Likelihood 50 5.16 2.024 0.575\n2 No Fear Appeal    Efficacy Message Likelihood 50 4.66 1.869 0.531\n3    Fear Appeal No Efficacy Message Likelihood 50 4.80 1.829 0.520\n4    Fear Appeal    Efficacy Message Likelihood 50 7.12 1.769 0.503\n\n\nWe won’t linger on these descriptive statistics because we inspected them in last week’s lab session.\nThe next steps are for us to perform our usual checks to make sure that the data are fit for analysis using ANOVA.\nThe first check we are going to perform is to establish whether there are any outliers or extreme values in the data set. Again, you have done this on numerous occasions before, so it is over to you to generate your own code to identify if there are any outliers or extreme values. As per when you generated the descriptive statistic, remember we have two factors, so when you invoke group_by() you will need to include the arguments (Fear, Efficacy).\n\n# *** ENTER YOUR OWN CODE FOR CHECKING FOR OUTLIERS AND EXTREME VALUES ***\n\nIf you have entered your code correctly, then it should generate the following:\n\n\n# A tibble: 2 × 6\n  Fear           Efficacy              Par Likelihood is.outlier is.extreme\n  &lt;fct&gt;          &lt;fct&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 No Fear Appeal No Efficacy Message    28         10 TRUE       FALSE     \n2 Fear Appeal    No Efficacy Message   126          0 TRUE       FALSE     \n\n\nThere are only two outliers in the data, participant 28 who received the combination of No Fear Appeal/No Efficacy Message and participant 126 who received the combination of Fear Appeal/No Efficacy Message. Fortunately, there are no extreme values in the data, so we are safe to proceed without having to remove or transform any scores.\nThe next check that we need to undertake is to ensure that the homogeneity of variance assumption has been met. As we have seen in previous lectures and labs, this is an important assumption for single-factor between-participants ANOVA, and the same is true for between-participants factorial ANOVA. Applied to the latter, this assumption states that the variances of the different conditions created through the combination of factors should not differ systematically from one another. For our 2 \\(\\times\\) 2 between-participants factorial design, this means that the variances for the four conditions created by the combination of our two factors should be roughly comparable.\nWe can test whether the data satisfy the homogeneity of variance assumption using Levene’s test, as we have done in previous lab classes. As you are familiar with the code for running Levene’s test, you can write this out for yourself. However, when you invoke Levene’s test using levene_test() you will need to enter (Likelihood ~ Fear * Efficacy) as the arguments. The argument Likelihood is our dependent measure, and the argument Fear * Efficacy tells the function that we want to run the test on the four conditions created through the combination of our two factors (that’s what the * is for; note you can also substitute the * with : which has the same meaning and yields the same result).\n\n# *** ENTER YOUR OWN CODE FOR RUNNING LEVENE'S TEST ***\n\nIf you have entered your code correctly, then it should return the following output:\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     3   196     0.231 0.875\n\n\nThe test result is nonsignificant (p = .875), which indicates that the variances of the four conditions do not differ appreciably from one another. Accordingly, based on Levene’s criterion, we can be satisfied that the homogeneity of variance assumption has been satisfied.\nThe next check we need to perform is to ensure that our data satisfy the normality assumption. We can do this by running a Shapiro Wilk test on the data for each of our four conditions to determine if the distribution of scores in each differs significantly from a normal distribution. Remember the rule for using this test—if there is less than \\(N\\) = 50 observations per condition, the Shapiro Wilk test is likely to yield an accurate test result, whereas if it is equal to or greater than this number, we are better off inspecting the QQ plots. Our sample size per condition is right on the threshold, so we will use both approaches to testing the normality assumption.\nYou have executed this test many times before, so once more it’s over to you to generate the code. Just remember once again that when invoking the function group_by you need to supply the arguments (Fear,Efficacy) given that we have two factors.\n\n# *** ENTER YOUR OWN CODE FOR RUNNING THE SHAPIRO WILK TEST ***\n\nIf you have executed your code correct, then you should see the following output:\n\n\n# A tibble: 4 × 5\n  Fear           Efficacy            variable   statistic       p\n  &lt;fct&gt;          &lt;fct&gt;               &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 No Fear Appeal No Efficacy Message Likelihood     0.936 0.00971\n2 No Fear Appeal Efficacy Message    Likelihood     0.958 0.0752 \n3 Fear Appeal    No Efficacy Message Likelihood     0.956 0.0587 \n4 Fear Appeal    Efficacy Message    Likelihood     0.949 0.0313 \n\n\nTwo of the test results are nonsignificant, indicating that the distributions of scores in the No Fear Appeal/Efficacy Message condition and Fear Appeal/No Efficacy Message condition do not differ significantly from a normal distribution (p = .075 and \\(p\\) = .059, respectively). However, the other two test results are significant, indicating that the distributions of scores in the No Fear Appeal/No Efficacy Message condition and Fear Appeal/Efficacy Message condition do differ significantly from a normal distribution (p = .010 and \\(p\\) = .031, respectively).\nLet’s not panic just yet. Remember, we are right on the threshold of accuracy for the Shapiro Wilk test, so we should take a look at the QQ plots before taking action.\nI am going to assist with you with this next bit. We generate QQ plots (these have been described previously, most recently in the Week 5 lab session, so I won’t go into the details of what they plot and how to interpret them here again) using the ggqqplot() function. Previously, when you have used this function, you only had a single factor and you would have written something like this: ggqqplot(covid19Data, \"Likelihood\", facet.by = \"Fear\"), where Fear in this context is the one and only factor in the study. However, we have two factors in our study, so to get what we want we have to pass through our two factors to facet.by as a vector using c(\"Fear\", \"Efficacy\"). The complete code is as follows:\n\n# Specify covid19Data as the data; \"Likelihood\" as the dependent measure; and facet.by the \"Fear\" and \"Efficacy\" factors to create separate plots for the four different conditions.   \nggqqplot(covid19Data, \"Likelihood\", facet.by = c(\"Fear\", \"Efficacy\"))\n\n\n\n\nThe QQ plots look very reasonable. The only mildly concerning plot is in the top left-hand corner, which plots the distribution of scores for the No Fear Appeal/No Efficacy Message Condition. Several of the data points at the top end of the scale depart from the diagonal criterion line. I know from experience inspecting these plots that this is not a radical departure, so I’m happy to proceed without taking further action in this instance.\nAll in all, our data are looking in good shape, so we are ready to perform our ANOVA.\n\n\nRunning the ANOVA\nTo run our ANOVA, we are going to use the aov() function that Richard introduced you to (in your Week 2 lab session) that you used to run a single-factor between-participants ANOVA. We can also use this function to run a between-participants factorial ANOVA.\nLet’s suppose, for a moment, that we only had a single factor in our study, Fear. To run the ANOVA we could use: covidModel = aov(data = covid19Data, Likelihood ~ Fear). Here we are asking R to run an ANOVA in which the data is covid19Data (the first argument), the dependent measure is Likelihood (the second argument), and the factor is Fear (the third argument), and we are storing the results in a variable called covidModel — the value to the left of the = sign. The ~ sign after Likelihood tells R that we have declared our dependent measure, and what follows after this symbol are the factors for the analysis.\nHow do we extend this to a two-factor study? Intuitively, you might think we would write something like this: covidModel = aov(data = covid19Data, Likelihood ~ Fear + Efficacy). Now, after the ~, we have added both factors in our study Fear + Efficacy seperated by a + sign. This is part of what we need, but not all that we need. What this code would do is generate the two main effects for Fear and Efficacy, but it would not test the interaction. To test the interaction, we need to modify the code following ~ as follows: Fear + Efficacy + Fear * Efficacy. The new bit + Fear * Efficacy tells R that we want to test the interaction as well (that’s what the * between the two factors indicates; we can also use : in place of * which has the same meaning).\nThe complete piece of code we need to run our ANOVA is as follows:\n\n# Specify our ANOVA model (see the main text for details)\ncovidModel = aov(data = covid19Data, Likelihood ~ Fear + Efficacy + Fear * Efficacy)\n# This bit prompts R to produce our summary ANOVA table\nsummary(covidModel)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nFear            1   55.1   55.12   15.68 0.000105 ***\nEfficacy        1   41.4   41.40   11.78 0.000732 ***\nFear:Efficacy   1   99.4   99.40   28.27 2.86e-07 ***\nResiduals     196  689.2    3.52                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s take a look at the resulting ANOVA table. The first column corresponds to the source of the effect:\n\nFear corresponds to the main effect of Fear.\nEfficacy corresponds to the main effect of Efficacy.\nFear:Efficacy corresponds to the interaction between the two factors.\nResiduals corresponds to the error term used to test the above effects.\n\nThe second column gives us the degrees of freedom for each effect indicated by the row (DF), the third column gives us the Sum of Squares (Sum Sq), the fourth column gives us the Mean Square (Mean Sq), the fifth column gives us the F value (F value), and the final column gives us the p value associated with each effect (PR\\(&gt;\\)F). Let’s run some sanity checks, starting with the Mean Square. These values should be equal to the Sum of Squares divided by the degrees of freedom for the effect indicated by the row, which they are (check for yourself). The F values should be equal to the Mean Square divided by the error term (3.52) for the effect indicated by the row, which they are once again (check for yourself).\nWe can see from the final column that both the main effects and the interaction are significant (all p &lt; .001). To help quantify the size of these effects, let’s get some effect size estimates using the effectsize() function from the effectsize package that we have used previously:\n\neffectsize(covidModel)\n\n# Effect Size for ANOVA (Type I)\n\nParameter     | Eta2 (partial) |       95% CI\n---------------------------------------------\nFear          |           0.07 | [0.03, 1.00]\nEfficacy      |           0.06 | [0.02, 1.00]\nFear:Efficacy |           0.13 | [0.06, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe \\(\\eta^2\\) effect size estimates for the main effect of fear, efficacy, and the interaction are, .07, .06, and .13, respectively, corresponding to medium-, medium-, and large-sized effects (reminder: 0.01 = small effect; 0.06 = medium effect; 0.14 = large effect).\nBecause the two main effects only comprise two levels, these results are easy to interpret as they involve a straightfoward pairwise comparison. Looking at the descriptive statistics we requested earlier, we can see that vaccination likelihood scores are higher in the presence of a fear appeal than in the absence of a fear appeal — this is the significant main effect of fear. We can also see that vaccination likelihood scores are higher in the presence of a self-efficacy message than in the absence of a self-efficacy message — this is the significant main effect of efficacy.\nThe interaction is less straightforward to interpret. To identify the reason for the interaction between factors, we need to proceed to a second stage of analysis in which we test the simple main effects of each factor.\n\n\nGenerating the simple main effects\nRemember that the simple main effects break down the main effects into their component parts. That is, we examine the effect of each factor at each level of the other factor. In a 2 \\(\\times\\) 2 factorial design, each factor has two simple main effects.\nWe will start by looking at the two simple main effects of fear:\n\nWe want to know whether there is a significant effect of fear (no fear appeal \\(vs.\\) fear appeal) at the no efficacy message level of the efficacy factor.\nWe want to know whether there is a significant effect of fear (no fear appeal \\(vs.\\) fear appeal) at the efficacy message level of the efficacy factor.\n\nWe can test these simple main effects using the testInteractions() function in the phia package (phia stands for Post-Hoc Interaciton Analysis) using the following code:\n\n# Get the simple main effects of \"Fear\" at each level of the \"Efficacy\"  factor\nsmeFear = testInteractions(covidModel, fixed = \"Efficacy\", across = \"Fear\")\n# Print the results\n(smeFear)\n\nF Test: \nP-value adjustment method: holm\n                    Value  Df Sum of Sq       F    Pr(&gt;F)    \nNo Efficacy Message  0.36   1      3.24  0.9214    0.3383    \n   Efficacy Message -2.46   1    151.29 43.0238 9.398e-10 ***\nResiduals                 196    689.22                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s clarify what we have just requested in the above code. The first argument we supplied to the function was our ANOVA model, covidModel. Next we specified which simple main effects we wanted to test via two further arguments to the function. Specifically, fixed = \"Efficacy tells the function that we want to fix (or hold constant) the level of the factor efficacy, and across = \"Fear\" tells the function that for each of these levels, we want to compare the difference between the no fear appeal and fear appeal levels of the fear factor. Notice that we have made a specific assignment to a variable called smeFear — the value before the = sign. The reason for that will become clear shortly.\nLet’s look at the output that has been produced. It’s basically another ANOVA table with our two simple main effects (No Efficacy Message and Efficacy Message, respectively) and also the error term (Residuals) from the initial ANOVA (remember from the Week 7 lecture that we use the same error term from the initial ANOVA to test the simple main effects — notice that Sum of Squares for Residuals is 689.22, which is the same as in the ANOVA table for our initial analysis).\nThe first row, shows the simple main effect of fear at the no efficacy message level of the efficacy factor. This simple main effect is nonsignificant (p = .338). Thus, in the absence of a self-efficacy message, the likelihood of vaccinating is unaffected by whether or not a person is a given a fear appeal.\nThe secod row, shows the simple main effect of fear at the efficacy message level of the efficacy factor. This simple main effect is significant (p &lt; .001). Looking at the descriptive statistics we generated earlier, we can see that this is because in the presence of a self-efficacy message vaccination likelihood scores are higher in the presence than in the absence of a fear appeal.\nWe can also request effect sizes for these simple main effects using the effectsize() function. This is why we saved our simple main effects to the variable smeFear. This allows us to get the effect sizes using the following bit of code:\n\neffectsize(smeFear)\n\n# Effect Size for ANOVA (Type I)\n\nParameter           | Eta2 (partial) |       95% CI\n---------------------------------------------------\nNo Efficacy Message |       4.68e-03 | [0.00, 1.00]\nEfficacy Message    |           0.18 | [0.11, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe effect size for the first simple main effect, which was nonsignificant, is nonsurprisingly very small (\\(\\eta^2\\) = .00), whereas the effect size for the second simple main effect, which was significant, is large (\\(\\eta^2\\) = .18).\nNext, we want to look at the two simple main effects of efficacy:\n\nWe want to know whether there is a significant effect of efficacy (no efficacy message \\(vs.\\) efficacy message) at the no fear appeal level of the fear factor.\nWe want to know whether there is a significant effect of efficacy (no efficacy message \\(vs.\\) efficacy message) at the fear appeal level of the fear factor.\n\nWe can get these simple main effects using the following code:\n\n# Get the simple main effects of \"Efficacy\" at each level of the \"Fear\" factor\nsmeEfficacy = testInteractions(covidModel, fixed = \"Fear\", across = \"Efficacy\")\n(smeEfficacy)\n\nF Test: \nP-value adjustment method: holm\n               Value  Df Sum of Sq       F    Pr(&gt;F)    \nNo Fear Appeal  0.50   1      6.25  1.7774     0.184    \n   Fear Appeal -2.32   1    134.56 38.2661 7.058e-09 ***\nResiduals            196    689.22                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis time, to generate the simple main effects of interest we set the second argument to fixed = \"Fear which tells the function that we want to fix (or hold constant) the level of the factor fear, and we set the value of the third argument to across = \"Fear\" which tells the function that for each of those levels, we want to compare the difference between the no efficacy message and efficacy message levels of the efficacy factor. Notice that we have once again made a specific assignment to a variable called smeEfficacy — the value before the = sign, so we can get effect sizes for these simple main effects later.\nLet’s once again look at the output that has been produced.\nThe first row, shows the simple main effect of efficacy at the no fear appeal level of the fear factor. This simple main effect is nonsignificant (p = .184). Thus, in the absence of a fear appeal, the likelihood of vaccinating is unaffected by whether or not a person is given a self-efficacy message.\nThe secod row, shows the simple main effect of efficacy at the fear appeal level of the fear factor. This simple main effect is significant (p &lt; .001). Looking at the descriptive statistics we generated earlier, we can see that this is because in the presence of a fear appeal vaccination likelihood scores are higher in the presence than in the absence of a self-efficacy message.\nWe can get the effect sizes of these simple main effects as follows:\n\neffectsize(smeEfficacy)\n\n# Effect Size for ANOVA (Type I)\n\nParameter      | Eta2 (partial) |       95% CI\n----------------------------------------------\nNo Fear Appeal |       8.99e-03 | [0.00, 1.00]\nFear Appeal    |           0.16 | [0.09, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe effect size for the first simple main effect, which was nonsignificant, is nonsurprisingly very small (\\(\\eta^2\\) = .01), whereas the effect size for the second simple main effect, which was significant, is large (\\(\\eta^2\\) = .16).\n\n\nHow to write up the results\n\n\n\nFigure 1. Vaccination likelihood scores as a function of the fear and efficacy manipulations. Error bars represent 95% confidence intervals.\n\n\nFigure 1 shows vaccination likelihood scores as a function of the fear and efficacy manipulations. These data were subjected to a 2 (fear: no fear appeal \\(vs.\\) fear appeal) \\(\\times\\) 2 (efficacy: no efficacy message \\(vs.\\) efficacy message) between-participants Analysis of Variance. There was a significant main effect of fear, F(1, 196) = 15.68, p &lt; .001, \\(\\eta^2\\) = .07, with vaccination likelihood scores being higher in the presence than in the absence of a fear appeal, a significant main effect of efficacy, F(1, 196) = 11.78, p &lt; .001, \\(\\eta^2\\) = .06, with vaccination likelihood scores being higher in the presence than in the absence of a self-efficacy message, and a significant interaction between the two factors, F(1, 196) = 28.27, p &lt; .001, \\(\\eta^2\\) = .13.\nTo scrutinise the interaction, a simple main effects analysis was undertaken. In the absence of an efficacy message, there was no significant effect of fear, F(1, 196) = .92, p = .338, \\(\\eta^2\\) = .00, whereas in the presence of an efficacy message there was a significant effect of fear, F(1, 196) = 43.02, p &lt; .001, \\(\\eta^2\\) = .18, with vaccination likelihood scores being higher in the presence than in the absence of a fear appeal. Mirroring these results, in the absence of a fear appeal, there was no significant effect of efficacy, F(1, 196) = 1.78, p = .184, \\(\\eta^2\\) = .01, whereas in the presence of a fear appeal there was a significant effect of efficacy, F(1, 196) = 38.27, p &lt; .001, \\(\\eta^2\\) = .16, with vaccination likelihood scores being higher in the presence than in the absence of an efficacy message.\nHence, people receiving a fear appeal had a higher likelihood of vaccinating against COVID-19 than those that did not receive a fear appeal, but only when the fear appeal was combined with a self-efficacy enhancing message.\n\n\nAdditional tasks\nThe final task is for you to generate the line graph of the data shown in Figure 1 above. This was generated using the code we supplied in last week’s lab session. However, we are now using confidence intervals for the error bars, so you will need to modify this aspect of the code.\n\n# *** ENTER YOUR OWN CODE FOR GENERTING THE LINE GRAPH IN FIGURE 1 ***\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\nFinal remarks\nIn this lab session, we have analysed the simplest instance of a two-factor between-participants design—namely a 2 \\(\\times\\) 2 design. But what happens if one or both of the factors have more than two levels? In this case, the analysis is a bit more involved. If the main effect of a factor with three or more levels is significant, then we will need to perform follow-up tests (planned comparisons or post-hoc tests), just like we did for a one-factor between-participants ANOVA to interpret the significant main effect. Another implication is that, if the interaction is significant, the simple main effects for any factor with three or more levels will require follow-up tests. Suppose we have a 2 (factor A: Level A\\(_{1}\\) vs. Level A\\(_{2}\\)) \\(\\times\\) 3 (factor B: Level B\\(_{1}\\) vs. Level B\\(_{2}\\) vs. Level B\\(_{3}\\)) between participants factorial design. The three simple main effects of factor A will involve only pairwise comparisons (namely A\\(_{1}\\) vs. A\\(_{2}\\), at levels B\\(_{1}\\), B\\(_{2}\\), and B\\(_{3}\\)). However, the two simple main effects of factor B will encompass comparisons between three levels (namely B\\(_{1}\\) vs. B\\(_{2}\\) vs B\\(_{3}\\) at levels A\\(_{1}\\) and A\\(_{2}\\)). These latter simple main effects are like running one-factor ANOVA’s on factors with three levels—if the test result is significant, it only tells us that the means differ, but we need to perform follow-up tests to identify the location of those differences. We will consider this issue in more detail in the next lecture and lab class.\nFor now, well done! We’ll see you again next week.\nFYI: at the end of the week I will upload a version of these lab materials to the lab folder that contains the full working code for the analyses."
  },
  {
    "objectID": "PSYC214/Week6.html",
    "href": "PSYC214/Week6.html",
    "title": "6. Introduction to Factorial Designs and Interactions",
    "section": "",
    "text": "Here’s one of my favorite quotes for those times when you feel like you have been confronted with an impossible obstacle. It’s about turning obstacles to your advantage to get where you need to go.\n![Marcus Aurelius—the philosopher king and the last of the five good emperors.}(images/MarcusAurelius.png){width=50%}"
  },
  {
    "objectID": "PSYC214/Week6.html#learning-objectives",
    "href": "PSYC214/Week6.html#learning-objectives",
    "title": "6. Introduction to Factorial Designs and Interactions",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this Week’s lecture, we provided a conceptual introduction to factorial designs and interactions (deferring a discussion of the statistical mechanics of factorial ANOVA for future weeks). You learned that the outcomes of a factorial ANOVA include:\n\nMain effects: the mean differences among the levels of one factor are called the main effect of that factor. Main effects provide information about the independent effects of each factor. A two-factor study has two main effects, one for each of the two factors.\nInteraction: sometimes, one factor has a direct influence on the effect of a second factor, producing an interaction between factors. An interaction occurs whenever two factors, acting together, produce mean differences not explained by the main effects of the two factors.\nSimple main effects: the simple main effects break down the main effects into their component parts. They reveal the effect of one factor at each level of the second factor. A simple main effects analysis allows us to determine how two factors are combining to influence the dependent variable.\n\nIn factorial designs, data visualization becomes even more important than in single-factor designs. If a factorial ANOVA produces a significant interaction, then the easiest way to interpret it is by graphing the data in terms of an interaction plot and studying the simple main effects. Interaction plots are either graphed as line plots, like the ones presented in the lecture, or as bar plots. In this lab class, we will show you how to generate both kinds of plots in an appropriate format for presenting in your reports. You will also obtain experience of detecting/interpreting interactions by looking at each of the simple main effects in graphed data."
  },
  {
    "objectID": "PSYC214/Week6.html#todays-lab-activities",
    "href": "PSYC214/Week6.html#todays-lab-activities",
    "title": "6. Introduction to Factorial Designs and Interactions",
    "section": "Today’s Lab Activities",
    "text": "Today’s Lab Activities\n\nA 2 \\(\\times\\) 2 factorial study\nLet’s start by revisiting the COVID-19 vaccination example presented in the lecture. In that example, a researcher is interested in identifying effective strategies for encouraging members of the general public to get vaccinated against COVID-19. The researcher decides to test an often employed intervention to reduce risky intentions or behaviours known as a fear appeal. A fear appeal is a persuasive message that attempts to induce fear in message recipients by emphasizing the potential danger and harms that will befall them if they do not adopt the messages’ recommendations. Graphic images on cigarette packages (e.g., a diseased eye, cancerous lungs, or a damaged heart) encouraging people to quit smoking are a classic example of a fear appeal. The researcher crafts a fear appeal—a short verbal message—drawing people’s attention to the various risks associated with contracting COVID-19, including long-term debilitating symptoms (so-called “long COVID”) such as lack of attention, concentration, and lethargy, as well as potential death. The researcher includes in the message a graphic image of a patient in a distressed state on a ventilator in a hospital bed. The researcher wants to know if a group of participants that receive the fear appeal express a greater intention to get vaccinated against COVID-19 than a group of participants that do not receive the fear appeal. The researcher plans to measure such intentions by asking people how likely it is that they would get vaccinated against COVID-19 on a scale ranging from 0 (Very Unlikely) to 10 (Very Likely).\nHowever, the researcher knows that for fear appeals to be effective, they typically must be accompanied by a self-efficacy message—a statement that assures message recipients that they are capable of performing the fear appeal’s recommended actions and/or that performing the recommended actions will result in desirable consequences. For example, cigarette packets as well as arousing fear using graphic images also direct smokers to resources to help them to quit smoking and highlight the benefits of doing so (the self-efficacy message component). Accordingly, the researcher creates a self-efficacy message that emphasizes to message recipients how easy it is for them to get vaccinated, how to do so, and the benefits that will be obtained once they have been immunized.\nThe researcher wants to know what independent or interactive effects the fear appeal and self-efficacy message may have on COVID-19 vaccination intentions, so she decides to run a 2 \\(\\times\\) 2 fully between-participants factorial study involving a total of N = 200 participants, with the following factors:\n\nFear: no fear appeal vs. fear appeal\nEfficacy: no efficacy message vs. efficacy message\n\nThis results in four different groups of participants. One group (N = 50) receives no fear appeal and no efficacy message. A second group (N = 50) receives the fear appeal but no efficacy message. A third group (N = 50) receives no fear appeal but does receive an efficacy message. A fourth group (N = 50) receives both the fear appeal and the efficacy message. All groups of participants then indicate their intention to vaccinate against COVID-19 using the vaccination intention measure described above. The four conditions are summarized in the table below.\n![}(images/DataMatrix.png){width=75%}\nFirst things first, let’s load the data set:\n\n# Import the data\ndata = read_csv(\"data/fearVac.csv\")\n# View the data\n(data)\n\n# A tibble: 200 × 4\n     Par Fear           Efficacy            Intention\n   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;                   &lt;dbl&gt;\n 1     1 No Fear Appeal No Efficacy Message         6\n 2     2 No Fear Appeal No Efficacy Message         4\n 3     3 No Fear Appeal No Efficacy Message         7\n 4     4 No Fear Appeal No Efficacy Message         8\n 5     5 No Fear Appeal No Efficacy Message         5\n 6     6 No Fear Appeal No Efficacy Message         6\n 7     7 No Fear Appeal No Efficacy Message         2\n 8     8 No Fear Appeal No Efficacy Message         6\n 9     9 No Fear Appeal No Efficacy Message         5\n10    10 No Fear Appeal No Efficacy Message         3\n# … with 190 more rows\n\n\nTake a look at the table summarising the data. Beneath each variable name, R tells us the variable type. There is an issue here in that the variables Fear and Efficacy are designated as character variables &lt;chr&gt;. This simply means that they are represented as strings of text. However, we want these variables to be represented as factors &lt;fct&gt;. Furthermore, we want to re-order the levels of each factor, as R will otherwise represent these in alphabetical order, which is not what we want. For Fear, we want the levels to-be-ordered “no fear appeal” –&gt; “fear appeal” and for Efficacy we want the levels to-be-ordered “no efficacy message” –&gt; “efficacy message”. The next code will convert the variables into factors and re-order the levels:\n\n# Convert Fear and Efficacy into factors and ensure \"No Fear Appeal\" is Level 1 of the Fear factor\n# and \"No Efficacy Message\" is Level 1 of the Efficacy factor\ndata$Fear     = factor(data$Fear, levels = c(\"No Fear Appeal\",\"Fear Appeal\"))\ndata$Efficacy = factor(data$Efficacy, levels = c(\"No Efficacy Message\",\"Efficacy Message\"))\n\nRight, lets take a look at our data set again:\n\n(data)\n\n# A tibble: 200 × 4\n     Par Fear           Efficacy            Intention\n   &lt;dbl&gt; &lt;fct&gt;          &lt;fct&gt;                   &lt;dbl&gt;\n 1     1 No Fear Appeal No Efficacy Message         6\n 2     2 No Fear Appeal No Efficacy Message         4\n 3     3 No Fear Appeal No Efficacy Message         7\n 4     4 No Fear Appeal No Efficacy Message         8\n 5     5 No Fear Appeal No Efficacy Message         5\n 6     6 No Fear Appeal No Efficacy Message         6\n 7     7 No Fear Appeal No Efficacy Message         2\n 8     8 No Fear Appeal No Efficacy Message         6\n 9     9 No Fear Appeal No Efficacy Message         5\n10    10 No Fear Appeal No Efficacy Message         3\n# … with 190 more rows\n\n\nLook at the variable type beneath the variable name. Notice that the Fear and Efficacy variables are now represented as factors  as we requested.\nYou can inspect the full data set by typing view(data) in the console. I suggest you do this now, so that you can see how the data is organized.\nNow we have inspected the raw data, let’s generate some descriptive statistics which we will store in a dataframe called descriptives. Specifically, we want the number of cases n, the mean, the standard deviation sd, the standard error se, and the confidence intervals ci.\n\ndescriptives = data %&gt;%\n  # Organise output by Fear and Efficacy\n  group_by(Fear,Efficacy) %&gt;%\n  # Get the mean, sd, se, and ci\n  get_summary_stats(Intention, show = c(\"mean\", \"sd\", \"se\", \"ci\"))\n  # Round the results to *at least* two-decimal places\n  options(digits = 4) \n  # Print the results\n  print.data.frame(descriptives)\n\n            Fear            Efficacy  variable  n mean    sd    se    ci\n1 No Fear Appeal No Efficacy Message Intention 50 5.16 2.024 0.286 0.575\n2 No Fear Appeal    Efficacy Message Intention 50 4.66 1.869 0.264 0.531\n3    Fear Appeal No Efficacy Message Intention 50 4.80 1.829 0.259 0.520\n4    Fear Appeal    Efficacy Message Intention 50 7.12 1.769 0.250 0.503"
  },
  {
    "objectID": "PSYC214/Week6.html#plotting-line-graphs",
    "href": "PSYC214/Week6.html#plotting-line-graphs",
    "title": "6. Introduction to Factorial Designs and Interactions",
    "section": "Plotting line graphs",
    "text": "Plotting line graphs\nOkay, let’s get plotting the data!\nIn most statistics textbooks, the standard advice when it comes to plotting the results of a factorial experiment is that you should use a line plot, like the ones in the lecture, as this makes it easier to spot an interaction (or lack thereof) between factors. As well as plotting the means, we also want to include error bars illustrating the variability in the data. In the example next, we are using standard errors. Run the following code and it should produce the graph below. Pay attention to the comments in the code, so it is clear what each element is doing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# jitt is used later to add a small amount of spatial jitter to our lines, data points, and error bars to prevent them from overlapping\njitt = position_dodge(0.1) \n# Using the dataframe called descriptives, place \"Fear\" on the x-axis, \"mean\" on the y-axis, and group the data \n# according to the two levels of the \"Efficacy\" factor \nggplot(data = descriptives, mapping = aes(x = Fear, y = mean, group = Efficacy)) +\n    # geom_line produces two lines: one for \"no efficacy message\" and one for \"efficacy message\"\n    geom_line(size = 1, position = jitt, aes(color = Efficacy)) + \n    # geom_point adds mean data points to our lines\n    geom_point(size = 3, position = jitt, aes(color = Efficacy)) +\n    # geom_errorbar adds error bars to our geom_points - here we are using the standard error\n    geom_errorbar(position = jitt, mapping = aes(ymin = mean - se, ymax = mean + se, color = Efficacy), size = 1, width = .05) +\n    # Here we are manually setting the colour of the lines, data points, and error bars\n    scale_color_manual(values=c(\"#355C7D\", \"#F67280\")) +\n    # Change y-axis lower and upper limits \n    ylim(0,10) +\n    # Manually set the x- and y-axis titles\n    labs(x = \"Fear\", y = \"Vaccination Intention\") +\n    # Use black and white theme for background\n    theme_bw() +\n    # The theme function allows us to set various properties of our figure manually \n    theme(panel.grid.major = element_blank(), # Removes the major gridlines\n          panel.grid.minor = element_blank(), # Removes the minor gridlines\n          legend.box.background = element_rect(colour = \"black\"), # Adds a black border around the legend\n          legend.position = \"bottom\", # Positions the legend at the bottom of the graph\n          legend.direction = \"vertical\") # Orients the legend in the vertical direction\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThis looks good. At this point, we might wish to save our figure so that we can export it elsewhere (e.g., a Word document or a PowerPoint presentation). Arguably, the most useful file format for this purpose is the Portable Network Graphic, or PNG for short. You can save the graph as a PNG file as follows:\n\nggsave(\"FearAppealLinePlot.png\", height = 6, width = 5, dpi = 300)\n\nAs you can see, we need to specify a name for the file, “FearAppealLinePlot.png”, that includes the .png extension at the end. We also have the option of specifying the desired height and width of the figure in inches, and the dpi, which stands for “Dots-Per-Inch” and corresponds to the resolution of the image. I recommend always setting the dpi to 300.\nLet’s take a closer look at the line graph we have produced. Remember from the lecture that a useful way to spot an interaction in a line graph is to determine whether the lines are parallel or not. If the lines are non-parallel, then this is an indication of the presence of an interaction. In our case, the lines are indeed non-parallel, suggesting that the fear and efficacy factors are combining with one another to influence the dependent variable. To break down the interaction, we can look at the simple main effects (if you have forgotten what these are, you may want to consult the lecture slides).\nFor these data, there are four simple main effects. For simplicity, we will characterise these effects using the operators =, &lt;, and &gt;. For example, condition A = condition B implies that the means for the two conditions are (roughly) the same; condition A &lt; condition B implies that the mean for condition A is less than the mean for condition B; whereas condition A &gt; condition B implies that the mean for condition A is greater than the mean for condition B. We will treat relatively small differences between means as indicating that they do not differ, with relatively large differences between means indicating that they do differ.\nFor our data, there are four simple main effects (two for the fear factor and two for the efficacy factor):\n\nThe simple main effect of fear (no fear appeal vs. fear appeal), at the no efficacy message level of the efficacy factor: no fear appeal = fear appeal\nThe simple main effect of fear (no fear appeal vs. fear appeal), at the efficacy message level of the efficacy factor: no fear appeal &lt; fear appeal\nThe simple main effect of efficacy (no efficacy message vs. efficacy message), at the no fear appeal level of the fear factor: no efficacy message = efficacy message\nThe simple main effect of efficacy (no efficacy message vs. efficacy message), at the fear appeal level of the fear factor: no efficacy message &lt; efficacy message\n\nThe simple main effects indicate that the effect of the fear factor varies according to the different levels of the efficacy factor (and vice versa), which is indicative of an interaction between the two factors.\nMore specifically, what the data show is that in the absence of a self-efficacy message, there is no effect of the fear appeal on vaccination intentions; that is, vaccination intentions do not differ between the no fear appeal and fear appeal conditions. However, in the presence of a self-efficacy message there is an effect of the fear appeal on vaccination intentions; that is, vaccination intentions are higher in the fear appeal condition than in the no fear appeal condition.\nThus, our data suggest that fear appeals are an effective strategy for increasing COVID-19 vaccination intentions, provided that they are accompanied by a self-efficacy message.\nBefore moving on, I will emphasise, as I did in the lecture, that we would not know for certain whether there “really” is an interaction until we subjected our data to a factorial ANOVA and looked at the significance value of the interaction effect.\nBack to plotting!\nI’m a big fan of using colour in graphs, but unfortunately the APA doesn’t share this sentiment. The APA guidlines state that you should not use colour in your figures, so I have been a very naughty boy by adding a touch of colour to my figure. We can easily resolve this by changing the hexademical codes for our colours to black and grey (#000000, #CCCCCC). The problem is that if we have more than two factors, and hence more than two lines in our plot, we have to use different shades of grey for at least two of the lines, and it can sometimes become difficult to visually distinguish them.\nAnother approach is to keep the colour of the lines identical (e.g., black) but vary the shape of the mean data points. In our current line graph, we used circles (or dots) as the markers for our mean data points. However, as can be seen from the figure below, there are various different shapes we can use in our plots.\n![}(images/BuiltInShapes.png){width=100%}\nNow, we will generate our line graph again but this time using different shapes (circles vs. squares) rather than different colours to distinguish the different lines for the no efficacy message and efficacy message levels of the Efficacy factor. The code below is very similar to that which we used earlier, so for clarity I have only commented on those elements that have changed.\n\njitt = position_dodge(0.1)\nggplot(data = descriptives, mapping = aes(x = Fear, y = mean, group = Efficacy)) +\n  # Here is the main bit of code that has changed from the last figure.\n  # Previously we used aes(color = Efficacy) in geom_line and geom_point to\n  # tell R to produce different colors for each level of the efficacy factor.\n  # Now we are using aes(shape = Efficacy), which tells R to use different shapes instead.\n  # Note also the custom specification of shapes below.\n  ########################################################################\n  geom_line(size = 1, position = jitt, aes(shape = Efficacy)) +\n  geom_point(size = 3, position = jitt, aes(shape = Efficacy)) +\n  # Custom values for the shapes: 15 = a black square, 16 = a black circle\n  scale_shape_manual(values = c(15, 16)) +\n  ########################################################################\n  geom_errorbar(position = jitt, mapping = aes(ymin = mean - se, ymax = mean + se, color = Efficacy), size = 1, width = .05) +\n  # We are also manually setting the color of both lines, data points, and error bars to black\n  scale_color_manual(values=c(\"#000000\", \"#000000\")) +\n  ylim(0,10) +\n  labs(x = \"Fear\", y = \"Vaccination Intention\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), \n        legend.box.background = element_rect(colour = \"black\"), \n        legend.position = \"bottom\", \n        legend.direction = \"vertical\") \n\n\n\n\nIdeally, I would like the data points to be larger in this figure. You can do this by adjusting the size argument in geom_point above—it’s currently set to a value of 3, which is quite small. However, if we increase it further it will obscure our standard error error bars. The solution is that we could plot the standard deviation in the error bars instead (but they take up a lot of visualization space) or the confidence intervals (which will be a bit bigger than the standard errors). I recommend the latter. We requested both of these descriptive statistics earlier, so if you wanted to replot the data with confidence intervals, then you would change se to ci (there are two of these values that must be changed) in the call to geom_errorbar and increase the size value in geom_point to, say, 5 or 6.\nOkay, that’s enough of line plots for now—let’s look at how to plot the same data as a bar graph instead."
  },
  {
    "objectID": "PSYC214/Week6.html#plotting-bar-graphs",
    "href": "PSYC214/Week6.html#plotting-bar-graphs",
    "title": "6. Introduction to Factorial Designs and Interactions",
    "section": "Plotting bar graphs",
    "text": "Plotting bar graphs\nAlthough most textbooks will recommend that you plot the results of a factorial experiment as a line graph, in practice bar graphs are a popular option too. There are no rules regarding which type of graph you should use, so I recommend plotting the type that you find easiest to read. Personally, I find bar graphs easier to read than line graphs. Bar graphs tend to work well when you have factors with a small number of levels, but when you have one or more factors with many levels, a line graph will typically be a better option.\nLet’s plot the results of our factorial study as a bar graph. You can do so using the following code:\n\n# Using the dataframe called descriptives, place \"Fear\" on the x-axis, \"mean\" on the y-axis, and group the data \n# according to the two levels of the \"Efficacy\" factor \nggplot(data = descriptives, mapping = aes(x = Fear, y = mean, fill = Efficacy)) +\n  # geom_col is used to create our bar plots. Width and position_dodge control the degree of spatial \n  # separation of the columns\n  geom_col(width = 0.45, position = position_dodge(0.55)) +\n  # geom_errorbar adds error bars to our geom_cols - here we are using the standard error\n  geom_errorbar(mapping = aes(ymin = mean - se, ymax = mean + se),\n      size = .5, width = .1, position = position_dodge(.55)) +\n  # Here we are manually setting the colour of the columns and error bars\n  scale_fill_manual(values = c(\"#355C7D\", \"#F67280\")) +\n  # We are using scale_y_continuous this time to set the y-axis limits because we want to remove the\n  # white space that ggplot leaves between 0 and the x-axis line using expand = c(0,0). \n  scale_y_continuous(limits = c(0,10.5), expand = c(0,0)) +\n  # Manually set the x- and y-axis titles\n  labs(x = \"Fear\", y = \"Vaccination Intention\") +\n  # Use black and white theme for background\n  theme_bw() +\n  # The theme function allows us to set various properties of our figure manually \n  theme(panel.grid.major = element_blank(), # Removes the major gridlines\n        panel.grid.minor = element_blank(), # Removes the minor gridlines\n        legend.box.background = element_rect(colour = \"black\"), # Adds a black border around the legend\n        legend.position = \"bottom\", # Positions the legend at the bottom of the graph\n        legend.direction = \"vertical\") # Orients the legend in the vertical direction\n\n\n\n\nHow would you spot an interaction in a bar graph? Imagine that there are two lines: one resting on top of the blue bars and one resting on top of the pink bars. If those lines are non-parallel, then you most probably have an interaction.\nI couldn’t resist using some colour once again in our bar graph, but just a reminder that for plotting graphs for your lab reports or third year dissertation you should use APA compatible colours, such as black and grey (#000000, #CCCCCC). Colour is set in ggplot using either named colours or more commonly hexadecimal colour codes (as we have been using here). If you search online for “hexadecimal colour palettes” it will return images containing a selection of colours and the hexadecimal codes for using them in ggplot. Try the following resource: http://derekogle.com/NCGraphing/resources/colors.\nThe colours I have used here were taken from the palette below, which I accessed online (there are many other palettes to choose from if you are fussy about colours, which I tend to be).\n![}(images/ColourPalette.png){width=100%}\nBefore moving on, let’s save our most recent figure.\n\nggsave(\"FearAppealBarPlot.png\", height = 6, width = 5, dpi = 300)"
  },
  {
    "objectID": "PSYC214/Week6.html#plotting-multiple-graphs-and-interpreting-simple-main-effects",
    "href": "PSYC214/Week6.html#plotting-multiple-graphs-and-interpreting-simple-main-effects",
    "title": "6. Introduction to Factorial Designs and Interactions",
    "section": "Plotting multiple graphs and interpreting simple main effects",
    "text": "Plotting multiple graphs and interpreting simple main effects\nSometimes, we want to plot multiple graphs within the same figure. In ggplot, we can plot multiple graphs using the facet_wrap command, which you may have encountered in Richard’s lab classes. For the first example, I am going to generate three different line plots in the same figure, each illustrating an interaction between factors. I am then going to get you to identify the simple main effects in each graph, as we did earlier.\nIn this example, we have three examples of alternative hypothetical interactions between the Fear and Efficacy manipulations for our COVID-19 experiment. The data are contained in the following tibble (note, we already have the means, so we don’t need to generate any descriptive statistics):\n\nthreeInteractions = tibble(\n  Example = c(\"A\",\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\"),\n  Fear = c(\"No Fear Appeal\",\"No Fear Appeal\",\"Fear Appeal\",\"Fear Appeal\",\n        \"No Fear Appeal\",\"No Fear Appeal\",\"Fear Appeal\",\"Fear Appeal\",\n        \"No Fear Appeal\",\"No Fear Appeal\",\"Fear Appeal\",\"Fear Appeal\"),\n  Efficacy = c(\"No Efficacy Message\",\"Efficacy Message\",\"No Efficacy Message\",\"Efficacy Message\",\n        \"No Efficacy Message\",\"Efficacy Message\",\"No Efficacy Message\",\"Efficacy Message\",\n        \"No Efficacy Message\",\"Efficacy Message\",\"No Efficacy Message\",\"Efficacy Message\"),\n  Mean  = c(4.2,9,4,4.5,4.75,5.25,1,9,2,8,8,2)\n)\n# Make sure \"Example\", \"Fear\", and \"Efficacy\" are coded as factors\nthreeInteractions$Example  = factor(threeInteractions$Example) # \"Example\" is already organised alphabetically, \n# which is what we want\nthreeInteractions$Fear     = factor(threeInteractions$Fear, levels = c(\"No Fear Appeal\",\"Fear Appeal\"))\nthreeInteractions$Efficacy = factor(threeInteractions$Efficacy, levels = c(\"No Efficacy Message\",\"Efficacy Message\"))\n\nLet’s take a look at the data:\n\n(threeInteractions)\n\n# A tibble: 12 × 4\n   Example Fear           Efficacy             Mean\n   &lt;fct&gt;   &lt;fct&gt;          &lt;fct&gt;               &lt;dbl&gt;\n 1 A       No Fear Appeal No Efficacy Message  4.2 \n 2 A       No Fear Appeal Efficacy Message     9   \n 3 A       Fear Appeal    No Efficacy Message  4   \n 4 A       Fear Appeal    Efficacy Message     4.5 \n 5 B       No Fear Appeal No Efficacy Message  4.75\n 6 B       No Fear Appeal Efficacy Message     5.25\n 7 B       Fear Appeal    No Efficacy Message  1   \n 8 B       Fear Appeal    Efficacy Message     9   \n 9 C       No Fear Appeal No Efficacy Message  2   \n10 C       No Fear Appeal Efficacy Message     8   \n11 C       Fear Appeal    No Efficacy Message  8   \n12 C       Fear Appeal    Efficacy Message     2   \n\n\nWe want to generate a single figure that contains three graphs in different panels, one panel for example A, one panel for example B, and one panel for example C. The code required to generate the figure is much the same as for our initial line graph example, but we are now going to use the command facet_wrap(Example) to instruct ggplot to generate separate graphs for each of our three examples of an interaction. The code for generating the figure is below:\n\n# Using the dataframe called threeInteractions, place \"Fear\" on the x-axis, \"Mean\" on the y-axis, \n# and group the data according to the two levels of \"Efficacy\" \nggplot(data = threeInteractions, mapping = aes(x = Fear, y = Mean, group = Efficacy)) +\n  # geom_line produces two lines: one for \"No Efficacy Message\" and one for \"Efficacy Message\"\n  geom_line(size = 1, aes(color = Efficacy)) +\n  # geom_point adds mean data points to our lines\n  geom_point(size = 3, aes(color = Efficacy)) +\n  # Here we are manually setting the colour of the lines and data points\n  scale_color_manual(values=c(\"#355C7D\", \"#F67280\")) +\n  # This tells ggplot to plot the figure with different panels for Examples A, B, and C on a single row.\n  # You can also specify the number of columns as an option.\n  ###################################\n  facet_wrap(~Example, nrow = 1) +\n  ###################################\n  # Change y-axis lower and upper limits\n  ylim(0,10) +\n  # Manually set the x-axis, y-axis, and legend titles\n  labs(x = \"Fear\", y = \"Vaccination Intention\", color = \"Efficacy\") +\n  # Use black and white theme for background\n  theme_bw() +\n  # The theme function allows us to set various properties of our figure manually\n  theme(panel.grid.major = element_blank(), # Removes the major gridlines\n        panel.grid.minor = element_blank(), # Removes the minor gridlines\n        legend.box.background = element_rect(colour = \"black\"), # Adds a black border around the legend\n        legend.position = \"bottom\", # Positions the legend at the bottom of the graph\n        legend.direction = \"vertical\") # Orients the legend in the vertical direction\n\n\n\n\nRight, this is a good point to give you some practice extracting simple main effects. So, for each of the examples I want you to identify the pattern of the four simple main effects as we did earlier using the =, &lt;, and &gt; operators. Don’t worry if you find this a bit tricky, it takes some time to get your head around simple main effects. If you get stuck, give us a shout!\n\nExample A\n\nSimple main effect of Fear at the No Efficacy Message level of Efficacy: No Fear Appeal = Fear Appeal\nSimple main effect of Fear at the Efficacy Message level of Efficacy: _______\nSimple main effect of Efficacy at the No Fear Appeal level of Fear: _______\nSimple main effect of Efficacy at the Fear Appeal level of Fear _______\n\n\n\nExample B\n\nSimple main effect of Fear at the No Efficacy Message level of Efficacy: _______\nSimple main effect of Fear at the Efficacy Message level of Efficacy: _______\nSimple main effect of Efficacy at the No Fear Appeal level of Fear: _______\nSimple main effect of Efficacy at the Fear Appeal level of Fear _______\n\n\n\nExample C\n\nSimple main effect of Fear at the No Efficacy Message level of Efficacy: _______\nSimple main effect of Fear at the Efficacy Message level of Efficacy: _______\nSimple main effect of Efficacy at the No Fear Appeal level of Fear: _______\nSimple main effect of Efficacy at the Fear Appeal level of Fear _______\n\nBefore moving on, let’s save our most recent figure.\n\nggsave(\"ThreeInteractionsLinePlot.png\", height = 6, width = 8, dpi = 300)\n\nOkay, we are almost finished but I want to conclude the activities by showing you how to create multiple bar graphs within the same figure. We will once again use the data illustrating the three different types of interactions. The code required to generate the figure is much the same as for our initial bar graph example, but we are now going to use the command facet_wrap(Example) to instruct ggplot to generate separate graphs for each of our three examples of an interaction. The code for generating the figure is below:\n\n# Using the dataframe called threeInteractions, place \"Fear\" on the x-axis, \"Mean\" on the y-axis, \n# and group the data according to the two levels of \"Efficacy\" \nggplot(data = threeInteractions, mapping = aes(x = Fear, y = Mean, fill = Efficacy)) +\n  # geom_col is used to create our bar plots. Width and position_dodge control the degree of spatial\n  # separation of the columns\n  geom_col(width = 0.45, position = position_dodge(0.55)) +\n  # Here we are manually setting the colour of the columns \n  scale_fill_manual(values = c(\"#355C7D\", \"#F67280\")) +\n  # This tells ggplot to plot the figure with different panels for Examples A, B, and C on a single row.\n  # You can also specify the number of columns as an option.\n  ###################################\n  facet_wrap(~Example, nrow = 1) +\n  ###################################\n  # We are using scale_y_continuous this time to set the y-axis limits because we want to remove the\n  # white space that ggplot leaves between 0 and the x-axis line using expand = c(0,0).\n  scale_y_continuous(limits = c(0,10.5), expand = c(0,0)) +\n  # Manually set the x-axis, y-axis, and legend titles\n  labs(x = \"Fear\", y = \"Vaccination Intention\", fill = \"Efficacy\") +\n  # Use black and white theme for background\n  theme_bw() +\n  # The theme function allows us to set various properties of our figure manually\n  theme(panel.grid.major = element_blank(), # Removes the major gridlines\n        panel.grid.minor = element_blank(), # Removes the minor gridlines\n        legend.box.background = element_rect(colour = \"black\"), # Adds a black border around the legend\n        legend.position = \"bottom\", # Positions the legend at the bottom of the graph\n        legend.direction = \"vertical\") # Orients the legend in the vertical direction\n\n\n\n\nBefore closing, let’s save our most recent figure to the directory.\n\nggsave(\"ThreeInteractionsBarPlot.png\", height = 6, width = 8, dpi = 300)\n\nThere are a couple of additional things I should mention regarding graphs. Statistics textbooks generally recommend that you should only plot all of the data, which is necessary to discern an interaction, if you actually have a significant interaction, otherwise you should plot whatever main effects are significant. I strongly disagree with this recommendation and it seems that most other researchers do as well because I rarely if ever see this advice being followed in published research. Whether your interaction is significant or not, I recommend graphing all of the data, simply because that gives your audience a complete picture of the results. Thus, even if only the main effects are significant you should plot all the results, so both the significant main effects and the absence of a significant interaction are visible to your audience. The other thing to mention is that a graph is generally always preferred to presenting your results in a table for the simple reason that “a picture speaks a thousand words”—graphs are easier to read than tables. And remember, APA style dictates that we can’t present the same data in different formats—that is, you should not present the same data as both a table and a graph. When forced to choose, graphs win hands down. Just make sure you include both the means and a measure of variability (e.g., standard deviation, standard error, or confidence interval) in the form of error bars.\nOkay, that’s the end of the exercises for today’s session. Well done! Please bear in mind, we will be using the code employed today in future sessions to generate the plots for our data, but I won’t give you the code—you will have to adpat the code I have given you here."
  },
  {
    "objectID": "PSYC214/Week6.html#further-tasks",
    "href": "PSYC214/Week6.html#further-tasks",
    "title": "6. Introduction to Factorial Designs and Interactions",
    "section": "Further tasks",
    "text": "Further tasks\nIf you still have time remaining, consider completing the following activity. In all of the plotting exercises above we used the theme function in ggplot to set various properties of our plots (see the bottom of each code segment). This function is incredibly useful and can be used to change all the non-data components of a plot (titles, labels, fonts, background, gridlines, and legends). To learn more about the arguments that theme accepts, type help(theme) into the console. Your additional tasks, should you choose to accept them, are as follows. Modify the code we used to generate the first line plot so that:\n\nThe x-axis title (i.e., Fear) is presented in size 14 bold font.\nThe x-axis text (i.e., No Fear Appeal, Fear Appeal) is presented in size 12 black font.\n\nThe y-axis title (i.e., Vaccination Intention) is presented in size 14 bold font.\nThe y-axis text (i.e., the numbers on the y-axis) is presented in size 12 black font.\nThe legend title (i.e., Efficacy) is presented in size 14 bold font.\n\nWell done folks! We’ve covered a lot of ground today, that’s all until our next session. Enjoy the rest of the week!"
  },
  {
    "objectID": "PSYC214/Week5.html",
    "href": "PSYC214/Week5.html",
    "title": "5. Revision of One-Factor ANOVA",
    "section": "",
    "text": "I have noticed Richard’s penchant for commencing these sessions with some words of philosophical wisdom to motivate and inspire you and will continue the trend. Here’s one of my favorite quotes from Frederick Nietzsche. It embodies the Stoic philosophical principle of Amor Fati, which is a mindset that entails a commitment to making the best out of anything that happens, whether good or bad. It’s about treating every moment, no matter how challenging and difficult, as something to be embraced rather than avoided."
  },
  {
    "objectID": "PSYC214/Week5.html#a-between-participants-single-factor-study",
    "href": "PSYC214/Week5.html#a-between-participants-single-factor-study",
    "title": "5. Revision of One-Factor ANOVA",
    "section": "A Between-Participants Single-Factor Study",
    "text": "A Between-Participants Single-Factor Study\nA short-term memory researcher wants to know whether objectively grouping lists of verbal items improves verbal short-term memory performance. She administers a serial recall task to three different groups of participants (described below). In the study phase of this task, participants are given a list of six words to study (e.g., gorilla, leprosy, nursery, radio, botany, calcium) spoken at a rate of one word per second, with a brief pause between each word. In the test phase, immediately after the final item has been spoken, participants must recall the list of words (by speaking them aloud) in their original presentation order. The dependent measure of interest is verbal recall accuracy: the percentage of words recalled in their correct position in the study list. For example, suppose the participant recalled gorilla, leprosy, radio, nursery, botany, calcium in response to the above study list. The first two items (gorilla, leprosy) and last two items (botany, calcium) have been recalled in their correct positions, but the middle two items (nursery, radio) have exchanged positions with one another, and are therefore incorrect. Thus, in this example, the participant recalled 4/6 = 67% of items accurately. The researcher administers to each participant a total of 20 serial recall study-test trials, with each study list comprising the same set of words, but presented in a different random order. The final score for each participant is the percentage of words recalled accurately, averaged across all 20 study-test trials.\nThere are 90 participants in total, allocated randomly and evenly to one of the following three conditions:\n\nUngrouped: in this condition, the study lists are presented in an ungrouped fashion. That is, the temporal pause separating each spoken word in study lists is always 0.5 seconds.\n2-2-2: in this condition, the study lists are organised into three groups, each containing two items. This is achieved by extending the temporal pauses after the second and fourth items in study lists from 0.5 seconds to 1.5 seconds.\n3-3: in this condition, the study lists are organised into two groups, each containing three items. This is achieved by extending the temporal pause after the third item in study lists from 0.5 seconds to 1.5 seconds.\n\nThe researcher wants to know whether (a) grouping study lists improves verbal recall accuracy compared to an ungrouped scenario, and (b) whether the degree of improvement depends on the type of grouping used (grouping in twos \\(vs.\\) grouping in threes). The researcher has chosen to run the study as a between-participants design rather than a within-participants design because she is worried about the possibility of carryover effects. Specifically, if the study was run as a within-participants design there is a risk that participants may carry over a grouping strategy from one condtion into another condition. For example, having completed the 3-3 condition, participants may then spontaneously group the ungrouped lists in the ungrouped condition into threes. This would make interpretation of the results of the experiment difficult, hence the decision to use a between-participants design.\nThe researcher has two hypotheses she wants to test. Specifically, she predicts that (1) verbal recall accuracy will be higher in the 2-2-2 condition than in the ungrouped condition, and (2) that verbal recall accuracy will be higher, in turn, in the 3-3 condition than in the 2-2-2 condition. She has thus decided a priori to conduct planned comparisons comparing accuracy for the ungrouped vs. 2-2-2 conditions, and the 2-2-2 vs. 3-3 conditions.\nFirst things first, let’s load the data set:\n\n# Import the data\ngroupingData = read_csv(\"data/Grouping.csv\")\n# Make sure Condition is a factor with levels ordered: ungrouped, 2-2-2, 3-3\ngroupingData$Condition = factor(groupingData$Condition, levels = c(\"Ungrouped\",\"2-2-2\",\"3-3\"))\n# Print the data\n(groupingData)\n\n# A tibble: 90 × 3\n   Participant Condition Accuracy\n         &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;\n 1           1 Ungrouped     62.7\n 2           2 Ungrouped     53.7\n 3           3 Ungrouped     68.7\n 4           4 Ungrouped     77.3\n 5           5 Ungrouped     60.2\n 6           6 Ungrouped     63.7\n 7           7 Ungrouped     46.9\n 8           8 Ungrouped     67.4\n 9           9 Ungrouped     60.4\n10          10 Ungrouped     49.5\n# … with 80 more rows\n\n\nWe have three columns: Participant (the participant number ranging from 1 to 90), Condition (the condition to which each participant was allocated: ungrouped vs. 2-2-2 vs. 3-3), and Accuracy (the percentage of list items recalled in the correct order, averaged across all study-test trials). You can take a closer look at the data using view(groupingData) which will open up the full data set in a new window.\n\nDescriptives statistics and assumption checks\nLet’s begin by calling some descriptive statistics. Specifically, we want the mean and sd of the Accuracy dependent variable, as a function of our Condition factor. We can obtain these quantities using the get_summary_stats() function in the rstatix package:\n\n# Get descriptive statistics\ndescriptives = groupingData %&gt;%\n  # Organise the output by Condition\n  group_by(Condition) %&gt;%\n  # Request mean and standard deviation\n  get_summary_stats(Accuracy, show = c(\"mean\", \"sd\"))\n  # Round the results to *at least* two-decimal places\n  options(digits = 4) \n  # Print the results\n  print.data.frame(descriptives)\n\n  Condition variable  n  mean     sd\n1 Ungrouped Accuracy 30 61.99 11.416\n2     2-2-2 Accuracy 30 63.75  9.177\n3       3-3 Accuracy 30 72.19  9.441\n\n\nOkay, let’s take a look at these descriptive statistics. Starting with the means, we can see that the means for the ungrouped and 2-2-2 conditions are very similar—it does not look like organizing study lists into three groups of two items improved performance. However, the mean for the 3-3 condition is much larger than the means for the ungrouped and 2-2-2 conditions, suggesting that organising the study list into two groups of three items did improve performance. Turning to the standard deviations, these are very similar for each of the three conditions, so it does not look like the data violate the homogeneity of variance assumption. However, let’s run Levene’s test for equality of variances to verify if that is indeed the case.\nWe can execute Levene’s test on our data using the levene_test() function in the rstatix package:\n\n# Levene's test for homogeneity of variance\ngroupingData %&gt;% \n  levene_test(Accuracy ~ Condition)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    87      1.24 0.295\n\n\nThe test result is nonsignificant (p = .295), suggesting that the variances do not differ systematically. In short, using Levene’s test criterion, we can be satisfied that the homogeneity of variance assumption has been met.\nSo far so good, let’s now check to see if there are any outliers or extreme values in the data for each of our three conditions. We can do this using the identify_outliers function contained in the rstatix package:\n\ngroupingData %&gt;% \n  # Organise the output by Condition\n  group_by(Condition) %&gt;%\n  # Identify outliers and extreme values of our dependent measure\n  identify_outliers(Accuracy)\n\n# A tibble: 1 × 5\n  Condition Participant Accuracy is.outlier is.extreme\n  &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 3-3                82     96.3 TRUE       FALSE     \n\n\nWe have a single outlier in the 3-3 condition—one participant’s score is quite a bit larger than the other scores in the distribution. However, as it is only a single non-extreme value it will not exert undue influence on the data and we can safely proceeed without having to remove, replace, or transform it.\nLet us now establish whether our data satisfy the assumption of normality. To do this, we can run the Shapiro-Wilk test, which you will recall tests whether data are normally distributed. We can do this using the shapiro_test() function in the rstatix package:\n\n# Shapiro Wilk test for normality\ngroupingData %&gt;%\n  # Organise output by Condition\n  group_by(Condition) %&gt;%\n  # Run Shapiro Wilk tests\n  shapiro_test(Accuracy)\n\n# A tibble: 3 × 4\n  Condition variable statistic      p\n  &lt;fct&gt;     &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Ungrouped Accuracy     0.938 0.0788\n2 2-2-2     Accuracy     0.971 0.563 \n3 3-3       Accuracy     0.974 0.648 \n\n\nThe test statistics are nonsignificant for all three conditions (all p &gt; .05), although only marginally so for the ungrouped condition. This indicates that the distributions of scores do not differ significantly from a normal distribution. We can therefore be satisfied that our data meet the assumption of normality. Remember that the Shapiro-Wilk test is sensitive to sample size—as sample size increases, the test becomes less accurate. The accepted rule of thumb is that if the number of observations (i.e., participants) in each condition is less than 50, then the Shapiro-Wilk test should provide an accurate test result. However, if the number of observations is above this value, then the test result may be inaccurate, so you should instead inspect the QQ plots of the data to determine if the normality assumption has been met. We have 30 observations per condition, so we can safely rely on the results of the Shapiro-Wilk test in this instance. Accordingly, we won’t bother looking at the QQ plots on this occasion.\n\n\nANOVA and planned comparisons\nOkay, we have performed our checks and we are now ready to run our ANOVA. We can do this using the aov function:\n\n# Our model with Accuracy as the DV and Condition as the factor\ngroupingModel = aov(data = groupingData, Accuracy ~ Condition) \n# Ensure we get exact p values rather than scientific notation\noptions(scipen = 999)\n# Create model summary\nsummary(groupingModel)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nCondition    2   1783     891     8.8 0.00033 ***\nResiduals   87   8807     101                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLooking at the table, the F value is 8.805, which is rather large as \\(F\\) values go, so it comes as no surprise to find when we look at the p value that the effect of condition is significant (p &lt; .001). The large F value and small p value imply that the effect of condition is large, but let’s verify this by looking at the effect size. We can do this using the effectsize() function in the effectsize package:\n\n# *** ENTER YOUR OWN CODE HERE FOR CALCULATIING THE ETA-SQUARED EFFECT SIZE FOR THE EFFECT OF CONDITION ***\n\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nCondition | 0.17 | [0.06, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe \\(\\eta^2\\) effect size is 0.17, which represents a large effect consistent with our expectations based on the results of our ANOVA (reminder: 0.01 = small effect; 0.06 = medium effect; 0.14 = large effect).\nRemember that because there are three levels in our condition factor, the ANOVA only tells us that there are differences between the means of our groups, but not where those differences are located. So, we need to conduct some follow-up tests. In this case, the researcher decided to conduct planned comparisons comparing the ungrouped and 2-2-2 conditions, and the 2-2-2 and 3-3 conditions. We can perform these comparisons using the pairwise_t_test() function in the rstatix package:\n\ngroupingData %&gt;% \n  # Execute independent-samples t-tests - remember to set pool.sd = FALSE \n  # and var.equal = TRUE\n  pairwise_t_test(Accuracy ~ Condition, pool.sd = FALSE, var.equal = TRUE, \n    # Just generate the two comparisons of interest: ungrouped vs. 2-2-2\n    # and 2-2-2 vs. 3-3\n    comparisons = list(c(\"Ungrouped\",\"2-2-2\"), c(\"2-2-2\",\"3-3\"))) %&gt;%\n  # Ensure we get exact p values rather than scientific notation\n  p_format(digits = 4, leading.zero = FALSE) \n\n# A tibble: 2 × 10\n  .y.      group1    group2    n1    n2 statistic    df p       p.adj p.adj.si…¹\n* &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     \n1 Accuracy Ungrouped 2-2-2     30    30    -0.657    58 .514    .514  ns        \n2 Accuracy 2-2-2     3-3       30    30    -3.51     58 .000873 .002  **        \n# … with abbreviated variable name ¹​p.adj.signif\n\n\nSince we are only performing two planned comparisons, I haven’t bothered to invoke the Bonferroni correction (annoyingly, the table includes the Bonferroni adjusted p value in the penultimate column, p.adj, despite the fact that we haven’t request it explicitly! Just make sure if you are not using the correction that you read the values from the column labelled p). Consistent with our intuitions when inspecting the descriptive statistics, we can see from inspection of the table that the comparison between the ungrouped and 2-2-2 conditions is nonsignificant (p = .514), whereas the comparison between the 2-2-2 and 3-3 conditions is significant (p &lt; .001).\nThe next step is to get the Cohen’s d effect size for each of our planned comparisons. We can get the Cohen’s d effect size for the ungrouped vs. 2-2-2 comparison using the following code which makes use of the cohens_d() function from the rstatix package:\n\n# Let's break down what the piece of code below is doing:\n# The function cohens_d() requires that we specify the two groups we wish to produce our d value for along with \n# the dependent variable.\n# The first argument, groupingData$Accuracy[groupingData$Condition == \"Ungrouped\"], tells R that for the first\n# group we want the variable Accuracy in the data set groupingData to be the dependent variable (groupingData$Accuracy) and we want the first group to be the ungrouped condition ([groupingData$Condition == \"Ungrouped\"])\n# The second argument, groupingData$Accuracy[groupingData$Condition == \"2-2-2\"], tells R that for the second \n# group we also want the variable Accuracy in the data set groupingData to be the dependent variable (groupingData$Accuracy) and we want the second group to be the 2-2-2 condition ([groupingData$Condition == \"2-2-2\"])\n\n# Cohen's d for ungrouped vs. 2-2-2\ncohens_d(groupingData$Accuracy[groupingData$Condition == \"Ungrouped\"],groupingData$Accuracy[groupingData$Condition == \"2-2-2\"])\n\nCohen's d |        95% CI\n-------------------------\n-0.17     | [-0.68, 0.34]\n\n- Estimated using pooled SD.\n\n\nFor the ungrouped vs. 2-2-2 comparison, the Cohen’s d effect size is equal to –0.17, which represents a small effect (reminder: 0.2 = small effect, 0.5 = moderate effect, 0.8 = large effect; Cohen, 1988).\nNow, let’s get the Cohen’s d effect size for the 2-2-2 vs. 3-3 comparison:\n\n# *** ENTER YOUR OWN CODE HERE FOR CALCULATING THE COHEN'S D EFFECT SIZE FOR THE 2-2-2 VS. 3-3 COMPARISON ***\n\n\n\nCohen's d |         95% CI\n--------------------------\n-0.91     | [-1.43, -0.37]\n\n- Estimated using pooled SD.\n\n\nFor the 2-2-2 vs. 3-3 comparison, the Cohen’s d effect size is equal to –0.91, which represents a large effect.\n\n\nWriting up the results\nA single-factor between-participants Analysis of Variance (ANOVA) was performed, with condition as the independent variable (control vs. 2-2-2 vs. 3-3) and verbal recall accuracy as the dependent variable. There was a significant effect of condition, F(2, 87) = 8.81, p &lt; .001, \\(\\eta^2\\) = .17. Planned comparisons revealed that verbal recall accuracy did not differ significantly between the ungrouped condition (M = 62.00, SD = 11.40) and the 2-2-2 condition (M = 63.80, SD = 9.18), t(58) = –0.657, \\(p\\) = .514, d = –.17, whereas verbal recall accuracy was higher in the 3-3 condition (M = 72.20, SD = 9.44) than in the 2-2-2 condition, t(58) = –3.51, \\(p\\) &lt; .001, d = –.91.\nNote: you can also include the 95% CIs for the \\(\\eta^2\\) and Cohen’s d effect sizes (as Richard has showed you in previous labs) but I haven’t included them above. Their inclusion is optional and since I have not requested you include them in your PSYC204 Short Report CWA, I thought you might think me hypocritical if I included them here (I may be many things, but I’m not a hypocrite).\nOkay, that’s all the exercises for the first data set complete, well done, but there’s no time for a break because we have another data set to contend with."
  },
  {
    "objectID": "PSYC214/Week5.html#further-tasks",
    "href": "PSYC214/Week5.html#further-tasks",
    "title": "5. Revision of One-Factor ANOVA",
    "section": "Further tasks",
    "text": "Further tasks\nWe have covered a lot of material in today’s session and I don’t expect you to have time to complete any further tasks. However, if you have managed to reach this point and still have time remaining, here are two things you can do (you should borrow and adapt the code from previous weeks for these two tasks):\n\nGenerate a bar figure plotting the results for the grouping study. Plot the condition means and standard error error bars.\nGenerate a line figure plotting the results for the word length study. Plot the condition means and standard error error bars.\n\nTime to relax now and pat yourself on the back for a job well done! See you in next week’s lab session."
  }
]